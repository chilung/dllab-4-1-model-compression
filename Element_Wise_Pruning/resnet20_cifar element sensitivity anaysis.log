2018-10-27 01:02:13,846 - Log file for this run: /home/ccma/Chilung/1022/distiller/examples/classifier_compression/logs/2018.10.27-010213/2018.10.27-010213.log
2018-10-27 01:02:13,846 - Number of CPUs: 8
2018-10-27 01:02:13,862 - Number of GPUs: 1
2018-10-27 01:02:13,862 - CUDA version: 8.0.61
2018-10-27 01:02:13,862 - CUDNN version: 7102
2018-10-27 01:02:13,862 - Kernel: 4.13.0-38-generic
2018-10-27 01:02:13,862 - Python: 3.5.2 (default, Nov 23 2017, 16:37:01) 
[GCC 5.4.0 20160609]
2018-10-27 01:02:13,862 - PyTorch: 0.4.0
2018-10-27 01:02:13,862 - Numpy: 1.14.3
2018-10-27 01:02:13,872 - Git is dirty
2018-10-27 01:02:13,873 - Active Git branch: master
2018-10-27 01:02:13,876 - Git commit: 8bf95d12172fb6e82a00ce40007953e23d9648c7
2018-10-27 01:02:13,877 - App args: ['compress_classifier.py', '--resume=../../../resnet20_cifar_baseline/checkpoint.pth.tar', '-a', 'resnet20_cifar', '../../../data.cifar10', '-j', '12', '--sense=element']
2018-10-27 01:02:13,877 - ==> using cifar10 dataset
2018-10-27 01:02:13,877 - => creating resnet20_cifar model for CIFAR10
2018-10-27 01:02:16,241 - => loading checkpoint ../../../resnet20_cifar_baseline/checkpoint.pth.tar
2018-10-27 01:02:16,337 - Checkpoint keys:
arch
	epoch
	compression_sched
	state_dict
	optimizer
	best_top1
2018-10-27 01:02:16,337 -    best top@1: 91.530
2018-10-27 01:02:16,338 - Loaded compression schedule from checkpoint (epoch 299)
2018-10-27 01:02:16,338 - => loaded checkpoint '../../../resnet20_cifar_baseline/checkpoint.pth.tar' (epoch 299)
2018-10-27 01:02:16,341 - Optimizer Type: <class 'torch.optim.sgd.SGD'>
2018-10-27 01:02:16,341 - Optimizer Args: {'lr': 0.1, 'weight_decay': 0.0001, 'momentum': 0.9, 'dampening': 0, 'nesterov': False}
2018-10-27 01:02:17,557 - Dataset sizes:
	training=45000
	validation=5000
	test=10000
2018-10-27 01:02:17,557 - Running sensitivity tests
2018-10-27 01:02:17,565 - Testing sensitivity of module.conv1.weight [0.0% sparsity]
2018-10-27 01:02:17,566 - --- test ---------------------
2018-10-27 01:02:17,566 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:18,009 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:02:18,117 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:02:18,219 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:02:18,400 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:02:18,428 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:02:18,428 - Testing sensitivity of module.conv1.weight [5.0% sparsity]
2018-10-27 01:02:18,432 - --- test ---------------------
2018-10-27 01:02:18,432 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:18,865 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:02:18,968 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:02:19,066 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:02:19,157 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:02:19,195 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:02:19,195 - Testing sensitivity of module.conv1.weight [10.0% sparsity]
2018-10-27 01:02:19,197 - --- test ---------------------
2018-10-27 01:02:19,197 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:19,656 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:02:19,759 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:02:19,858 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:02:19,949 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:02:19,976 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:02:19,977 - Testing sensitivity of module.conv1.weight [15.0% sparsity]
2018-10-27 01:02:19,979 - --- test ---------------------
2018-10-27 01:02:19,980 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:20,412 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:02:20,515 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:02:20,614 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:02:20,706 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:02:20,734 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:02:20,734 - Testing sensitivity of module.conv1.weight [20.0% sparsity]
2018-10-27 01:02:20,736 - --- test ---------------------
2018-10-27 01:02:20,737 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:21,163 - Test: [   10/   39]    Loss 0.551767    Top1 91.289062    Top5 99.687500    
2018-10-27 01:02:21,266 - Test: [   20/   39]    Loss 0.552920    Top1 91.523438    Top5 99.570312    
2018-10-27 01:02:21,365 - Test: [   30/   39]    Loss 0.546259    Top1 91.510417    Top5 99.661458    
2018-10-27 01:02:21,456 - Test: [   40/   39]    Loss 0.541548    Top1 91.520000    Top5 99.640000    
2018-10-27 01:02:21,483 - ==> Top1: 91.520    Top5: 99.640    Loss: 0.542

2018-10-27 01:02:21,484 - Testing sensitivity of module.conv1.weight [25.0% sparsity]
2018-10-27 01:02:21,487 - --- test ---------------------
2018-10-27 01:02:21,487 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:21,926 - Test: [   10/   39]    Loss 0.551859    Top1 91.289062    Top5 99.687500    
2018-10-27 01:02:22,028 - Test: [   20/   39]    Loss 0.553664    Top1 91.503906    Top5 99.589844    
2018-10-27 01:02:22,127 - Test: [   30/   39]    Loss 0.546667    Top1 91.484375    Top5 99.674479    
2018-10-27 01:02:22,218 - Test: [   40/   39]    Loss 0.542234    Top1 91.460000    Top5 99.650000    
2018-10-27 01:02:22,246 - ==> Top1: 91.460    Top5: 99.650    Loss: 0.542

2018-10-27 01:02:22,247 - Testing sensitivity of module.conv1.weight [30.0% sparsity]
2018-10-27 01:02:22,250 - --- test ---------------------
2018-10-27 01:02:22,250 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:22,685 - Test: [   10/   39]    Loss 0.551715    Top1 91.289062    Top5 99.687500    
2018-10-27 01:02:22,788 - Test: [   20/   39]    Loss 0.552332    Top1 91.562500    Top5 99.589844    
2018-10-27 01:02:22,887 - Test: [   30/   39]    Loss 0.545351    Top1 91.549479    Top5 99.674479    
2018-10-27 01:02:22,978 - Test: [   40/   39]    Loss 0.541049    Top1 91.510000    Top5 99.650000    
2018-10-27 01:02:23,005 - ==> Top1: 91.510    Top5: 99.650    Loss: 0.541

2018-10-27 01:02:23,006 - Testing sensitivity of module.conv1.weight [35.0% sparsity]
2018-10-27 01:02:23,008 - --- test ---------------------
2018-10-27 01:02:23,009 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:23,448 - Test: [   10/   39]    Loss 0.551635    Top1 91.210938    Top5 99.687500    
2018-10-27 01:02:23,550 - Test: [   20/   39]    Loss 0.552333    Top1 91.503906    Top5 99.570312    
2018-10-27 01:02:23,649 - Test: [   30/   39]    Loss 0.545918    Top1 91.510417    Top5 99.661458    
2018-10-27 01:02:23,741 - Test: [   40/   39]    Loss 0.540274    Top1 91.420000    Top5 99.640000    
2018-10-27 01:02:23,769 - ==> Top1: 91.420    Top5: 99.640    Loss: 0.540

2018-10-27 01:02:23,769 - Testing sensitivity of module.conv1.weight [40.0% sparsity]
2018-10-27 01:02:23,772 - --- test ---------------------
2018-10-27 01:02:23,772 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:24,221 - Test: [   10/   39]    Loss 0.553749    Top1 91.210938    Top5 99.609375    
2018-10-27 01:02:24,323 - Test: [   20/   39]    Loss 0.552885    Top1 91.523438    Top5 99.550781    
2018-10-27 01:02:24,421 - Test: [   30/   39]    Loss 0.546575    Top1 91.523438    Top5 99.648438    
2018-10-27 01:02:24,512 - Test: [   40/   39]    Loss 0.541378    Top1 91.520000    Top5 99.630000    
2018-10-27 01:02:24,540 - ==> Top1: 91.520    Top5: 99.630    Loss: 0.541

2018-10-27 01:02:24,541 - Testing sensitivity of module.conv1.weight [45.0% sparsity]
2018-10-27 01:02:24,543 - --- test ---------------------
2018-10-27 01:02:24,544 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:24,990 - Test: [   10/   39]    Loss 0.566756    Top1 90.703125    Top5 99.648438    
2018-10-27 01:02:25,093 - Test: [   20/   39]    Loss 0.564172    Top1 90.937500    Top5 99.550781    
2018-10-27 01:02:25,192 - Test: [   30/   39]    Loss 0.553892    Top1 91.054688    Top5 99.648438    
2018-10-27 01:02:25,283 - Test: [   40/   39]    Loss 0.549525    Top1 91.120000    Top5 99.620000    
2018-10-27 01:02:25,311 - ==> Top1: 91.120    Top5: 99.620    Loss: 0.550

2018-10-27 01:02:25,312 - Testing sensitivity of module.conv1.weight [50.0% sparsity]
2018-10-27 01:02:25,315 - --- test ---------------------
2018-10-27 01:02:25,315 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:25,758 - Test: [   10/   39]    Loss 0.594250    Top1 90.390625    Top5 99.648438    
2018-10-27 01:02:25,861 - Test: [   20/   39]    Loss 0.585025    Top1 90.585938    Top5 99.511719    
2018-10-27 01:02:25,960 - Test: [   30/   39]    Loss 0.574724    Top1 90.716146    Top5 99.609375    
2018-10-27 01:02:26,051 - Test: [   40/   39]    Loss 0.572831    Top1 90.750000    Top5 99.570000    
2018-10-27 01:02:26,079 - ==> Top1: 90.750    Top5: 99.570    Loss: 0.573

2018-10-27 01:02:26,080 - Testing sensitivity of module.conv1.weight [55.0% sparsity]
2018-10-27 01:02:26,083 - --- test ---------------------
2018-10-27 01:02:26,084 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:26,507 - Test: [   10/   39]    Loss 0.600622    Top1 90.000000    Top5 99.609375    
2018-10-27 01:02:26,609 - Test: [   20/   39]    Loss 0.593125    Top1 90.351562    Top5 99.570312    
2018-10-27 01:02:26,708 - Test: [   30/   39]    Loss 0.580901    Top1 90.455729    Top5 99.648438    
2018-10-27 01:02:26,799 - Test: [   40/   39]    Loss 0.577209    Top1 90.580000    Top5 99.620000    
2018-10-27 01:02:26,837 - ==> Top1: 90.580    Top5: 99.620    Loss: 0.577

2018-10-27 01:02:26,837 - Testing sensitivity of module.conv1.weight [60.0% sparsity]
2018-10-27 01:02:26,839 - --- test ---------------------
2018-10-27 01:02:26,839 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:27,274 - Test: [   10/   39]    Loss 0.628859    Top1 89.726562    Top5 99.726562    
2018-10-27 01:02:27,376 - Test: [   20/   39]    Loss 0.607500    Top1 90.312500    Top5 99.609375    
2018-10-27 01:02:27,475 - Test: [   30/   39]    Loss 0.602502    Top1 90.195312    Top5 99.700521    
2018-10-27 01:02:27,566 - Test: [   40/   39]    Loss 0.594288    Top1 90.340000    Top5 99.690000    
2018-10-27 01:02:27,593 - ==> Top1: 90.340    Top5: 99.690    Loss: 0.594

2018-10-27 01:02:27,594 - Testing sensitivity of module.conv1.weight [65.0% sparsity]
2018-10-27 01:02:27,596 - --- test ---------------------
2018-10-27 01:02:27,597 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:28,031 - Test: [   10/   39]    Loss 0.655590    Top1 89.531250    Top5 99.687500    
2018-10-27 01:02:28,134 - Test: [   20/   39]    Loss 0.624185    Top1 90.117188    Top5 99.609375    
2018-10-27 01:02:28,234 - Test: [   30/   39]    Loss 0.617441    Top1 90.026042    Top5 99.674479    
2018-10-27 01:02:28,325 - Test: [   40/   39]    Loss 0.610602    Top1 90.230000    Top5 99.620000    
2018-10-27 01:02:28,361 - ==> Top1: 90.230    Top5: 99.620    Loss: 0.611

2018-10-27 01:02:28,361 - Testing sensitivity of module.conv1.weight [70.0% sparsity]
2018-10-27 01:02:28,364 - --- test ---------------------
2018-10-27 01:02:28,364 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:28,790 - Test: [   10/   39]    Loss 0.673283    Top1 89.257812    Top5 99.492188    
2018-10-27 01:02:28,894 - Test: [   20/   39]    Loss 0.643845    Top1 90.019531    Top5 99.531250    
2018-10-27 01:02:28,994 - Test: [   30/   39]    Loss 0.638907    Top1 90.013021    Top5 99.557292    
2018-10-27 01:02:29,085 - Test: [   40/   39]    Loss 0.633945    Top1 90.240000    Top5 99.570000    
2018-10-27 01:02:29,113 - ==> Top1: 90.240    Top5: 99.570    Loss: 0.634

2018-10-27 01:02:29,114 - Testing sensitivity of module.conv1.weight [75.0% sparsity]
2018-10-27 01:02:29,117 - --- test ---------------------
2018-10-27 01:02:29,117 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:29,563 - Test: [   10/   39]    Loss 0.926535    Top1 85.742188    Top5 99.062500    
2018-10-27 01:02:29,664 - Test: [   20/   39]    Loss 0.853541    Top1 86.816406    Top5 99.238281    
2018-10-27 01:02:29,764 - Test: [   30/   39]    Loss 0.847806    Top1 86.666667    Top5 99.375000    
2018-10-27 01:02:29,855 - Test: [   40/   39]    Loss 0.839970    Top1 86.800000    Top5 99.360000    
2018-10-27 01:02:29,894 - ==> Top1: 86.800    Top5: 99.360    Loss: 0.840

2018-10-27 01:02:29,895 - Testing sensitivity of module.conv1.weight [80.0% sparsity]
2018-10-27 01:02:29,897 - --- test ---------------------
2018-10-27 01:02:29,898 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:30,331 - Test: [   10/   39]    Loss 0.936290    Top1 85.585938    Top5 99.218750    
2018-10-27 01:02:30,434 - Test: [   20/   39]    Loss 0.867537    Top1 86.503906    Top5 99.218750    
2018-10-27 01:02:30,533 - Test: [   30/   39]    Loss 0.854062    Top1 86.614583    Top5 99.348958    
2018-10-27 01:02:30,625 - Test: [   40/   39]    Loss 0.854188    Top1 86.750000    Top5 99.310000    
2018-10-27 01:02:30,652 - ==> Top1: 86.750    Top5: 99.310    Loss: 0.854

2018-10-27 01:02:30,653 - Testing sensitivity of module.conv1.weight [85.0% sparsity]
2018-10-27 01:02:30,655 - --- test ---------------------
2018-10-27 01:02:30,655 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:31,084 - Test: [   10/   39]    Loss 1.490765    Top1 80.273438    Top5 98.476562    
2018-10-27 01:02:31,187 - Test: [   20/   39]    Loss 1.408923    Top1 80.625000    Top5 98.613281    
2018-10-27 01:02:31,286 - Test: [   30/   39]    Loss 1.396286    Top1 80.390625    Top5 98.710938    
2018-10-27 01:02:31,379 - Test: [   40/   39]    Loss 1.395065    Top1 80.190000    Top5 98.690000    
2018-10-27 01:02:31,407 - ==> Top1: 80.190    Top5: 98.690    Loss: 1.395

2018-10-27 01:02:31,408 - Testing sensitivity of module.conv1.weight [90.0% sparsity]
2018-10-27 01:02:31,411 - --- test ---------------------
2018-10-27 01:02:31,411 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:31,872 - Test: [   10/   39]    Loss 1.723758    Top1 76.835938    Top5 97.968750    
2018-10-27 01:02:31,974 - Test: [   20/   39]    Loss 1.698349    Top1 77.011719    Top5 97.851562    
2018-10-27 01:02:32,074 - Test: [   30/   39]    Loss 1.707228    Top1 76.510417    Top5 97.929688    
2018-10-27 01:02:32,166 - Test: [   40/   39]    Loss 1.728500    Top1 76.530000    Top5 97.920000    
2018-10-27 01:02:32,192 - ==> Top1: 76.530    Top5: 97.920    Loss: 1.728

2018-10-27 01:02:32,207 - Testing sensitivity of module.layer1.0.conv1.weight [0.0% sparsity]
2018-10-27 01:02:32,211 - --- test ---------------------
2018-10-27 01:02:32,211 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:32,622 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:02:32,735 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:02:32,838 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:02:32,930 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:02:32,931 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:02:32,934 - Testing sensitivity of module.layer1.0.conv1.weight [5.0% sparsity]
2018-10-27 01:02:32,936 - --- test ---------------------
2018-10-27 01:02:32,937 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:33,400 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:02:33,503 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:02:33,603 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:02:33,697 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:02:33,724 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:02:33,725 - Testing sensitivity of module.layer1.0.conv1.weight [10.0% sparsity]
2018-10-27 01:02:33,734 - --- test ---------------------
2018-10-27 01:02:33,734 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:34,165 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:02:34,267 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:02:34,368 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:02:34,459 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:02:34,486 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:02:34,487 - Testing sensitivity of module.layer1.0.conv1.weight [15.0% sparsity]
2018-10-27 01:02:34,490 - --- test ---------------------
2018-10-27 01:02:34,490 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:34,937 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:02:35,042 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:02:35,143 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:02:35,235 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:02:35,263 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:02:35,264 - Testing sensitivity of module.layer1.0.conv1.weight [20.0% sparsity]
2018-10-27 01:02:35,267 - --- test ---------------------
2018-10-27 01:02:35,267 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:35,702 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:02:35,806 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:02:35,907 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:02:35,999 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:02:36,026 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:02:36,027 - Testing sensitivity of module.layer1.0.conv1.weight [25.0% sparsity]
2018-10-27 01:02:36,029 - --- test ---------------------
2018-10-27 01:02:36,029 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:36,468 - Test: [   10/   39]    Loss 0.551868    Top1 91.289062    Top5 99.687500    
2018-10-27 01:02:36,575 - Test: [   20/   39]    Loss 0.552952    Top1 91.542969    Top5 99.570312    
2018-10-27 01:02:36,677 - Test: [   30/   39]    Loss 0.546306    Top1 91.523438    Top5 99.661458    
2018-10-27 01:02:36,769 - Test: [   40/   39]    Loss 0.541578    Top1 91.530000    Top5 99.640000    
2018-10-27 01:02:36,796 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:02:36,797 - Testing sensitivity of module.layer1.0.conv1.weight [30.0% sparsity]
2018-10-27 01:02:36,800 - --- test ---------------------
2018-10-27 01:02:36,800 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:37,243 - Test: [   10/   39]    Loss 0.551635    Top1 91.328125    Top5 99.687500    
2018-10-27 01:02:37,346 - Test: [   20/   39]    Loss 0.552781    Top1 91.562500    Top5 99.570312    
2018-10-27 01:02:37,446 - Test: [   30/   39]    Loss 0.546187    Top1 91.562500    Top5 99.661458    
2018-10-27 01:02:37,538 - Test: [   40/   39]    Loss 0.541571    Top1 91.550000    Top5 99.640000    
2018-10-27 01:02:37,566 - ==> Top1: 91.550    Top5: 99.640    Loss: 0.542

2018-10-27 01:02:37,566 - Testing sensitivity of module.layer1.0.conv1.weight [35.0% sparsity]
2018-10-27 01:02:37,569 - --- test ---------------------
2018-10-27 01:02:37,569 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:38,002 - Test: [   10/   39]    Loss 0.552285    Top1 91.289062    Top5 99.687500    
2018-10-27 01:02:38,106 - Test: [   20/   39]    Loss 0.552969    Top1 91.523438    Top5 99.570312    
2018-10-27 01:02:38,206 - Test: [   30/   39]    Loss 0.546425    Top1 91.497396    Top5 99.661458    
2018-10-27 01:02:38,297 - Test: [   40/   39]    Loss 0.541795    Top1 91.530000    Top5 99.640000    
2018-10-27 01:02:38,325 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:02:38,326 - Testing sensitivity of module.layer1.0.conv1.weight [40.0% sparsity]
2018-10-27 01:02:38,329 - --- test ---------------------
2018-10-27 01:02:38,329 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:38,776 - Test: [   10/   39]    Loss 0.552543    Top1 91.210938    Top5 99.687500    
2018-10-27 01:02:38,878 - Test: [   20/   39]    Loss 0.553355    Top1 91.484375    Top5 99.570312    
2018-10-27 01:02:38,978 - Test: [   30/   39]    Loss 0.546750    Top1 91.471354    Top5 99.661458    
2018-10-27 01:02:39,069 - Test: [   40/   39]    Loss 0.541770    Top1 91.490000    Top5 99.640000    
2018-10-27 01:02:39,096 - ==> Top1: 91.490    Top5: 99.640    Loss: 0.542

2018-10-27 01:02:39,097 - Testing sensitivity of module.layer1.0.conv1.weight [45.0% sparsity]
2018-10-27 01:02:39,100 - --- test ---------------------
2018-10-27 01:02:39,100 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:39,536 - Test: [   10/   39]    Loss 0.553550    Top1 91.132812    Top5 99.687500    
2018-10-27 01:02:39,638 - Test: [   20/   39]    Loss 0.553723    Top1 91.425781    Top5 99.589844    
2018-10-27 01:02:39,738 - Test: [   30/   39]    Loss 0.546857    Top1 91.432292    Top5 99.674479    
2018-10-27 01:02:39,831 - Test: [   40/   39]    Loss 0.541623    Top1 91.470000    Top5 99.650000    
2018-10-27 01:02:39,858 - ==> Top1: 91.470    Top5: 99.650    Loss: 0.542

2018-10-27 01:02:39,859 - Testing sensitivity of module.layer1.0.conv1.weight [50.0% sparsity]
2018-10-27 01:02:39,862 - --- test ---------------------
2018-10-27 01:02:39,862 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:40,312 - Test: [   10/   39]    Loss 0.552681    Top1 90.937500    Top5 99.687500    
2018-10-27 01:02:40,415 - Test: [   20/   39]    Loss 0.554497    Top1 91.328125    Top5 99.570312    
2018-10-27 01:02:40,514 - Test: [   30/   39]    Loss 0.546935    Top1 91.367188    Top5 99.661458    
2018-10-27 01:02:40,606 - Test: [   40/   39]    Loss 0.542630    Top1 91.380000    Top5 99.640000    
2018-10-27 01:02:40,636 - ==> Top1: 91.380    Top5: 99.640    Loss: 0.543

2018-10-27 01:02:40,637 - Testing sensitivity of module.layer1.0.conv1.weight [55.0% sparsity]
2018-10-27 01:02:40,641 - --- test ---------------------
2018-10-27 01:02:40,641 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:41,082 - Test: [   10/   39]    Loss 0.556985    Top1 91.015625    Top5 99.687500    
2018-10-27 01:02:41,186 - Test: [   20/   39]    Loss 0.557769    Top1 91.289062    Top5 99.570312    
2018-10-27 01:02:41,286 - Test: [   30/   39]    Loss 0.549389    Top1 91.315104    Top5 99.661458    
2018-10-27 01:02:41,377 - Test: [   40/   39]    Loss 0.545996    Top1 91.330000    Top5 99.640000    
2018-10-27 01:02:41,405 - ==> Top1: 91.330    Top5: 99.640    Loss: 0.546

2018-10-27 01:02:41,405 - Testing sensitivity of module.layer1.0.conv1.weight [60.0% sparsity]
2018-10-27 01:02:41,407 - --- test ---------------------
2018-10-27 01:02:41,407 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:41,857 - Test: [   10/   39]    Loss 0.552568    Top1 91.015625    Top5 99.687500    
2018-10-27 01:02:41,960 - Test: [   20/   39]    Loss 0.556812    Top1 91.328125    Top5 99.609375    
2018-10-27 01:02:42,061 - Test: [   30/   39]    Loss 0.550235    Top1 91.341146    Top5 99.687500    
2018-10-27 01:02:42,153 - Test: [   40/   39]    Loss 0.545884    Top1 91.390000    Top5 99.660000    
2018-10-27 01:02:42,180 - ==> Top1: 91.390    Top5: 99.660    Loss: 0.546

2018-10-27 01:02:42,181 - Testing sensitivity of module.layer1.0.conv1.weight [65.0% sparsity]
2018-10-27 01:02:42,183 - --- test ---------------------
2018-10-27 01:02:42,183 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:42,620 - Test: [   10/   39]    Loss 0.555732    Top1 90.976562    Top5 99.687500    
2018-10-27 01:02:42,722 - Test: [   20/   39]    Loss 0.557869    Top1 91.269531    Top5 99.589844    
2018-10-27 01:02:42,822 - Test: [   30/   39]    Loss 0.551387    Top1 91.263021    Top5 99.661458    
2018-10-27 01:02:42,915 - Test: [   40/   39]    Loss 0.547788    Top1 91.320000    Top5 99.650000    
2018-10-27 01:02:42,948 - ==> Top1: 91.320    Top5: 99.650    Loss: 0.548

2018-10-27 01:02:42,949 - Testing sensitivity of module.layer1.0.conv1.weight [70.0% sparsity]
2018-10-27 01:02:42,953 - --- test ---------------------
2018-10-27 01:02:42,953 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:43,413 - Test: [   10/   39]    Loss 0.556012    Top1 90.937500    Top5 99.687500    
2018-10-27 01:02:43,517 - Test: [   20/   39]    Loss 0.556839    Top1 91.210938    Top5 99.570312    
2018-10-27 01:02:43,619 - Test: [   30/   39]    Loss 0.551471    Top1 91.223958    Top5 99.648438    
2018-10-27 01:02:43,714 - Test: [   40/   39]    Loss 0.547948    Top1 91.320000    Top5 99.640000    
2018-10-27 01:02:43,741 - ==> Top1: 91.320    Top5: 99.640    Loss: 0.548

2018-10-27 01:02:43,742 - Testing sensitivity of module.layer1.0.conv1.weight [75.0% sparsity]
2018-10-27 01:02:43,745 - --- test ---------------------
2018-10-27 01:02:43,745 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:44,183 - Test: [   10/   39]    Loss 0.571575    Top1 91.132812    Top5 99.648438    
2018-10-27 01:02:44,288 - Test: [   20/   39]    Loss 0.569214    Top1 91.093750    Top5 99.570312    
2018-10-27 01:02:44,391 - Test: [   30/   39]    Loss 0.561898    Top1 91.171875    Top5 99.648438    
2018-10-27 01:02:44,482 - Test: [   40/   39]    Loss 0.560316    Top1 91.150000    Top5 99.620000    
2018-10-27 01:02:44,510 - ==> Top1: 91.150    Top5: 99.620    Loss: 0.560

2018-10-27 01:02:44,511 - Testing sensitivity of module.layer1.0.conv1.weight [80.0% sparsity]
2018-10-27 01:02:44,514 - --- test ---------------------
2018-10-27 01:02:44,514 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:44,967 - Test: [   10/   39]    Loss 0.588674    Top1 90.546875    Top5 99.492188    
2018-10-27 01:02:45,071 - Test: [   20/   39]    Loss 0.585113    Top1 90.722656    Top5 99.453125    
2018-10-27 01:02:45,170 - Test: [   30/   39]    Loss 0.572486    Top1 90.833333    Top5 99.544271    
2018-10-27 01:02:45,262 - Test: [   40/   39]    Loss 0.573905    Top1 90.760000    Top5 99.540000    
2018-10-27 01:02:45,290 - ==> Top1: 90.760    Top5: 99.540    Loss: 0.574

2018-10-27 01:02:45,291 - Testing sensitivity of module.layer1.0.conv1.weight [85.0% sparsity]
2018-10-27 01:02:45,294 - --- test ---------------------
2018-10-27 01:02:45,294 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:45,732 - Test: [   10/   39]    Loss 0.598376    Top1 90.351562    Top5 99.687500    
2018-10-27 01:02:45,835 - Test: [   20/   39]    Loss 0.595364    Top1 90.605469    Top5 99.570312    
2018-10-27 01:02:45,935 - Test: [   30/   39]    Loss 0.583954    Top1 90.677083    Top5 99.609375    
2018-10-27 01:02:46,026 - Test: [   40/   39]    Loss 0.584629    Top1 90.580000    Top5 99.590000    
2018-10-27 01:02:46,053 - ==> Top1: 90.580    Top5: 99.590    Loss: 0.585

2018-10-27 01:02:46,054 - Testing sensitivity of module.layer1.0.conv1.weight [90.0% sparsity]
2018-10-27 01:02:46,058 - --- test ---------------------
2018-10-27 01:02:46,058 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:46,495 - Test: [   10/   39]    Loss 0.606962    Top1 89.921875    Top5 99.570312    
2018-10-27 01:02:46,597 - Test: [   20/   39]    Loss 0.600172    Top1 90.273438    Top5 99.433594    
2018-10-27 01:02:46,697 - Test: [   30/   39]    Loss 0.587681    Top1 90.442708    Top5 99.544271    
2018-10-27 01:02:46,791 - Test: [   40/   39]    Loss 0.585939    Top1 90.440000    Top5 99.540000    
2018-10-27 01:02:46,821 - ==> Top1: 90.440    Top5: 99.540    Loss: 0.586

2018-10-27 01:02:46,831 - Testing sensitivity of module.layer1.0.conv2.weight [0.0% sparsity]
2018-10-27 01:02:46,833 - --- test ---------------------
2018-10-27 01:02:46,834 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:47,277 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:02:47,386 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:02:47,489 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:02:47,585 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:02:47,613 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:02:47,614 - Testing sensitivity of module.layer1.0.conv2.weight [5.0% sparsity]
2018-10-27 01:02:47,617 - --- test ---------------------
2018-10-27 01:02:47,617 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:48,066 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:02:48,170 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:02:48,271 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:02:48,366 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:02:48,393 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:02:48,394 - Testing sensitivity of module.layer1.0.conv2.weight [10.0% sparsity]
2018-10-27 01:02:48,397 - --- test ---------------------
2018-10-27 01:02:48,398 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:48,846 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:02:48,950 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:02:49,051 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:02:49,143 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:02:49,169 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:02:49,170 - Testing sensitivity of module.layer1.0.conv2.weight [15.0% sparsity]
2018-10-27 01:02:49,172 - --- test ---------------------
2018-10-27 01:02:49,172 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:49,615 - Test: [   10/   39]    Loss 0.551865    Top1 91.289062    Top5 99.687500    
2018-10-27 01:02:49,720 - Test: [   20/   39]    Loss 0.552957    Top1 91.542969    Top5 99.570312    
2018-10-27 01:02:49,820 - Test: [   30/   39]    Loss 0.546279    Top1 91.523438    Top5 99.661458    
2018-10-27 01:02:49,912 - Test: [   40/   39]    Loss 0.541525    Top1 91.530000    Top5 99.640000    
2018-10-27 01:02:49,940 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:02:49,941 - Testing sensitivity of module.layer1.0.conv2.weight [20.0% sparsity]
2018-10-27 01:02:49,944 - --- test ---------------------
2018-10-27 01:02:49,945 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:50,387 - Test: [   10/   39]    Loss 0.551778    Top1 91.289062    Top5 99.687500    
2018-10-27 01:02:50,489 - Test: [   20/   39]    Loss 0.552725    Top1 91.523438    Top5 99.570312    
2018-10-27 01:02:50,589 - Test: [   30/   39]    Loss 0.546134    Top1 91.536458    Top5 99.661458    
2018-10-27 01:02:50,681 - Test: [   40/   39]    Loss 0.541474    Top1 91.520000    Top5 99.640000    
2018-10-27 01:02:50,708 - ==> Top1: 91.520    Top5: 99.640    Loss: 0.541

2018-10-27 01:02:50,709 - Testing sensitivity of module.layer1.0.conv2.weight [25.0% sparsity]
2018-10-27 01:02:50,712 - --- test ---------------------
2018-10-27 01:02:50,712 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:51,160 - Test: [   10/   39]    Loss 0.551788    Top1 91.210938    Top5 99.687500    
2018-10-27 01:02:51,268 - Test: [   20/   39]    Loss 0.553071    Top1 91.542969    Top5 99.570312    
2018-10-27 01:02:51,379 - Test: [   30/   39]    Loss 0.546680    Top1 91.575521    Top5 99.661458    
2018-10-27 01:02:51,474 - Test: [   40/   39]    Loss 0.541924    Top1 91.550000    Top5 99.640000    
2018-10-27 01:02:51,501 - ==> Top1: 91.550    Top5: 99.640    Loss: 0.542

2018-10-27 01:02:51,502 - Testing sensitivity of module.layer1.0.conv2.weight [30.0% sparsity]
2018-10-27 01:02:51,505 - --- test ---------------------
2018-10-27 01:02:51,505 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:51,943 - Test: [   10/   39]    Loss 0.551598    Top1 91.250000    Top5 99.687500    
2018-10-27 01:02:52,046 - Test: [   20/   39]    Loss 0.553033    Top1 91.562500    Top5 99.589844    
2018-10-27 01:02:52,150 - Test: [   30/   39]    Loss 0.546260    Top1 91.588542    Top5 99.674479    
2018-10-27 01:02:52,245 - Test: [   40/   39]    Loss 0.541058    Top1 91.540000    Top5 99.650000    
2018-10-27 01:02:52,273 - ==> Top1: 91.540    Top5: 99.650    Loss: 0.541

2018-10-27 01:02:52,274 - Testing sensitivity of module.layer1.0.conv2.weight [35.0% sparsity]
2018-10-27 01:02:52,276 - --- test ---------------------
2018-10-27 01:02:52,276 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:52,729 - Test: [   10/   39]    Loss 0.553651    Top1 91.289062    Top5 99.687500    
2018-10-27 01:02:52,835 - Test: [   20/   39]    Loss 0.554756    Top1 91.464844    Top5 99.570312    
2018-10-27 01:02:52,940 - Test: [   30/   39]    Loss 0.547333    Top1 91.484375    Top5 99.661458    
2018-10-27 01:02:53,036 - Test: [   40/   39]    Loss 0.541850    Top1 91.470000    Top5 99.640000    
2018-10-27 01:02:53,063 - ==> Top1: 91.470    Top5: 99.640    Loss: 0.542

2018-10-27 01:02:53,064 - Testing sensitivity of module.layer1.0.conv2.weight [40.0% sparsity]
2018-10-27 01:02:53,067 - --- test ---------------------
2018-10-27 01:02:53,067 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:53,507 - Test: [   10/   39]    Loss 0.557237    Top1 91.132812    Top5 99.687500    
2018-10-27 01:02:53,611 - Test: [   20/   39]    Loss 0.556594    Top1 91.347656    Top5 99.589844    
2018-10-27 01:02:53,712 - Test: [   30/   39]    Loss 0.549063    Top1 91.354167    Top5 99.674479    
2018-10-27 01:02:53,804 - Test: [   40/   39]    Loss 0.543814    Top1 91.370000    Top5 99.650000    
2018-10-27 01:02:53,831 - ==> Top1: 91.370    Top5: 99.650    Loss: 0.544

2018-10-27 01:02:53,832 - Testing sensitivity of module.layer1.0.conv2.weight [45.0% sparsity]
2018-10-27 01:02:53,834 - --- test ---------------------
2018-10-27 01:02:53,834 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:54,264 - Test: [   10/   39]    Loss 0.558386    Top1 90.976562    Top5 99.687500    
2018-10-27 01:02:54,368 - Test: [   20/   39]    Loss 0.556766    Top1 91.289062    Top5 99.609375    
2018-10-27 01:02:54,468 - Test: [   30/   39]    Loss 0.549812    Top1 91.289062    Top5 99.687500    
2018-10-27 01:02:54,560 - Test: [   40/   39]    Loss 0.545101    Top1 91.330000    Top5 99.660000    
2018-10-27 01:02:54,588 - ==> Top1: 91.330    Top5: 99.660    Loss: 0.545

2018-10-27 01:02:54,589 - Testing sensitivity of module.layer1.0.conv2.weight [50.0% sparsity]
2018-10-27 01:02:54,592 - --- test ---------------------
2018-10-27 01:02:54,592 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:55,025 - Test: [   10/   39]    Loss 0.561379    Top1 90.820312    Top5 99.765625    
2018-10-27 01:02:55,126 - Test: [   20/   39]    Loss 0.557447    Top1 91.250000    Top5 99.667969    
2018-10-27 01:02:55,226 - Test: [   30/   39]    Loss 0.551792    Top1 91.210938    Top5 99.726562    
2018-10-27 01:02:55,318 - Test: [   40/   39]    Loss 0.548171    Top1 91.250000    Top5 99.690000    
2018-10-27 01:02:55,346 - ==> Top1: 91.250    Top5: 99.690    Loss: 0.548

2018-10-27 01:02:55,347 - Testing sensitivity of module.layer1.0.conv2.weight [55.0% sparsity]
2018-10-27 01:02:55,349 - --- test ---------------------
2018-10-27 01:02:55,350 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:55,790 - Test: [   10/   39]    Loss 0.570859    Top1 90.585938    Top5 99.726562    
2018-10-27 01:02:55,893 - Test: [   20/   39]    Loss 0.564462    Top1 91.113281    Top5 99.648438    
2018-10-27 01:02:55,993 - Test: [   30/   39]    Loss 0.560112    Top1 91.028646    Top5 99.700521    
2018-10-27 01:02:56,086 - Test: [   40/   39]    Loss 0.555486    Top1 91.110000    Top5 99.670000    
2018-10-27 01:02:56,113 - ==> Top1: 91.110    Top5: 99.670    Loss: 0.555

2018-10-27 01:02:56,114 - Testing sensitivity of module.layer1.0.conv2.weight [60.0% sparsity]
2018-10-27 01:02:56,116 - --- test ---------------------
2018-10-27 01:02:56,116 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:56,563 - Test: [   10/   39]    Loss 0.576226    Top1 90.898438    Top5 99.726562    
2018-10-27 01:02:56,670 - Test: [   20/   39]    Loss 0.568797    Top1 91.308594    Top5 99.648438    
2018-10-27 01:02:56,779 - Test: [   30/   39]    Loss 0.563850    Top1 91.184896    Top5 99.713542    
2018-10-27 01:02:56,878 - Test: [   40/   39]    Loss 0.558726    Top1 91.170000    Top5 99.690000    
2018-10-27 01:02:56,879 - ==> Top1: 91.170    Top5: 99.690    Loss: 0.559

2018-10-27 01:02:56,882 - Testing sensitivity of module.layer1.0.conv2.weight [65.0% sparsity]
2018-10-27 01:02:56,885 - --- test ---------------------
2018-10-27 01:02:56,885 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:57,363 - Test: [   10/   39]    Loss 0.577119    Top1 91.093750    Top5 99.531250    
2018-10-27 01:02:57,471 - Test: [   20/   39]    Loss 0.571859    Top1 91.230469    Top5 99.531250    
2018-10-27 01:02:57,575 - Test: [   30/   39]    Loss 0.568768    Top1 91.171875    Top5 99.609375    
2018-10-27 01:02:57,670 - Test: [   40/   39]    Loss 0.561463    Top1 91.160000    Top5 99.600000    
2018-10-27 01:02:57,697 - ==> Top1: 91.160    Top5: 99.600    Loss: 0.561

2018-10-27 01:02:57,698 - Testing sensitivity of module.layer1.0.conv2.weight [70.0% sparsity]
2018-10-27 01:02:57,701 - --- test ---------------------
2018-10-27 01:02:57,701 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:58,145 - Test: [   10/   39]    Loss 0.573204    Top1 91.328125    Top5 99.453125    
2018-10-27 01:02:58,252 - Test: [   20/   39]    Loss 0.571311    Top1 91.250000    Top5 99.472656    
2018-10-27 01:02:58,352 - Test: [   30/   39]    Loss 0.572526    Top1 91.119792    Top5 99.583333    
2018-10-27 01:02:58,444 - Test: [   40/   39]    Loss 0.565966    Top1 91.220000    Top5 99.570000    
2018-10-27 01:02:58,471 - ==> Top1: 91.220    Top5: 99.570    Loss: 0.566

2018-10-27 01:02:58,472 - Testing sensitivity of module.layer1.0.conv2.weight [75.0% sparsity]
2018-10-27 01:02:58,475 - --- test ---------------------
2018-10-27 01:02:58,475 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:58,952 - Test: [   10/   39]    Loss 0.575316    Top1 90.742188    Top5 99.570312    
2018-10-27 01:02:59,056 - Test: [   20/   39]    Loss 0.573797    Top1 90.976562    Top5 99.550781    
2018-10-27 01:02:59,156 - Test: [   30/   39]    Loss 0.572490    Top1 91.015625    Top5 99.609375    
2018-10-27 01:02:59,254 - Test: [   40/   39]    Loss 0.566868    Top1 91.120000    Top5 99.590000    
2018-10-27 01:02:59,283 - ==> Top1: 91.120    Top5: 99.590    Loss: 0.567

2018-10-27 01:02:59,283 - Testing sensitivity of module.layer1.0.conv2.weight [80.0% sparsity]
2018-10-27 01:02:59,287 - --- test ---------------------
2018-10-27 01:02:59,287 - 10000 samples (256 per mini-batch)
2018-10-27 01:02:59,754 - Test: [   10/   39]    Loss 0.578721    Top1 90.585938    Top5 99.531250    
2018-10-27 01:02:59,862 - Test: [   20/   39]    Loss 0.573377    Top1 90.722656    Top5 99.492188    
2018-10-27 01:02:59,967 - Test: [   30/   39]    Loss 0.571702    Top1 90.703125    Top5 99.583333    
2018-10-27 01:03:00,065 - Test: [   40/   39]    Loss 0.568130    Top1 90.840000    Top5 99.560000    
2018-10-27 01:03:00,092 - ==> Top1: 90.840    Top5: 99.560    Loss: 0.568

2018-10-27 01:03:00,093 - Testing sensitivity of module.layer1.0.conv2.weight [85.0% sparsity]
2018-10-27 01:03:00,096 - --- test ---------------------
2018-10-27 01:03:00,096 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:00,559 - Test: [   10/   39]    Loss 0.599615    Top1 90.000000    Top5 99.492188    
2018-10-27 01:03:00,667 - Test: [   20/   39]    Loss 0.590932    Top1 90.351562    Top5 99.472656    
2018-10-27 01:03:00,773 - Test: [   30/   39]    Loss 0.593980    Top1 90.208333    Top5 99.557292    
2018-10-27 01:03:00,869 - Test: [   40/   39]    Loss 0.589571    Top1 90.440000    Top5 99.530000    
2018-10-27 01:03:00,897 - ==> Top1: 90.440    Top5: 99.530    Loss: 0.590

2018-10-27 01:03:00,897 - Testing sensitivity of module.layer1.0.conv2.weight [90.0% sparsity]
2018-10-27 01:03:00,900 - --- test ---------------------
2018-10-27 01:03:00,901 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:01,360 - Test: [   10/   39]    Loss 0.606040    Top1 89.804688    Top5 99.531250    
2018-10-27 01:03:01,468 - Test: [   20/   39]    Loss 0.599569    Top1 90.019531    Top5 99.453125    
2018-10-27 01:03:01,573 - Test: [   30/   39]    Loss 0.608306    Top1 89.752604    Top5 99.518229    
2018-10-27 01:03:01,669 - Test: [   40/   39]    Loss 0.606905    Top1 89.930000    Top5 99.500000    
2018-10-27 01:03:01,697 - ==> Top1: 89.930    Top5: 99.500    Loss: 0.607

2018-10-27 01:03:01,712 - Testing sensitivity of module.layer1.1.conv1.weight [0.0% sparsity]
2018-10-27 01:03:01,716 - --- test ---------------------
2018-10-27 01:03:01,716 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:02,160 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:03:02,268 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:03:02,372 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:03:02,468 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:03:02,496 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:03:02,496 - Testing sensitivity of module.layer1.1.conv1.weight [5.0% sparsity]
2018-10-27 01:03:02,500 - --- test ---------------------
2018-10-27 01:03:02,500 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:02,974 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:03:03,077 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:03:03,177 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:03:03,270 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:03:03,303 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:03:03,303 - Testing sensitivity of module.layer1.1.conv1.weight [10.0% sparsity]
2018-10-27 01:03:03,306 - --- test ---------------------
2018-10-27 01:03:03,307 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:03,727 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:03:03,831 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:03:03,931 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:03:04,023 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:03:04,051 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:03:04,052 - Testing sensitivity of module.layer1.1.conv1.weight [15.0% sparsity]
2018-10-27 01:03:04,054 - --- test ---------------------
2018-10-27 01:03:04,054 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:04,526 - Test: [   10/   39]    Loss 0.551975    Top1 91.289062    Top5 99.687500    
2018-10-27 01:03:04,629 - Test: [   20/   39]    Loss 0.553020    Top1 91.523438    Top5 99.570312    
2018-10-27 01:03:04,740 - Test: [   30/   39]    Loss 0.546360    Top1 91.510417    Top5 99.661458    
2018-10-27 01:03:04,835 - Test: [   40/   39]    Loss 0.541572    Top1 91.510000    Top5 99.640000    
2018-10-27 01:03:04,863 - ==> Top1: 91.510    Top5: 99.640    Loss: 0.542

2018-10-27 01:03:04,864 - Testing sensitivity of module.layer1.1.conv1.weight [20.0% sparsity]
2018-10-27 01:03:04,867 - --- test ---------------------
2018-10-27 01:03:04,867 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:05,313 - Test: [   10/   39]    Loss 0.551668    Top1 91.289062    Top5 99.687500    
2018-10-27 01:03:05,419 - Test: [   20/   39]    Loss 0.552756    Top1 91.523438    Top5 99.570312    
2018-10-27 01:03:05,523 - Test: [   30/   39]    Loss 0.546314    Top1 91.484375    Top5 99.661458    
2018-10-27 01:03:05,618 - Test: [   40/   39]    Loss 0.541663    Top1 91.500000    Top5 99.640000    
2018-10-27 01:03:05,646 - ==> Top1: 91.500    Top5: 99.640    Loss: 0.542

2018-10-27 01:03:05,647 - Testing sensitivity of module.layer1.1.conv1.weight [25.0% sparsity]
2018-10-27 01:03:05,650 - --- test ---------------------
2018-10-27 01:03:05,650 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:06,123 - Test: [   10/   39]    Loss 0.552029    Top1 91.250000    Top5 99.687500    
2018-10-27 01:03:06,232 - Test: [   20/   39]    Loss 0.552897    Top1 91.503906    Top5 99.589844    
2018-10-27 01:03:06,336 - Test: [   30/   39]    Loss 0.546381    Top1 91.497396    Top5 99.674479    
2018-10-27 01:03:06,437 - Test: [   40/   39]    Loss 0.541678    Top1 91.490000    Top5 99.650000    
2018-10-27 01:03:06,466 - ==> Top1: 91.490    Top5: 99.650    Loss: 0.542

2018-10-27 01:03:06,467 - Testing sensitivity of module.layer1.1.conv1.weight [30.0% sparsity]
2018-10-27 01:03:06,470 - --- test ---------------------
2018-10-27 01:03:06,470 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:06,949 - Test: [   10/   39]    Loss 0.552051    Top1 91.250000    Top5 99.687500    
2018-10-27 01:03:07,058 - Test: [   20/   39]    Loss 0.553612    Top1 91.503906    Top5 99.589844    
2018-10-27 01:03:07,158 - Test: [   30/   39]    Loss 0.546618    Top1 91.471354    Top5 99.674479    
2018-10-27 01:03:07,250 - Test: [   40/   39]    Loss 0.541979    Top1 91.480000    Top5 99.650000    
2018-10-27 01:03:07,278 - ==> Top1: 91.480    Top5: 99.650    Loss: 0.542

2018-10-27 01:03:07,278 - Testing sensitivity of module.layer1.1.conv1.weight [35.0% sparsity]
2018-10-27 01:03:07,281 - --- test ---------------------
2018-10-27 01:03:07,281 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:07,748 - Test: [   10/   39]    Loss 0.551748    Top1 91.250000    Top5 99.687500    
2018-10-27 01:03:07,855 - Test: [   20/   39]    Loss 0.554451    Top1 91.425781    Top5 99.589844    
2018-10-27 01:03:07,960 - Test: [   30/   39]    Loss 0.547435    Top1 91.445312    Top5 99.674479    
2018-10-27 01:03:08,057 - Test: [   40/   39]    Loss 0.542605    Top1 91.450000    Top5 99.650000    
2018-10-27 01:03:08,085 - ==> Top1: 91.450    Top5: 99.650    Loss: 0.543

2018-10-27 01:03:08,086 - Testing sensitivity of module.layer1.1.conv1.weight [40.0% sparsity]
2018-10-27 01:03:08,088 - --- test ---------------------
2018-10-27 01:03:08,089 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:08,570 - Test: [   10/   39]    Loss 0.552045    Top1 91.250000    Top5 99.648438    
2018-10-27 01:03:08,678 - Test: [   20/   39]    Loss 0.554070    Top1 91.503906    Top5 99.570312    
2018-10-27 01:03:08,782 - Test: [   30/   39]    Loss 0.548135    Top1 91.471354    Top5 99.661458    
2018-10-27 01:03:08,877 - Test: [   40/   39]    Loss 0.542705    Top1 91.460000    Top5 99.640000    
2018-10-27 01:03:08,905 - ==> Top1: 91.460    Top5: 99.640    Loss: 0.543

2018-10-27 01:03:08,905 - Testing sensitivity of module.layer1.1.conv1.weight [45.0% sparsity]
2018-10-27 01:03:08,908 - --- test ---------------------
2018-10-27 01:03:08,909 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:09,345 - Test: [   10/   39]    Loss 0.551455    Top1 91.250000    Top5 99.648438    
2018-10-27 01:03:09,449 - Test: [   20/   39]    Loss 0.553072    Top1 91.464844    Top5 99.570312    
2018-10-27 01:03:09,549 - Test: [   30/   39]    Loss 0.546875    Top1 91.419271    Top5 99.661458    
2018-10-27 01:03:09,641 - Test: [   40/   39]    Loss 0.542135    Top1 91.440000    Top5 99.640000    
2018-10-27 01:03:09,668 - ==> Top1: 91.440    Top5: 99.640    Loss: 0.542

2018-10-27 01:03:09,669 - Testing sensitivity of module.layer1.1.conv1.weight [50.0% sparsity]
2018-10-27 01:03:09,671 - --- test ---------------------
2018-10-27 01:03:09,672 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:10,119 - Test: [   10/   39]    Loss 0.552259    Top1 91.093750    Top5 99.648438    
2018-10-27 01:03:10,225 - Test: [   20/   39]    Loss 0.551873    Top1 91.367188    Top5 99.570312    
2018-10-27 01:03:10,330 - Test: [   30/   39]    Loss 0.546077    Top1 91.367188    Top5 99.661458    
2018-10-27 01:03:10,426 - Test: [   40/   39]    Loss 0.542207    Top1 91.410000    Top5 99.650000    
2018-10-27 01:03:10,453 - ==> Top1: 91.410    Top5: 99.650    Loss: 0.542

2018-10-27 01:03:10,454 - Testing sensitivity of module.layer1.1.conv1.weight [55.0% sparsity]
2018-10-27 01:03:10,456 - --- test ---------------------
2018-10-27 01:03:10,456 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:10,904 - Test: [   10/   39]    Loss 0.552189    Top1 90.781250    Top5 99.609375    
2018-10-27 01:03:11,007 - Test: [   20/   39]    Loss 0.552307    Top1 91.132812    Top5 99.531250    
2018-10-27 01:03:11,106 - Test: [   30/   39]    Loss 0.547013    Top1 91.171875    Top5 99.635417    
2018-10-27 01:03:11,198 - Test: [   40/   39]    Loss 0.543344    Top1 91.260000    Top5 99.630000    
2018-10-27 01:03:11,226 - ==> Top1: 91.260    Top5: 99.630    Loss: 0.543

2018-10-27 01:03:11,227 - Testing sensitivity of module.layer1.1.conv1.weight [60.0% sparsity]
2018-10-27 01:03:11,230 - --- test ---------------------
2018-10-27 01:03:11,230 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:11,665 - Test: [   10/   39]    Loss 0.549897    Top1 90.976562    Top5 99.687500    
2018-10-27 01:03:11,767 - Test: [   20/   39]    Loss 0.551294    Top1 91.328125    Top5 99.589844    
2018-10-27 01:03:11,868 - Test: [   30/   39]    Loss 0.550286    Top1 91.328125    Top5 99.674479    
2018-10-27 01:03:11,960 - Test: [   40/   39]    Loss 0.543602    Top1 91.350000    Top5 99.670000    
2018-10-27 01:03:11,987 - ==> Top1: 91.350    Top5: 99.670    Loss: 0.544

2018-10-27 01:03:11,988 - Testing sensitivity of module.layer1.1.conv1.weight [65.0% sparsity]
2018-10-27 01:03:11,991 - --- test ---------------------
2018-10-27 01:03:11,991 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:12,415 - Test: [   10/   39]    Loss 0.552434    Top1 91.093750    Top5 99.570312    
2018-10-27 01:03:12,520 - Test: [   20/   39]    Loss 0.554400    Top1 91.367188    Top5 99.550781    
2018-10-27 01:03:12,620 - Test: [   30/   39]    Loss 0.554269    Top1 91.354167    Top5 99.622396    
2018-10-27 01:03:12,712 - Test: [   40/   39]    Loss 0.547615    Top1 91.260000    Top5 99.620000    
2018-10-27 01:03:12,740 - ==> Top1: 91.260    Top5: 99.620    Loss: 0.548

2018-10-27 01:03:12,741 - Testing sensitivity of module.layer1.1.conv1.weight [70.0% sparsity]
2018-10-27 01:03:12,743 - --- test ---------------------
2018-10-27 01:03:12,743 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:13,179 - Test: [   10/   39]    Loss 0.556601    Top1 90.976562    Top5 99.648438    
2018-10-27 01:03:13,282 - Test: [   20/   39]    Loss 0.557946    Top1 91.210938    Top5 99.550781    
2018-10-27 01:03:13,382 - Test: [   30/   39]    Loss 0.560389    Top1 91.119792    Top5 99.622396    
2018-10-27 01:03:13,474 - Test: [   40/   39]    Loss 0.549994    Top1 91.090000    Top5 99.610000    
2018-10-27 01:03:13,503 - ==> Top1: 91.090    Top5: 99.610    Loss: 0.550

2018-10-27 01:03:13,503 - Testing sensitivity of module.layer1.1.conv1.weight [75.0% sparsity]
2018-10-27 01:03:13,506 - --- test ---------------------
2018-10-27 01:03:13,507 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:13,938 - Test: [   10/   39]    Loss 0.557699    Top1 90.976562    Top5 99.570312    
2018-10-27 01:03:14,042 - Test: [   20/   39]    Loss 0.563228    Top1 91.074219    Top5 99.472656    
2018-10-27 01:03:14,148 - Test: [   30/   39]    Loss 0.566869    Top1 90.924479    Top5 99.583333    
2018-10-27 01:03:14,245 - Test: [   40/   39]    Loss 0.556675    Top1 90.970000    Top5 99.570000    
2018-10-27 01:03:14,284 - ==> Top1: 90.970    Top5: 99.570    Loss: 0.557

2018-10-27 01:03:14,285 - Testing sensitivity of module.layer1.1.conv1.weight [80.0% sparsity]
2018-10-27 01:03:14,286 - --- test ---------------------
2018-10-27 01:03:14,287 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:14,732 - Test: [   10/   39]    Loss 0.565300    Top1 90.546875    Top5 99.648438    
2018-10-27 01:03:14,840 - Test: [   20/   39]    Loss 0.570299    Top1 90.820312    Top5 99.453125    
2018-10-27 01:03:14,943 - Test: [   30/   39]    Loss 0.574461    Top1 90.768229    Top5 99.570312    
2018-10-27 01:03:15,039 - Test: [   40/   39]    Loss 0.563823    Top1 90.800000    Top5 99.570000    
2018-10-27 01:03:15,069 - ==> Top1: 90.800    Top5: 99.570    Loss: 0.564

2018-10-27 01:03:15,069 - Testing sensitivity of module.layer1.1.conv1.weight [85.0% sparsity]
2018-10-27 01:03:15,072 - --- test ---------------------
2018-10-27 01:03:15,072 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:15,538 - Test: [   10/   39]    Loss 0.627142    Top1 89.648438    Top5 99.492188    
2018-10-27 01:03:15,641 - Test: [   20/   39]    Loss 0.624260    Top1 90.058594    Top5 99.472656    
2018-10-27 01:03:15,743 - Test: [   30/   39]    Loss 0.621023    Top1 89.921875    Top5 99.583333    
2018-10-27 01:03:15,837 - Test: [   40/   39]    Loss 0.608923    Top1 90.060000    Top5 99.610000    
2018-10-27 01:03:15,864 - ==> Top1: 90.060    Top5: 99.610    Loss: 0.609

2018-10-27 01:03:15,865 - Testing sensitivity of module.layer1.1.conv1.weight [90.0% sparsity]
2018-10-27 01:03:15,867 - --- test ---------------------
2018-10-27 01:03:15,868 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:16,305 - Test: [   10/   39]    Loss 0.760312    Top1 87.734375    Top5 99.414062    
2018-10-27 01:03:16,408 - Test: [   20/   39]    Loss 0.738175    Top1 88.496094    Top5 99.453125    
2018-10-27 01:03:16,508 - Test: [   30/   39]    Loss 0.732747    Top1 88.255208    Top5 99.518229    
2018-10-27 01:03:16,600 - Test: [   40/   39]    Loss 0.718561    Top1 88.370000    Top5 99.560000    
2018-10-27 01:03:16,627 - ==> Top1: 88.370    Top5: 99.560    Loss: 0.719

2018-10-27 01:03:16,642 - Testing sensitivity of module.layer1.1.conv2.weight [0.0% sparsity]
2018-10-27 01:03:16,646 - --- test ---------------------
2018-10-27 01:03:16,646 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:17,038 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:03:17,143 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:03:17,245 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:03:17,338 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:03:17,366 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:03:17,367 - Testing sensitivity of module.layer1.1.conv2.weight [5.0% sparsity]
2018-10-27 01:03:17,369 - --- test ---------------------
2018-10-27 01:03:17,370 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:17,823 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:03:17,929 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:03:18,029 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:03:18,121 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:03:18,149 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:03:18,150 - Testing sensitivity of module.layer1.1.conv2.weight [10.0% sparsity]
2018-10-27 01:03:18,152 - --- test ---------------------
2018-10-27 01:03:18,153 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:18,612 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:03:18,716 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:03:18,817 - Test: [   30/   39]    Loss 0.546294    Top1 91.523438    Top5 99.661458    
2018-10-27 01:03:18,908 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:03:18,936 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:03:18,937 - Testing sensitivity of module.layer1.1.conv2.weight [15.0% sparsity]
2018-10-27 01:03:18,939 - --- test ---------------------
2018-10-27 01:03:18,940 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:19,377 - Test: [   10/   39]    Loss 0.551866    Top1 91.289062    Top5 99.687500    
2018-10-27 01:03:19,480 - Test: [   20/   39]    Loss 0.552965    Top1 91.542969    Top5 99.570312    
2018-10-27 01:03:19,580 - Test: [   30/   39]    Loss 0.546309    Top1 91.523438    Top5 99.661458    
2018-10-27 01:03:19,672 - Test: [   40/   39]    Loss 0.541603    Top1 91.530000    Top5 99.640000    
2018-10-27 01:03:19,699 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:03:19,700 - Testing sensitivity of module.layer1.1.conv2.weight [20.0% sparsity]
2018-10-27 01:03:19,702 - --- test ---------------------
2018-10-27 01:03:19,702 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:20,136 - Test: [   10/   39]    Loss 0.551924    Top1 91.250000    Top5 99.687500    
2018-10-27 01:03:20,240 - Test: [   20/   39]    Loss 0.553095    Top1 91.503906    Top5 99.570312    
2018-10-27 01:03:20,340 - Test: [   30/   39]    Loss 0.546512    Top1 91.510417    Top5 99.661458    
2018-10-27 01:03:20,433 - Test: [   40/   39]    Loss 0.541690    Top1 91.520000    Top5 99.640000    
2018-10-27 01:03:20,460 - ==> Top1: 91.520    Top5: 99.640    Loss: 0.542

2018-10-27 01:03:20,461 - Testing sensitivity of module.layer1.1.conv2.weight [25.0% sparsity]
2018-10-27 01:03:20,463 - --- test ---------------------
2018-10-27 01:03:20,464 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:20,900 - Test: [   10/   39]    Loss 0.551302    Top1 91.250000    Top5 99.687500    
2018-10-27 01:03:21,003 - Test: [   20/   39]    Loss 0.552723    Top1 91.503906    Top5 99.589844    
2018-10-27 01:03:21,103 - Test: [   30/   39]    Loss 0.546105    Top1 91.484375    Top5 99.674479    
2018-10-27 01:03:21,195 - Test: [   40/   39]    Loss 0.541429    Top1 91.480000    Top5 99.650000    
2018-10-27 01:03:21,221 - ==> Top1: 91.480    Top5: 99.650    Loss: 0.541

2018-10-27 01:03:21,222 - Testing sensitivity of module.layer1.1.conv2.weight [30.0% sparsity]
2018-10-27 01:03:21,224 - --- test ---------------------
2018-10-27 01:03:21,224 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:21,651 - Test: [   10/   39]    Loss 0.551288    Top1 91.289062    Top5 99.687500    
2018-10-27 01:03:21,754 - Test: [   20/   39]    Loss 0.552842    Top1 91.523438    Top5 99.570312    
2018-10-27 01:03:21,854 - Test: [   30/   39]    Loss 0.546481    Top1 91.510417    Top5 99.661458    
2018-10-27 01:03:21,946 - Test: [   40/   39]    Loss 0.541948    Top1 91.530000    Top5 99.640000    
2018-10-27 01:03:21,974 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:03:21,975 - Testing sensitivity of module.layer1.1.conv2.weight [35.0% sparsity]
2018-10-27 01:03:21,978 - --- test ---------------------
2018-10-27 01:03:21,978 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:22,420 - Test: [   10/   39]    Loss 0.551547    Top1 91.289062    Top5 99.687500    
2018-10-27 01:03:22,530 - Test: [   20/   39]    Loss 0.552578    Top1 91.523438    Top5 99.589844    
2018-10-27 01:03:22,632 - Test: [   30/   39]    Loss 0.546051    Top1 91.510417    Top5 99.674479    
2018-10-27 01:03:22,724 - Test: [   40/   39]    Loss 0.541658    Top1 91.530000    Top5 99.650000    
2018-10-27 01:03:22,752 - ==> Top1: 91.530    Top5: 99.650    Loss: 0.542

2018-10-27 01:03:22,752 - Testing sensitivity of module.layer1.1.conv2.weight [40.0% sparsity]
2018-10-27 01:03:22,755 - --- test ---------------------
2018-10-27 01:03:22,756 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:23,190 - Test: [   10/   39]    Loss 0.552423    Top1 91.250000    Top5 99.687500    
2018-10-27 01:03:23,296 - Test: [   20/   39]    Loss 0.552729    Top1 91.464844    Top5 99.589844    
2018-10-27 01:03:23,396 - Test: [   30/   39]    Loss 0.545897    Top1 91.458333    Top5 99.674479    
2018-10-27 01:03:23,487 - Test: [   40/   39]    Loss 0.541368    Top1 91.490000    Top5 99.650000    
2018-10-27 01:03:23,515 - ==> Top1: 91.490    Top5: 99.650    Loss: 0.541

2018-10-27 01:03:23,516 - Testing sensitivity of module.layer1.1.conv2.weight [45.0% sparsity]
2018-10-27 01:03:23,518 - --- test ---------------------
2018-10-27 01:03:23,518 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:23,955 - Test: [   10/   39]    Loss 0.549612    Top1 91.289062    Top5 99.687500    
2018-10-27 01:03:24,061 - Test: [   20/   39]    Loss 0.550568    Top1 91.601562    Top5 99.589844    
2018-10-27 01:03:24,161 - Test: [   30/   39]    Loss 0.544351    Top1 91.588542    Top5 99.674479    
2018-10-27 01:03:24,254 - Test: [   40/   39]    Loss 0.540126    Top1 91.610000    Top5 99.650000    
2018-10-27 01:03:24,293 - ==> Top1: 91.610    Top5: 99.650    Loss: 0.540

2018-10-27 01:03:24,294 - Testing sensitivity of module.layer1.1.conv2.weight [50.0% sparsity]
2018-10-27 01:03:24,296 - --- test ---------------------
2018-10-27 01:03:24,296 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:24,760 - Test: [   10/   39]    Loss 0.549414    Top1 91.289062    Top5 99.687500    
2018-10-27 01:03:24,864 - Test: [   20/   39]    Loss 0.549839    Top1 91.582031    Top5 99.589844    
2018-10-27 01:03:24,968 - Test: [   30/   39]    Loss 0.543754    Top1 91.549479    Top5 99.674479    
2018-10-27 01:03:25,068 - Test: [   40/   39]    Loss 0.539507    Top1 91.540000    Top5 99.650000    
2018-10-27 01:03:25,100 - ==> Top1: 91.540    Top5: 99.650    Loss: 0.540

2018-10-27 01:03:25,101 - Testing sensitivity of module.layer1.1.conv2.weight [55.0% sparsity]
2018-10-27 01:03:25,103 - --- test ---------------------
2018-10-27 01:03:25,104 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:25,561 - Test: [   10/   39]    Loss 0.557046    Top1 90.937500    Top5 99.687500    
2018-10-27 01:03:25,665 - Test: [   20/   39]    Loss 0.555355    Top1 91.250000    Top5 99.589844    
2018-10-27 01:03:25,766 - Test: [   30/   39]    Loss 0.547224    Top1 91.276042    Top5 99.674479    
2018-10-27 01:03:25,858 - Test: [   40/   39]    Loss 0.542702    Top1 91.300000    Top5 99.660000    
2018-10-27 01:03:25,887 - ==> Top1: 91.300    Top5: 99.660    Loss: 0.543

2018-10-27 01:03:25,888 - Testing sensitivity of module.layer1.1.conv2.weight [60.0% sparsity]
2018-10-27 01:03:25,890 - --- test ---------------------
2018-10-27 01:03:25,891 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:26,340 - Test: [   10/   39]    Loss 0.558366    Top1 91.054688    Top5 99.687500    
2018-10-27 01:03:26,442 - Test: [   20/   39]    Loss 0.556921    Top1 91.289062    Top5 99.609375    
2018-10-27 01:03:26,547 - Test: [   30/   39]    Loss 0.548393    Top1 91.341146    Top5 99.687500    
2018-10-27 01:03:26,640 - Test: [   40/   39]    Loss 0.543130    Top1 91.410000    Top5 99.670000    
2018-10-27 01:03:26,668 - ==> Top1: 91.410    Top5: 99.670    Loss: 0.543

2018-10-27 01:03:26,668 - Testing sensitivity of module.layer1.1.conv2.weight [65.0% sparsity]
2018-10-27 01:03:26,670 - --- test ---------------------
2018-10-27 01:03:26,670 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:27,112 - Test: [   10/   39]    Loss 0.557403    Top1 90.859375    Top5 99.648438    
2018-10-27 01:03:27,214 - Test: [   20/   39]    Loss 0.558232    Top1 91.210938    Top5 99.570312    
2018-10-27 01:03:27,315 - Test: [   30/   39]    Loss 0.548474    Top1 91.276042    Top5 99.661458    
2018-10-27 01:03:27,407 - Test: [   40/   39]    Loss 0.544359    Top1 91.340000    Top5 99.650000    
2018-10-27 01:03:27,434 - ==> Top1: 91.340    Top5: 99.650    Loss: 0.544

2018-10-27 01:03:27,435 - Testing sensitivity of module.layer1.1.conv2.weight [70.0% sparsity]
2018-10-27 01:03:27,438 - --- test ---------------------
2018-10-27 01:03:27,438 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:27,881 - Test: [   10/   39]    Loss 0.549682    Top1 91.171875    Top5 99.648438    
2018-10-27 01:03:27,983 - Test: [   20/   39]    Loss 0.553229    Top1 91.367188    Top5 99.570312    
2018-10-27 01:03:28,082 - Test: [   30/   39]    Loss 0.545025    Top1 91.458333    Top5 99.661458    
2018-10-27 01:03:28,175 - Test: [   40/   39]    Loss 0.540954    Top1 91.510000    Top5 99.660000    
2018-10-27 01:03:28,203 - ==> Top1: 91.510    Top5: 99.660    Loss: 0.541

2018-10-27 01:03:28,204 - Testing sensitivity of module.layer1.1.conv2.weight [75.0% sparsity]
2018-10-27 01:03:28,206 - --- test ---------------------
2018-10-27 01:03:28,207 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:28,649 - Test: [   10/   39]    Loss 0.549767    Top1 90.898438    Top5 99.687500    
2018-10-27 01:03:28,751 - Test: [   20/   39]    Loss 0.555482    Top1 91.230469    Top5 99.550781    
2018-10-27 01:03:28,851 - Test: [   30/   39]    Loss 0.549622    Top1 91.380208    Top5 99.648438    
2018-10-27 01:03:28,943 - Test: [   40/   39]    Loss 0.545532    Top1 91.450000    Top5 99.650000    
2018-10-27 01:03:28,970 - ==> Top1: 91.450    Top5: 99.650    Loss: 0.546

2018-10-27 01:03:28,971 - Testing sensitivity of module.layer1.1.conv2.weight [80.0% sparsity]
2018-10-27 01:03:28,974 - --- test ---------------------
2018-10-27 01:03:28,974 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:29,407 - Test: [   10/   39]    Loss 0.560192    Top1 90.625000    Top5 99.648438    
2018-10-27 01:03:29,510 - Test: [   20/   39]    Loss 0.566101    Top1 90.917969    Top5 99.570312    
2018-10-27 01:03:29,610 - Test: [   30/   39]    Loss 0.561165    Top1 90.950521    Top5 99.661458    
2018-10-27 01:03:29,701 - Test: [   40/   39]    Loss 0.559664    Top1 90.990000    Top5 99.660000    
2018-10-27 01:03:29,729 - ==> Top1: 90.990    Top5: 99.660    Loss: 0.560

2018-10-27 01:03:29,730 - Testing sensitivity of module.layer1.1.conv2.weight [85.0% sparsity]
2018-10-27 01:03:29,732 - --- test ---------------------
2018-10-27 01:03:29,733 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:30,169 - Test: [   10/   39]    Loss 0.570853    Top1 90.507812    Top5 99.648438    
2018-10-27 01:03:30,272 - Test: [   20/   39]    Loss 0.572216    Top1 90.800781    Top5 99.589844    
2018-10-27 01:03:30,372 - Test: [   30/   39]    Loss 0.564140    Top1 90.963542    Top5 99.648438    
2018-10-27 01:03:30,464 - Test: [   40/   39]    Loss 0.563272    Top1 90.960000    Top5 99.650000    
2018-10-27 01:03:30,493 - ==> Top1: 90.960    Top5: 99.650    Loss: 0.563

2018-10-27 01:03:30,493 - Testing sensitivity of module.layer1.1.conv2.weight [90.0% sparsity]
2018-10-27 01:03:30,496 - --- test ---------------------
2018-10-27 01:03:30,496 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:30,956 - Test: [   10/   39]    Loss 0.611404    Top1 89.453125    Top5 99.609375    
2018-10-27 01:03:31,059 - Test: [   20/   39]    Loss 0.611700    Top1 90.058594    Top5 99.511719    
2018-10-27 01:03:31,158 - Test: [   30/   39]    Loss 0.603285    Top1 90.130208    Top5 99.531250    
2018-10-27 01:03:31,250 - Test: [   40/   39]    Loss 0.609091    Top1 90.140000    Top5 99.570000    
2018-10-27 01:03:31,276 - ==> Top1: 90.140    Top5: 99.570    Loss: 0.609

2018-10-27 01:03:31,287 - Testing sensitivity of module.layer1.2.conv1.weight [0.0% sparsity]
2018-10-27 01:03:31,289 - --- test ---------------------
2018-10-27 01:03:31,290 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:31,709 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:03:31,812 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:03:31,913 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:03:32,004 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:03:32,031 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:03:32,032 - Testing sensitivity of module.layer1.2.conv1.weight [5.0% sparsity]
2018-10-27 01:03:32,035 - --- test ---------------------
2018-10-27 01:03:32,035 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:32,465 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:03:32,568 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:03:32,668 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:03:32,760 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:03:32,788 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:03:32,789 - Testing sensitivity of module.layer1.2.conv1.weight [10.0% sparsity]
2018-10-27 01:03:32,791 - --- test ---------------------
2018-10-27 01:03:32,792 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:33,232 - Test: [   10/   39]    Loss 0.551879    Top1 91.328125    Top5 99.687500    
2018-10-27 01:03:33,336 - Test: [   20/   39]    Loss 0.552997    Top1 91.562500    Top5 99.570312    
2018-10-27 01:03:33,436 - Test: [   30/   39]    Loss 0.546314    Top1 91.523438    Top5 99.661458    
2018-10-27 01:03:33,527 - Test: [   40/   39]    Loss 0.541615    Top1 91.540000    Top5 99.640000    
2018-10-27 01:03:33,555 - ==> Top1: 91.540    Top5: 99.640    Loss: 0.542

2018-10-27 01:03:33,556 - Testing sensitivity of module.layer1.2.conv1.weight [15.0% sparsity]
2018-10-27 01:03:33,559 - --- test ---------------------
2018-10-27 01:03:33,559 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:34,004 - Test: [   10/   39]    Loss 0.551679    Top1 91.289062    Top5 99.687500    
2018-10-27 01:03:34,107 - Test: [   20/   39]    Loss 0.553253    Top1 91.542969    Top5 99.570312    
2018-10-27 01:03:34,207 - Test: [   30/   39]    Loss 0.546502    Top1 91.510417    Top5 99.661458    
2018-10-27 01:03:34,300 - Test: [   40/   39]    Loss 0.541972    Top1 91.490000    Top5 99.640000    
2018-10-27 01:03:34,327 - ==> Top1: 91.490    Top5: 99.640    Loss: 0.542

2018-10-27 01:03:34,327 - Testing sensitivity of module.layer1.2.conv1.weight [20.0% sparsity]
2018-10-27 01:03:34,330 - --- test ---------------------
2018-10-27 01:03:34,331 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:34,761 - Test: [   10/   39]    Loss 0.552076    Top1 91.250000    Top5 99.687500    
2018-10-27 01:03:34,867 - Test: [   20/   39]    Loss 0.554496    Top1 91.503906    Top5 99.589844    
2018-10-27 01:03:34,968 - Test: [   30/   39]    Loss 0.547706    Top1 91.510417    Top5 99.674479    
2018-10-27 01:03:35,061 - Test: [   40/   39]    Loss 0.543145    Top1 91.510000    Top5 99.650000    
2018-10-27 01:03:35,088 - ==> Top1: 91.510    Top5: 99.650    Loss: 0.543

2018-10-27 01:03:35,089 - Testing sensitivity of module.layer1.2.conv1.weight [25.0% sparsity]
2018-10-27 01:03:35,091 - --- test ---------------------
2018-10-27 01:03:35,092 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:35,534 - Test: [   10/   39]    Loss 0.550852    Top1 91.250000    Top5 99.687500    
2018-10-27 01:03:35,643 - Test: [   20/   39]    Loss 0.553215    Top1 91.503906    Top5 99.589844    
2018-10-27 01:03:35,752 - Test: [   30/   39]    Loss 0.547093    Top1 91.523438    Top5 99.674479    
2018-10-27 01:03:35,851 - Test: [   40/   39]    Loss 0.542077    Top1 91.520000    Top5 99.650000    
2018-10-27 01:03:35,879 - ==> Top1: 91.520    Top5: 99.650    Loss: 0.542

2018-10-27 01:03:35,880 - Testing sensitivity of module.layer1.2.conv1.weight [30.0% sparsity]
2018-10-27 01:03:35,882 - --- test ---------------------
2018-10-27 01:03:35,883 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:36,335 - Test: [   10/   39]    Loss 0.549466    Top1 91.015625    Top5 99.687500    
2018-10-27 01:03:36,441 - Test: [   20/   39]    Loss 0.554090    Top1 91.445312    Top5 99.589844    
2018-10-27 01:03:36,547 - Test: [   30/   39]    Loss 0.547002    Top1 91.445312    Top5 99.674479    
2018-10-27 01:03:36,644 - Test: [   40/   39]    Loss 0.542350    Top1 91.410000    Top5 99.650000    
2018-10-27 01:03:36,684 - ==> Top1: 91.410    Top5: 99.650    Loss: 0.542

2018-10-27 01:03:36,685 - Testing sensitivity of module.layer1.2.conv1.weight [35.0% sparsity]
2018-10-27 01:03:36,688 - --- test ---------------------
2018-10-27 01:03:36,688 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:37,143 - Test: [   10/   39]    Loss 0.550014    Top1 91.093750    Top5 99.687500    
2018-10-27 01:03:37,250 - Test: [   20/   39]    Loss 0.551610    Top1 91.445312    Top5 99.589844    
2018-10-27 01:03:37,353 - Test: [   30/   39]    Loss 0.545439    Top1 91.354167    Top5 99.674479    
2018-10-27 01:03:37,447 - Test: [   40/   39]    Loss 0.540413    Top1 91.370000    Top5 99.660000    
2018-10-27 01:03:37,475 - ==> Top1: 91.370    Top5: 99.660    Loss: 0.540

2018-10-27 01:03:37,475 - Testing sensitivity of module.layer1.2.conv1.weight [40.0% sparsity]
2018-10-27 01:03:37,478 - --- test ---------------------
2018-10-27 01:03:37,478 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:37,926 - Test: [   10/   39]    Loss 0.549391    Top1 91.171875    Top5 99.648438    
2018-10-27 01:03:38,033 - Test: [   20/   39]    Loss 0.553512    Top1 91.503906    Top5 99.589844    
2018-10-27 01:03:38,139 - Test: [   30/   39]    Loss 0.547112    Top1 91.380208    Top5 99.674479    
2018-10-27 01:03:38,236 - Test: [   40/   39]    Loss 0.540443    Top1 91.410000    Top5 99.660000    
2018-10-27 01:03:38,272 - ==> Top1: 91.410    Top5: 99.660    Loss: 0.540

2018-10-27 01:03:38,273 - Testing sensitivity of module.layer1.2.conv1.weight [45.0% sparsity]
2018-10-27 01:03:38,274 - --- test ---------------------
2018-10-27 01:03:38,275 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:38,715 - Test: [   10/   39]    Loss 0.553361    Top1 91.054688    Top5 99.609375    
2018-10-27 01:03:38,823 - Test: [   20/   39]    Loss 0.557405    Top1 91.250000    Top5 99.570312    
2018-10-27 01:03:38,928 - Test: [   30/   39]    Loss 0.551469    Top1 91.210938    Top5 99.648438    
2018-10-27 01:03:39,026 - Test: [   40/   39]    Loss 0.544529    Top1 91.250000    Top5 99.640000    
2018-10-27 01:03:39,055 - ==> Top1: 91.250    Top5: 99.640    Loss: 0.545

2018-10-27 01:03:39,055 - Testing sensitivity of module.layer1.2.conv1.weight [50.0% sparsity]
2018-10-27 01:03:39,058 - --- test ---------------------
2018-10-27 01:03:39,058 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:39,515 - Test: [   10/   39]    Loss 0.570925    Top1 90.742188    Top5 99.570312    
2018-10-27 01:03:39,618 - Test: [   20/   39]    Loss 0.572172    Top1 91.035156    Top5 99.570312    
2018-10-27 01:03:39,718 - Test: [   30/   39]    Loss 0.565412    Top1 90.898438    Top5 99.661458    
2018-10-27 01:03:39,810 - Test: [   40/   39]    Loss 0.558308    Top1 90.940000    Top5 99.660000    
2018-10-27 01:03:39,837 - ==> Top1: 90.940    Top5: 99.660    Loss: 0.558

2018-10-27 01:03:39,838 - Testing sensitivity of module.layer1.2.conv1.weight [55.0% sparsity]
2018-10-27 01:03:39,841 - --- test ---------------------
2018-10-27 01:03:39,841 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:40,292 - Test: [   10/   39]    Loss 0.576785    Top1 90.546875    Top5 99.609375    
2018-10-27 01:03:40,395 - Test: [   20/   39]    Loss 0.575167    Top1 90.898438    Top5 99.570312    
2018-10-27 01:03:40,495 - Test: [   30/   39]    Loss 0.569227    Top1 90.820312    Top5 99.648438    
2018-10-27 01:03:40,588 - Test: [   40/   39]    Loss 0.562401    Top1 90.950000    Top5 99.650000    
2018-10-27 01:03:40,615 - ==> Top1: 90.950    Top5: 99.650    Loss: 0.562

2018-10-27 01:03:40,616 - Testing sensitivity of module.layer1.2.conv1.weight [60.0% sparsity]
2018-10-27 01:03:40,619 - --- test ---------------------
2018-10-27 01:03:40,619 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:41,068 - Test: [   10/   39]    Loss 0.599538    Top1 90.195312    Top5 99.335938    
2018-10-27 01:03:41,173 - Test: [   20/   39]    Loss 0.599398    Top1 90.351562    Top5 99.433594    
2018-10-27 01:03:41,274 - Test: [   30/   39]    Loss 0.595563    Top1 90.377604    Top5 99.557292    
2018-10-27 01:03:41,367 - Test: [   40/   39]    Loss 0.588206    Top1 90.490000    Top5 99.580000    
2018-10-27 01:03:41,394 - ==> Top1: 90.490    Top5: 99.580    Loss: 0.588

2018-10-27 01:03:41,395 - Testing sensitivity of module.layer1.2.conv1.weight [65.0% sparsity]
2018-10-27 01:03:41,397 - --- test ---------------------
2018-10-27 01:03:41,398 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:41,838 - Test: [   10/   39]    Loss 0.673429    Top1 89.335938    Top5 99.531250    
2018-10-27 01:03:41,941 - Test: [   20/   39]    Loss 0.662902    Top1 89.531250    Top5 99.472656    
2018-10-27 01:03:42,041 - Test: [   30/   39]    Loss 0.655623    Top1 89.466146    Top5 99.570312    
2018-10-27 01:03:42,134 - Test: [   40/   39]    Loss 0.643257    Top1 89.720000    Top5 99.580000    
2018-10-27 01:03:42,161 - ==> Top1: 89.720    Top5: 99.580    Loss: 0.643

2018-10-27 01:03:42,162 - Testing sensitivity of module.layer1.2.conv1.weight [70.0% sparsity]
2018-10-27 01:03:42,164 - --- test ---------------------
2018-10-27 01:03:42,164 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:42,606 - Test: [   10/   39]    Loss 0.703800    Top1 88.945312    Top5 99.531250    
2018-10-27 01:03:42,712 - Test: [   20/   39]    Loss 0.693964    Top1 89.003906    Top5 99.492188    
2018-10-27 01:03:42,812 - Test: [   30/   39]    Loss 0.683751    Top1 89.231771    Top5 99.583333    
2018-10-27 01:03:42,904 - Test: [   40/   39]    Loss 0.672278    Top1 89.220000    Top5 99.570000    
2018-10-27 01:03:42,932 - ==> Top1: 89.220    Top5: 99.570    Loss: 0.672

2018-10-27 01:03:42,932 - Testing sensitivity of module.layer1.2.conv1.weight [75.0% sparsity]
2018-10-27 01:03:42,934 - --- test ---------------------
2018-10-27 01:03:42,934 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:43,372 - Test: [   10/   39]    Loss 0.758769    Top1 87.617188    Top5 99.648438    
2018-10-27 01:03:43,475 - Test: [   20/   39]    Loss 0.760201    Top1 88.027344    Top5 99.531250    
2018-10-27 01:03:43,579 - Test: [   30/   39]    Loss 0.744942    Top1 88.216146    Top5 99.531250    
2018-10-27 01:03:43,674 - Test: [   40/   39]    Loss 0.733807    Top1 88.310000    Top5 99.500000    
2018-10-27 01:03:43,704 - ==> Top1: 88.310    Top5: 99.500    Loss: 0.734

2018-10-27 01:03:43,705 - Testing sensitivity of module.layer1.2.conv1.weight [80.0% sparsity]
2018-10-27 01:03:43,707 - --- test ---------------------
2018-10-27 01:03:43,707 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:44,158 - Test: [   10/   39]    Loss 0.706740    Top1 88.007812    Top5 99.531250    
2018-10-27 01:03:44,262 - Test: [   20/   39]    Loss 0.706193    Top1 88.613281    Top5 99.453125    
2018-10-27 01:03:44,363 - Test: [   30/   39]    Loss 0.700770    Top1 88.697917    Top5 99.479167    
2018-10-27 01:03:44,456 - Test: [   40/   39]    Loss 0.692189    Top1 88.740000    Top5 99.480000    
2018-10-27 01:03:44,492 - ==> Top1: 88.740    Top5: 99.480    Loss: 0.692

2018-10-27 01:03:44,493 - Testing sensitivity of module.layer1.2.conv1.weight [85.0% sparsity]
2018-10-27 01:03:44,496 - --- test ---------------------
2018-10-27 01:03:44,496 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:44,933 - Test: [   10/   39]    Loss 0.956889    Top1 85.507812    Top5 99.453125    
2018-10-27 01:03:45,037 - Test: [   20/   39]    Loss 0.957624    Top1 85.566406    Top5 99.335938    
2018-10-27 01:03:45,138 - Test: [   30/   39]    Loss 0.932427    Top1 85.781250    Top5 99.361979    
2018-10-27 01:03:45,232 - Test: [   40/   39]    Loss 0.918525    Top1 85.820000    Top5 99.330000    
2018-10-27 01:03:45,262 - ==> Top1: 85.820    Top5: 99.330    Loss: 0.919

2018-10-27 01:03:45,262 - Testing sensitivity of module.layer1.2.conv1.weight [90.0% sparsity]
2018-10-27 01:03:45,265 - --- test ---------------------
2018-10-27 01:03:45,265 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:45,700 - Test: [   10/   39]    Loss 3.127176    Top1 63.750000    Top5 96.718750    
2018-10-27 01:03:45,803 - Test: [   20/   39]    Loss 3.093488    Top1 64.511719    Top5 96.582031    
2018-10-27 01:03:45,904 - Test: [   30/   39]    Loss 3.020937    Top1 64.453125    Top5 96.822917    
2018-10-27 01:03:45,996 - Test: [   40/   39]    Loss 3.003294    Top1 64.240000    Top5 96.830000    
2018-10-27 01:03:46,023 - ==> Top1: 64.240    Top5: 96.830    Loss: 3.003

2018-10-27 01:03:46,038 - Testing sensitivity of module.layer1.2.conv2.weight [0.0% sparsity]
2018-10-27 01:03:46,042 - --- test ---------------------
2018-10-27 01:03:46,042 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:46,478 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:03:46,584 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:03:46,684 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:03:46,776 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:03:46,802 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:03:46,803 - Testing sensitivity of module.layer1.2.conv2.weight [5.0% sparsity]
2018-10-27 01:03:46,805 - --- test ---------------------
2018-10-27 01:03:46,806 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:47,242 - Test: [   10/   39]    Loss 0.551825    Top1 91.289062    Top5 99.687500    
2018-10-27 01:03:47,348 - Test: [   20/   39]    Loss 0.552920    Top1 91.523438    Top5 99.589844    
2018-10-27 01:03:47,450 - Test: [   30/   39]    Loss 0.546291    Top1 91.497396    Top5 99.674479    
2018-10-27 01:03:47,546 - Test: [   40/   39]    Loss 0.541521    Top1 91.500000    Top5 99.650000    
2018-10-27 01:03:47,573 - ==> Top1: 91.500    Top5: 99.650    Loss: 0.542

2018-10-27 01:03:47,574 - Testing sensitivity of module.layer1.2.conv2.weight [10.0% sparsity]
2018-10-27 01:03:47,576 - --- test ---------------------
2018-10-27 01:03:47,577 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:48,032 - Test: [   10/   39]    Loss 0.551971    Top1 91.289062    Top5 99.687500    
2018-10-27 01:03:48,144 - Test: [   20/   39]    Loss 0.553079    Top1 91.562500    Top5 99.589844    
2018-10-27 01:03:48,247 - Test: [   30/   39]    Loss 0.546360    Top1 91.562500    Top5 99.674479    
2018-10-27 01:03:48,339 - Test: [   40/   39]    Loss 0.541626    Top1 91.550000    Top5 99.650000    
2018-10-27 01:03:48,368 - ==> Top1: 91.550    Top5: 99.650    Loss: 0.542

2018-10-27 01:03:48,369 - Testing sensitivity of module.layer1.2.conv2.weight [15.0% sparsity]
2018-10-27 01:03:48,372 - --- test ---------------------
2018-10-27 01:03:48,372 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:48,808 - Test: [   10/   39]    Loss 0.552649    Top1 91.250000    Top5 99.687500    
2018-10-27 01:03:48,911 - Test: [   20/   39]    Loss 0.553282    Top1 91.503906    Top5 99.570312    
2018-10-27 01:03:49,011 - Test: [   30/   39]    Loss 0.546538    Top1 91.484375    Top5 99.661458    
2018-10-27 01:03:49,103 - Test: [   40/   39]    Loss 0.541872    Top1 91.480000    Top5 99.640000    
2018-10-27 01:03:49,131 - ==> Top1: 91.480    Top5: 99.640    Loss: 0.542

2018-10-27 01:03:49,131 - Testing sensitivity of module.layer1.2.conv2.weight [20.0% sparsity]
2018-10-27 01:03:49,134 - --- test ---------------------
2018-10-27 01:03:49,134 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:49,580 - Test: [   10/   39]    Loss 0.552887    Top1 91.289062    Top5 99.687500    
2018-10-27 01:03:49,683 - Test: [   20/   39]    Loss 0.553566    Top1 91.542969    Top5 99.570312    
2018-10-27 01:03:49,783 - Test: [   30/   39]    Loss 0.546827    Top1 91.523438    Top5 99.661458    
2018-10-27 01:03:49,876 - Test: [   40/   39]    Loss 0.542310    Top1 91.510000    Top5 99.640000    
2018-10-27 01:03:49,903 - ==> Top1: 91.510    Top5: 99.640    Loss: 0.542

2018-10-27 01:03:49,904 - Testing sensitivity of module.layer1.2.conv2.weight [25.0% sparsity]
2018-10-27 01:03:49,906 - --- test ---------------------
2018-10-27 01:03:49,906 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:50,344 - Test: [   10/   39]    Loss 0.551770    Top1 91.328125    Top5 99.687500    
2018-10-27 01:03:50,450 - Test: [   20/   39]    Loss 0.553124    Top1 91.562500    Top5 99.570312    
2018-10-27 01:03:50,559 - Test: [   30/   39]    Loss 0.546544    Top1 91.562500    Top5 99.661458    
2018-10-27 01:03:50,660 - Test: [   40/   39]    Loss 0.542180    Top1 91.560000    Top5 99.640000    
2018-10-27 01:03:50,701 - ==> Top1: 91.560    Top5: 99.640    Loss: 0.542

2018-10-27 01:03:50,702 - Testing sensitivity of module.layer1.2.conv2.weight [30.0% sparsity]
2018-10-27 01:03:50,705 - --- test ---------------------
2018-10-27 01:03:50,705 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:51,142 - Test: [   10/   39]    Loss 0.552131    Top1 91.328125    Top5 99.687500    
2018-10-27 01:03:51,250 - Test: [   20/   39]    Loss 0.553207    Top1 91.542969    Top5 99.570312    
2018-10-27 01:03:51,359 - Test: [   30/   39]    Loss 0.546666    Top1 91.536458    Top5 99.661458    
2018-10-27 01:03:51,455 - Test: [   40/   39]    Loss 0.542491    Top1 91.540000    Top5 99.640000    
2018-10-27 01:03:51,483 - ==> Top1: 91.540    Top5: 99.640    Loss: 0.542

2018-10-27 01:03:51,483 - Testing sensitivity of module.layer1.2.conv2.weight [35.0% sparsity]
2018-10-27 01:03:51,486 - --- test ---------------------
2018-10-27 01:03:51,486 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:51,925 - Test: [   10/   39]    Loss 0.552230    Top1 91.250000    Top5 99.687500    
2018-10-27 01:03:52,028 - Test: [   20/   39]    Loss 0.553819    Top1 91.464844    Top5 99.570312    
2018-10-27 01:03:52,131 - Test: [   30/   39]    Loss 0.547523    Top1 91.471354    Top5 99.661458    
2018-10-27 01:03:52,225 - Test: [   40/   39]    Loss 0.543452    Top1 91.480000    Top5 99.640000    
2018-10-27 01:03:52,251 - ==> Top1: 91.480    Top5: 99.640    Loss: 0.543

2018-10-27 01:03:52,252 - Testing sensitivity of module.layer1.2.conv2.weight [40.0% sparsity]
2018-10-27 01:03:52,255 - --- test ---------------------
2018-10-27 01:03:52,255 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:52,700 - Test: [   10/   39]    Loss 0.554707    Top1 91.210938    Top5 99.687500    
2018-10-27 01:03:52,803 - Test: [   20/   39]    Loss 0.555119    Top1 91.406250    Top5 99.570312    
2018-10-27 01:03:52,904 - Test: [   30/   39]    Loss 0.548262    Top1 91.471354    Top5 99.661458    
2018-10-27 01:03:52,996 - Test: [   40/   39]    Loss 0.544355    Top1 91.510000    Top5 99.640000    
2018-10-27 01:03:53,023 - ==> Top1: 91.510    Top5: 99.640    Loss: 0.544

2018-10-27 01:03:53,024 - Testing sensitivity of module.layer1.2.conv2.weight [45.0% sparsity]
2018-10-27 01:03:53,027 - --- test ---------------------
2018-10-27 01:03:53,027 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:53,508 - Test: [   10/   39]    Loss 0.553685    Top1 91.328125    Top5 99.687500    
2018-10-27 01:03:53,611 - Test: [   20/   39]    Loss 0.555178    Top1 91.464844    Top5 99.589844    
2018-10-27 01:03:53,711 - Test: [   30/   39]    Loss 0.549130    Top1 91.497396    Top5 99.674479    
2018-10-27 01:03:53,803 - Test: [   40/   39]    Loss 0.545727    Top1 91.510000    Top5 99.650000    
2018-10-27 01:03:53,846 - ==> Top1: 91.510    Top5: 99.650    Loss: 0.546

2018-10-27 01:03:53,846 - Testing sensitivity of module.layer1.2.conv2.weight [50.0% sparsity]
2018-10-27 01:03:53,848 - --- test ---------------------
2018-10-27 01:03:53,848 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:54,278 - Test: [   10/   39]    Loss 0.554254    Top1 91.328125    Top5 99.648438    
2018-10-27 01:03:54,381 - Test: [   20/   39]    Loss 0.556327    Top1 91.406250    Top5 99.570312    
2018-10-27 01:03:54,481 - Test: [   30/   39]    Loss 0.549969    Top1 91.458333    Top5 99.661458    
2018-10-27 01:03:54,573 - Test: [   40/   39]    Loss 0.546049    Top1 91.450000    Top5 99.640000    
2018-10-27 01:03:54,600 - ==> Top1: 91.450    Top5: 99.640    Loss: 0.546

2018-10-27 01:03:54,601 - Testing sensitivity of module.layer1.2.conv2.weight [55.0% sparsity]
2018-10-27 01:03:54,604 - --- test ---------------------
2018-10-27 01:03:54,605 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:55,042 - Test: [   10/   39]    Loss 0.553928    Top1 91.328125    Top5 99.570312    
2018-10-27 01:03:55,145 - Test: [   20/   39]    Loss 0.555189    Top1 91.367188    Top5 99.531250    
2018-10-27 01:03:55,245 - Test: [   30/   39]    Loss 0.549951    Top1 91.406250    Top5 99.622396    
2018-10-27 01:03:55,338 - Test: [   40/   39]    Loss 0.546901    Top1 91.440000    Top5 99.620000    
2018-10-27 01:03:55,366 - ==> Top1: 91.440    Top5: 99.620    Loss: 0.547

2018-10-27 01:03:55,367 - Testing sensitivity of module.layer1.2.conv2.weight [60.0% sparsity]
2018-10-27 01:03:55,370 - --- test ---------------------
2018-10-27 01:03:55,371 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:55,801 - Test: [   10/   39]    Loss 0.552784    Top1 91.367188    Top5 99.570312    
2018-10-27 01:03:55,905 - Test: [   20/   39]    Loss 0.555124    Top1 91.445312    Top5 99.511719    
2018-10-27 01:03:56,005 - Test: [   30/   39]    Loss 0.551315    Top1 91.432292    Top5 99.609375    
2018-10-27 01:03:56,098 - Test: [   40/   39]    Loss 0.547628    Top1 91.440000    Top5 99.610000    
2018-10-27 01:03:56,124 - ==> Top1: 91.440    Top5: 99.610    Loss: 0.548

2018-10-27 01:03:56,125 - Testing sensitivity of module.layer1.2.conv2.weight [65.0% sparsity]
2018-10-27 01:03:56,128 - --- test ---------------------
2018-10-27 01:03:56,128 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:56,570 - Test: [   10/   39]    Loss 0.559282    Top1 91.054688    Top5 99.648438    
2018-10-27 01:03:56,674 - Test: [   20/   39]    Loss 0.556486    Top1 91.250000    Top5 99.570312    
2018-10-27 01:03:56,774 - Test: [   30/   39]    Loss 0.553048    Top1 91.223958    Top5 99.648438    
2018-10-27 01:03:56,867 - Test: [   40/   39]    Loss 0.549414    Top1 91.210000    Top5 99.640000    
2018-10-27 01:03:56,894 - ==> Top1: 91.210    Top5: 99.640    Loss: 0.549

2018-10-27 01:03:56,895 - Testing sensitivity of module.layer1.2.conv2.weight [70.0% sparsity]
2018-10-27 01:03:56,897 - --- test ---------------------
2018-10-27 01:03:56,897 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:57,322 - Test: [   10/   39]    Loss 0.559450    Top1 90.976562    Top5 99.570312    
2018-10-27 01:03:57,425 - Test: [   20/   39]    Loss 0.558244    Top1 91.132812    Top5 99.531250    
2018-10-27 01:03:57,525 - Test: [   30/   39]    Loss 0.556438    Top1 91.184896    Top5 99.609375    
2018-10-27 01:03:57,617 - Test: [   40/   39]    Loss 0.552705    Top1 91.230000    Top5 99.600000    
2018-10-27 01:03:57,645 - ==> Top1: 91.230    Top5: 99.600    Loss: 0.553

2018-10-27 01:03:57,645 - Testing sensitivity of module.layer1.2.conv2.weight [75.0% sparsity]
2018-10-27 01:03:57,648 - --- test ---------------------
2018-10-27 01:03:57,649 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:58,094 - Test: [   10/   39]    Loss 0.574755    Top1 91.093750    Top5 99.453125    
2018-10-27 01:03:58,199 - Test: [   20/   39]    Loss 0.573793    Top1 91.035156    Top5 99.472656    
2018-10-27 01:03:58,303 - Test: [   30/   39]    Loss 0.570576    Top1 90.976562    Top5 99.557292    
2018-10-27 01:03:58,396 - Test: [   40/   39]    Loss 0.567526    Top1 91.070000    Top5 99.550000    
2018-10-27 01:03:58,424 - ==> Top1: 91.070    Top5: 99.550    Loss: 0.568

2018-10-27 01:03:58,425 - Testing sensitivity of module.layer1.2.conv2.weight [80.0% sparsity]
2018-10-27 01:03:58,426 - --- test ---------------------
2018-10-27 01:03:58,427 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:58,862 - Test: [   10/   39]    Loss 0.584739    Top1 90.625000    Top5 99.492188    
2018-10-27 01:03:58,965 - Test: [   20/   39]    Loss 0.586972    Top1 90.664062    Top5 99.472656    
2018-10-27 01:03:59,065 - Test: [   30/   39]    Loss 0.585955    Top1 90.638021    Top5 99.557292    
2018-10-27 01:03:59,157 - Test: [   40/   39]    Loss 0.583122    Top1 90.750000    Top5 99.550000    
2018-10-27 01:03:59,185 - ==> Top1: 90.750    Top5: 99.550    Loss: 0.583

2018-10-27 01:03:59,186 - Testing sensitivity of module.layer1.2.conv2.weight [85.0% sparsity]
2018-10-27 01:03:59,188 - --- test ---------------------
2018-10-27 01:03:59,188 - 10000 samples (256 per mini-batch)
2018-10-27 01:03:59,633 - Test: [   10/   39]    Loss 0.594143    Top1 90.468750    Top5 99.609375    
2018-10-27 01:03:59,737 - Test: [   20/   39]    Loss 0.594218    Top1 90.449219    Top5 99.531250    
2018-10-27 01:03:59,837 - Test: [   30/   39]    Loss 0.595872    Top1 90.312500    Top5 99.583333    
2018-10-27 01:03:59,929 - Test: [   40/   39]    Loss 0.591471    Top1 90.520000    Top5 99.560000    
2018-10-27 01:03:59,957 - ==> Top1: 90.520    Top5: 99.560    Loss: 0.591

2018-10-27 01:03:59,958 - Testing sensitivity of module.layer1.2.conv2.weight [90.0% sparsity]
2018-10-27 01:03:59,961 - --- test ---------------------
2018-10-27 01:03:59,961 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:00,396 - Test: [   10/   39]    Loss 0.655731    Top1 89.414062    Top5 99.609375    
2018-10-27 01:04:00,500 - Test: [   20/   39]    Loss 0.646803    Top1 89.707031    Top5 99.531250    
2018-10-27 01:04:00,600 - Test: [   30/   39]    Loss 0.648895    Top1 89.479167    Top5 99.557292    
2018-10-27 01:04:00,692 - Test: [   40/   39]    Loss 0.640307    Top1 89.750000    Top5 99.530000    
2018-10-27 01:04:00,720 - ==> Top1: 89.750    Top5: 99.530    Loss: 0.640

2018-10-27 01:04:00,735 - Testing sensitivity of module.layer2.0.conv1.weight [0.0% sparsity]
2018-10-27 01:04:00,739 - --- test ---------------------
2018-10-27 01:04:00,740 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:01,174 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:04:01,281 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:04:01,385 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:04:01,478 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:04:01,506 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:04:01,507 - Testing sensitivity of module.layer2.0.conv1.weight [5.0% sparsity]
2018-10-27 01:04:01,510 - --- test ---------------------
2018-10-27 01:04:01,510 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:01,949 - Test: [   10/   39]    Loss 0.552273    Top1 91.328125    Top5 99.687500    
2018-10-27 01:04:02,054 - Test: [   20/   39]    Loss 0.553255    Top1 91.542969    Top5 99.570312    
2018-10-27 01:04:02,154 - Test: [   30/   39]    Loss 0.546648    Top1 91.497396    Top5 99.661458    
2018-10-27 01:04:02,246 - Test: [   40/   39]    Loss 0.541963    Top1 91.490000    Top5 99.640000    
2018-10-27 01:04:02,273 - ==> Top1: 91.490    Top5: 99.640    Loss: 0.542

2018-10-27 01:04:02,274 - Testing sensitivity of module.layer2.0.conv1.weight [10.0% sparsity]
2018-10-27 01:04:02,277 - --- test ---------------------
2018-10-27 01:04:02,277 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:02,709 - Test: [   10/   39]    Loss 0.552499    Top1 91.367188    Top5 99.687500    
2018-10-27 01:04:02,812 - Test: [   20/   39]    Loss 0.552727    Top1 91.542969    Top5 99.570312    
2018-10-27 01:04:02,912 - Test: [   30/   39]    Loss 0.546393    Top1 91.497396    Top5 99.661458    
2018-10-27 01:04:03,004 - Test: [   40/   39]    Loss 0.541714    Top1 91.490000    Top5 99.640000    
2018-10-27 01:04:03,030 - ==> Top1: 91.490    Top5: 99.640    Loss: 0.542

2018-10-27 01:04:03,031 - Testing sensitivity of module.layer2.0.conv1.weight [15.0% sparsity]
2018-10-27 01:04:03,034 - --- test ---------------------
2018-10-27 01:04:03,034 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:03,465 - Test: [   10/   39]    Loss 0.553734    Top1 91.289062    Top5 99.687500    
2018-10-27 01:04:03,568 - Test: [   20/   39]    Loss 0.553250    Top1 91.542969    Top5 99.570312    
2018-10-27 01:04:03,668 - Test: [   30/   39]    Loss 0.546534    Top1 91.510417    Top5 99.661458    
2018-10-27 01:04:03,761 - Test: [   40/   39]    Loss 0.542706    Top1 91.500000    Top5 99.640000    
2018-10-27 01:04:03,788 - ==> Top1: 91.500    Top5: 99.640    Loss: 0.543

2018-10-27 01:04:03,788 - Testing sensitivity of module.layer2.0.conv1.weight [20.0% sparsity]
2018-10-27 01:04:03,791 - --- test ---------------------
2018-10-27 01:04:03,791 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:04,238 - Test: [   10/   39]    Loss 0.554227    Top1 91.171875    Top5 99.687500    
2018-10-27 01:04:04,342 - Test: [   20/   39]    Loss 0.552961    Top1 91.464844    Top5 99.570312    
2018-10-27 01:04:04,442 - Test: [   30/   39]    Loss 0.545462    Top1 91.523438    Top5 99.661458    
2018-10-27 01:04:04,534 - Test: [   40/   39]    Loss 0.543158    Top1 91.560000    Top5 99.640000    
2018-10-27 01:04:04,561 - ==> Top1: 91.560    Top5: 99.640    Loss: 0.543

2018-10-27 01:04:04,562 - Testing sensitivity of module.layer2.0.conv1.weight [25.0% sparsity]
2018-10-27 01:04:04,565 - --- test ---------------------
2018-10-27 01:04:04,566 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:05,019 - Test: [   10/   39]    Loss 0.553793    Top1 91.132812    Top5 99.687500    
2018-10-27 01:04:05,124 - Test: [   20/   39]    Loss 0.553938    Top1 91.367188    Top5 99.589844    
2018-10-27 01:04:05,224 - Test: [   30/   39]    Loss 0.547206    Top1 91.445312    Top5 99.674479    
2018-10-27 01:04:05,316 - Test: [   40/   39]    Loss 0.541531    Top1 91.450000    Top5 99.660000    
2018-10-27 01:04:05,345 - ==> Top1: 91.450    Top5: 99.660    Loss: 0.542

2018-10-27 01:04:05,345 - Testing sensitivity of module.layer2.0.conv1.weight [30.0% sparsity]
2018-10-27 01:04:05,348 - --- test ---------------------
2018-10-27 01:04:05,348 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:05,792 - Test: [   10/   39]    Loss 0.554755    Top1 91.210938    Top5 99.609375    
2018-10-27 01:04:05,895 - Test: [   20/   39]    Loss 0.555327    Top1 91.347656    Top5 99.550781    
2018-10-27 01:04:05,995 - Test: [   30/   39]    Loss 0.547789    Top1 91.432292    Top5 99.635417    
2018-10-27 01:04:06,090 - Test: [   40/   39]    Loss 0.542990    Top1 91.420000    Top5 99.640000    
2018-10-27 01:04:06,118 - ==> Top1: 91.420    Top5: 99.640    Loss: 0.543

2018-10-27 01:04:06,119 - Testing sensitivity of module.layer2.0.conv1.weight [35.0% sparsity]
2018-10-27 01:04:06,122 - --- test ---------------------
2018-10-27 01:04:06,122 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:06,627 - Test: [   10/   39]    Loss 0.560957    Top1 91.132812    Top5 99.726562    
2018-10-27 01:04:06,739 - Test: [   20/   39]    Loss 0.559824    Top1 91.308594    Top5 99.589844    
2018-10-27 01:04:06,845 - Test: [   30/   39]    Loss 0.551746    Top1 91.328125    Top5 99.674479    
2018-10-27 01:04:06,937 - Test: [   40/   39]    Loss 0.548080    Top1 91.320000    Top5 99.670000    
2018-10-27 01:04:06,965 - ==> Top1: 91.320    Top5: 99.670    Loss: 0.548

2018-10-27 01:04:06,966 - Testing sensitivity of module.layer2.0.conv1.weight [40.0% sparsity]
2018-10-27 01:04:06,968 - --- test ---------------------
2018-10-27 01:04:06,968 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:07,446 - Test: [   10/   39]    Loss 0.566846    Top1 91.015625    Top5 99.687500    
2018-10-27 01:04:07,550 - Test: [   20/   39]    Loss 0.565440    Top1 91.328125    Top5 99.570312    
2018-10-27 01:04:07,653 - Test: [   30/   39]    Loss 0.556722    Top1 91.341146    Top5 99.661458    
2018-10-27 01:04:07,748 - Test: [   40/   39]    Loss 0.553528    Top1 91.230000    Top5 99.660000    
2018-10-27 01:04:07,777 - ==> Top1: 91.230    Top5: 99.660    Loss: 0.554

2018-10-27 01:04:07,777 - Testing sensitivity of module.layer2.0.conv1.weight [45.0% sparsity]
2018-10-27 01:04:07,780 - --- test ---------------------
2018-10-27 01:04:07,781 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:08,354 - Test: [   10/   39]    Loss 0.574121    Top1 90.781250    Top5 99.609375    
2018-10-27 01:04:08,468 - Test: [   20/   39]    Loss 0.579063    Top1 90.996094    Top5 99.472656    
2018-10-27 01:04:08,578 - Test: [   30/   39]    Loss 0.570100    Top1 90.976562    Top5 99.596354    
2018-10-27 01:04:08,679 - Test: [   40/   39]    Loss 0.563929    Top1 90.950000    Top5 99.610000    
2018-10-27 01:04:08,727 - ==> Top1: 90.950    Top5: 99.610    Loss: 0.564

2018-10-27 01:04:08,727 - Testing sensitivity of module.layer2.0.conv1.weight [50.0% sparsity]
2018-10-27 01:04:08,731 - --- test ---------------------
2018-10-27 01:04:08,732 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:09,194 - Test: [   10/   39]    Loss 0.595668    Top1 90.625000    Top5 99.531250    
2018-10-27 01:04:09,297 - Test: [   20/   39]    Loss 0.599689    Top1 90.859375    Top5 99.472656    
2018-10-27 01:04:09,397 - Test: [   30/   39]    Loss 0.589804    Top1 90.781250    Top5 99.583333    
2018-10-27 01:04:09,489 - Test: [   40/   39]    Loss 0.584605    Top1 90.660000    Top5 99.610000    
2018-10-27 01:04:09,517 - ==> Top1: 90.660    Top5: 99.610    Loss: 0.585

2018-10-27 01:04:09,518 - Testing sensitivity of module.layer2.0.conv1.weight [55.0% sparsity]
2018-10-27 01:04:09,520 - --- test ---------------------
2018-10-27 01:04:09,520 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:10,024 - Test: [   10/   39]    Loss 0.633940    Top1 89.570312    Top5 99.492188    
2018-10-27 01:04:10,139 - Test: [   20/   39]    Loss 0.634248    Top1 89.882812    Top5 99.394531    
2018-10-27 01:04:10,248 - Test: [   30/   39]    Loss 0.626192    Top1 89.869792    Top5 99.518229    
2018-10-27 01:04:10,348 - Test: [   40/   39]    Loss 0.617616    Top1 89.840000    Top5 99.540000    
2018-10-27 01:04:10,379 - ==> Top1: 89.840    Top5: 99.540    Loss: 0.618

2018-10-27 01:04:10,380 - Testing sensitivity of module.layer2.0.conv1.weight [60.0% sparsity]
2018-10-27 01:04:10,384 - --- test ---------------------
2018-10-27 01:04:10,385 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:10,839 - Test: [   10/   39]    Loss 0.694725    Top1 88.554688    Top5 99.531250    
2018-10-27 01:04:10,943 - Test: [   20/   39]    Loss 0.683419    Top1 89.003906    Top5 99.414062    
2018-10-27 01:04:11,043 - Test: [   30/   39]    Loss 0.677219    Top1 89.101562    Top5 99.492188    
2018-10-27 01:04:11,136 - Test: [   40/   39]    Loss 0.671898    Top1 89.130000    Top5 99.510000    
2018-10-27 01:04:11,164 - ==> Top1: 89.130    Top5: 99.510    Loss: 0.672

2018-10-27 01:04:11,164 - Testing sensitivity of module.layer2.0.conv1.weight [65.0% sparsity]
2018-10-27 01:04:11,167 - --- test ---------------------
2018-10-27 01:04:11,168 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:11,603 - Test: [   10/   39]    Loss 0.723359    Top1 88.281250    Top5 99.531250    
2018-10-27 01:04:11,705 - Test: [   20/   39]    Loss 0.711575    Top1 88.710938    Top5 99.375000    
2018-10-27 01:04:11,806 - Test: [   30/   39]    Loss 0.706530    Top1 88.658854    Top5 99.466146    
2018-10-27 01:04:11,898 - Test: [   40/   39]    Loss 0.710076    Top1 88.560000    Top5 99.500000    
2018-10-27 01:04:11,924 - ==> Top1: 88.560    Top5: 99.500    Loss: 0.710

2018-10-27 01:04:11,925 - Testing sensitivity of module.layer2.0.conv1.weight [70.0% sparsity]
2018-10-27 01:04:11,928 - --- test ---------------------
2018-10-27 01:04:11,928 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:12,378 - Test: [   10/   39]    Loss 0.777357    Top1 87.460938    Top5 99.375000    
2018-10-27 01:04:12,482 - Test: [   20/   39]    Loss 0.762079    Top1 87.832031    Top5 99.218750    
2018-10-27 01:04:12,582 - Test: [   30/   39]    Loss 0.758082    Top1 87.708333    Top5 99.361979    
2018-10-27 01:04:12,674 - Test: [   40/   39]    Loss 0.766676    Top1 87.720000    Top5 99.370000    
2018-10-27 01:04:12,700 - ==> Top1: 87.720    Top5: 99.370    Loss: 0.767

2018-10-27 01:04:12,701 - Testing sensitivity of module.layer2.0.conv1.weight [75.0% sparsity]
2018-10-27 01:04:12,704 - --- test ---------------------
2018-10-27 01:04:12,704 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:13,168 - Test: [   10/   39]    Loss 0.843890    Top1 86.406250    Top5 99.296875    
2018-10-27 01:04:13,272 - Test: [   20/   39]    Loss 0.830007    Top1 86.640625    Top5 99.160156    
2018-10-27 01:04:13,378 - Test: [   30/   39]    Loss 0.828131    Top1 86.445312    Top5 99.270833    
2018-10-27 01:04:13,478 - Test: [   40/   39]    Loss 0.837704    Top1 86.570000    Top5 99.260000    
2018-10-27 01:04:13,506 - ==> Top1: 86.570    Top5: 99.260    Loss: 0.838

2018-10-27 01:04:13,507 - Testing sensitivity of module.layer2.0.conv1.weight [80.0% sparsity]
2018-10-27 01:04:13,510 - --- test ---------------------
2018-10-27 01:04:13,510 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:13,952 - Test: [   10/   39]    Loss 1.033950    Top1 84.179688    Top5 99.023438    
2018-10-27 01:04:14,057 - Test: [   20/   39]    Loss 1.004917    Top1 84.531250    Top5 98.925781    
2018-10-27 01:04:14,157 - Test: [   30/   39]    Loss 1.019417    Top1 84.296875    Top5 99.062500    
2018-10-27 01:04:14,249 - Test: [   40/   39]    Loss 1.012730    Top1 84.240000    Top5 99.090000    
2018-10-27 01:04:14,276 - ==> Top1: 84.240    Top5: 99.090    Loss: 1.013

2018-10-27 01:04:14,277 - Testing sensitivity of module.layer2.0.conv1.weight [85.0% sparsity]
2018-10-27 01:04:14,280 - --- test ---------------------
2018-10-27 01:04:14,281 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:14,723 - Test: [   10/   39]    Loss 1.723337    Top1 75.039062    Top5 97.890625    
2018-10-27 01:04:14,826 - Test: [   20/   39]    Loss 1.709171    Top1 75.000000    Top5 97.910156    
2018-10-27 01:04:14,926 - Test: [   30/   39]    Loss 1.732747    Top1 74.856771    Top5 97.903646    
2018-10-27 01:04:15,019 - Test: [   40/   39]    Loss 1.735998    Top1 74.820000    Top5 98.020000    
2018-10-27 01:04:15,046 - ==> Top1: 74.820    Top5: 98.020    Loss: 1.736

2018-10-27 01:04:15,047 - Testing sensitivity of module.layer2.0.conv1.weight [90.0% sparsity]
2018-10-27 01:04:15,050 - --- test ---------------------
2018-10-27 01:04:15,050 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:15,484 - Test: [   10/   39]    Loss 3.988569    Top1 53.906250    Top5 91.250000    
2018-10-27 01:04:15,589 - Test: [   20/   39]    Loss 3.960543    Top1 54.394531    Top5 91.582031    
2018-10-27 01:04:15,690 - Test: [   30/   39]    Loss 3.912924    Top1 54.960937    Top5 91.419271    
2018-10-27 01:04:15,783 - Test: [   40/   39]    Loss 3.900700    Top1 54.940000    Top5 91.400000    
2018-10-27 01:04:15,811 - ==> Top1: 54.940    Top5: 91.400    Loss: 3.901

2018-10-27 01:04:15,826 - Testing sensitivity of module.layer2.0.conv2.weight [0.0% sparsity]
2018-10-27 01:04:15,830 - --- test ---------------------
2018-10-27 01:04:15,830 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:16,288 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:04:16,396 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:04:16,497 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:04:16,591 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:04:16,620 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:04:16,620 - Testing sensitivity of module.layer2.0.conv2.weight [5.0% sparsity]
2018-10-27 01:04:16,623 - --- test ---------------------
2018-10-27 01:04:16,623 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:17,101 - Test: [   10/   39]    Loss 0.551774    Top1 91.328125    Top5 99.687500    
2018-10-27 01:04:17,206 - Test: [   20/   39]    Loss 0.552814    Top1 91.562500    Top5 99.570312    
2018-10-27 01:04:17,310 - Test: [   30/   39]    Loss 0.546118    Top1 91.536458    Top5 99.661458    
2018-10-27 01:04:17,403 - Test: [   40/   39]    Loss 0.541495    Top1 91.540000    Top5 99.640000    
2018-10-27 01:04:17,432 - ==> Top1: 91.540    Top5: 99.640    Loss: 0.541

2018-10-27 01:04:17,432 - Testing sensitivity of module.layer2.0.conv2.weight [10.0% sparsity]
2018-10-27 01:04:17,435 - --- test ---------------------
2018-10-27 01:04:17,436 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:17,920 - Test: [   10/   39]    Loss 0.551625    Top1 91.289062    Top5 99.687500    
2018-10-27 01:04:18,028 - Test: [   20/   39]    Loss 0.552808    Top1 91.542969    Top5 99.570312    
2018-10-27 01:04:18,137 - Test: [   30/   39]    Loss 0.546137    Top1 91.510417    Top5 99.661458    
2018-10-27 01:04:18,236 - Test: [   40/   39]    Loss 0.541341    Top1 91.490000    Top5 99.640000    
2018-10-27 01:04:18,269 - ==> Top1: 91.490    Top5: 99.640    Loss: 0.541

2018-10-27 01:04:18,269 - Testing sensitivity of module.layer2.0.conv2.weight [15.0% sparsity]
2018-10-27 01:04:18,273 - --- test ---------------------
2018-10-27 01:04:18,273 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:18,738 - Test: [   10/   39]    Loss 0.552701    Top1 91.328125    Top5 99.687500    
2018-10-27 01:04:18,851 - Test: [   20/   39]    Loss 0.553328    Top1 91.542969    Top5 99.570312    
2018-10-27 01:04:18,956 - Test: [   30/   39]    Loss 0.546824    Top1 91.406250    Top5 99.661458    
2018-10-27 01:04:19,049 - Test: [   40/   39]    Loss 0.541590    Top1 91.410000    Top5 99.650000    
2018-10-27 01:04:19,080 - ==> Top1: 91.410    Top5: 99.650    Loss: 0.542

2018-10-27 01:04:19,081 - Testing sensitivity of module.layer2.0.conv2.weight [20.0% sparsity]
2018-10-27 01:04:19,084 - --- test ---------------------
2018-10-27 01:04:19,084 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:19,535 - Test: [   10/   39]    Loss 0.556435    Top1 91.289062    Top5 99.687500    
2018-10-27 01:04:19,639 - Test: [   20/   39]    Loss 0.555149    Top1 91.523438    Top5 99.589844    
2018-10-27 01:04:19,739 - Test: [   30/   39]    Loss 0.548046    Top1 91.419271    Top5 99.674479    
2018-10-27 01:04:19,831 - Test: [   40/   39]    Loss 0.542273    Top1 91.390000    Top5 99.670000    
2018-10-27 01:04:19,859 - ==> Top1: 91.390    Top5: 99.670    Loss: 0.542

2018-10-27 01:04:19,859 - Testing sensitivity of module.layer2.0.conv2.weight [25.0% sparsity]
2018-10-27 01:04:19,862 - --- test ---------------------
2018-10-27 01:04:19,863 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:20,324 - Test: [   10/   39]    Loss 0.557776    Top1 91.367188    Top5 99.648438    
2018-10-27 01:04:20,436 - Test: [   20/   39]    Loss 0.555837    Top1 91.523438    Top5 99.570312    
2018-10-27 01:04:20,544 - Test: [   30/   39]    Loss 0.547782    Top1 91.510417    Top5 99.661458    
2018-10-27 01:04:20,641 - Test: [   40/   39]    Loss 0.542578    Top1 91.480000    Top5 99.640000    
2018-10-27 01:04:20,681 - ==> Top1: 91.480    Top5: 99.640    Loss: 0.543

2018-10-27 01:04:20,681 - Testing sensitivity of module.layer2.0.conv2.weight [30.0% sparsity]
2018-10-27 01:04:20,684 - --- test ---------------------
2018-10-27 01:04:20,685 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:21,171 - Test: [   10/   39]    Loss 0.561079    Top1 91.445312    Top5 99.687500    
2018-10-27 01:04:21,277 - Test: [   20/   39]    Loss 0.557788    Top1 91.582031    Top5 99.589844    
2018-10-27 01:04:21,382 - Test: [   30/   39]    Loss 0.549931    Top1 91.549479    Top5 99.674479    
2018-10-27 01:04:21,481 - Test: [   40/   39]    Loss 0.543880    Top1 91.470000    Top5 99.670000    
2018-10-27 01:04:21,508 - ==> Top1: 91.470    Top5: 99.670    Loss: 0.544

2018-10-27 01:04:21,509 - Testing sensitivity of module.layer2.0.conv2.weight [35.0% sparsity]
2018-10-27 01:04:21,512 - --- test ---------------------
2018-10-27 01:04:21,512 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:21,962 - Test: [   10/   39]    Loss 0.564688    Top1 91.132812    Top5 99.687500    
2018-10-27 01:04:22,066 - Test: [   20/   39]    Loss 0.560657    Top1 91.503906    Top5 99.570312    
2018-10-27 01:04:22,172 - Test: [   30/   39]    Loss 0.553621    Top1 91.406250    Top5 99.661458    
2018-10-27 01:04:22,272 - Test: [   40/   39]    Loss 0.546661    Top1 91.400000    Top5 99.660000    
2018-10-27 01:04:22,299 - ==> Top1: 91.400    Top5: 99.660    Loss: 0.547

2018-10-27 01:04:22,300 - Testing sensitivity of module.layer2.0.conv2.weight [40.0% sparsity]
2018-10-27 01:04:22,303 - --- test ---------------------
2018-10-27 01:04:22,304 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:22,788 - Test: [   10/   39]    Loss 0.570454    Top1 91.171875    Top5 99.687500    
2018-10-27 01:04:22,892 - Test: [   20/   39]    Loss 0.566088    Top1 91.347656    Top5 99.570312    
2018-10-27 01:04:23,001 - Test: [   30/   39]    Loss 0.558786    Top1 91.289062    Top5 99.661458    
2018-10-27 01:04:23,101 - Test: [   40/   39]    Loss 0.552771    Top1 91.290000    Top5 99.660000    
2018-10-27 01:04:23,140 - ==> Top1: 91.290    Top5: 99.660    Loss: 0.553

2018-10-27 01:04:23,144 - Testing sensitivity of module.layer2.0.conv2.weight [45.0% sparsity]
2018-10-27 01:04:23,147 - --- test ---------------------
2018-10-27 01:04:23,147 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:23,600 - Test: [   10/   39]    Loss 0.573110    Top1 90.898438    Top5 99.687500    
2018-10-27 01:04:23,704 - Test: [   20/   39]    Loss 0.569874    Top1 91.113281    Top5 99.570312    
2018-10-27 01:04:23,813 - Test: [   30/   39]    Loss 0.561206    Top1 91.093750    Top5 99.661458    
2018-10-27 01:04:23,912 - Test: [   40/   39]    Loss 0.554382    Top1 91.170000    Top5 99.660000    
2018-10-27 01:04:23,941 - ==> Top1: 91.170    Top5: 99.660    Loss: 0.554

2018-10-27 01:04:23,941 - Testing sensitivity of module.layer2.0.conv2.weight [50.0% sparsity]
2018-10-27 01:04:23,945 - --- test ---------------------
2018-10-27 01:04:23,945 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:24,391 - Test: [   10/   39]    Loss 0.591769    Top1 90.625000    Top5 99.726562    
2018-10-27 01:04:24,495 - Test: [   20/   39]    Loss 0.586683    Top1 90.917969    Top5 99.570312    
2018-10-27 01:04:24,598 - Test: [   30/   39]    Loss 0.574636    Top1 90.872396    Top5 99.648438    
2018-10-27 01:04:24,693 - Test: [   40/   39]    Loss 0.567597    Top1 90.960000    Top5 99.660000    
2018-10-27 01:04:24,727 - ==> Top1: 90.960    Top5: 99.660    Loss: 0.568

2018-10-27 01:04:24,728 - Testing sensitivity of module.layer2.0.conv2.weight [55.0% sparsity]
2018-10-27 01:04:24,731 - --- test ---------------------
2018-10-27 01:04:24,731 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:25,202 - Test: [   10/   39]    Loss 0.607459    Top1 90.546875    Top5 99.726562    
2018-10-27 01:04:25,315 - Test: [   20/   39]    Loss 0.599881    Top1 90.566406    Top5 99.609375    
2018-10-27 01:04:25,419 - Test: [   30/   39]    Loss 0.587053    Top1 90.585938    Top5 99.700521    
2018-10-27 01:04:25,511 - Test: [   40/   39]    Loss 0.579788    Top1 90.680000    Top5 99.690000    
2018-10-27 01:04:25,538 - ==> Top1: 90.680    Top5: 99.690    Loss: 0.580

2018-10-27 01:04:25,539 - Testing sensitivity of module.layer2.0.conv2.weight [60.0% sparsity]
2018-10-27 01:04:25,541 - --- test ---------------------
2018-10-27 01:04:25,541 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:25,998 - Test: [   10/   39]    Loss 0.634257    Top1 90.351562    Top5 99.648438    
2018-10-27 01:04:26,104 - Test: [   20/   39]    Loss 0.623227    Top1 90.390625    Top5 99.550781    
2018-10-27 01:04:26,210 - Test: [   30/   39]    Loss 0.610920    Top1 90.468750    Top5 99.661458    
2018-10-27 01:04:26,311 - Test: [   40/   39]    Loss 0.604368    Top1 90.460000    Top5 99.650000    
2018-10-27 01:04:26,339 - ==> Top1: 90.460    Top5: 99.650    Loss: 0.604

2018-10-27 01:04:26,340 - Testing sensitivity of module.layer2.0.conv2.weight [65.0% sparsity]
2018-10-27 01:04:26,342 - --- test ---------------------
2018-10-27 01:04:26,342 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:26,808 - Test: [   10/   39]    Loss 0.666083    Top1 89.921875    Top5 99.648438    
2018-10-27 01:04:26,913 - Test: [   20/   39]    Loss 0.652560    Top1 89.980469    Top5 99.511719    
2018-10-27 01:04:27,022 - Test: [   30/   39]    Loss 0.643704    Top1 90.039062    Top5 99.622396    
2018-10-27 01:04:27,120 - Test: [   40/   39]    Loss 0.632997    Top1 90.050000    Top5 99.620000    
2018-10-27 01:04:27,148 - ==> Top1: 90.050    Top5: 99.620    Loss: 0.633

2018-10-27 01:04:27,149 - Testing sensitivity of module.layer2.0.conv2.weight [70.0% sparsity]
2018-10-27 01:04:27,152 - --- test ---------------------
2018-10-27 01:04:27,153 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:27,600 - Test: [   10/   39]    Loss 0.754896    Top1 88.437500    Top5 99.609375    
2018-10-27 01:04:27,704 - Test: [   20/   39]    Loss 0.741616    Top1 88.496094    Top5 99.414062    
2018-10-27 01:04:27,808 - Test: [   30/   39]    Loss 0.728983    Top1 88.606771    Top5 99.544271    
2018-10-27 01:04:27,903 - Test: [   40/   39]    Loss 0.710390    Top1 88.660000    Top5 99.570000    
2018-10-27 01:04:27,930 - ==> Top1: 88.660    Top5: 99.570    Loss: 0.710

2018-10-27 01:04:27,931 - Testing sensitivity of module.layer2.0.conv2.weight [75.0% sparsity]
2018-10-27 01:04:27,934 - --- test ---------------------
2018-10-27 01:04:27,934 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:28,387 - Test: [   10/   39]    Loss 0.858713    Top1 86.796875    Top5 99.375000    
2018-10-27 01:04:28,490 - Test: [   20/   39]    Loss 0.848701    Top1 87.089844    Top5 99.238281    
2018-10-27 01:04:28,594 - Test: [   30/   39]    Loss 0.850175    Top1 86.992188    Top5 99.388021    
2018-10-27 01:04:28,694 - Test: [   40/   39]    Loss 0.826272    Top1 86.970000    Top5 99.430000    
2018-10-27 01:04:28,725 - ==> Top1: 86.970    Top5: 99.430    Loss: 0.826

2018-10-27 01:04:28,726 - Testing sensitivity of module.layer2.0.conv2.weight [80.0% sparsity]
2018-10-27 01:04:28,729 - --- test ---------------------
2018-10-27 01:04:28,729 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:29,181 - Test: [   10/   39]    Loss 1.128504    Top1 83.359375    Top5 99.335938    
2018-10-27 01:04:29,286 - Test: [   20/   39]    Loss 1.120376    Top1 83.710938    Top5 99.121094    
2018-10-27 01:04:29,387 - Test: [   30/   39]    Loss 1.119070    Top1 83.710938    Top5 99.257812    
2018-10-27 01:04:29,479 - Test: [   40/   39]    Loss 1.084451    Top1 83.770000    Top5 99.290000    
2018-10-27 01:04:29,507 - ==> Top1: 83.770    Top5: 99.290    Loss: 1.084

2018-10-27 01:04:29,508 - Testing sensitivity of module.layer2.0.conv2.weight [85.0% sparsity]
2018-10-27 01:04:29,511 - --- test ---------------------
2018-10-27 01:04:29,511 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:29,957 - Test: [   10/   39]    Loss 1.602632    Top1 78.710938    Top5 98.984375    
2018-10-27 01:04:30,062 - Test: [   20/   39]    Loss 1.599282    Top1 78.847656    Top5 98.769531    
2018-10-27 01:04:30,162 - Test: [   30/   39]    Loss 1.588225    Top1 78.710938    Top5 98.997396    
2018-10-27 01:04:30,254 - Test: [   40/   39]    Loss 1.549359    Top1 78.880000    Top5 99.030000    
2018-10-27 01:04:30,280 - ==> Top1: 78.880    Top5: 99.030    Loss: 1.549

2018-10-27 01:04:30,281 - Testing sensitivity of module.layer2.0.conv2.weight [90.0% sparsity]
2018-10-27 01:04:30,284 - --- test ---------------------
2018-10-27 01:04:30,285 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:30,758 - Test: [   10/   39]    Loss 2.273726    Top1 70.703125    Top5 98.125000    
2018-10-27 01:04:30,862 - Test: [   20/   39]    Loss 2.264802    Top1 70.839844    Top5 97.753906    
2018-10-27 01:04:30,969 - Test: [   30/   39]    Loss 2.278664    Top1 70.677083    Top5 97.929688    
2018-10-27 01:04:31,066 - Test: [   40/   39]    Loss 2.250431    Top1 70.870000    Top5 97.920000    
2018-10-27 01:04:31,093 - ==> Top1: 70.870    Top5: 97.920    Loss: 2.250

2018-10-27 01:04:31,110 - Testing sensitivity of module.layer2.0.downsample.0.weight [0.0% sparsity]
2018-10-27 01:04:31,114 - --- test ---------------------
2018-10-27 01:04:31,114 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:31,602 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:04:31,714 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:04:31,824 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:04:31,923 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:04:31,953 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:04:31,953 - Testing sensitivity of module.layer2.0.downsample.0.weight [5.0% sparsity]
2018-10-27 01:04:31,955 - --- test ---------------------
2018-10-27 01:04:31,956 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:32,421 - Test: [   10/   39]    Loss 0.551579    Top1 91.289062    Top5 99.687500    
2018-10-27 01:04:32,524 - Test: [   20/   39]    Loss 0.552739    Top1 91.542969    Top5 99.589844    
2018-10-27 01:04:32,625 - Test: [   30/   39]    Loss 0.546175    Top1 91.536458    Top5 99.674479    
2018-10-27 01:04:32,719 - Test: [   40/   39]    Loss 0.541280    Top1 91.540000    Top5 99.650000    
2018-10-27 01:04:32,747 - ==> Top1: 91.540    Top5: 99.650    Loss: 0.541

2018-10-27 01:04:32,748 - Testing sensitivity of module.layer2.0.downsample.0.weight [10.0% sparsity]
2018-10-27 01:04:32,751 - --- test ---------------------
2018-10-27 01:04:32,751 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:33,262 - Test: [   10/   39]    Loss 0.552702    Top1 91.328125    Top5 99.687500    
2018-10-27 01:04:33,370 - Test: [   20/   39]    Loss 0.553483    Top1 91.503906    Top5 99.589844    
2018-10-27 01:04:33,474 - Test: [   30/   39]    Loss 0.546945    Top1 91.510417    Top5 99.674479    
2018-10-27 01:04:33,570 - Test: [   40/   39]    Loss 0.542054    Top1 91.510000    Top5 99.650000    
2018-10-27 01:04:33,602 - ==> Top1: 91.510    Top5: 99.650    Loss: 0.542

2018-10-27 01:04:33,603 - Testing sensitivity of module.layer2.0.downsample.0.weight [15.0% sparsity]
2018-10-27 01:04:33,606 - --- test ---------------------
2018-10-27 01:04:33,607 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:34,054 - Test: [   10/   39]    Loss 0.551347    Top1 91.328125    Top5 99.687500    
2018-10-27 01:04:34,160 - Test: [   20/   39]    Loss 0.552477    Top1 91.562500    Top5 99.589844    
2018-10-27 01:04:34,262 - Test: [   30/   39]    Loss 0.546144    Top1 91.575521    Top5 99.674479    
2018-10-27 01:04:34,354 - Test: [   40/   39]    Loss 0.541416    Top1 91.580000    Top5 99.650000    
2018-10-27 01:04:34,383 - ==> Top1: 91.580    Top5: 99.650    Loss: 0.541

2018-10-27 01:04:34,383 - Testing sensitivity of module.layer2.0.downsample.0.weight [20.0% sparsity]
2018-10-27 01:04:34,386 - --- test ---------------------
2018-10-27 01:04:34,386 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:34,828 - Test: [   10/   39]    Loss 0.552707    Top1 91.210938    Top5 99.687500    
2018-10-27 01:04:34,931 - Test: [   20/   39]    Loss 0.553403    Top1 91.523438    Top5 99.589844    
2018-10-27 01:04:35,031 - Test: [   30/   39]    Loss 0.547137    Top1 91.471354    Top5 99.674479    
2018-10-27 01:04:35,123 - Test: [   40/   39]    Loss 0.541344    Top1 91.500000    Top5 99.650000    
2018-10-27 01:04:35,160 - ==> Top1: 91.500    Top5: 99.650    Loss: 0.541

2018-10-27 01:04:35,161 - Testing sensitivity of module.layer2.0.downsample.0.weight [25.0% sparsity]
2018-10-27 01:04:35,164 - --- test ---------------------
2018-10-27 01:04:35,165 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:35,602 - Test: [   10/   39]    Loss 0.550176    Top1 91.210938    Top5 99.687500    
2018-10-27 01:04:35,705 - Test: [   20/   39]    Loss 0.553034    Top1 91.562500    Top5 99.589844    
2018-10-27 01:04:35,805 - Test: [   30/   39]    Loss 0.548093    Top1 91.484375    Top5 99.674479    
2018-10-27 01:04:35,897 - Test: [   40/   39]    Loss 0.542094    Top1 91.540000    Top5 99.650000    
2018-10-27 01:04:35,926 - ==> Top1: 91.540    Top5: 99.650    Loss: 0.542

2018-10-27 01:04:35,926 - Testing sensitivity of module.layer2.0.downsample.0.weight [30.0% sparsity]
2018-10-27 01:04:35,928 - --- test ---------------------
2018-10-27 01:04:35,928 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:36,359 - Test: [   10/   39]    Loss 0.552159    Top1 91.250000    Top5 99.609375    
2018-10-27 01:04:36,462 - Test: [   20/   39]    Loss 0.557081    Top1 91.386719    Top5 99.570312    
2018-10-27 01:04:36,562 - Test: [   30/   39]    Loss 0.552166    Top1 91.380208    Top5 99.661458    
2018-10-27 01:04:36,653 - Test: [   40/   39]    Loss 0.547027    Top1 91.420000    Top5 99.640000    
2018-10-27 01:04:36,682 - ==> Top1: 91.420    Top5: 99.640    Loss: 0.547

2018-10-27 01:04:36,683 - Testing sensitivity of module.layer2.0.downsample.0.weight [35.0% sparsity]
2018-10-27 01:04:36,685 - --- test ---------------------
2018-10-27 01:04:36,686 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:37,139 - Test: [   10/   39]    Loss 0.547684    Top1 91.210938    Top5 99.570312    
2018-10-27 01:04:37,243 - Test: [   20/   39]    Loss 0.554201    Top1 91.367188    Top5 99.492188    
2018-10-27 01:04:37,342 - Test: [   30/   39]    Loss 0.550607    Top1 91.380208    Top5 99.609375    
2018-10-27 01:04:37,434 - Test: [   40/   39]    Loss 0.545326    Top1 91.460000    Top5 99.610000    
2018-10-27 01:04:37,462 - ==> Top1: 91.460    Top5: 99.610    Loss: 0.545

2018-10-27 01:04:37,462 - Testing sensitivity of module.layer2.0.downsample.0.weight [40.0% sparsity]
2018-10-27 01:04:37,465 - --- test ---------------------
2018-10-27 01:04:37,465 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:37,900 - Test: [   10/   39]    Loss 0.547912    Top1 91.054688    Top5 99.492188    
2018-10-27 01:04:38,003 - Test: [   20/   39]    Loss 0.557547    Top1 91.171875    Top5 99.453125    
2018-10-27 01:04:38,103 - Test: [   30/   39]    Loss 0.553699    Top1 91.223958    Top5 99.570312    
2018-10-27 01:04:38,195 - Test: [   40/   39]    Loss 0.548286    Top1 91.280000    Top5 99.590000    
2018-10-27 01:04:38,223 - ==> Top1: 91.280    Top5: 99.590    Loss: 0.548

2018-10-27 01:04:38,224 - Testing sensitivity of module.layer2.0.downsample.0.weight [45.0% sparsity]
2018-10-27 01:04:38,227 - --- test ---------------------
2018-10-27 01:04:38,227 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:38,686 - Test: [   10/   39]    Loss 0.550795    Top1 91.171875    Top5 99.570312    
2018-10-27 01:04:38,789 - Test: [   20/   39]    Loss 0.557761    Top1 91.386719    Top5 99.511719    
2018-10-27 01:04:38,889 - Test: [   30/   39]    Loss 0.554529    Top1 91.432292    Top5 99.609375    
2018-10-27 01:04:38,981 - Test: [   40/   39]    Loss 0.549544    Top1 91.420000    Top5 99.610000    
2018-10-27 01:04:39,009 - ==> Top1: 91.420    Top5: 99.610    Loss: 0.550

2018-10-27 01:04:39,010 - Testing sensitivity of module.layer2.0.downsample.0.weight [50.0% sparsity]
2018-10-27 01:04:39,012 - --- test ---------------------
2018-10-27 01:04:39,013 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:39,461 - Test: [   10/   39]    Loss 0.556357    Top1 91.171875    Top5 99.570312    
2018-10-27 01:04:39,565 - Test: [   20/   39]    Loss 0.559287    Top1 91.503906    Top5 99.531250    
2018-10-27 01:04:39,666 - Test: [   30/   39]    Loss 0.557323    Top1 91.380208    Top5 99.635417    
2018-10-27 01:04:39,759 - Test: [   40/   39]    Loss 0.549645    Top1 91.390000    Top5 99.650000    
2018-10-27 01:04:39,787 - ==> Top1: 91.390    Top5: 99.650    Loss: 0.550

2018-10-27 01:04:39,788 - Testing sensitivity of module.layer2.0.downsample.0.weight [55.0% sparsity]
2018-10-27 01:04:39,791 - --- test ---------------------
2018-10-27 01:04:39,791 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:40,241 - Test: [   10/   39]    Loss 0.563438    Top1 91.132812    Top5 99.609375    
2018-10-27 01:04:40,344 - Test: [   20/   39]    Loss 0.567815    Top1 91.386719    Top5 99.550781    
2018-10-27 01:04:40,444 - Test: [   30/   39]    Loss 0.566951    Top1 91.210938    Top5 99.648438    
2018-10-27 01:04:40,536 - Test: [   40/   39]    Loss 0.561718    Top1 91.190000    Top5 99.650000    
2018-10-27 01:04:40,564 - ==> Top1: 91.190    Top5: 99.650    Loss: 0.562

2018-10-27 01:04:40,565 - Testing sensitivity of module.layer2.0.downsample.0.weight [60.0% sparsity]
2018-10-27 01:04:40,567 - --- test ---------------------
2018-10-27 01:04:40,568 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:41,018 - Test: [   10/   39]    Loss 0.567280    Top1 91.093750    Top5 99.492188    
2018-10-27 01:04:41,122 - Test: [   20/   39]    Loss 0.575675    Top1 91.347656    Top5 99.433594    
2018-10-27 01:04:41,223 - Test: [   30/   39]    Loss 0.573778    Top1 91.119792    Top5 99.570312    
2018-10-27 01:04:41,316 - Test: [   40/   39]    Loss 0.566714    Top1 91.100000    Top5 99.590000    
2018-10-27 01:04:41,344 - ==> Top1: 91.100    Top5: 99.590    Loss: 0.567

2018-10-27 01:04:41,344 - Testing sensitivity of module.layer2.0.downsample.0.weight [65.0% sparsity]
2018-10-27 01:04:41,347 - --- test ---------------------
2018-10-27 01:04:41,347 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:41,793 - Test: [   10/   39]    Loss 0.580182    Top1 90.937500    Top5 99.375000    
2018-10-27 01:04:41,896 - Test: [   20/   39]    Loss 0.593450    Top1 90.937500    Top5 99.375000    
2018-10-27 01:04:41,995 - Test: [   30/   39]    Loss 0.593301    Top1 90.846354    Top5 99.505208    
2018-10-27 01:04:42,087 - Test: [   40/   39]    Loss 0.583047    Top1 90.880000    Top5 99.530000    
2018-10-27 01:04:42,115 - ==> Top1: 90.880    Top5: 99.530    Loss: 0.583

2018-10-27 01:04:42,116 - Testing sensitivity of module.layer2.0.downsample.0.weight [70.0% sparsity]
2018-10-27 01:04:42,118 - --- test ---------------------
2018-10-27 01:04:42,119 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:42,552 - Test: [   10/   39]    Loss 0.609566    Top1 90.156250    Top5 99.375000    
2018-10-27 01:04:42,657 - Test: [   20/   39]    Loss 0.619886    Top1 90.351562    Top5 99.375000    
2018-10-27 01:04:42,757 - Test: [   30/   39]    Loss 0.619267    Top1 90.299479    Top5 99.492188    
2018-10-27 01:04:42,849 - Test: [   40/   39]    Loss 0.612633    Top1 90.330000    Top5 99.520000    
2018-10-27 01:04:42,877 - ==> Top1: 90.330    Top5: 99.520    Loss: 0.613

2018-10-27 01:04:42,878 - Testing sensitivity of module.layer2.0.downsample.0.weight [75.0% sparsity]
2018-10-27 01:04:42,881 - --- test ---------------------
2018-10-27 01:04:42,881 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:43,317 - Test: [   10/   39]    Loss 0.690319    Top1 89.414062    Top5 99.335938    
2018-10-27 01:04:43,420 - Test: [   20/   39]    Loss 0.694671    Top1 89.628906    Top5 99.335938    
2018-10-27 01:04:43,521 - Test: [   30/   39]    Loss 0.699355    Top1 89.414062    Top5 99.414062    
2018-10-27 01:04:43,613 - Test: [   40/   39]    Loss 0.688294    Top1 89.540000    Top5 99.450000    
2018-10-27 01:04:43,641 - ==> Top1: 89.540    Top5: 99.450    Loss: 0.688

2018-10-27 01:04:43,642 - Testing sensitivity of module.layer2.0.downsample.0.weight [80.0% sparsity]
2018-10-27 01:04:43,645 - --- test ---------------------
2018-10-27 01:04:43,646 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:44,082 - Test: [   10/   39]    Loss 0.728632    Top1 88.750000    Top5 99.218750    
2018-10-27 01:04:44,186 - Test: [   20/   39]    Loss 0.750075    Top1 88.750000    Top5 99.238281    
2018-10-27 01:04:44,286 - Test: [   30/   39]    Loss 0.749141    Top1 88.697917    Top5 99.283854    
2018-10-27 01:04:44,377 - Test: [   40/   39]    Loss 0.745191    Top1 88.640000    Top5 99.330000    
2018-10-27 01:04:44,406 - ==> Top1: 88.640    Top5: 99.330    Loss: 0.745

2018-10-27 01:04:44,407 - Testing sensitivity of module.layer2.0.downsample.0.weight [85.0% sparsity]
2018-10-27 01:04:44,410 - --- test ---------------------
2018-10-27 01:04:44,410 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:44,839 - Test: [   10/   39]    Loss 0.840269    Top1 86.992188    Top5 99.218750    
2018-10-27 01:04:44,942 - Test: [   20/   39]    Loss 0.862317    Top1 86.972656    Top5 99.082031    
2018-10-27 01:04:45,042 - Test: [   30/   39]    Loss 0.869615    Top1 86.588542    Top5 99.153646    
2018-10-27 01:04:45,134 - Test: [   40/   39]    Loss 0.861531    Top1 86.710000    Top5 99.190000    
2018-10-27 01:04:45,163 - ==> Top1: 86.710    Top5: 99.190    Loss: 0.862

2018-10-27 01:04:45,164 - Testing sensitivity of module.layer2.0.downsample.0.weight [90.0% sparsity]
2018-10-27 01:04:45,166 - --- test ---------------------
2018-10-27 01:04:45,167 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:45,620 - Test: [   10/   39]    Loss 0.992416    Top1 85.039062    Top5 98.750000    
2018-10-27 01:04:45,723 - Test: [   20/   39]    Loss 1.012364    Top1 84.980469    Top5 98.730469    
2018-10-27 01:04:45,823 - Test: [   30/   39]    Loss 1.035921    Top1 84.518229    Top5 98.828125    
2018-10-27 01:04:45,914 - Test: [   40/   39]    Loss 1.013979    Top1 84.550000    Top5 98.900000    
2018-10-27 01:04:45,941 - ==> Top1: 84.550    Top5: 98.900    Loss: 1.014

2018-10-27 01:04:45,956 - Testing sensitivity of module.layer2.1.conv1.weight [0.0% sparsity]
2018-10-27 01:04:45,960 - --- test ---------------------
2018-10-27 01:04:45,961 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:46,389 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:04:46,493 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:04:46,594 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:04:46,686 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:04:46,715 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:04:46,715 - Testing sensitivity of module.layer2.1.conv1.weight [5.0% sparsity]
2018-10-27 01:04:46,718 - --- test ---------------------
2018-10-27 01:04:46,718 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:47,154 - Test: [   10/   39]    Loss 0.551750    Top1 91.289062    Top5 99.687500    
2018-10-27 01:04:47,257 - Test: [   20/   39]    Loss 0.553048    Top1 91.523438    Top5 99.570312    
2018-10-27 01:04:47,358 - Test: [   30/   39]    Loss 0.546375    Top1 91.510417    Top5 99.661458    
2018-10-27 01:04:47,451 - Test: [   40/   39]    Loss 0.541629    Top1 91.510000    Top5 99.640000    
2018-10-27 01:04:47,480 - ==> Top1: 91.510    Top5: 99.640    Loss: 0.542

2018-10-27 01:04:47,481 - Testing sensitivity of module.layer2.1.conv1.weight [10.0% sparsity]
2018-10-27 01:04:47,484 - --- test ---------------------
2018-10-27 01:04:47,485 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:47,936 - Test: [   10/   39]    Loss 0.551575    Top1 91.328125    Top5 99.687500    
2018-10-27 01:04:48,039 - Test: [   20/   39]    Loss 0.553201    Top1 91.562500    Top5 99.589844    
2018-10-27 01:04:48,140 - Test: [   30/   39]    Loss 0.546545    Top1 91.575521    Top5 99.674479    
2018-10-27 01:04:48,233 - Test: [   40/   39]    Loss 0.541754    Top1 91.560000    Top5 99.650000    
2018-10-27 01:04:48,261 - ==> Top1: 91.560    Top5: 99.650    Loss: 0.542

2018-10-27 01:04:48,262 - Testing sensitivity of module.layer2.1.conv1.weight [15.0% sparsity]
2018-10-27 01:04:48,264 - --- test ---------------------
2018-10-27 01:04:48,265 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:48,713 - Test: [   10/   39]    Loss 0.551733    Top1 91.289062    Top5 99.687500    
2018-10-27 01:04:48,817 - Test: [   20/   39]    Loss 0.553014    Top1 91.523438    Top5 99.570312    
2018-10-27 01:04:48,918 - Test: [   30/   39]    Loss 0.546544    Top1 91.510417    Top5 99.661458    
2018-10-27 01:04:49,010 - Test: [   40/   39]    Loss 0.541951    Top1 91.520000    Top5 99.640000    
2018-10-27 01:04:49,037 - ==> Top1: 91.520    Top5: 99.640    Loss: 0.542

2018-10-27 01:04:49,038 - Testing sensitivity of module.layer2.1.conv1.weight [20.0% sparsity]
2018-10-27 01:04:49,040 - --- test ---------------------
2018-10-27 01:04:49,041 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:49,464 - Test: [   10/   39]    Loss 0.551815    Top1 91.328125    Top5 99.687500    
2018-10-27 01:04:49,569 - Test: [   20/   39]    Loss 0.553425    Top1 91.464844    Top5 99.570312    
2018-10-27 01:04:49,670 - Test: [   30/   39]    Loss 0.546703    Top1 91.497396    Top5 99.661458    
2018-10-27 01:04:49,763 - Test: [   40/   39]    Loss 0.542505    Top1 91.530000    Top5 99.650000    
2018-10-27 01:04:49,791 - ==> Top1: 91.530    Top5: 99.650    Loss: 0.543

2018-10-27 01:04:49,793 - Testing sensitivity of module.layer2.1.conv1.weight [25.0% sparsity]
2018-10-27 01:04:49,796 - --- test ---------------------
2018-10-27 01:04:49,797 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:50,243 - Test: [   10/   39]    Loss 0.552621    Top1 91.328125    Top5 99.648438    
2018-10-27 01:04:50,346 - Test: [   20/   39]    Loss 0.553944    Top1 91.464844    Top5 99.570312    
2018-10-27 01:04:50,447 - Test: [   30/   39]    Loss 0.546646    Top1 91.445312    Top5 99.661458    
2018-10-27 01:04:50,540 - Test: [   40/   39]    Loss 0.542134    Top1 91.540000    Top5 99.650000    
2018-10-27 01:04:50,568 - ==> Top1: 91.540    Top5: 99.650    Loss: 0.542

2018-10-27 01:04:50,569 - Testing sensitivity of module.layer2.1.conv1.weight [30.0% sparsity]
2018-10-27 01:04:50,572 - --- test ---------------------
2018-10-27 01:04:50,572 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:51,016 - Test: [   10/   39]    Loss 0.551542    Top1 91.171875    Top5 99.687500    
2018-10-27 01:04:51,120 - Test: [   20/   39]    Loss 0.552266    Top1 91.425781    Top5 99.589844    
2018-10-27 01:04:51,222 - Test: [   30/   39]    Loss 0.545957    Top1 91.406250    Top5 99.674479    
2018-10-27 01:04:51,315 - Test: [   40/   39]    Loss 0.542592    Top1 91.460000    Top5 99.660000    
2018-10-27 01:04:51,343 - ==> Top1: 91.460    Top5: 99.660    Loss: 0.543

2018-10-27 01:04:51,344 - Testing sensitivity of module.layer2.1.conv1.weight [35.0% sparsity]
2018-10-27 01:04:51,347 - --- test ---------------------
2018-10-27 01:04:51,348 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:51,798 - Test: [   10/   39]    Loss 0.559237    Top1 90.976562    Top5 99.609375    
2018-10-27 01:04:51,903 - Test: [   20/   39]    Loss 0.558006    Top1 91.328125    Top5 99.570312    
2018-10-27 01:04:52,004 - Test: [   30/   39]    Loss 0.550490    Top1 91.380208    Top5 99.648438    
2018-10-27 01:04:52,097 - Test: [   40/   39]    Loss 0.545640    Top1 91.430000    Top5 99.630000    
2018-10-27 01:04:52,125 - ==> Top1: 91.430    Top5: 99.630    Loss: 0.546

2018-10-27 01:04:52,126 - Testing sensitivity of module.layer2.1.conv1.weight [40.0% sparsity]
2018-10-27 01:04:52,129 - --- test ---------------------
2018-10-27 01:04:52,129 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:52,555 - Test: [   10/   39]    Loss 0.560126    Top1 91.093750    Top5 99.648438    
2018-10-27 01:04:52,659 - Test: [   20/   39]    Loss 0.559030    Top1 91.269531    Top5 99.589844    
2018-10-27 01:04:52,760 - Test: [   30/   39]    Loss 0.548565    Top1 91.328125    Top5 99.661458    
2018-10-27 01:04:52,853 - Test: [   40/   39]    Loss 0.543846    Top1 91.320000    Top5 99.640000    
2018-10-27 01:04:52,882 - ==> Top1: 91.320    Top5: 99.640    Loss: 0.544

2018-10-27 01:04:52,882 - Testing sensitivity of module.layer2.1.conv1.weight [45.0% sparsity]
2018-10-27 01:04:52,884 - --- test ---------------------
2018-10-27 01:04:52,884 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:53,319 - Test: [   10/   39]    Loss 0.555973    Top1 91.250000    Top5 99.609375    
2018-10-27 01:04:53,424 - Test: [   20/   39]    Loss 0.557251    Top1 91.367188    Top5 99.570312    
2018-10-27 01:04:53,525 - Test: [   30/   39]    Loss 0.545773    Top1 91.419271    Top5 99.648438    
2018-10-27 01:04:53,618 - Test: [   40/   39]    Loss 0.540295    Top1 91.430000    Top5 99.630000    
2018-10-27 01:04:53,648 - ==> Top1: 91.430    Top5: 99.630    Loss: 0.540

2018-10-27 01:04:53,649 - Testing sensitivity of module.layer2.1.conv1.weight [50.0% sparsity]
2018-10-27 01:04:53,653 - --- test ---------------------
2018-10-27 01:04:53,653 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:54,119 - Test: [   10/   39]    Loss 0.558155    Top1 91.210938    Top5 99.648438    
2018-10-27 01:04:54,226 - Test: [   20/   39]    Loss 0.559367    Top1 91.289062    Top5 99.589844    
2018-10-27 01:04:54,330 - Test: [   30/   39]    Loss 0.547818    Top1 91.302083    Top5 99.648438    
2018-10-27 01:04:54,423 - Test: [   40/   39]    Loss 0.542664    Top1 91.320000    Top5 99.640000    
2018-10-27 01:04:54,450 - ==> Top1: 91.320    Top5: 99.640    Loss: 0.543

2018-10-27 01:04:54,451 - Testing sensitivity of module.layer2.1.conv1.weight [55.0% sparsity]
2018-10-27 01:04:54,454 - --- test ---------------------
2018-10-27 01:04:54,454 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:54,887 - Test: [   10/   39]    Loss 0.560773    Top1 91.015625    Top5 99.687500    
2018-10-27 01:04:54,992 - Test: [   20/   39]    Loss 0.563788    Top1 91.210938    Top5 99.589844    
2018-10-27 01:04:55,096 - Test: [   30/   39]    Loss 0.554671    Top1 91.184896    Top5 99.674479    
2018-10-27 01:04:55,190 - Test: [   40/   39]    Loss 0.546178    Top1 91.160000    Top5 99.650000    
2018-10-27 01:04:55,218 - ==> Top1: 91.160    Top5: 99.650    Loss: 0.546

2018-10-27 01:04:55,218 - Testing sensitivity of module.layer2.1.conv1.weight [60.0% sparsity]
2018-10-27 01:04:55,222 - --- test ---------------------
2018-10-27 01:04:55,222 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:55,670 - Test: [   10/   39]    Loss 0.565141    Top1 90.703125    Top5 99.570312    
2018-10-27 01:04:55,774 - Test: [   20/   39]    Loss 0.561665    Top1 90.859375    Top5 99.531250    
2018-10-27 01:04:55,875 - Test: [   30/   39]    Loss 0.554676    Top1 90.846354    Top5 99.622396    
2018-10-27 01:04:55,967 - Test: [   40/   39]    Loss 0.550329    Top1 90.950000    Top5 99.620000    
2018-10-27 01:04:55,995 - ==> Top1: 90.950    Top5: 99.620    Loss: 0.550

2018-10-27 01:04:55,996 - Testing sensitivity of module.layer2.1.conv1.weight [65.0% sparsity]
2018-10-27 01:04:55,999 - --- test ---------------------
2018-10-27 01:04:55,999 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:56,424 - Test: [   10/   39]    Loss 0.578902    Top1 90.820312    Top5 99.453125    
2018-10-27 01:04:56,529 - Test: [   20/   39]    Loss 0.571728    Top1 90.703125    Top5 99.492188    
2018-10-27 01:04:56,629 - Test: [   30/   39]    Loss 0.566707    Top1 90.716146    Top5 99.609375    
2018-10-27 01:04:56,722 - Test: [   40/   39]    Loss 0.561989    Top1 90.790000    Top5 99.580000    
2018-10-27 01:04:56,750 - ==> Top1: 90.790    Top5: 99.580    Loss: 0.562

2018-10-27 01:04:56,750 - Testing sensitivity of module.layer2.1.conv1.weight [70.0% sparsity]
2018-10-27 01:04:56,753 - --- test ---------------------
2018-10-27 01:04:56,754 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:57,187 - Test: [   10/   39]    Loss 0.589827    Top1 90.664062    Top5 99.453125    
2018-10-27 01:04:57,290 - Test: [   20/   39]    Loss 0.581319    Top1 90.820312    Top5 99.433594    
2018-10-27 01:04:57,391 - Test: [   30/   39]    Loss 0.575314    Top1 90.729167    Top5 99.544271    
2018-10-27 01:04:57,484 - Test: [   40/   39]    Loss 0.571664    Top1 90.790000    Top5 99.550000    
2018-10-27 01:04:57,512 - ==> Top1: 90.790    Top5: 99.550    Loss: 0.572

2018-10-27 01:04:57,513 - Testing sensitivity of module.layer2.1.conv1.weight [75.0% sparsity]
2018-10-27 01:04:57,516 - --- test ---------------------
2018-10-27 01:04:57,516 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:57,949 - Test: [   10/   39]    Loss 0.615057    Top1 89.843750    Top5 99.375000    
2018-10-27 01:04:58,054 - Test: [   20/   39]    Loss 0.603290    Top1 90.273438    Top5 99.414062    
2018-10-27 01:04:58,154 - Test: [   30/   39]    Loss 0.608114    Top1 90.221354    Top5 99.518229    
2018-10-27 01:04:58,247 - Test: [   40/   39]    Loss 0.602812    Top1 90.290000    Top5 99.540000    
2018-10-27 01:04:58,274 - ==> Top1: 90.290    Top5: 99.540    Loss: 0.603

2018-10-27 01:04:58,275 - Testing sensitivity of module.layer2.1.conv1.weight [80.0% sparsity]
2018-10-27 01:04:58,277 - --- test ---------------------
2018-10-27 01:04:58,278 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:58,722 - Test: [   10/   39]    Loss 0.635006    Top1 89.843750    Top5 99.335938    
2018-10-27 01:04:58,826 - Test: [   20/   39]    Loss 0.617098    Top1 90.039062    Top5 99.296875    
2018-10-27 01:04:58,927 - Test: [   30/   39]    Loss 0.629224    Top1 89.895833    Top5 99.401042    
2018-10-27 01:04:59,020 - Test: [   40/   39]    Loss 0.626951    Top1 89.980000    Top5 99.460000    
2018-10-27 01:04:59,048 - ==> Top1: 89.980    Top5: 99.460    Loss: 0.627

2018-10-27 01:04:59,049 - Testing sensitivity of module.layer2.1.conv1.weight [85.0% sparsity]
2018-10-27 01:04:59,052 - --- test ---------------------
2018-10-27 01:04:59,052 - 10000 samples (256 per mini-batch)
2018-10-27 01:04:59,498 - Test: [   10/   39]    Loss 0.711432    Top1 88.632812    Top5 99.140625    
2018-10-27 01:04:59,602 - Test: [   20/   39]    Loss 0.690893    Top1 88.867188    Top5 99.082031    
2018-10-27 01:04:59,703 - Test: [   30/   39]    Loss 0.709587    Top1 88.828125    Top5 99.231771    
2018-10-27 01:04:59,796 - Test: [   40/   39]    Loss 0.699654    Top1 88.950000    Top5 99.330000    
2018-10-27 01:04:59,823 - ==> Top1: 88.950    Top5: 99.330    Loss: 0.700

2018-10-27 01:04:59,824 - Testing sensitivity of module.layer2.1.conv1.weight [90.0% sparsity]
2018-10-27 01:04:59,827 - --- test ---------------------
2018-10-27 01:04:59,827 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:00,257 - Test: [   10/   39]    Loss 0.804770    Top1 87.226562    Top5 99.218750    
2018-10-27 01:05:00,361 - Test: [   20/   39]    Loss 0.793024    Top1 87.207031    Top5 99.042969    
2018-10-27 01:05:00,462 - Test: [   30/   39]    Loss 0.810449    Top1 86.966146    Top5 99.192708    
2018-10-27 01:05:00,555 - Test: [   40/   39]    Loss 0.797659    Top1 87.190000    Top5 99.260000    
2018-10-27 01:05:00,583 - ==> Top1: 87.190    Top5: 99.260    Loss: 0.798

2018-10-27 01:05:00,597 - Testing sensitivity of module.layer2.1.conv2.weight [0.0% sparsity]
2018-10-27 01:05:00,600 - --- test ---------------------
2018-10-27 01:05:00,600 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:01,027 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:05:01,131 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:05:01,231 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:05:01,322 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:05:01,350 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:05:01,351 - Testing sensitivity of module.layer2.1.conv2.weight [5.0% sparsity]
2018-10-27 01:05:01,353 - --- test ---------------------
2018-10-27 01:05:01,354 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:01,791 - Test: [   10/   39]    Loss 0.551924    Top1 91.328125    Top5 99.687500    
2018-10-27 01:05:01,894 - Test: [   20/   39]    Loss 0.553147    Top1 91.542969    Top5 99.589844    
2018-10-27 01:05:01,994 - Test: [   30/   39]    Loss 0.546506    Top1 91.523438    Top5 99.674479    
2018-10-27 01:05:02,085 - Test: [   40/   39]    Loss 0.541649    Top1 91.510000    Top5 99.650000    
2018-10-27 01:05:02,112 - ==> Top1: 91.510    Top5: 99.650    Loss: 0.542

2018-10-27 01:05:02,113 - Testing sensitivity of module.layer2.1.conv2.weight [10.0% sparsity]
2018-10-27 01:05:02,116 - --- test ---------------------
2018-10-27 01:05:02,116 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:02,672 - Test: [   10/   39]    Loss 0.552469    Top1 91.289062    Top5 99.687500    
2018-10-27 01:05:02,775 - Test: [   20/   39]    Loss 0.553523    Top1 91.523438    Top5 99.570312    
2018-10-27 01:05:02,876 - Test: [   30/   39]    Loss 0.546829    Top1 91.497396    Top5 99.661458    
2018-10-27 01:05:02,968 - Test: [   40/   39]    Loss 0.542204    Top1 91.510000    Top5 99.640000    
2018-10-27 01:05:02,998 - ==> Top1: 91.510    Top5: 99.640    Loss: 0.542

2018-10-27 01:05:02,998 - Testing sensitivity of module.layer2.1.conv2.weight [15.0% sparsity]
2018-10-27 01:05:03,001 - --- test ---------------------
2018-10-27 01:05:03,001 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:03,519 - Test: [   10/   39]    Loss 0.552397    Top1 91.328125    Top5 99.687500    
2018-10-27 01:05:03,623 - Test: [   20/   39]    Loss 0.553816    Top1 91.523438    Top5 99.570312    
2018-10-27 01:05:03,723 - Test: [   30/   39]    Loss 0.547543    Top1 91.536458    Top5 99.661458    
2018-10-27 01:05:03,815 - Test: [   40/   39]    Loss 0.543103    Top1 91.530000    Top5 99.650000    
2018-10-27 01:05:03,847 - ==> Top1: 91.530    Top5: 99.650    Loss: 0.543

2018-10-27 01:05:03,848 - Testing sensitivity of module.layer2.1.conv2.weight [20.0% sparsity]
2018-10-27 01:05:03,850 - --- test ---------------------
2018-10-27 01:05:03,851 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:04,288 - Test: [   10/   39]    Loss 0.552648    Top1 91.367188    Top5 99.687500    
2018-10-27 01:05:04,391 - Test: [   20/   39]    Loss 0.553531    Top1 91.582031    Top5 99.570312    
2018-10-27 01:05:04,491 - Test: [   30/   39]    Loss 0.547338    Top1 91.588542    Top5 99.661458    
2018-10-27 01:05:04,583 - Test: [   40/   39]    Loss 0.542736    Top1 91.580000    Top5 99.650000    
2018-10-27 01:05:04,611 - ==> Top1: 91.580    Top5: 99.650    Loss: 0.543

2018-10-27 01:05:04,612 - Testing sensitivity of module.layer2.1.conv2.weight [25.0% sparsity]
2018-10-27 01:05:04,616 - --- test ---------------------
2018-10-27 01:05:04,616 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:05,056 - Test: [   10/   39]    Loss 0.553742    Top1 91.406250    Top5 99.687500    
2018-10-27 01:05:05,159 - Test: [   20/   39]    Loss 0.554451    Top1 91.601562    Top5 99.589844    
2018-10-27 01:05:05,259 - Test: [   30/   39]    Loss 0.548039    Top1 91.523438    Top5 99.674479    
2018-10-27 01:05:05,351 - Test: [   40/   39]    Loss 0.543056    Top1 91.540000    Top5 99.660000    
2018-10-27 01:05:05,389 - ==> Top1: 91.540    Top5: 99.660    Loss: 0.543

2018-10-27 01:05:05,389 - Testing sensitivity of module.layer2.1.conv2.weight [30.0% sparsity]
2018-10-27 01:05:05,392 - --- test ---------------------
2018-10-27 01:05:05,393 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:05,827 - Test: [   10/   39]    Loss 0.552361    Top1 91.406250    Top5 99.687500    
2018-10-27 01:05:05,930 - Test: [   20/   39]    Loss 0.552875    Top1 91.503906    Top5 99.589844    
2018-10-27 01:05:06,030 - Test: [   30/   39]    Loss 0.547156    Top1 91.484375    Top5 99.661458    
2018-10-27 01:05:06,123 - Test: [   40/   39]    Loss 0.542916    Top1 91.510000    Top5 99.650000    
2018-10-27 01:05:06,152 - ==> Top1: 91.510    Top5: 99.650    Loss: 0.543

2018-10-27 01:05:06,152 - Testing sensitivity of module.layer2.1.conv2.weight [35.0% sparsity]
2018-10-27 01:05:06,156 - --- test ---------------------
2018-10-27 01:05:06,156 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:06,600 - Test: [   10/   39]    Loss 0.554758    Top1 91.328125    Top5 99.648438    
2018-10-27 01:05:06,703 - Test: [   20/   39]    Loss 0.557100    Top1 91.503906    Top5 99.531250    
2018-10-27 01:05:06,803 - Test: [   30/   39]    Loss 0.552220    Top1 91.432292    Top5 99.609375    
2018-10-27 01:05:06,895 - Test: [   40/   39]    Loss 0.548341    Top1 91.470000    Top5 99.610000    
2018-10-27 01:05:06,924 - ==> Top1: 91.470    Top5: 99.610    Loss: 0.548

2018-10-27 01:05:06,924 - Testing sensitivity of module.layer2.1.conv2.weight [40.0% sparsity]
2018-10-27 01:05:06,927 - --- test ---------------------
2018-10-27 01:05:06,927 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:07,367 - Test: [   10/   39]    Loss 0.554759    Top1 91.328125    Top5 99.687500    
2018-10-27 01:05:07,470 - Test: [   20/   39]    Loss 0.554652    Top1 91.464844    Top5 99.570312    
2018-10-27 01:05:07,570 - Test: [   30/   39]    Loss 0.551269    Top1 91.432292    Top5 99.635417    
2018-10-27 01:05:07,662 - Test: [   40/   39]    Loss 0.547401    Top1 91.480000    Top5 99.630000    
2018-10-27 01:05:07,691 - ==> Top1: 91.480    Top5: 99.630    Loss: 0.547

2018-10-27 01:05:07,691 - Testing sensitivity of module.layer2.1.conv2.weight [45.0% sparsity]
2018-10-27 01:05:07,693 - --- test ---------------------
2018-10-27 01:05:07,694 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:08,123 - Test: [   10/   39]    Loss 0.556479    Top1 91.171875    Top5 99.570312    
2018-10-27 01:05:08,226 - Test: [   20/   39]    Loss 0.557446    Top1 91.406250    Top5 99.511719    
2018-10-27 01:05:08,326 - Test: [   30/   39]    Loss 0.553348    Top1 91.367188    Top5 99.596354    
2018-10-27 01:05:08,418 - Test: [   40/   39]    Loss 0.549350    Top1 91.420000    Top5 99.600000    
2018-10-27 01:05:08,448 - ==> Top1: 91.420    Top5: 99.600    Loss: 0.549

2018-10-27 01:05:08,448 - Testing sensitivity of module.layer2.1.conv2.weight [50.0% sparsity]
2018-10-27 01:05:08,451 - --- test ---------------------
2018-10-27 01:05:08,452 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:08,888 - Test: [   10/   39]    Loss 0.552030    Top1 91.132812    Top5 99.531250    
2018-10-27 01:05:08,992 - Test: [   20/   39]    Loss 0.555630    Top1 91.328125    Top5 99.472656    
2018-10-27 01:05:09,092 - Test: [   30/   39]    Loss 0.552179    Top1 91.315104    Top5 99.570312    
2018-10-27 01:05:09,184 - Test: [   40/   39]    Loss 0.549288    Top1 91.370000    Top5 99.580000    
2018-10-27 01:05:09,211 - ==> Top1: 91.370    Top5: 99.580    Loss: 0.549

2018-10-27 01:05:09,212 - Testing sensitivity of module.layer2.1.conv2.weight [55.0% sparsity]
2018-10-27 01:05:09,215 - --- test ---------------------
2018-10-27 01:05:09,216 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:09,644 - Test: [   10/   39]    Loss 0.553594    Top1 91.093750    Top5 99.492188    
2018-10-27 01:05:09,747 - Test: [   20/   39]    Loss 0.560058    Top1 91.230469    Top5 99.472656    
2018-10-27 01:05:09,847 - Test: [   30/   39]    Loss 0.560669    Top1 91.197917    Top5 99.557292    
2018-10-27 01:05:09,939 - Test: [   40/   39]    Loss 0.557693    Top1 91.230000    Top5 99.570000    
2018-10-27 01:05:09,967 - ==> Top1: 91.230    Top5: 99.570    Loss: 0.558

2018-10-27 01:05:09,967 - Testing sensitivity of module.layer2.1.conv2.weight [60.0% sparsity]
2018-10-27 01:05:09,970 - --- test ---------------------
2018-10-27 01:05:09,971 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:10,434 - Test: [   10/   39]    Loss 0.558283    Top1 91.093750    Top5 99.531250    
2018-10-27 01:05:10,537 - Test: [   20/   39]    Loss 0.565947    Top1 91.171875    Top5 99.492188    
2018-10-27 01:05:10,637 - Test: [   30/   39]    Loss 0.568901    Top1 91.145833    Top5 99.557292    
2018-10-27 01:05:10,729 - Test: [   40/   39]    Loss 0.565118    Top1 91.190000    Top5 99.570000    
2018-10-27 01:05:10,758 - ==> Top1: 91.190    Top5: 99.570    Loss: 0.565

2018-10-27 01:05:10,759 - Testing sensitivity of module.layer2.1.conv2.weight [65.0% sparsity]
2018-10-27 01:05:10,762 - --- test ---------------------
2018-10-27 01:05:10,763 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:11,197 - Test: [   10/   39]    Loss 0.558387    Top1 90.859375    Top5 99.492188    
2018-10-27 01:05:11,300 - Test: [   20/   39]    Loss 0.565639    Top1 91.074219    Top5 99.472656    
2018-10-27 01:05:11,400 - Test: [   30/   39]    Loss 0.572070    Top1 91.015625    Top5 99.557292    
2018-10-27 01:05:11,492 - Test: [   40/   39]    Loss 0.569100    Top1 91.180000    Top5 99.550000    
2018-10-27 01:05:11,520 - ==> Top1: 91.180    Top5: 99.550    Loss: 0.569

2018-10-27 01:05:11,520 - Testing sensitivity of module.layer2.1.conv2.weight [70.0% sparsity]
2018-10-27 01:05:11,523 - --- test ---------------------
2018-10-27 01:05:11,524 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:11,959 - Test: [   10/   39]    Loss 0.582286    Top1 90.742188    Top5 99.453125    
2018-10-27 01:05:12,062 - Test: [   20/   39]    Loss 0.585609    Top1 91.035156    Top5 99.414062    
2018-10-27 01:05:12,162 - Test: [   30/   39]    Loss 0.595570    Top1 90.794271    Top5 99.505208    
2018-10-27 01:05:12,255 - Test: [   40/   39]    Loss 0.593047    Top1 90.900000    Top5 99.500000    
2018-10-27 01:05:12,283 - ==> Top1: 90.900    Top5: 99.500    Loss: 0.593

2018-10-27 01:05:12,284 - Testing sensitivity of module.layer2.1.conv2.weight [75.0% sparsity]
2018-10-27 01:05:12,286 - --- test ---------------------
2018-10-27 01:05:12,286 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:12,747 - Test: [   10/   39]    Loss 0.606271    Top1 90.468750    Top5 99.453125    
2018-10-27 01:05:12,850 - Test: [   20/   39]    Loss 0.609016    Top1 90.546875    Top5 99.394531    
2018-10-27 01:05:12,950 - Test: [   30/   39]    Loss 0.620447    Top1 90.299479    Top5 99.479167    
2018-10-27 01:05:13,042 - Test: [   40/   39]    Loss 0.616616    Top1 90.500000    Top5 99.470000    
2018-10-27 01:05:13,070 - ==> Top1: 90.500    Top5: 99.470    Loss: 0.617

2018-10-27 01:05:13,071 - Testing sensitivity of module.layer2.1.conv2.weight [80.0% sparsity]
2018-10-27 01:05:13,074 - --- test ---------------------
2018-10-27 01:05:13,075 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:13,510 - Test: [   10/   39]    Loss 0.637884    Top1 89.960938    Top5 99.492188    
2018-10-27 01:05:13,614 - Test: [   20/   39]    Loss 0.639164    Top1 90.195312    Top5 99.316406    
2018-10-27 01:05:13,714 - Test: [   30/   39]    Loss 0.653142    Top1 89.895833    Top5 99.427083    
2018-10-27 01:05:13,806 - Test: [   40/   39]    Loss 0.646147    Top1 90.030000    Top5 99.440000    
2018-10-27 01:05:13,834 - ==> Top1: 90.030    Top5: 99.440    Loss: 0.646

2018-10-27 01:05:13,835 - Testing sensitivity of module.layer2.1.conv2.weight [85.0% sparsity]
2018-10-27 01:05:13,838 - --- test ---------------------
2018-10-27 01:05:13,838 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:14,296 - Test: [   10/   39]    Loss 0.665832    Top1 89.179688    Top5 99.453125    
2018-10-27 01:05:14,400 - Test: [   20/   39]    Loss 0.668406    Top1 89.316406    Top5 99.238281    
2018-10-27 01:05:14,509 - Test: [   30/   39]    Loss 0.684958    Top1 89.114583    Top5 99.375000    
2018-10-27 01:05:14,609 - Test: [   40/   39]    Loss 0.671393    Top1 89.310000    Top5 99.390000    
2018-10-27 01:05:14,638 - ==> Top1: 89.310    Top5: 99.390    Loss: 0.671

2018-10-27 01:05:14,638 - Testing sensitivity of module.layer2.1.conv2.weight [90.0% sparsity]
2018-10-27 01:05:14,641 - --- test ---------------------
2018-10-27 01:05:14,642 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:15,095 - Test: [   10/   39]    Loss 0.760862    Top1 87.109375    Top5 99.414062    
2018-10-27 01:05:15,199 - Test: [   20/   39]    Loss 0.758841    Top1 87.500000    Top5 99.121094    
2018-10-27 01:05:15,299 - Test: [   30/   39]    Loss 0.787862    Top1 87.343750    Top5 99.244792    
2018-10-27 01:05:15,391 - Test: [   40/   39]    Loss 0.766027    Top1 87.590000    Top5 99.290000    
2018-10-27 01:05:15,419 - ==> Top1: 87.590    Top5: 99.290    Loss: 0.766

2018-10-27 01:05:15,429 - Testing sensitivity of module.layer2.2.conv1.weight [0.0% sparsity]
2018-10-27 01:05:15,433 - --- test ---------------------
2018-10-27 01:05:15,433 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:15,861 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:05:15,966 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:05:16,066 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:05:16,163 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:05:16,192 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:05:16,192 - Testing sensitivity of module.layer2.2.conv1.weight [5.0% sparsity]
2018-10-27 01:05:16,195 - --- test ---------------------
2018-10-27 01:05:16,195 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:16,662 - Test: [   10/   39]    Loss 0.551633    Top1 91.328125    Top5 99.687500    
2018-10-27 01:05:16,767 - Test: [   20/   39]    Loss 0.552805    Top1 91.562500    Top5 99.570312    
2018-10-27 01:05:16,869 - Test: [   30/   39]    Loss 0.546227    Top1 91.536458    Top5 99.661458    
2018-10-27 01:05:16,962 - Test: [   40/   39]    Loss 0.541477    Top1 91.530000    Top5 99.640000    
2018-10-27 01:05:17,003 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.541

2018-10-27 01:05:17,004 - Testing sensitivity of module.layer2.2.conv1.weight [10.0% sparsity]
2018-10-27 01:05:17,007 - --- test ---------------------
2018-10-27 01:05:17,007 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:17,474 - Test: [   10/   39]    Loss 0.551266    Top1 91.367188    Top5 99.687500    
2018-10-27 01:05:17,584 - Test: [   20/   39]    Loss 0.552686    Top1 91.542969    Top5 99.570312    
2018-10-27 01:05:17,688 - Test: [   30/   39]    Loss 0.546066    Top1 91.549479    Top5 99.661458    
2018-10-27 01:05:17,786 - Test: [   40/   39]    Loss 0.541332    Top1 91.530000    Top5 99.640000    
2018-10-27 01:05:17,814 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.541

2018-10-27 01:05:17,814 - Testing sensitivity of module.layer2.2.conv1.weight [15.0% sparsity]
2018-10-27 01:05:17,818 - --- test ---------------------
2018-10-27 01:05:17,818 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:18,304 - Test: [   10/   39]    Loss 0.551175    Top1 91.367188    Top5 99.687500    
2018-10-27 01:05:18,416 - Test: [   20/   39]    Loss 0.552010    Top1 91.562500    Top5 99.570312    
2018-10-27 01:05:18,517 - Test: [   30/   39]    Loss 0.545812    Top1 91.510417    Top5 99.661458    
2018-10-27 01:05:18,611 - Test: [   40/   39]    Loss 0.541173    Top1 91.530000    Top5 99.640000    
2018-10-27 01:05:18,638 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.541

2018-10-27 01:05:18,639 - Testing sensitivity of module.layer2.2.conv1.weight [20.0% sparsity]
2018-10-27 01:05:18,641 - --- test ---------------------
2018-10-27 01:05:18,642 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:19,109 - Test: [   10/   39]    Loss 0.553906    Top1 91.289062    Top5 99.687500    
2018-10-27 01:05:19,215 - Test: [   20/   39]    Loss 0.554220    Top1 91.562500    Top5 99.570312    
2018-10-27 01:05:19,321 - Test: [   30/   39]    Loss 0.547294    Top1 91.549479    Top5 99.661458    
2018-10-27 01:05:19,416 - Test: [   40/   39]    Loss 0.542524    Top1 91.500000    Top5 99.640000    
2018-10-27 01:05:19,443 - ==> Top1: 91.500    Top5: 99.640    Loss: 0.543

2018-10-27 01:05:19,444 - Testing sensitivity of module.layer2.2.conv1.weight [25.0% sparsity]
2018-10-27 01:05:19,447 - --- test ---------------------
2018-10-27 01:05:19,447 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:19,914 - Test: [   10/   39]    Loss 0.551851    Top1 91.289062    Top5 99.687500    
2018-10-27 01:05:20,025 - Test: [   20/   39]    Loss 0.552398    Top1 91.445312    Top5 99.570312    
2018-10-27 01:05:20,134 - Test: [   30/   39]    Loss 0.547740    Top1 91.445312    Top5 99.661458    
2018-10-27 01:05:20,226 - Test: [   40/   39]    Loss 0.543719    Top1 91.420000    Top5 99.640000    
2018-10-27 01:05:20,259 - ==> Top1: 91.420    Top5: 99.640    Loss: 0.544

2018-10-27 01:05:20,259 - Testing sensitivity of module.layer2.2.conv1.weight [30.0% sparsity]
2018-10-27 01:05:20,261 - --- test ---------------------
2018-10-27 01:05:20,262 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:20,731 - Test: [   10/   39]    Loss 0.554501    Top1 91.015625    Top5 99.687500    
2018-10-27 01:05:20,839 - Test: [   20/   39]    Loss 0.554632    Top1 91.386719    Top5 99.570312    
2018-10-27 01:05:20,945 - Test: [   30/   39]    Loss 0.549487    Top1 91.419271    Top5 99.661458    
2018-10-27 01:05:21,038 - Test: [   40/   39]    Loss 0.545622    Top1 91.380000    Top5 99.640000    
2018-10-27 01:05:21,067 - ==> Top1: 91.380    Top5: 99.640    Loss: 0.546

2018-10-27 01:05:21,067 - Testing sensitivity of module.layer2.2.conv1.weight [35.0% sparsity]
2018-10-27 01:05:21,070 - --- test ---------------------
2018-10-27 01:05:21,071 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:21,527 - Test: [   10/   39]    Loss 0.554634    Top1 91.132812    Top5 99.726562    
2018-10-27 01:05:21,637 - Test: [   20/   39]    Loss 0.553302    Top1 91.445312    Top5 99.570312    
2018-10-27 01:05:21,746 - Test: [   30/   39]    Loss 0.547386    Top1 91.445312    Top5 99.648438    
2018-10-27 01:05:21,840 - Test: [   40/   39]    Loss 0.542985    Top1 91.450000    Top5 99.640000    
2018-10-27 01:05:21,868 - ==> Top1: 91.450    Top5: 99.640    Loss: 0.543

2018-10-27 01:05:21,869 - Testing sensitivity of module.layer2.2.conv1.weight [40.0% sparsity]
2018-10-27 01:05:21,872 - --- test ---------------------
2018-10-27 01:05:21,873 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:22,344 - Test: [   10/   39]    Loss 0.558110    Top1 91.093750    Top5 99.648438    
2018-10-27 01:05:22,447 - Test: [   20/   39]    Loss 0.554755    Top1 91.367188    Top5 99.570312    
2018-10-27 01:05:22,549 - Test: [   30/   39]    Loss 0.547779    Top1 91.419271    Top5 99.648438    
2018-10-27 01:05:22,646 - Test: [   40/   39]    Loss 0.543345    Top1 91.390000    Top5 99.650000    
2018-10-27 01:05:22,675 - ==> Top1: 91.390    Top5: 99.650    Loss: 0.543

2018-10-27 01:05:22,676 - Testing sensitivity of module.layer2.2.conv1.weight [45.0% sparsity]
2018-10-27 01:05:22,679 - --- test ---------------------
2018-10-27 01:05:22,679 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:23,122 - Test: [   10/   39]    Loss 0.558889    Top1 91.054688    Top5 99.648438    
2018-10-27 01:05:23,226 - Test: [   20/   39]    Loss 0.560165    Top1 91.191406    Top5 99.492188    
2018-10-27 01:05:23,326 - Test: [   30/   39]    Loss 0.555359    Top1 91.236979    Top5 99.609375    
2018-10-27 01:05:23,419 - Test: [   40/   39]    Loss 0.548283    Top1 91.260000    Top5 99.610000    
2018-10-27 01:05:23,447 - ==> Top1: 91.260    Top5: 99.610    Loss: 0.548

2018-10-27 01:05:23,447 - Testing sensitivity of module.layer2.2.conv1.weight [50.0% sparsity]
2018-10-27 01:05:23,450 - --- test ---------------------
2018-10-27 01:05:23,451 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:23,903 - Test: [   10/   39]    Loss 0.573790    Top1 91.054688    Top5 99.570312    
2018-10-27 01:05:24,006 - Test: [   20/   39]    Loss 0.569920    Top1 91.289062    Top5 99.492188    
2018-10-27 01:05:24,107 - Test: [   30/   39]    Loss 0.563417    Top1 91.197917    Top5 99.570312    
2018-10-27 01:05:24,204 - Test: [   40/   39]    Loss 0.559234    Top1 91.170000    Top5 99.580000    
2018-10-27 01:05:24,244 - ==> Top1: 91.170    Top5: 99.580    Loss: 0.559

2018-10-27 01:05:24,244 - Testing sensitivity of module.layer2.2.conv1.weight [55.0% sparsity]
2018-10-27 01:05:24,251 - --- test ---------------------
2018-10-27 01:05:24,251 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:24,710 - Test: [   10/   39]    Loss 0.599642    Top1 90.703125    Top5 99.570312    
2018-10-27 01:05:24,816 - Test: [   20/   39]    Loss 0.587108    Top1 90.917969    Top5 99.531250    
2018-10-27 01:05:24,916 - Test: [   30/   39]    Loss 0.575755    Top1 90.833333    Top5 99.609375    
2018-10-27 01:05:25,009 - Test: [   40/   39]    Loss 0.573314    Top1 90.800000    Top5 99.610000    
2018-10-27 01:05:25,035 - ==> Top1: 90.800    Top5: 99.610    Loss: 0.573

2018-10-27 01:05:25,036 - Testing sensitivity of module.layer2.2.conv1.weight [60.0% sparsity]
2018-10-27 01:05:25,039 - --- test ---------------------
2018-10-27 01:05:25,039 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:25,492 - Test: [   10/   39]    Loss 0.627906    Top1 90.078125    Top5 99.531250    
2018-10-27 01:05:25,595 - Test: [   20/   39]    Loss 0.607756    Top1 90.332031    Top5 99.511719    
2018-10-27 01:05:25,703 - Test: [   30/   39]    Loss 0.595425    Top1 90.377604    Top5 99.583333    
2018-10-27 01:05:25,799 - Test: [   40/   39]    Loss 0.596474    Top1 90.500000    Top5 99.580000    
2018-10-27 01:05:25,827 - ==> Top1: 90.500    Top5: 99.580    Loss: 0.596

2018-10-27 01:05:25,828 - Testing sensitivity of module.layer2.2.conv1.weight [65.0% sparsity]
2018-10-27 01:05:25,831 - --- test ---------------------
2018-10-27 01:05:25,831 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:26,295 - Test: [   10/   39]    Loss 0.669285    Top1 89.062500    Top5 99.296875    
2018-10-27 01:05:26,403 - Test: [   20/   39]    Loss 0.643723    Top1 89.394531    Top5 99.394531    
2018-10-27 01:05:26,513 - Test: [   30/   39]    Loss 0.629284    Top1 89.570312    Top5 99.479167    
2018-10-27 01:05:26,606 - Test: [   40/   39]    Loss 0.626638    Top1 89.740000    Top5 99.460000    
2018-10-27 01:05:26,643 - ==> Top1: 89.740    Top5: 99.460    Loss: 0.627

2018-10-27 01:05:26,643 - Testing sensitivity of module.layer2.2.conv1.weight [70.0% sparsity]
2018-10-27 01:05:26,646 - --- test ---------------------
2018-10-27 01:05:26,646 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:27,078 - Test: [   10/   39]    Loss 0.712950    Top1 88.554688    Top5 99.218750    
2018-10-27 01:05:27,183 - Test: [   20/   39]    Loss 0.687849    Top1 88.925781    Top5 99.335938    
2018-10-27 01:05:27,283 - Test: [   30/   39]    Loss 0.670023    Top1 89.127604    Top5 99.375000    
2018-10-27 01:05:27,376 - Test: [   40/   39]    Loss 0.664835    Top1 89.310000    Top5 99.360000    
2018-10-27 01:05:27,403 - ==> Top1: 89.310    Top5: 99.360    Loss: 0.665

2018-10-27 01:05:27,404 - Testing sensitivity of module.layer2.2.conv1.weight [75.0% sparsity]
2018-10-27 01:05:27,407 - --- test ---------------------
2018-10-27 01:05:27,408 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:27,855 - Test: [   10/   39]    Loss 0.764220    Top1 87.695312    Top5 99.179688    
2018-10-27 01:05:27,960 - Test: [   20/   39]    Loss 0.740021    Top1 88.183594    Top5 99.277344    
2018-10-27 01:05:28,061 - Test: [   30/   39]    Loss 0.719262    Top1 88.359375    Top5 99.283854    
2018-10-27 01:05:28,155 - Test: [   40/   39]    Loss 0.710683    Top1 88.580000    Top5 99.290000    
2018-10-27 01:05:28,183 - ==> Top1: 88.580    Top5: 99.290    Loss: 0.711

2018-10-27 01:05:28,183 - Testing sensitivity of module.layer2.2.conv1.weight [80.0% sparsity]
2018-10-27 01:05:28,186 - --- test ---------------------
2018-10-27 01:05:28,187 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:28,656 - Test: [   10/   39]    Loss 1.051846    Top1 84.492188    Top5 98.710938    
2018-10-27 01:05:28,761 - Test: [   20/   39]    Loss 0.997680    Top1 85.039062    Top5 98.925781    
2018-10-27 01:05:28,866 - Test: [   30/   39]    Loss 0.976819    Top1 85.208333    Top5 98.867188    
2018-10-27 01:05:28,960 - Test: [   40/   39]    Loss 0.951210    Top1 85.260000    Top5 98.870000    
2018-10-27 01:05:28,988 - ==> Top1: 85.260    Top5: 98.870    Loss: 0.951

2018-10-27 01:05:28,989 - Testing sensitivity of module.layer2.2.conv1.weight [85.0% sparsity]
2018-10-27 01:05:28,992 - --- test ---------------------
2018-10-27 01:05:28,992 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:29,447 - Test: [   10/   39]    Loss 2.595777    Top1 69.179688    Top5 94.765625    
2018-10-27 01:05:29,550 - Test: [   20/   39]    Loss 2.478555    Top1 69.648438    Top5 95.507812    
2018-10-27 01:05:29,651 - Test: [   30/   39]    Loss 2.443274    Top1 69.700521    Top5 95.533854    
2018-10-27 01:05:29,744 - Test: [   40/   39]    Loss 2.422872    Top1 69.240000    Top5 95.600000    
2018-10-27 01:05:29,773 - ==> Top1: 69.240    Top5: 95.600    Loss: 2.423

2018-10-27 01:05:29,774 - Testing sensitivity of module.layer2.2.conv1.weight [90.0% sparsity]
2018-10-27 01:05:29,777 - --- test ---------------------
2018-10-27 01:05:29,778 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:30,278 - Test: [   10/   39]    Loss 8.861474    Top1 32.851562    Top5 75.625000    
2018-10-27 01:05:30,386 - Test: [   20/   39]    Loss 8.546521    Top1 34.023438    Top5 76.445312    
2018-10-27 01:05:30,492 - Test: [   30/   39]    Loss 8.570468    Top1 33.554688    Top5 76.315104    
2018-10-27 01:05:30,588 - Test: [   40/   39]    Loss 8.570459    Top1 33.410000    Top5 76.260000    
2018-10-27 01:05:30,618 - ==> Top1: 33.410    Top5: 76.260    Loss: 8.570

2018-10-27 01:05:30,633 - Testing sensitivity of module.layer2.2.conv2.weight [0.0% sparsity]
2018-10-27 01:05:30,637 - --- test ---------------------
2018-10-27 01:05:30,638 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:31,102 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:05:31,212 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:05:31,317 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:05:31,414 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:05:31,442 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:05:31,443 - Testing sensitivity of module.layer2.2.conv2.weight [5.0% sparsity]
2018-10-27 01:05:31,446 - --- test ---------------------
2018-10-27 01:05:31,446 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:31,888 - Test: [   10/   39]    Loss 0.551627    Top1 91.289062    Top5 99.687500    
2018-10-27 01:05:31,990 - Test: [   20/   39]    Loss 0.552919    Top1 91.523438    Top5 99.570312    
2018-10-27 01:05:32,090 - Test: [   30/   39]    Loss 0.546271    Top1 91.510417    Top5 99.661458    
2018-10-27 01:05:32,182 - Test: [   40/   39]    Loss 0.541524    Top1 91.530000    Top5 99.640000    
2018-10-27 01:05:32,210 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:05:32,210 - Testing sensitivity of module.layer2.2.conv2.weight [10.0% sparsity]
2018-10-27 01:05:32,213 - --- test ---------------------
2018-10-27 01:05:32,214 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:32,653 - Test: [   10/   39]    Loss 0.551668    Top1 91.250000    Top5 99.687500    
2018-10-27 01:05:32,758 - Test: [   20/   39]    Loss 0.552836    Top1 91.503906    Top5 99.570312    
2018-10-27 01:05:32,865 - Test: [   30/   39]    Loss 0.546486    Top1 91.523438    Top5 99.661458    
2018-10-27 01:05:32,966 - Test: [   40/   39]    Loss 0.541634    Top1 91.520000    Top5 99.640000    
2018-10-27 01:05:33,008 - ==> Top1: 91.520    Top5: 99.640    Loss: 0.542

2018-10-27 01:05:33,008 - Testing sensitivity of module.layer2.2.conv2.weight [15.0% sparsity]
2018-10-27 01:05:33,010 - --- test ---------------------
2018-10-27 01:05:33,011 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:33,497 - Test: [   10/   39]    Loss 0.551113    Top1 91.328125    Top5 99.687500    
2018-10-27 01:05:33,601 - Test: [   20/   39]    Loss 0.551743    Top1 91.562500    Top5 99.570312    
2018-10-27 01:05:33,701 - Test: [   30/   39]    Loss 0.545570    Top1 91.588542    Top5 99.661458    
2018-10-27 01:05:33,795 - Test: [   40/   39]    Loss 0.540723    Top1 91.560000    Top5 99.650000    
2018-10-27 01:05:33,822 - ==> Top1: 91.560    Top5: 99.650    Loss: 0.541

2018-10-27 01:05:33,823 - Testing sensitivity of module.layer2.2.conv2.weight [20.0% sparsity]
2018-10-27 01:05:33,826 - --- test ---------------------
2018-10-27 01:05:33,827 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:34,264 - Test: [   10/   39]    Loss 0.552570    Top1 91.210938    Top5 99.687500    
2018-10-27 01:05:34,371 - Test: [   20/   39]    Loss 0.553484    Top1 91.464844    Top5 99.570312    
2018-10-27 01:05:34,471 - Test: [   30/   39]    Loss 0.546849    Top1 91.432292    Top5 99.661458    
2018-10-27 01:05:34,564 - Test: [   40/   39]    Loss 0.541667    Top1 91.460000    Top5 99.640000    
2018-10-27 01:05:34,591 - ==> Top1: 91.460    Top5: 99.640    Loss: 0.542

2018-10-27 01:05:34,592 - Testing sensitivity of module.layer2.2.conv2.weight [25.0% sparsity]
2018-10-27 01:05:34,595 - --- test ---------------------
2018-10-27 01:05:34,595 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:35,038 - Test: [   10/   39]    Loss 0.554585    Top1 91.171875    Top5 99.648438    
2018-10-27 01:05:35,141 - Test: [   20/   39]    Loss 0.556000    Top1 91.464844    Top5 99.550781    
2018-10-27 01:05:35,241 - Test: [   30/   39]    Loss 0.549119    Top1 91.445312    Top5 99.648438    
2018-10-27 01:05:35,334 - Test: [   40/   39]    Loss 0.543341    Top1 91.450000    Top5 99.630000    
2018-10-27 01:05:35,367 - ==> Top1: 91.450    Top5: 99.630    Loss: 0.543

2018-10-27 01:05:35,368 - Testing sensitivity of module.layer2.2.conv2.weight [30.0% sparsity]
2018-10-27 01:05:35,370 - --- test ---------------------
2018-10-27 01:05:35,370 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:35,842 - Test: [   10/   39]    Loss 0.555022    Top1 91.250000    Top5 99.687500    
2018-10-27 01:05:35,948 - Test: [   20/   39]    Loss 0.556006    Top1 91.464844    Top5 99.570312    
2018-10-27 01:05:36,054 - Test: [   30/   39]    Loss 0.549809    Top1 91.393229    Top5 99.661458    
2018-10-27 01:05:36,148 - Test: [   40/   39]    Loss 0.543630    Top1 91.390000    Top5 99.640000    
2018-10-27 01:05:36,175 - ==> Top1: 91.390    Top5: 99.640    Loss: 0.544

2018-10-27 01:05:36,176 - Testing sensitivity of module.layer2.2.conv2.weight [35.0% sparsity]
2018-10-27 01:05:36,179 - --- test ---------------------
2018-10-27 01:05:36,179 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:36,649 - Test: [   10/   39]    Loss 0.555655    Top1 91.171875    Top5 99.687500    
2018-10-27 01:05:36,754 - Test: [   20/   39]    Loss 0.556320    Top1 91.406250    Top5 99.570312    
2018-10-27 01:05:36,855 - Test: [   30/   39]    Loss 0.550286    Top1 91.393229    Top5 99.661458    
2018-10-27 01:05:36,950 - Test: [   40/   39]    Loss 0.544566    Top1 91.390000    Top5 99.640000    
2018-10-27 01:05:36,981 - ==> Top1: 91.390    Top5: 99.640    Loss: 0.545

2018-10-27 01:05:36,981 - Testing sensitivity of module.layer2.2.conv2.weight [40.0% sparsity]
2018-10-27 01:05:36,985 - --- test ---------------------
2018-10-27 01:05:36,985 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:37,434 - Test: [   10/   39]    Loss 0.556675    Top1 91.093750    Top5 99.648438    
2018-10-27 01:05:37,541 - Test: [   20/   39]    Loss 0.557680    Top1 91.367188    Top5 99.570312    
2018-10-27 01:05:37,643 - Test: [   30/   39]    Loss 0.550786    Top1 91.367188    Top5 99.661458    
2018-10-27 01:05:37,735 - Test: [   40/   39]    Loss 0.544852    Top1 91.380000    Top5 99.650000    
2018-10-27 01:05:37,764 - ==> Top1: 91.380    Top5: 99.650    Loss: 0.545

2018-10-27 01:05:37,765 - Testing sensitivity of module.layer2.2.conv2.weight [45.0% sparsity]
2018-10-27 01:05:37,768 - --- test ---------------------
2018-10-27 01:05:37,769 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:38,232 - Test: [   10/   39]    Loss 0.559063    Top1 91.250000    Top5 99.609375    
2018-10-27 01:05:38,336 - Test: [   20/   39]    Loss 0.558076    Top1 91.406250    Top5 99.550781    
2018-10-27 01:05:38,437 - Test: [   30/   39]    Loss 0.550614    Top1 91.406250    Top5 99.648438    
2018-10-27 01:05:38,534 - Test: [   40/   39]    Loss 0.545702    Top1 91.470000    Top5 99.650000    
2018-10-27 01:05:38,561 - ==> Top1: 91.470    Top5: 99.650    Loss: 0.546

2018-10-27 01:05:38,562 - Testing sensitivity of module.layer2.2.conv2.weight [50.0% sparsity]
2018-10-27 01:05:38,565 - --- test ---------------------
2018-10-27 01:05:38,565 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:39,012 - Test: [   10/   39]    Loss 0.554129    Top1 91.054688    Top5 99.609375    
2018-10-27 01:05:39,120 - Test: [   20/   39]    Loss 0.555754    Top1 91.386719    Top5 99.531250    
2018-10-27 01:05:39,230 - Test: [   30/   39]    Loss 0.551033    Top1 91.354167    Top5 99.635417    
2018-10-27 01:05:39,330 - Test: [   40/   39]    Loss 0.545727    Top1 91.370000    Top5 99.630000    
2018-10-27 01:05:39,361 - ==> Top1: 91.370    Top5: 99.630    Loss: 0.546

2018-10-27 01:05:39,361 - Testing sensitivity of module.layer2.2.conv2.weight [55.0% sparsity]
2018-10-27 01:05:39,364 - --- test ---------------------
2018-10-27 01:05:39,365 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:39,862 - Test: [   10/   39]    Loss 0.555061    Top1 90.937500    Top5 99.570312    
2018-10-27 01:05:39,966 - Test: [   20/   39]    Loss 0.557277    Top1 91.250000    Top5 99.531250    
2018-10-27 01:05:40,068 - Test: [   30/   39]    Loss 0.552165    Top1 91.328125    Top5 99.622396    
2018-10-27 01:05:40,161 - Test: [   40/   39]    Loss 0.549916    Top1 91.320000    Top5 99.610000    
2018-10-27 01:05:40,189 - ==> Top1: 91.320    Top5: 99.610    Loss: 0.550

2018-10-27 01:05:40,190 - Testing sensitivity of module.layer2.2.conv2.weight [60.0% sparsity]
2018-10-27 01:05:40,193 - --- test ---------------------
2018-10-27 01:05:40,193 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:40,646 - Test: [   10/   39]    Loss 0.557892    Top1 90.781250    Top5 99.609375    
2018-10-27 01:05:40,749 - Test: [   20/   39]    Loss 0.560786    Top1 91.210938    Top5 99.550781    
2018-10-27 01:05:40,851 - Test: [   30/   39]    Loss 0.555161    Top1 91.328125    Top5 99.635417    
2018-10-27 01:05:40,945 - Test: [   40/   39]    Loss 0.551542    Top1 91.240000    Top5 99.610000    
2018-10-27 01:05:40,972 - ==> Top1: 91.240    Top5: 99.610    Loss: 0.552

2018-10-27 01:05:40,972 - Testing sensitivity of module.layer2.2.conv2.weight [65.0% sparsity]
2018-10-27 01:05:40,976 - --- test ---------------------
2018-10-27 01:05:40,976 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:41,440 - Test: [   10/   39]    Loss 0.565330    Top1 90.664062    Top5 99.609375    
2018-10-27 01:05:41,543 - Test: [   20/   39]    Loss 0.568317    Top1 91.093750    Top5 99.550781    
2018-10-27 01:05:41,644 - Test: [   30/   39]    Loss 0.563916    Top1 91.119792    Top5 99.622396    
2018-10-27 01:05:41,743 - Test: [   40/   39]    Loss 0.561303    Top1 91.090000    Top5 99.610000    
2018-10-27 01:05:41,771 - ==> Top1: 91.090    Top5: 99.610    Loss: 0.561

2018-10-27 01:05:41,771 - Testing sensitivity of module.layer2.2.conv2.weight [70.0% sparsity]
2018-10-27 01:05:41,774 - --- test ---------------------
2018-10-27 01:05:41,775 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:42,265 - Test: [   10/   39]    Loss 0.574653    Top1 90.742188    Top5 99.570312    
2018-10-27 01:05:42,369 - Test: [   20/   39]    Loss 0.582927    Top1 90.996094    Top5 99.492188    
2018-10-27 01:05:42,470 - Test: [   30/   39]    Loss 0.579324    Top1 90.963542    Top5 99.570312    
2018-10-27 01:05:42,563 - Test: [   40/   39]    Loss 0.576896    Top1 90.940000    Top5 99.570000    
2018-10-27 01:05:42,592 - ==> Top1: 90.940    Top5: 99.570    Loss: 0.577

2018-10-27 01:05:42,592 - Testing sensitivity of module.layer2.2.conv2.weight [75.0% sparsity]
2018-10-27 01:05:42,595 - --- test ---------------------
2018-10-27 01:05:42,596 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:43,072 - Test: [   10/   39]    Loss 0.587244    Top1 90.507812    Top5 99.492188    
2018-10-27 01:05:43,176 - Test: [   20/   39]    Loss 0.593904    Top1 90.742188    Top5 99.375000    
2018-10-27 01:05:43,277 - Test: [   30/   39]    Loss 0.592836    Top1 90.638021    Top5 99.492188    
2018-10-27 01:05:43,370 - Test: [   40/   39]    Loss 0.590429    Top1 90.600000    Top5 99.500000    
2018-10-27 01:05:43,398 - ==> Top1: 90.600    Top5: 99.500    Loss: 0.590

2018-10-27 01:05:43,398 - Testing sensitivity of module.layer2.2.conv2.weight [80.0% sparsity]
2018-10-27 01:05:43,401 - --- test ---------------------
2018-10-27 01:05:43,401 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:43,867 - Test: [   10/   39]    Loss 0.589938    Top1 90.351562    Top5 99.531250    
2018-10-27 01:05:43,971 - Test: [   20/   39]    Loss 0.601560    Top1 90.546875    Top5 99.472656    
2018-10-27 01:05:44,072 - Test: [   30/   39]    Loss 0.607662    Top1 90.338542    Top5 99.557292    
2018-10-27 01:05:44,165 - Test: [   40/   39]    Loss 0.603447    Top1 90.380000    Top5 99.560000    
2018-10-27 01:05:44,193 - ==> Top1: 90.380    Top5: 99.560    Loss: 0.603

2018-10-27 01:05:44,193 - Testing sensitivity of module.layer2.2.conv2.weight [85.0% sparsity]
2018-10-27 01:05:44,197 - --- test ---------------------
2018-10-27 01:05:44,197 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:44,657 - Test: [   10/   39]    Loss 0.643276    Top1 89.296875    Top5 99.414062    
2018-10-27 01:05:44,761 - Test: [   20/   39]    Loss 0.649369    Top1 89.628906    Top5 99.335938    
2018-10-27 01:05:44,862 - Test: [   30/   39]    Loss 0.666678    Top1 89.322917    Top5 99.466146    
2018-10-27 01:05:44,961 - Test: [   40/   39]    Loss 0.662088    Top1 89.350000    Top5 99.510000    
2018-10-27 01:05:44,990 - ==> Top1: 89.350    Top5: 99.510    Loss: 0.662

2018-10-27 01:05:44,991 - Testing sensitivity of module.layer2.2.conv2.weight [90.0% sparsity]
2018-10-27 01:05:44,994 - --- test ---------------------
2018-10-27 01:05:44,994 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:45,452 - Test: [   10/   39]    Loss 0.674089    Top1 88.789062    Top5 99.414062    
2018-10-27 01:05:45,562 - Test: [   20/   39]    Loss 0.677231    Top1 89.335938    Top5 99.296875    
2018-10-27 01:05:45,666 - Test: [   30/   39]    Loss 0.703782    Top1 88.932292    Top5 99.375000    
2018-10-27 01:05:45,763 - Test: [   40/   39]    Loss 0.702222    Top1 88.980000    Top5 99.420000    
2018-10-27 01:05:45,792 - ==> Top1: 88.980    Top5: 99.420    Loss: 0.702

2018-10-27 01:05:45,806 - Testing sensitivity of module.layer3.0.conv1.weight [0.0% sparsity]
2018-10-27 01:05:45,810 - --- test ---------------------
2018-10-27 01:05:45,810 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:46,281 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:05:46,386 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:05:46,488 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:05:46,581 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:05:46,610 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:05:46,610 - Testing sensitivity of module.layer3.0.conv1.weight [5.0% sparsity]
2018-10-27 01:05:46,613 - --- test ---------------------
2018-10-27 01:05:46,614 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:47,095 - Test: [   10/   39]    Loss 0.551546    Top1 91.328125    Top5 99.687500    
2018-10-27 01:05:47,201 - Test: [   20/   39]    Loss 0.552865    Top1 91.582031    Top5 99.570312    
2018-10-27 01:05:47,303 - Test: [   30/   39]    Loss 0.545921    Top1 91.575521    Top5 99.661458    
2018-10-27 01:05:47,396 - Test: [   40/   39]    Loss 0.541275    Top1 91.550000    Top5 99.640000    
2018-10-27 01:05:47,425 - ==> Top1: 91.550    Top5: 99.640    Loss: 0.541

2018-10-27 01:05:47,425 - Testing sensitivity of module.layer3.0.conv1.weight [10.0% sparsity]
2018-10-27 01:05:47,429 - --- test ---------------------
2018-10-27 01:05:47,429 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:47,879 - Test: [   10/   39]    Loss 0.553102    Top1 91.250000    Top5 99.687500    
2018-10-27 01:05:47,986 - Test: [   20/   39]    Loss 0.553940    Top1 91.523438    Top5 99.570312    
2018-10-27 01:05:48,092 - Test: [   30/   39]    Loss 0.546510    Top1 91.536458    Top5 99.661458    
2018-10-27 01:05:48,191 - Test: [   40/   39]    Loss 0.541828    Top1 91.520000    Top5 99.640000    
2018-10-27 01:05:48,223 - ==> Top1: 91.520    Top5: 99.640    Loss: 0.542

2018-10-27 01:05:48,223 - Testing sensitivity of module.layer3.0.conv1.weight [15.0% sparsity]
2018-10-27 01:05:48,226 - --- test ---------------------
2018-10-27 01:05:48,227 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:48,694 - Test: [   10/   39]    Loss 0.551714    Top1 91.289062    Top5 99.687500    
2018-10-27 01:05:48,802 - Test: [   20/   39]    Loss 0.552537    Top1 91.464844    Top5 99.589844    
2018-10-27 01:05:48,908 - Test: [   30/   39]    Loss 0.545096    Top1 91.510417    Top5 99.674479    
2018-10-27 01:05:49,002 - Test: [   40/   39]    Loss 0.540852    Top1 91.510000    Top5 99.660000    
2018-10-27 01:05:49,040 - ==> Top1: 91.510    Top5: 99.660    Loss: 0.541

2018-10-27 01:05:49,041 - Testing sensitivity of module.layer3.0.conv1.weight [20.0% sparsity]
2018-10-27 01:05:49,044 - --- test ---------------------
2018-10-27 01:05:49,044 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:49,497 - Test: [   10/   39]    Loss 0.556966    Top1 91.250000    Top5 99.687500    
2018-10-27 01:05:49,602 - Test: [   20/   39]    Loss 0.557672    Top1 91.503906    Top5 99.570312    
2018-10-27 01:05:49,703 - Test: [   30/   39]    Loss 0.550569    Top1 91.471354    Top5 99.661458    
2018-10-27 01:05:49,797 - Test: [   40/   39]    Loss 0.544540    Top1 91.450000    Top5 99.650000    
2018-10-27 01:05:49,825 - ==> Top1: 91.450    Top5: 99.650    Loss: 0.545

2018-10-27 01:05:49,826 - Testing sensitivity of module.layer3.0.conv1.weight [25.0% sparsity]
2018-10-27 01:05:49,828 - --- test ---------------------
2018-10-27 01:05:49,828 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:50,291 - Test: [   10/   39]    Loss 0.557253    Top1 91.250000    Top5 99.687500    
2018-10-27 01:05:50,395 - Test: [   20/   39]    Loss 0.558252    Top1 91.523438    Top5 99.589844    
2018-10-27 01:05:50,501 - Test: [   30/   39]    Loss 0.551857    Top1 91.562500    Top5 99.674479    
2018-10-27 01:05:50,597 - Test: [   40/   39]    Loss 0.544889    Top1 91.520000    Top5 99.660000    
2018-10-27 01:05:50,625 - ==> Top1: 91.520    Top5: 99.660    Loss: 0.545

2018-10-27 01:05:50,626 - Testing sensitivity of module.layer3.0.conv1.weight [30.0% sparsity]
2018-10-27 01:05:50,629 - --- test ---------------------
2018-10-27 01:05:50,629 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:51,102 - Test: [   10/   39]    Loss 0.558190    Top1 91.250000    Top5 99.648438    
2018-10-27 01:05:51,207 - Test: [   20/   39]    Loss 0.558337    Top1 91.464844    Top5 99.570312    
2018-10-27 01:05:51,309 - Test: [   30/   39]    Loss 0.552375    Top1 91.445312    Top5 99.661458    
2018-10-27 01:05:51,404 - Test: [   40/   39]    Loss 0.545405    Top1 91.500000    Top5 99.650000    
2018-10-27 01:05:51,438 - ==> Top1: 91.500    Top5: 99.650    Loss: 0.545

2018-10-27 01:05:51,438 - Testing sensitivity of module.layer3.0.conv1.weight [35.0% sparsity]
2018-10-27 01:05:51,441 - --- test ---------------------
2018-10-27 01:05:51,442 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:51,905 - Test: [   10/   39]    Loss 0.560564    Top1 91.171875    Top5 99.726562    
2018-10-27 01:05:52,012 - Test: [   20/   39]    Loss 0.557163    Top1 91.601562    Top5 99.648438    
2018-10-27 01:05:52,114 - Test: [   30/   39]    Loss 0.550425    Top1 91.471354    Top5 99.713542    
2018-10-27 01:05:52,208 - Test: [   40/   39]    Loss 0.545124    Top1 91.550000    Top5 99.690000    
2018-10-27 01:05:52,240 - ==> Top1: 91.550    Top5: 99.690    Loss: 0.545

2018-10-27 01:05:52,241 - Testing sensitivity of module.layer3.0.conv1.weight [40.0% sparsity]
2018-10-27 01:05:52,243 - --- test ---------------------
2018-10-27 01:05:52,244 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:52,697 - Test: [   10/   39]    Loss 0.562979    Top1 91.210938    Top5 99.648438    
2018-10-27 01:05:52,802 - Test: [   20/   39]    Loss 0.556441    Top1 91.425781    Top5 99.570312    
2018-10-27 01:05:52,904 - Test: [   30/   39]    Loss 0.552704    Top1 91.328125    Top5 99.661458    
2018-10-27 01:05:53,001 - Test: [   40/   39]    Loss 0.546729    Top1 91.360000    Top5 99.660000    
2018-10-27 01:05:53,036 - ==> Top1: 91.360    Top5: 99.660    Loss: 0.547

2018-10-27 01:05:53,037 - Testing sensitivity of module.layer3.0.conv1.weight [45.0% sparsity]
2018-10-27 01:05:53,040 - --- test ---------------------
2018-10-27 01:05:53,041 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:53,485 - Test: [   10/   39]    Loss 0.561275    Top1 91.054688    Top5 99.648438    
2018-10-27 01:05:53,592 - Test: [   20/   39]    Loss 0.551135    Top1 91.308594    Top5 99.589844    
2018-10-27 01:05:53,693 - Test: [   30/   39]    Loss 0.547904    Top1 91.341146    Top5 99.661458    
2018-10-27 01:05:53,787 - Test: [   40/   39]    Loss 0.543113    Top1 91.400000    Top5 99.660000    
2018-10-27 01:05:53,814 - ==> Top1: 91.400    Top5: 99.660    Loss: 0.543

2018-10-27 01:05:53,815 - Testing sensitivity of module.layer3.0.conv1.weight [50.0% sparsity]
2018-10-27 01:05:53,817 - --- test ---------------------
2018-10-27 01:05:53,818 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:54,296 - Test: [   10/   39]    Loss 0.564950    Top1 90.742188    Top5 99.531250    
2018-10-27 01:05:54,404 - Test: [   20/   39]    Loss 0.553131    Top1 91.191406    Top5 99.492188    
2018-10-27 01:05:54,506 - Test: [   30/   39]    Loss 0.548736    Top1 91.223958    Top5 99.596354    
2018-10-27 01:05:54,600 - Test: [   40/   39]    Loss 0.545149    Top1 91.190000    Top5 99.610000    
2018-10-27 01:05:54,629 - ==> Top1: 91.190    Top5: 99.610    Loss: 0.545

2018-10-27 01:05:54,630 - Testing sensitivity of module.layer3.0.conv1.weight [55.0% sparsity]
2018-10-27 01:05:54,632 - --- test ---------------------
2018-10-27 01:05:54,632 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:55,098 - Test: [   10/   39]    Loss 0.575655    Top1 90.468750    Top5 99.570312    
2018-10-27 01:05:55,202 - Test: [   20/   39]    Loss 0.569391    Top1 90.742188    Top5 99.531250    
2018-10-27 01:05:55,304 - Test: [   30/   39]    Loss 0.558210    Top1 90.872396    Top5 99.583333    
2018-10-27 01:05:55,401 - Test: [   40/   39]    Loss 0.556719    Top1 90.880000    Top5 99.600000    
2018-10-27 01:05:55,428 - ==> Top1: 90.880    Top5: 99.600    Loss: 0.557

2018-10-27 01:05:55,429 - Testing sensitivity of module.layer3.0.conv1.weight [60.0% sparsity]
2018-10-27 01:05:55,432 - --- test ---------------------
2018-10-27 01:05:55,432 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:55,898 - Test: [   10/   39]    Loss 0.588222    Top1 90.234375    Top5 99.531250    
2018-10-27 01:05:56,002 - Test: [   20/   39]    Loss 0.583878    Top1 90.410156    Top5 99.492188    
2018-10-27 01:05:56,102 - Test: [   30/   39]    Loss 0.571328    Top1 90.690104    Top5 99.570312    
2018-10-27 01:05:56,195 - Test: [   40/   39]    Loss 0.573005    Top1 90.660000    Top5 99.590000    
2018-10-27 01:05:56,223 - ==> Top1: 90.660    Top5: 99.590    Loss: 0.573

2018-10-27 01:05:56,224 - Testing sensitivity of module.layer3.0.conv1.weight [65.0% sparsity]
2018-10-27 01:05:56,226 - --- test ---------------------
2018-10-27 01:05:56,226 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:56,677 - Test: [   10/   39]    Loss 0.658967    Top1 88.828125    Top5 99.492188    
2018-10-27 01:05:56,781 - Test: [   20/   39]    Loss 0.643388    Top1 89.101562    Top5 99.414062    
2018-10-27 01:05:56,881 - Test: [   30/   39]    Loss 0.622097    Top1 89.492188    Top5 99.466146    
2018-10-27 01:05:56,974 - Test: [   40/   39]    Loss 0.625198    Top1 89.540000    Top5 99.510000    
2018-10-27 01:05:57,002 - ==> Top1: 89.540    Top5: 99.510    Loss: 0.625

2018-10-27 01:05:57,002 - Testing sensitivity of module.layer3.0.conv1.weight [70.0% sparsity]
2018-10-27 01:05:57,005 - --- test ---------------------
2018-10-27 01:05:57,006 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:57,452 - Test: [   10/   39]    Loss 0.855460    Top1 86.718750    Top5 99.218750    
2018-10-27 01:05:57,556 - Test: [   20/   39]    Loss 0.832266    Top1 86.542969    Top5 99.140625    
2018-10-27 01:05:57,657 - Test: [   30/   39]    Loss 0.806786    Top1 86.875000    Top5 99.192708    
2018-10-27 01:05:57,751 - Test: [   40/   39]    Loss 0.795013    Top1 86.950000    Top5 99.230000    
2018-10-27 01:05:57,780 - ==> Top1: 86.950    Top5: 99.230    Loss: 0.795

2018-10-27 01:05:57,781 - Testing sensitivity of module.layer3.0.conv1.weight [75.0% sparsity]
2018-10-27 01:05:57,785 - --- test ---------------------
2018-10-27 01:05:57,785 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:58,253 - Test: [   10/   39]    Loss 0.914046    Top1 84.921875    Top5 99.218750    
2018-10-27 01:05:58,357 - Test: [   20/   39]    Loss 0.901154    Top1 85.292969    Top5 99.003906    
2018-10-27 01:05:58,462 - Test: [   30/   39]    Loss 0.889560    Top1 85.377604    Top5 99.049479    
2018-10-27 01:05:58,562 - Test: [   40/   39]    Loss 0.871685    Top1 85.460000    Top5 99.050000    
2018-10-27 01:05:58,590 - ==> Top1: 85.460    Top5: 99.050    Loss: 0.872

2018-10-27 01:05:58,590 - Testing sensitivity of module.layer3.0.conv1.weight [80.0% sparsity]
2018-10-27 01:05:58,593 - --- test ---------------------
2018-10-27 01:05:58,594 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:59,039 - Test: [   10/   39]    Loss 1.436886    Top1 78.984375    Top5 98.476562    
2018-10-27 01:05:59,142 - Test: [   20/   39]    Loss 1.434152    Top1 79.101562    Top5 98.378906    
2018-10-27 01:05:59,243 - Test: [   30/   39]    Loss 1.425881    Top1 79.296875    Top5 98.385417    
2018-10-27 01:05:59,335 - Test: [   40/   39]    Loss 1.413798    Top1 79.240000    Top5 98.440000    
2018-10-27 01:05:59,362 - ==> Top1: 79.240    Top5: 98.440    Loss: 1.414

2018-10-27 01:05:59,363 - Testing sensitivity of module.layer3.0.conv1.weight [85.0% sparsity]
2018-10-27 01:05:59,365 - --- test ---------------------
2018-10-27 01:05:59,366 - 10000 samples (256 per mini-batch)
2018-10-27 01:05:59,805 - Test: [   10/   39]    Loss 2.592246    Top1 65.898438    Top5 97.500000    
2018-10-27 01:05:59,918 - Test: [   20/   39]    Loss 2.595112    Top1 66.582031    Top5 97.324219    
2018-10-27 01:06:00,020 - Test: [   30/   39]    Loss 2.589653    Top1 66.692708    Top5 97.369792    
2018-10-27 01:06:00,115 - Test: [   40/   39]    Loss 2.589070    Top1 66.560000    Top5 97.380000    
2018-10-27 01:06:00,154 - ==> Top1: 66.560    Top5: 97.380    Loss: 2.589

2018-10-27 01:06:00,155 - Testing sensitivity of module.layer3.0.conv1.weight [90.0% sparsity]
2018-10-27 01:06:00,158 - --- test ---------------------
2018-10-27 01:06:00,158 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:00,624 - Test: [   10/   39]    Loss 9.292566    Top1 27.734375    Top5 92.539062    
2018-10-27 01:06:00,730 - Test: [   20/   39]    Loss 9.193894    Top1 27.929688    Top5 92.734375    
2018-10-27 01:06:00,831 - Test: [   30/   39]    Loss 9.192034    Top1 27.890625    Top5 92.682292    
2018-10-27 01:06:00,923 - Test: [   40/   39]    Loss 9.155307    Top1 27.990000    Top5 92.740000    
2018-10-27 01:06:00,952 - ==> Top1: 27.990    Top5: 92.740    Loss: 9.155

2018-10-27 01:06:00,967 - Testing sensitivity of module.layer3.0.conv2.weight [0.0% sparsity]
2018-10-27 01:06:00,971 - --- test ---------------------
2018-10-27 01:06:00,971 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:01,427 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:06:01,536 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:06:01,645 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:06:01,738 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:06:01,767 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:06:01,768 - Testing sensitivity of module.layer3.0.conv2.weight [5.0% sparsity]
2018-10-27 01:06:01,771 - --- test ---------------------
2018-10-27 01:06:01,771 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:02,226 - Test: [   10/   39]    Loss 0.551558    Top1 91.289062    Top5 99.687500    
2018-10-27 01:06:02,333 - Test: [   20/   39]    Loss 0.552369    Top1 91.562500    Top5 99.570312    
2018-10-27 01:06:02,436 - Test: [   30/   39]    Loss 0.545946    Top1 91.536458    Top5 99.661458    
2018-10-27 01:06:02,528 - Test: [   40/   39]    Loss 0.541194    Top1 91.520000    Top5 99.640000    
2018-10-27 01:06:02,556 - ==> Top1: 91.520    Top5: 99.640    Loss: 0.541

2018-10-27 01:06:02,557 - Testing sensitivity of module.layer3.0.conv2.weight [10.0% sparsity]
2018-10-27 01:06:02,560 - --- test ---------------------
2018-10-27 01:06:02,560 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:03,029 - Test: [   10/   39]    Loss 0.549625    Top1 91.289062    Top5 99.687500    
2018-10-27 01:06:03,135 - Test: [   20/   39]    Loss 0.550570    Top1 91.582031    Top5 99.570312    
2018-10-27 01:06:03,235 - Test: [   30/   39]    Loss 0.544207    Top1 91.562500    Top5 99.661458    
2018-10-27 01:06:03,327 - Test: [   40/   39]    Loss 0.539869    Top1 91.570000    Top5 99.640000    
2018-10-27 01:06:03,356 - ==> Top1: 91.570    Top5: 99.640    Loss: 0.540

2018-10-27 01:06:03,357 - Testing sensitivity of module.layer3.0.conv2.weight [15.0% sparsity]
2018-10-27 01:06:03,360 - --- test ---------------------
2018-10-27 01:06:03,360 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:03,807 - Test: [   10/   39]    Loss 0.548906    Top1 91.328125    Top5 99.687500    
2018-10-27 01:06:03,910 - Test: [   20/   39]    Loss 0.549498    Top1 91.523438    Top5 99.589844    
2018-10-27 01:06:04,010 - Test: [   30/   39]    Loss 0.544104    Top1 91.536458    Top5 99.661458    
2018-10-27 01:06:04,102 - Test: [   40/   39]    Loss 0.539443    Top1 91.550000    Top5 99.650000    
2018-10-27 01:06:04,132 - ==> Top1: 91.550    Top5: 99.650    Loss: 0.539

2018-10-27 01:06:04,133 - Testing sensitivity of module.layer3.0.conv2.weight [20.0% sparsity]
2018-10-27 01:06:04,136 - --- test ---------------------
2018-10-27 01:06:04,136 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:04,577 - Test: [   10/   39]    Loss 0.548611    Top1 91.093750    Top5 99.687500    
2018-10-27 01:06:04,680 - Test: [   20/   39]    Loss 0.550309    Top1 91.289062    Top5 99.589844    
2018-10-27 01:06:04,781 - Test: [   30/   39]    Loss 0.544567    Top1 91.367188    Top5 99.661458    
2018-10-27 01:06:04,873 - Test: [   40/   39]    Loss 0.538681    Top1 91.450000    Top5 99.650000    
2018-10-27 01:06:04,900 - ==> Top1: 91.450    Top5: 99.650    Loss: 0.539

2018-10-27 01:06:04,901 - Testing sensitivity of module.layer3.0.conv2.weight [25.0% sparsity]
2018-10-27 01:06:04,904 - --- test ---------------------
2018-10-27 01:06:04,904 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:05,357 - Test: [   10/   39]    Loss 0.556049    Top1 91.171875    Top5 99.687500    
2018-10-27 01:06:05,460 - Test: [   20/   39]    Loss 0.552607    Top1 91.328125    Top5 99.589844    
2018-10-27 01:06:05,565 - Test: [   30/   39]    Loss 0.546427    Top1 91.289062    Top5 99.661458    
2018-10-27 01:06:05,663 - Test: [   40/   39]    Loss 0.543242    Top1 91.370000    Top5 99.630000    
2018-10-27 01:06:05,690 - ==> Top1: 91.370    Top5: 99.630    Loss: 0.543

2018-10-27 01:06:05,691 - Testing sensitivity of module.layer3.0.conv2.weight [30.0% sparsity]
2018-10-27 01:06:05,694 - --- test ---------------------
2018-10-27 01:06:05,695 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:06,138 - Test: [   10/   39]    Loss 0.564466    Top1 91.054688    Top5 99.687500    
2018-10-27 01:06:06,240 - Test: [   20/   39]    Loss 0.558374    Top1 91.171875    Top5 99.589844    
2018-10-27 01:06:06,341 - Test: [   30/   39]    Loss 0.550995    Top1 91.236979    Top5 99.661458    
2018-10-27 01:06:06,433 - Test: [   40/   39]    Loss 0.550480    Top1 91.220000    Top5 99.620000    
2018-10-27 01:06:06,460 - ==> Top1: 91.220    Top5: 99.620    Loss: 0.550

2018-10-27 01:06:06,461 - Testing sensitivity of module.layer3.0.conv2.weight [35.0% sparsity]
2018-10-27 01:06:06,463 - --- test ---------------------
2018-10-27 01:06:06,463 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:06,908 - Test: [   10/   39]    Loss 0.573857    Top1 90.937500    Top5 99.648438    
2018-10-27 01:06:07,011 - Test: [   20/   39]    Loss 0.566650    Top1 91.132812    Top5 99.550781    
2018-10-27 01:06:07,111 - Test: [   30/   39]    Loss 0.559498    Top1 91.223958    Top5 99.635417    
2018-10-27 01:06:07,203 - Test: [   40/   39]    Loss 0.559593    Top1 91.260000    Top5 99.620000    
2018-10-27 01:06:07,231 - ==> Top1: 91.260    Top5: 99.620    Loss: 0.560

2018-10-27 01:06:07,232 - Testing sensitivity of module.layer3.0.conv2.weight [40.0% sparsity]
2018-10-27 01:06:07,235 - --- test ---------------------
2018-10-27 01:06:07,235 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:07,675 - Test: [   10/   39]    Loss 0.583469    Top1 90.781250    Top5 99.531250    
2018-10-27 01:06:07,778 - Test: [   20/   39]    Loss 0.575398    Top1 91.015625    Top5 99.472656    
2018-10-27 01:06:07,878 - Test: [   30/   39]    Loss 0.567977    Top1 91.067708    Top5 99.557292    
2018-10-27 01:06:07,970 - Test: [   40/   39]    Loss 0.571022    Top1 91.100000    Top5 99.550000    
2018-10-27 01:06:07,999 - ==> Top1: 91.100    Top5: 99.550    Loss: 0.571

2018-10-27 01:06:08,000 - Testing sensitivity of module.layer3.0.conv2.weight [45.0% sparsity]
2018-10-27 01:06:08,003 - --- test ---------------------
2018-10-27 01:06:08,003 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:08,456 - Test: [   10/   39]    Loss 0.585800    Top1 90.429688    Top5 99.492188    
2018-10-27 01:06:08,563 - Test: [   20/   39]    Loss 0.583307    Top1 90.625000    Top5 99.453125    
2018-10-27 01:06:08,663 - Test: [   30/   39]    Loss 0.574226    Top1 90.807292    Top5 99.531250    
2018-10-27 01:06:08,757 - Test: [   40/   39]    Loss 0.575734    Top1 90.940000    Top5 99.550000    
2018-10-27 01:06:08,787 - ==> Top1: 90.940    Top5: 99.550    Loss: 0.576

2018-10-27 01:06:08,788 - Testing sensitivity of module.layer3.0.conv2.weight [50.0% sparsity]
2018-10-27 01:06:08,791 - --- test ---------------------
2018-10-27 01:06:08,792 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:09,236 - Test: [   10/   39]    Loss 0.594516    Top1 90.429688    Top5 99.531250    
2018-10-27 01:06:09,340 - Test: [   20/   39]    Loss 0.595833    Top1 90.449219    Top5 99.453125    
2018-10-27 01:06:09,440 - Test: [   30/   39]    Loss 0.591843    Top1 90.598958    Top5 99.518229    
2018-10-27 01:06:09,532 - Test: [   40/   39]    Loss 0.591758    Top1 90.630000    Top5 99.540000    
2018-10-27 01:06:09,560 - ==> Top1: 90.630    Top5: 99.540    Loss: 0.592

2018-10-27 01:06:09,561 - Testing sensitivity of module.layer3.0.conv2.weight [55.0% sparsity]
2018-10-27 01:06:09,564 - --- test ---------------------
2018-10-27 01:06:09,564 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:10,047 - Test: [   10/   39]    Loss 0.615809    Top1 90.039062    Top5 99.375000    
2018-10-27 01:06:10,151 - Test: [   20/   39]    Loss 0.617766    Top1 90.136719    Top5 99.277344    
2018-10-27 01:06:10,251 - Test: [   30/   39]    Loss 0.609603    Top1 90.195312    Top5 99.440104    
2018-10-27 01:06:10,345 - Test: [   40/   39]    Loss 0.603017    Top1 90.280000    Top5 99.490000    
2018-10-27 01:06:10,374 - ==> Top1: 90.280    Top5: 99.490    Loss: 0.603

2018-10-27 01:06:10,374 - Testing sensitivity of module.layer3.0.conv2.weight [60.0% sparsity]
2018-10-27 01:06:10,377 - --- test ---------------------
2018-10-27 01:06:10,377 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:10,834 - Test: [   10/   39]    Loss 0.603905    Top1 89.179688    Top5 99.453125    
2018-10-27 01:06:10,943 - Test: [   20/   39]    Loss 0.610241    Top1 89.726562    Top5 99.355469    
2018-10-27 01:06:11,043 - Test: [   30/   39]    Loss 0.602201    Top1 89.947917    Top5 99.466146    
2018-10-27 01:06:11,136 - Test: [   40/   39]    Loss 0.593623    Top1 90.050000    Top5 99.470000    
2018-10-27 01:06:11,164 - ==> Top1: 90.050    Top5: 99.470    Loss: 0.594

2018-10-27 01:06:11,165 - Testing sensitivity of module.layer3.0.conv2.weight [65.0% sparsity]
2018-10-27 01:06:11,168 - --- test ---------------------
2018-10-27 01:06:11,168 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:11,640 - Test: [   10/   39]    Loss 0.596764    Top1 89.648438    Top5 99.296875    
2018-10-27 01:06:11,743 - Test: [   20/   39]    Loss 0.613126    Top1 89.785156    Top5 99.238281    
2018-10-27 01:06:11,843 - Test: [   30/   39]    Loss 0.613386    Top1 89.895833    Top5 99.348958    
2018-10-27 01:06:11,935 - Test: [   40/   39]    Loss 0.605942    Top1 89.940000    Top5 99.360000    
2018-10-27 01:06:11,962 - ==> Top1: 89.940    Top5: 99.360    Loss: 0.606

2018-10-27 01:06:11,964 - Testing sensitivity of module.layer3.0.conv2.weight [70.0% sparsity]
2018-10-27 01:06:11,968 - --- test ---------------------
2018-10-27 01:06:11,968 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:12,411 - Test: [   10/   39]    Loss 0.668313    Top1 88.671875    Top5 99.453125    
2018-10-27 01:06:12,514 - Test: [   20/   39]    Loss 0.695076    Top1 88.613281    Top5 99.277344    
2018-10-27 01:06:12,616 - Test: [   30/   39]    Loss 0.707123    Top1 88.489583    Top5 99.348958    
2018-10-27 01:06:12,712 - Test: [   40/   39]    Loss 0.691612    Top1 88.520000    Top5 99.370000    
2018-10-27 01:06:12,739 - ==> Top1: 88.520    Top5: 99.370    Loss: 0.692

2018-10-27 01:06:12,740 - Testing sensitivity of module.layer3.0.conv2.weight [75.0% sparsity]
2018-10-27 01:06:12,743 - --- test ---------------------
2018-10-27 01:06:12,743 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:13,196 - Test: [   10/   39]    Loss 0.721332    Top1 87.617188    Top5 99.296875    
2018-10-27 01:06:13,299 - Test: [   20/   39]    Loss 0.758276    Top1 87.734375    Top5 99.023438    
2018-10-27 01:06:13,400 - Test: [   30/   39]    Loss 0.778193    Top1 87.591146    Top5 99.101562    
2018-10-27 01:06:13,495 - Test: [   40/   39]    Loss 0.767253    Top1 87.670000    Top5 99.120000    
2018-10-27 01:06:13,523 - ==> Top1: 87.670    Top5: 99.120    Loss: 0.767

2018-10-27 01:06:13,524 - Testing sensitivity of module.layer3.0.conv2.weight [80.0% sparsity]
2018-10-27 01:06:13,527 - --- test ---------------------
2018-10-27 01:06:13,528 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:13,962 - Test: [   10/   39]    Loss 0.807093    Top1 86.015625    Top5 99.101562    
2018-10-27 01:06:14,064 - Test: [   20/   39]    Loss 0.842636    Top1 86.191406    Top5 98.964844    
2018-10-27 01:06:14,167 - Test: [   30/   39]    Loss 0.844468    Top1 86.067708    Top5 99.023438    
2018-10-27 01:06:14,263 - Test: [   40/   39]    Loss 0.846595    Top1 86.130000    Top5 99.030000    
2018-10-27 01:06:14,291 - ==> Top1: 86.130    Top5: 99.030    Loss: 0.847

2018-10-27 01:06:14,292 - Testing sensitivity of module.layer3.0.conv2.weight [85.0% sparsity]
2018-10-27 01:06:14,295 - --- test ---------------------
2018-10-27 01:06:14,295 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:14,774 - Test: [   10/   39]    Loss 1.011644    Top1 82.421875    Top5 98.750000    
2018-10-27 01:06:14,879 - Test: [   20/   39]    Loss 1.056964    Top1 82.382812    Top5 98.515625    
2018-10-27 01:06:14,979 - Test: [   30/   39]    Loss 1.071773    Top1 82.239583    Top5 98.619792    
2018-10-27 01:06:15,072 - Test: [   40/   39]    Loss 1.077743    Top1 82.350000    Top5 98.650000    
2018-10-27 01:06:15,111 - ==> Top1: 82.350    Top5: 98.650    Loss: 1.078

2018-10-27 01:06:15,112 - Testing sensitivity of module.layer3.0.conv2.weight [90.0% sparsity]
2018-10-27 01:06:15,115 - --- test ---------------------
2018-10-27 01:06:15,115 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:15,577 - Test: [   10/   39]    Loss 1.656819    Top1 75.585938    Top5 98.203125    
2018-10-27 01:06:15,682 - Test: [   20/   39]    Loss 1.620599    Top1 76.699219    Top5 97.851562    
2018-10-27 01:06:15,787 - Test: [   30/   39]    Loss 1.629910    Top1 76.054688    Top5 98.007812    
2018-10-27 01:06:15,883 - Test: [   40/   39]    Loss 1.623458    Top1 76.160000    Top5 97.980000    
2018-10-27 01:06:15,919 - ==> Top1: 76.160    Top5: 97.980    Loss: 1.623

2018-10-27 01:06:15,934 - Testing sensitivity of module.layer3.0.downsample.0.weight [0.0% sparsity]
2018-10-27 01:06:15,937 - --- test ---------------------
2018-10-27 01:06:15,937 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:16,393 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:06:16,498 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:06:16,600 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:06:16,701 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:06:16,729 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:06:16,729 - Testing sensitivity of module.layer3.0.downsample.0.weight [5.0% sparsity]
2018-10-27 01:06:16,732 - --- test ---------------------
2018-10-27 01:06:16,732 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:17,185 - Test: [   10/   39]    Loss 0.551909    Top1 91.250000    Top5 99.687500    
2018-10-27 01:06:17,292 - Test: [   20/   39]    Loss 0.552885    Top1 91.523438    Top5 99.570312    
2018-10-27 01:06:17,395 - Test: [   30/   39]    Loss 0.546265    Top1 91.510417    Top5 99.661458    
2018-10-27 01:06:17,488 - Test: [   40/   39]    Loss 0.541527    Top1 91.510000    Top5 99.640000    
2018-10-27 01:06:17,516 - ==> Top1: 91.510    Top5: 99.640    Loss: 0.542

2018-10-27 01:06:17,517 - Testing sensitivity of module.layer3.0.downsample.0.weight [10.0% sparsity]
2018-10-27 01:06:17,519 - --- test ---------------------
2018-10-27 01:06:17,519 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:17,981 - Test: [   10/   39]    Loss 0.552195    Top1 91.250000    Top5 99.687500    
2018-10-27 01:06:18,084 - Test: [   20/   39]    Loss 0.553302    Top1 91.523438    Top5 99.589844    
2018-10-27 01:06:18,185 - Test: [   30/   39]    Loss 0.546679    Top1 91.510417    Top5 99.674479    
2018-10-27 01:06:18,282 - Test: [   40/   39]    Loss 0.541865    Top1 91.520000    Top5 99.650000    
2018-10-27 01:06:18,310 - ==> Top1: 91.520    Top5: 99.650    Loss: 0.542

2018-10-27 01:06:18,311 - Testing sensitivity of module.layer3.0.downsample.0.weight [15.0% sparsity]
2018-10-27 01:06:18,313 - --- test ---------------------
2018-10-27 01:06:18,314 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:18,758 - Test: [   10/   39]    Loss 0.551797    Top1 91.250000    Top5 99.687500    
2018-10-27 01:06:18,864 - Test: [   20/   39]    Loss 0.553395    Top1 91.484375    Top5 99.589844    
2018-10-27 01:06:18,964 - Test: [   30/   39]    Loss 0.546665    Top1 91.484375    Top5 99.674479    
2018-10-27 01:06:19,056 - Test: [   40/   39]    Loss 0.542030    Top1 91.490000    Top5 99.650000    
2018-10-27 01:06:19,084 - ==> Top1: 91.490    Top5: 99.650    Loss: 0.542

2018-10-27 01:06:19,085 - Testing sensitivity of module.layer3.0.downsample.0.weight [20.0% sparsity]
2018-10-27 01:06:19,087 - --- test ---------------------
2018-10-27 01:06:19,087 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:19,562 - Test: [   10/   39]    Loss 0.552778    Top1 91.250000    Top5 99.687500    
2018-10-27 01:06:19,668 - Test: [   20/   39]    Loss 0.553827    Top1 91.503906    Top5 99.589844    
2018-10-27 01:06:19,769 - Test: [   30/   39]    Loss 0.547025    Top1 91.536458    Top5 99.674479    
2018-10-27 01:06:19,862 - Test: [   40/   39]    Loss 0.542678    Top1 91.550000    Top5 99.660000    
2018-10-27 01:06:19,890 - ==> Top1: 91.550    Top5: 99.660    Loss: 0.543

2018-10-27 01:06:19,891 - Testing sensitivity of module.layer3.0.downsample.0.weight [25.0% sparsity]
2018-10-27 01:06:19,894 - --- test ---------------------
2018-10-27 01:06:19,894 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:20,331 - Test: [   10/   39]    Loss 0.551923    Top1 91.289062    Top5 99.687500    
2018-10-27 01:06:20,443 - Test: [   20/   39]    Loss 0.554079    Top1 91.523438    Top5 99.570312    
2018-10-27 01:06:20,548 - Test: [   30/   39]    Loss 0.547167    Top1 91.510417    Top5 99.661458    
2018-10-27 01:06:20,646 - Test: [   40/   39]    Loss 0.542891    Top1 91.520000    Top5 99.640000    
2018-10-27 01:06:20,674 - ==> Top1: 91.520    Top5: 99.640    Loss: 0.543

2018-10-27 01:06:20,674 - Testing sensitivity of module.layer3.0.downsample.0.weight [30.0% sparsity]
2018-10-27 01:06:20,677 - --- test ---------------------
2018-10-27 01:06:20,678 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:21,137 - Test: [   10/   39]    Loss 0.551178    Top1 91.250000    Top5 99.687500    
2018-10-27 01:06:21,241 - Test: [   20/   39]    Loss 0.554749    Top1 91.445312    Top5 99.589844    
2018-10-27 01:06:21,342 - Test: [   30/   39]    Loss 0.548213    Top1 91.497396    Top5 99.674479    
2018-10-27 01:06:21,435 - Test: [   40/   39]    Loss 0.543865    Top1 91.530000    Top5 99.650000    
2018-10-27 01:06:21,463 - ==> Top1: 91.530    Top5: 99.650    Loss: 0.544

2018-10-27 01:06:21,464 - Testing sensitivity of module.layer3.0.downsample.0.weight [35.0% sparsity]
2018-10-27 01:06:21,467 - --- test ---------------------
2018-10-27 01:06:21,467 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:21,908 - Test: [   10/   39]    Loss 0.552974    Top1 91.210938    Top5 99.687500    
2018-10-27 01:06:22,014 - Test: [   20/   39]    Loss 0.557253    Top1 91.386719    Top5 99.570312    
2018-10-27 01:06:22,115 - Test: [   30/   39]    Loss 0.550269    Top1 91.432292    Top5 99.661458    
2018-10-27 01:06:22,210 - Test: [   40/   39]    Loss 0.545486    Top1 91.450000    Top5 99.650000    
2018-10-27 01:06:22,237 - ==> Top1: 91.450    Top5: 99.650    Loss: 0.545

2018-10-27 01:06:22,238 - Testing sensitivity of module.layer3.0.downsample.0.weight [40.0% sparsity]
2018-10-27 01:06:22,242 - --- test ---------------------
2018-10-27 01:06:22,242 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:22,707 - Test: [   10/   39]    Loss 0.556584    Top1 91.093750    Top5 99.687500    
2018-10-27 01:06:22,828 - Test: [   20/   39]    Loss 0.559619    Top1 91.386719    Top5 99.570312    
2018-10-27 01:06:22,937 - Test: [   30/   39]    Loss 0.552233    Top1 91.419271    Top5 99.661458    
2018-10-27 01:06:23,037 - Test: [   40/   39]    Loss 0.548349    Top1 91.430000    Top5 99.640000    
2018-10-27 01:06:23,066 - ==> Top1: 91.430    Top5: 99.640    Loss: 0.548

2018-10-27 01:06:23,066 - Testing sensitivity of module.layer3.0.downsample.0.weight [45.0% sparsity]
2018-10-27 01:06:23,069 - --- test ---------------------
2018-10-27 01:06:23,072 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:23,667 - Test: [   10/   39]    Loss 0.558204    Top1 91.132812    Top5 99.609375    
2018-10-27 01:06:23,780 - Test: [   20/   39]    Loss 0.559157    Top1 91.347656    Top5 99.531250    
2018-10-27 01:06:23,890 - Test: [   30/   39]    Loss 0.549942    Top1 91.354167    Top5 99.622396    
2018-10-27 01:06:23,990 - Test: [   40/   39]    Loss 0.545928    Top1 91.380000    Top5 99.610000    
2018-10-27 01:06:24,035 - ==> Top1: 91.380    Top5: 99.610    Loss: 0.546

2018-10-27 01:06:24,035 - Testing sensitivity of module.layer3.0.downsample.0.weight [50.0% sparsity]
2018-10-27 01:06:24,038 - --- test ---------------------
2018-10-27 01:06:24,038 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:24,573 - Test: [   10/   39]    Loss 0.558194    Top1 91.093750    Top5 99.648438    
2018-10-27 01:06:24,680 - Test: [   20/   39]    Loss 0.560850    Top1 91.367188    Top5 99.570312    
2018-10-27 01:06:24,790 - Test: [   30/   39]    Loss 0.551247    Top1 91.432292    Top5 99.648438    
2018-10-27 01:06:24,892 - Test: [   40/   39]    Loss 0.547287    Top1 91.400000    Top5 99.630000    
2018-10-27 01:06:24,963 - ==> Top1: 91.400    Top5: 99.630    Loss: 0.547

2018-10-27 01:06:24,963 - Testing sensitivity of module.layer3.0.downsample.0.weight [55.0% sparsity]
2018-10-27 01:06:24,966 - --- test ---------------------
2018-10-27 01:06:24,966 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:25,529 - Test: [   10/   39]    Loss 0.562705    Top1 91.132812    Top5 99.687500    
2018-10-27 01:06:25,641 - Test: [   20/   39]    Loss 0.564931    Top1 91.308594    Top5 99.589844    
2018-10-27 01:06:25,750 - Test: [   30/   39]    Loss 0.554586    Top1 91.419271    Top5 99.661458    
2018-10-27 01:06:25,853 - Test: [   40/   39]    Loss 0.550967    Top1 91.440000    Top5 99.640000    
2018-10-27 01:06:25,917 - ==> Top1: 91.440    Top5: 99.640    Loss: 0.551

2018-10-27 01:06:25,917 - Testing sensitivity of module.layer3.0.downsample.0.weight [60.0% sparsity]
2018-10-27 01:06:25,920 - --- test ---------------------
2018-10-27 01:06:25,921 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:26,442 - Test: [   10/   39]    Loss 0.575100    Top1 90.937500    Top5 99.648438    
2018-10-27 01:06:26,558 - Test: [   20/   39]    Loss 0.573608    Top1 91.054688    Top5 99.550781    
2018-10-27 01:06:26,668 - Test: [   30/   39]    Loss 0.560805    Top1 91.184896    Top5 99.635417    
2018-10-27 01:06:26,767 - Test: [   40/   39]    Loss 0.559146    Top1 91.180000    Top5 99.620000    
2018-10-27 01:06:26,805 - ==> Top1: 91.180    Top5: 99.620    Loss: 0.559

2018-10-27 01:06:26,805 - Testing sensitivity of module.layer3.0.downsample.0.weight [65.0% sparsity]
2018-10-27 01:06:26,808 - --- test ---------------------
2018-10-27 01:06:26,809 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:27,259 - Test: [   10/   39]    Loss 0.570809    Top1 90.859375    Top5 99.609375    
2018-10-27 01:06:27,369 - Test: [   20/   39]    Loss 0.574274    Top1 90.917969    Top5 99.511719    
2018-10-27 01:06:27,471 - Test: [   30/   39]    Loss 0.562703    Top1 91.054688    Top5 99.596354    
2018-10-27 01:06:27,565 - Test: [   40/   39]    Loss 0.561089    Top1 91.170000    Top5 99.590000    
2018-10-27 01:06:27,603 - ==> Top1: 91.170    Top5: 99.590    Loss: 0.561

2018-10-27 01:06:27,603 - Testing sensitivity of module.layer3.0.downsample.0.weight [70.0% sparsity]
2018-10-27 01:06:27,606 - --- test ---------------------
2018-10-27 01:06:27,606 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:28,064 - Test: [   10/   39]    Loss 0.580974    Top1 90.742188    Top5 99.609375    
2018-10-27 01:06:28,169 - Test: [   20/   39]    Loss 0.582385    Top1 90.800781    Top5 99.550781    
2018-10-27 01:06:28,271 - Test: [   30/   39]    Loss 0.568596    Top1 90.898438    Top5 99.609375    
2018-10-27 01:06:28,365 - Test: [   40/   39]    Loss 0.569148    Top1 91.010000    Top5 99.610000    
2018-10-27 01:06:28,392 - ==> Top1: 91.010    Top5: 99.610    Loss: 0.569

2018-10-27 01:06:28,393 - Testing sensitivity of module.layer3.0.downsample.0.weight [75.0% sparsity]
2018-10-27 01:06:28,396 - --- test ---------------------
2018-10-27 01:06:28,396 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:28,864 - Test: [   10/   39]    Loss 0.601249    Top1 90.429688    Top5 99.531250    
2018-10-27 01:06:28,969 - Test: [   20/   39]    Loss 0.604495    Top1 90.605469    Top5 99.511719    
2018-10-27 01:06:29,070 - Test: [   30/   39]    Loss 0.589928    Top1 90.807292    Top5 99.583333    
2018-10-27 01:06:29,167 - Test: [   40/   39]    Loss 0.591912    Top1 90.970000    Top5 99.580000    
2018-10-27 01:06:29,195 - ==> Top1: 90.970    Top5: 99.580    Loss: 0.592

2018-10-27 01:06:29,196 - Testing sensitivity of module.layer3.0.downsample.0.weight [80.0% sparsity]
2018-10-27 01:06:29,198 - --- test ---------------------
2018-10-27 01:06:29,199 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:29,657 - Test: [   10/   39]    Loss 0.612471    Top1 90.195312    Top5 99.531250    
2018-10-27 01:06:29,766 - Test: [   20/   39]    Loss 0.615638    Top1 90.488281    Top5 99.492188    
2018-10-27 01:06:29,869 - Test: [   30/   39]    Loss 0.600675    Top1 90.572917    Top5 99.570312    
2018-10-27 01:06:29,962 - Test: [   40/   39]    Loss 0.603722    Top1 90.660000    Top5 99.550000    
2018-10-27 01:06:29,963 - ==> Top1: 90.660    Top5: 99.550    Loss: 0.604

2018-10-27 01:06:29,994 - Testing sensitivity of module.layer3.0.downsample.0.weight [85.0% sparsity]
2018-10-27 01:06:29,997 - --- test ---------------------
2018-10-27 01:06:29,998 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:30,465 - Test: [   10/   39]    Loss 0.616378    Top1 90.039062    Top5 99.570312    
2018-10-27 01:06:30,569 - Test: [   20/   39]    Loss 0.616320    Top1 90.332031    Top5 99.511719    
2018-10-27 01:06:30,671 - Test: [   30/   39]    Loss 0.600192    Top1 90.468750    Top5 99.583333    
2018-10-27 01:06:30,769 - Test: [   40/   39]    Loss 0.602427    Top1 90.580000    Top5 99.560000    
2018-10-27 01:06:30,804 - ==> Top1: 90.580    Top5: 99.560    Loss: 0.602

2018-10-27 01:06:30,805 - Testing sensitivity of module.layer3.0.downsample.0.weight [90.0% sparsity]
2018-10-27 01:06:30,806 - --- test ---------------------
2018-10-27 01:06:30,806 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:31,273 - Test: [   10/   39]    Loss 0.635357    Top1 89.882812    Top5 99.609375    
2018-10-27 01:06:31,377 - Test: [   20/   39]    Loss 0.634561    Top1 90.156250    Top5 99.531250    
2018-10-27 01:06:31,487 - Test: [   30/   39]    Loss 0.615927    Top1 90.260417    Top5 99.570312    
2018-10-27 01:06:31,587 - Test: [   40/   39]    Loss 0.618180    Top1 90.410000    Top5 99.540000    
2018-10-27 01:06:31,616 - ==> Top1: 90.410    Top5: 99.540    Loss: 0.618

2018-10-27 01:06:31,631 - Testing sensitivity of module.layer3.1.conv1.weight [0.0% sparsity]
2018-10-27 01:06:31,635 - --- test ---------------------
2018-10-27 01:06:31,636 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:32,185 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:06:32,298 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:06:32,408 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:06:32,509 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:06:32,554 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:06:32,554 - Testing sensitivity of module.layer3.1.conv1.weight [5.0% sparsity]
2018-10-27 01:06:32,557 - --- test ---------------------
2018-10-27 01:06:32,557 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:33,074 - Test: [   10/   39]    Loss 0.552171    Top1 91.250000    Top5 99.687500    
2018-10-27 01:06:33,186 - Test: [   20/   39]    Loss 0.553458    Top1 91.484375    Top5 99.570312    
2018-10-27 01:06:33,296 - Test: [   30/   39]    Loss 0.546733    Top1 91.497396    Top5 99.661458    
2018-10-27 01:06:33,395 - Test: [   40/   39]    Loss 0.542036    Top1 91.530000    Top5 99.640000    
2018-10-27 01:06:33,436 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:06:33,438 - Testing sensitivity of module.layer3.1.conv1.weight [10.0% sparsity]
2018-10-27 01:06:33,441 - --- test ---------------------
2018-10-27 01:06:33,442 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:33,899 - Test: [   10/   39]    Loss 0.549970    Top1 91.367188    Top5 99.687500    
2018-10-27 01:06:34,003 - Test: [   20/   39]    Loss 0.551745    Top1 91.562500    Top5 99.570312    
2018-10-27 01:06:34,108 - Test: [   30/   39]    Loss 0.545168    Top1 91.523438    Top5 99.661458    
2018-10-27 01:06:34,208 - Test: [   40/   39]    Loss 0.540191    Top1 91.490000    Top5 99.640000    
2018-10-27 01:06:34,249 - ==> Top1: 91.490    Top5: 99.640    Loss: 0.540

2018-10-27 01:06:34,249 - Testing sensitivity of module.layer3.1.conv1.weight [15.0% sparsity]
2018-10-27 01:06:34,252 - --- test ---------------------
2018-10-27 01:06:34,252 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:34,718 - Test: [   10/   39]    Loss 0.553113    Top1 91.250000    Top5 99.687500    
2018-10-27 01:06:34,821 - Test: [   20/   39]    Loss 0.554592    Top1 91.484375    Top5 99.570312    
2018-10-27 01:06:34,921 - Test: [   30/   39]    Loss 0.547400    Top1 91.471354    Top5 99.661458    
2018-10-27 01:06:35,014 - Test: [   40/   39]    Loss 0.541809    Top1 91.550000    Top5 99.640000    
2018-10-27 01:06:35,015 - ==> Top1: 91.550    Top5: 99.640    Loss: 0.542

2018-10-27 01:06:35,019 - Testing sensitivity of module.layer3.1.conv1.weight [20.0% sparsity]
2018-10-27 01:06:35,021 - --- test ---------------------
2018-10-27 01:06:35,022 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:35,538 - Test: [   10/   39]    Loss 0.550053    Top1 91.367188    Top5 99.687500    
2018-10-27 01:06:35,649 - Test: [   20/   39]    Loss 0.553155    Top1 91.562500    Top5 99.570312    
2018-10-27 01:06:35,751 - Test: [   30/   39]    Loss 0.545257    Top1 91.510417    Top5 99.661458    
2018-10-27 01:06:35,843 - Test: [   40/   39]    Loss 0.538969    Top1 91.540000    Top5 99.650000    
2018-10-27 01:06:35,871 - ==> Top1: 91.540    Top5: 99.650    Loss: 0.539

2018-10-27 01:06:35,872 - Testing sensitivity of module.layer3.1.conv1.weight [25.0% sparsity]
2018-10-27 01:06:35,876 - --- test ---------------------
2018-10-27 01:06:35,876 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:36,344 - Test: [   10/   39]    Loss 0.543352    Top1 91.210938    Top5 99.687500    
2018-10-27 01:06:36,449 - Test: [   20/   39]    Loss 0.548294    Top1 91.523438    Top5 99.570312    
2018-10-27 01:06:36,550 - Test: [   30/   39]    Loss 0.541639    Top1 91.432292    Top5 99.648438    
2018-10-27 01:06:36,643 - Test: [   40/   39]    Loss 0.535417    Top1 91.510000    Top5 99.640000    
2018-10-27 01:06:36,678 - ==> Top1: 91.510    Top5: 99.640    Loss: 0.535

2018-10-27 01:06:36,679 - Testing sensitivity of module.layer3.1.conv1.weight [30.0% sparsity]
2018-10-27 01:06:36,681 - --- test ---------------------
2018-10-27 01:06:36,682 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:37,138 - Test: [   10/   39]    Loss 0.540622    Top1 91.328125    Top5 99.687500    
2018-10-27 01:06:37,251 - Test: [   20/   39]    Loss 0.545996    Top1 91.464844    Top5 99.589844    
2018-10-27 01:06:37,359 - Test: [   30/   39]    Loss 0.538143    Top1 91.341146    Top5 99.674479    
2018-10-27 01:06:37,456 - Test: [   40/   39]    Loss 0.532517    Top1 91.430000    Top5 99.660000    
2018-10-27 01:06:37,498 - ==> Top1: 91.430    Top5: 99.660    Loss: 0.533

2018-10-27 01:06:37,499 - Testing sensitivity of module.layer3.1.conv1.weight [35.0% sparsity]
2018-10-27 01:06:37,501 - --- test ---------------------
2018-10-27 01:06:37,502 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:37,970 - Test: [   10/   39]    Loss 0.545247    Top1 91.054688    Top5 99.570312    
2018-10-27 01:06:38,073 - Test: [   20/   39]    Loss 0.543249    Top1 91.269531    Top5 99.531250    
2018-10-27 01:06:38,177 - Test: [   30/   39]    Loss 0.536910    Top1 91.171875    Top5 99.635417    
2018-10-27 01:06:38,275 - Test: [   40/   39]    Loss 0.530624    Top1 91.250000    Top5 99.640000    
2018-10-27 01:06:38,333 - ==> Top1: 91.250    Top5: 99.640    Loss: 0.531

2018-10-27 01:06:38,334 - Testing sensitivity of module.layer3.1.conv1.weight [40.0% sparsity]
2018-10-27 01:06:38,337 - --- test ---------------------
2018-10-27 01:06:38,338 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:38,918 - Test: [   10/   39]    Loss 0.536193    Top1 91.015625    Top5 99.570312    
2018-10-27 01:06:39,029 - Test: [   20/   39]    Loss 0.538603    Top1 91.308594    Top5 99.511719    
2018-10-27 01:06:39,138 - Test: [   30/   39]    Loss 0.532456    Top1 91.145833    Top5 99.622396    
2018-10-27 01:06:39,242 - Test: [   40/   39]    Loss 0.526883    Top1 91.220000    Top5 99.640000    
2018-10-27 01:06:39,285 - ==> Top1: 91.220    Top5: 99.640    Loss: 0.527

2018-10-27 01:06:39,286 - Testing sensitivity of module.layer3.1.conv1.weight [45.0% sparsity]
2018-10-27 01:06:39,289 - --- test ---------------------
2018-10-27 01:06:39,290 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:39,804 - Test: [   10/   39]    Loss 0.527143    Top1 91.093750    Top5 99.531250    
2018-10-27 01:06:39,910 - Test: [   20/   39]    Loss 0.525725    Top1 91.269531    Top5 99.453125    
2018-10-27 01:06:40,022 - Test: [   30/   39]    Loss 0.519552    Top1 91.289062    Top5 99.557292    
2018-10-27 01:06:40,122 - Test: [   40/   39]    Loss 0.516387    Top1 91.320000    Top5 99.560000    
2018-10-27 01:06:40,181 - ==> Top1: 91.320    Top5: 99.560    Loss: 0.516

2018-10-27 01:06:40,182 - Testing sensitivity of module.layer3.1.conv1.weight [50.0% sparsity]
2018-10-27 01:06:40,185 - --- test ---------------------
2018-10-27 01:06:40,186 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:40,766 - Test: [   10/   39]    Loss 0.530005    Top1 90.664062    Top5 99.648438    
2018-10-27 01:06:40,878 - Test: [   20/   39]    Loss 0.529273    Top1 90.898438    Top5 99.550781    
2018-10-27 01:06:40,987 - Test: [   30/   39]    Loss 0.528775    Top1 90.872396    Top5 99.622396    
2018-10-27 01:06:41,088 - Test: [   40/   39]    Loss 0.519270    Top1 90.940000    Top5 99.630000    
2018-10-27 01:06:41,136 - ==> Top1: 90.940    Top5: 99.630    Loss: 0.519

2018-10-27 01:06:41,136 - Testing sensitivity of module.layer3.1.conv1.weight [55.0% sparsity]
2018-10-27 01:06:41,140 - --- test ---------------------
2018-10-27 01:06:41,140 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:41,657 - Test: [   10/   39]    Loss 0.522975    Top1 90.273438    Top5 99.570312    
2018-10-27 01:06:41,770 - Test: [   20/   39]    Loss 0.515339    Top1 90.527344    Top5 99.550781    
2018-10-27 01:06:41,880 - Test: [   30/   39]    Loss 0.514513    Top1 90.598958    Top5 99.648438    
2018-10-27 01:06:41,981 - Test: [   40/   39]    Loss 0.510945    Top1 90.740000    Top5 99.630000    
2018-10-27 01:06:42,022 - ==> Top1: 90.740    Top5: 99.630    Loss: 0.511

2018-10-27 01:06:42,022 - Testing sensitivity of module.layer3.1.conv1.weight [60.0% sparsity]
2018-10-27 01:06:42,026 - --- test ---------------------
2018-10-27 01:06:42,026 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:42,517 - Test: [   10/   39]    Loss 0.526138    Top1 89.765625    Top5 99.492188    
2018-10-27 01:06:42,628 - Test: [   20/   39]    Loss 0.512700    Top1 90.000000    Top5 99.472656    
2018-10-27 01:06:42,733 - Test: [   30/   39]    Loss 0.510577    Top1 89.947917    Top5 99.518229    
2018-10-27 01:06:42,826 - Test: [   40/   39]    Loss 0.509051    Top1 90.100000    Top5 99.520000    
2018-10-27 01:06:42,856 - ==> Top1: 90.100    Top5: 99.520    Loss: 0.509

2018-10-27 01:06:42,856 - Testing sensitivity of module.layer3.1.conv1.weight [65.0% sparsity]
2018-10-27 01:06:42,859 - --- test ---------------------
2018-10-27 01:06:42,859 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:43,331 - Test: [   10/   39]    Loss 0.513393    Top1 89.648438    Top5 99.375000    
2018-10-27 01:06:43,436 - Test: [   20/   39]    Loss 0.492039    Top1 89.726562    Top5 99.453125    
2018-10-27 01:06:43,545 - Test: [   30/   39]    Loss 0.485697    Top1 89.752604    Top5 99.505208    
2018-10-27 01:06:43,642 - Test: [   40/   39]    Loss 0.485070    Top1 89.750000    Top5 99.510000    
2018-10-27 01:06:43,670 - ==> Top1: 89.750    Top5: 99.510    Loss: 0.485

2018-10-27 01:06:43,671 - Testing sensitivity of module.layer3.1.conv1.weight [70.0% sparsity]
2018-10-27 01:06:43,675 - --- test ---------------------
2018-10-27 01:06:43,676 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:44,154 - Test: [   10/   39]    Loss 0.528753    Top1 87.500000    Top5 99.101562    
2018-10-27 01:06:44,261 - Test: [   20/   39]    Loss 0.509440    Top1 87.734375    Top5 99.238281    
2018-10-27 01:06:44,366 - Test: [   30/   39]    Loss 0.497434    Top1 87.929688    Top5 99.322917    
2018-10-27 01:06:44,461 - Test: [   40/   39]    Loss 0.496980    Top1 87.990000    Top5 99.310000    
2018-10-27 01:06:44,488 - ==> Top1: 87.990    Top5: 99.310    Loss: 0.497

2018-10-27 01:06:44,489 - Testing sensitivity of module.layer3.1.conv1.weight [75.0% sparsity]
2018-10-27 01:06:44,491 - --- test ---------------------
2018-10-27 01:06:44,492 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:44,961 - Test: [   10/   39]    Loss 0.621404    Top1 82.460938    Top5 98.320312    
2018-10-27 01:06:45,064 - Test: [   20/   39]    Loss 0.613641    Top1 83.261719    Top5 98.554688    
2018-10-27 01:06:45,165 - Test: [   30/   39]    Loss 0.601421    Top1 83.333333    Top5 98.723958    
2018-10-27 01:06:45,264 - Test: [   40/   39]    Loss 0.598686    Top1 83.150000    Top5 98.740000    
2018-10-27 01:06:45,309 - ==> Top1: 83.150    Top5: 98.740    Loss: 0.599

2018-10-27 01:06:45,309 - Testing sensitivity of module.layer3.1.conv1.weight [80.0% sparsity]
2018-10-27 01:06:45,314 - --- test ---------------------
2018-10-27 01:06:45,314 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:45,793 - Test: [   10/   39]    Loss 0.840344    Top1 73.750000    Top5 97.500000    
2018-10-27 01:06:45,905 - Test: [   20/   39]    Loss 0.843939    Top1 73.828125    Top5 97.500000    
2018-10-27 01:06:46,013 - Test: [   30/   39]    Loss 0.830288    Top1 74.088542    Top5 97.747396    
2018-10-27 01:06:46,115 - Test: [   40/   39]    Loss 0.820585    Top1 73.940000    Top5 97.680000    
2018-10-27 01:06:46,164 - ==> Top1: 73.940    Top5: 97.680    Loss: 0.821

2018-10-27 01:06:46,164 - Testing sensitivity of module.layer3.1.conv1.weight [85.0% sparsity]
2018-10-27 01:06:46,167 - --- test ---------------------
2018-10-27 01:06:46,167 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:46,711 - Test: [   10/   39]    Loss 1.382127    Top1 52.734375    Top5 93.125000    
2018-10-27 01:06:46,822 - Test: [   20/   39]    Loss 1.382789    Top1 52.578125    Top5 92.910156    
2018-10-27 01:06:46,931 - Test: [   30/   39]    Loss 1.380710    Top1 52.421875    Top5 92.929688    
2018-10-27 01:06:47,030 - Test: [   40/   39]    Loss 1.370631    Top1 52.420000    Top5 92.650000    
2018-10-27 01:06:47,078 - ==> Top1: 52.420    Top5: 92.650    Loss: 1.371

2018-10-27 01:06:47,079 - Testing sensitivity of module.layer3.1.conv1.weight [90.0% sparsity]
2018-10-27 01:06:47,082 - --- test ---------------------
2018-10-27 01:06:47,083 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:47,557 - Test: [   10/   39]    Loss 2.093847    Top1 28.632813    Top5 79.218750    
2018-10-27 01:06:47,660 - Test: [   20/   39]    Loss 2.078132    Top1 29.218750    Top5 79.726562    
2018-10-27 01:06:47,768 - Test: [   30/   39]    Loss 2.079967    Top1 29.192708    Top5 79.830729    
2018-10-27 01:06:47,869 - Test: [   40/   39]    Loss 2.076877    Top1 29.180000    Top5 79.490000    
2018-10-27 01:06:47,937 - ==> Top1: 29.180    Top5: 79.490    Loss: 2.077

2018-10-27 01:06:47,953 - Testing sensitivity of module.layer3.1.conv2.weight [0.0% sparsity]
2018-10-27 01:06:47,957 - --- test ---------------------
2018-10-27 01:06:47,957 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:48,534 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:06:48,648 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:06:48,755 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:06:48,849 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:06:48,877 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:06:48,877 - Testing sensitivity of module.layer3.1.conv2.weight [5.0% sparsity]
2018-10-27 01:06:48,880 - --- test ---------------------
2018-10-27 01:06:48,880 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:49,354 - Test: [   10/   39]    Loss 0.552108    Top1 91.289062    Top5 99.687500    
2018-10-27 01:06:49,458 - Test: [   20/   39]    Loss 0.553079    Top1 91.542969    Top5 99.570312    
2018-10-27 01:06:49,561 - Test: [   30/   39]    Loss 0.546483    Top1 91.523438    Top5 99.661458    
2018-10-27 01:06:49,662 - Test: [   40/   39]    Loss 0.541783    Top1 91.530000    Top5 99.640000    
2018-10-27 01:06:49,710 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:06:49,711 - Testing sensitivity of module.layer3.1.conv2.weight [10.0% sparsity]
2018-10-27 01:06:49,713 - --- test ---------------------
2018-10-27 01:06:49,714 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:50,163 - Test: [   10/   39]    Loss 0.552021    Top1 91.367188    Top5 99.687500    
2018-10-27 01:06:50,269 - Test: [   20/   39]    Loss 0.552851    Top1 91.582031    Top5 99.570312    
2018-10-27 01:06:50,370 - Test: [   30/   39]    Loss 0.545961    Top1 91.549479    Top5 99.661458    
2018-10-27 01:06:50,462 - Test: [   40/   39]    Loss 0.541653    Top1 91.540000    Top5 99.650000    
2018-10-27 01:06:50,490 - ==> Top1: 91.540    Top5: 99.650    Loss: 0.542

2018-10-27 01:06:50,491 - Testing sensitivity of module.layer3.1.conv2.weight [15.0% sparsity]
2018-10-27 01:06:50,494 - --- test ---------------------
2018-10-27 01:06:50,494 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:50,926 - Test: [   10/   39]    Loss 0.552834    Top1 91.367188    Top5 99.687500    
2018-10-27 01:06:51,031 - Test: [   20/   39]    Loss 0.553208    Top1 91.562500    Top5 99.570312    
2018-10-27 01:06:51,131 - Test: [   30/   39]    Loss 0.546210    Top1 91.536458    Top5 99.661458    
2018-10-27 01:06:51,227 - Test: [   40/   39]    Loss 0.541708    Top1 91.530000    Top5 99.650000    
2018-10-27 01:06:51,264 - ==> Top1: 91.530    Top5: 99.650    Loss: 0.542

2018-10-27 01:06:51,265 - Testing sensitivity of module.layer3.1.conv2.weight [20.0% sparsity]
2018-10-27 01:06:51,268 - --- test ---------------------
2018-10-27 01:06:51,268 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:51,733 - Test: [   10/   39]    Loss 0.553081    Top1 91.250000    Top5 99.687500    
2018-10-27 01:06:51,840 - Test: [   20/   39]    Loss 0.553207    Top1 91.484375    Top5 99.570312    
2018-10-27 01:06:51,944 - Test: [   30/   39]    Loss 0.546284    Top1 91.445312    Top5 99.661458    
2018-10-27 01:06:52,037 - Test: [   40/   39]    Loss 0.541346    Top1 91.470000    Top5 99.650000    
2018-10-27 01:06:52,069 - ==> Top1: 91.470    Top5: 99.650    Loss: 0.541

2018-10-27 01:06:52,069 - Testing sensitivity of module.layer3.1.conv2.weight [25.0% sparsity]
2018-10-27 01:06:52,072 - --- test ---------------------
2018-10-27 01:06:52,072 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:52,564 - Test: [   10/   39]    Loss 0.552755    Top1 91.210938    Top5 99.648438    
2018-10-27 01:06:52,669 - Test: [   20/   39]    Loss 0.552937    Top1 91.484375    Top5 99.550781    
2018-10-27 01:06:52,770 - Test: [   30/   39]    Loss 0.544917    Top1 91.406250    Top5 99.635417    
2018-10-27 01:06:52,867 - Test: [   40/   39]    Loss 0.540596    Top1 91.420000    Top5 99.630000    
2018-10-27 01:06:52,894 - ==> Top1: 91.420    Top5: 99.630    Loss: 0.541

2018-10-27 01:06:52,895 - Testing sensitivity of module.layer3.1.conv2.weight [30.0% sparsity]
2018-10-27 01:06:52,898 - --- test ---------------------
2018-10-27 01:06:52,899 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:53,371 - Test: [   10/   39]    Loss 0.554242    Top1 91.054688    Top5 99.648438    
2018-10-27 01:06:53,476 - Test: [   20/   39]    Loss 0.553957    Top1 91.347656    Top5 99.550781    
2018-10-27 01:06:53,578 - Test: [   30/   39]    Loss 0.546084    Top1 91.341146    Top5 99.661458    
2018-10-27 01:06:53,673 - Test: [   40/   39]    Loss 0.541906    Top1 91.330000    Top5 99.650000    
2018-10-27 01:06:53,704 - ==> Top1: 91.330    Top5: 99.650    Loss: 0.542

2018-10-27 01:06:53,704 - Testing sensitivity of module.layer3.1.conv2.weight [35.0% sparsity]
2018-10-27 01:06:53,707 - --- test ---------------------
2018-10-27 01:06:53,708 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:54,181 - Test: [   10/   39]    Loss 0.565393    Top1 91.132812    Top5 99.609375    
2018-10-27 01:06:54,288 - Test: [   20/   39]    Loss 0.566901    Top1 91.406250    Top5 99.531250    
2018-10-27 01:06:54,389 - Test: [   30/   39]    Loss 0.557652    Top1 91.341146    Top5 99.648438    
2018-10-27 01:06:54,481 - Test: [   40/   39]    Loss 0.553617    Top1 91.360000    Top5 99.640000    
2018-10-27 01:06:54,509 - ==> Top1: 91.360    Top5: 99.640    Loss: 0.554

2018-10-27 01:06:54,510 - Testing sensitivity of module.layer3.1.conv2.weight [40.0% sparsity]
2018-10-27 01:06:54,512 - --- test ---------------------
2018-10-27 01:06:54,512 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:54,965 - Test: [   10/   39]    Loss 0.565768    Top1 91.210938    Top5 99.609375    
2018-10-27 01:06:55,067 - Test: [   20/   39]    Loss 0.568276    Top1 91.367188    Top5 99.511719    
2018-10-27 01:06:55,168 - Test: [   30/   39]    Loss 0.559365    Top1 91.367188    Top5 99.622396    
2018-10-27 01:06:55,264 - Test: [   40/   39]    Loss 0.554289    Top1 91.320000    Top5 99.610000    
2018-10-27 01:06:55,292 - ==> Top1: 91.320    Top5: 99.610    Loss: 0.554

2018-10-27 01:06:55,293 - Testing sensitivity of module.layer3.1.conv2.weight [45.0% sparsity]
2018-10-27 01:06:55,296 - --- test ---------------------
2018-10-27 01:06:55,296 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:55,737 - Test: [   10/   39]    Loss 0.568638    Top1 91.289062    Top5 99.609375    
2018-10-27 01:06:55,842 - Test: [   20/   39]    Loss 0.572690    Top1 91.484375    Top5 99.531250    
2018-10-27 01:06:55,943 - Test: [   30/   39]    Loss 0.562561    Top1 91.393229    Top5 99.648438    
2018-10-27 01:06:56,038 - Test: [   40/   39]    Loss 0.557728    Top1 91.360000    Top5 99.640000    
2018-10-27 01:06:56,084 - ==> Top1: 91.360    Top5: 99.640    Loss: 0.558

2018-10-27 01:06:56,084 - Testing sensitivity of module.layer3.1.conv2.weight [50.0% sparsity]
2018-10-27 01:06:56,088 - --- test ---------------------
2018-10-27 01:06:56,088 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:56,632 - Test: [   10/   39]    Loss 0.576200    Top1 91.289062    Top5 99.648438    
2018-10-27 01:06:56,745 - Test: [   20/   39]    Loss 0.579954    Top1 91.406250    Top5 99.550781    
2018-10-27 01:06:56,854 - Test: [   30/   39]    Loss 0.569129    Top1 91.419271    Top5 99.648438    
2018-10-27 01:06:56,955 - Test: [   40/   39]    Loss 0.565542    Top1 91.360000    Top5 99.640000    
2018-10-27 01:06:56,987 - ==> Top1: 91.360    Top5: 99.640    Loss: 0.566

2018-10-27 01:06:56,987 - Testing sensitivity of module.layer3.1.conv2.weight [55.0% sparsity]
2018-10-27 01:06:56,990 - --- test ---------------------
2018-10-27 01:06:56,991 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:57,551 - Test: [   10/   39]    Loss 0.597315    Top1 91.015625    Top5 99.648438    
2018-10-27 01:06:57,663 - Test: [   20/   39]    Loss 0.599861    Top1 91.269531    Top5 99.570312    
2018-10-27 01:06:57,771 - Test: [   30/   39]    Loss 0.590425    Top1 91.184896    Top5 99.648438    
2018-10-27 01:06:57,872 - Test: [   40/   39]    Loss 0.583917    Top1 91.170000    Top5 99.640000    
2018-10-27 01:06:57,903 - ==> Top1: 91.170    Top5: 99.640    Loss: 0.584

2018-10-27 01:06:57,904 - Testing sensitivity of module.layer3.1.conv2.weight [60.0% sparsity]
2018-10-27 01:06:57,907 - --- test ---------------------
2018-10-27 01:06:57,907 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:58,443 - Test: [   10/   39]    Loss 0.620879    Top1 90.781250    Top5 99.531250    
2018-10-27 01:06:58,556 - Test: [   20/   39]    Loss 0.628038    Top1 91.015625    Top5 99.492188    
2018-10-27 01:06:58,665 - Test: [   30/   39]    Loss 0.616060    Top1 90.976562    Top5 99.596354    
2018-10-27 01:06:58,766 - Test: [   40/   39]    Loss 0.608462    Top1 90.860000    Top5 99.600000    
2018-10-27 01:06:58,797 - ==> Top1: 90.860    Top5: 99.600    Loss: 0.608

2018-10-27 01:06:58,797 - Testing sensitivity of module.layer3.1.conv2.weight [65.0% sparsity]
2018-10-27 01:06:58,801 - --- test ---------------------
2018-10-27 01:06:58,801 - 10000 samples (256 per mini-batch)
2018-10-27 01:06:59,285 - Test: [   10/   39]    Loss 0.630801    Top1 90.703125    Top5 99.531250    
2018-10-27 01:06:59,390 - Test: [   20/   39]    Loss 0.640766    Top1 90.937500    Top5 99.472656    
2018-10-27 01:06:59,491 - Test: [   30/   39]    Loss 0.629552    Top1 90.989583    Top5 99.583333    
2018-10-27 01:06:59,584 - Test: [   40/   39]    Loss 0.623204    Top1 90.940000    Top5 99.580000    
2018-10-27 01:06:59,613 - ==> Top1: 90.940    Top5: 99.580    Loss: 0.623

2018-10-27 01:06:59,613 - Testing sensitivity of module.layer3.1.conv2.weight [70.0% sparsity]
2018-10-27 01:06:59,616 - --- test ---------------------
2018-10-27 01:06:59,616 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:00,117 - Test: [   10/   39]    Loss 0.666570    Top1 90.351562    Top5 99.531250    
2018-10-27 01:07:00,220 - Test: [   20/   39]    Loss 0.682824    Top1 90.585938    Top5 99.394531    
2018-10-27 01:07:00,321 - Test: [   30/   39]    Loss 0.677716    Top1 90.664062    Top5 99.505208    
2018-10-27 01:07:00,415 - Test: [   40/   39]    Loss 0.669609    Top1 90.580000    Top5 99.520000    
2018-10-27 01:07:00,444 - ==> Top1: 90.580    Top5: 99.520    Loss: 0.670

2018-10-27 01:07:00,445 - Testing sensitivity of module.layer3.1.conv2.weight [75.0% sparsity]
2018-10-27 01:07:00,448 - --- test ---------------------
2018-10-27 01:07:00,449 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:00,905 - Test: [   10/   39]    Loss 0.685902    Top1 90.117188    Top5 99.296875    
2018-10-27 01:07:01,010 - Test: [   20/   39]    Loss 0.707340    Top1 90.195312    Top5 99.238281    
2018-10-27 01:07:01,110 - Test: [   30/   39]    Loss 0.703484    Top1 90.182292    Top5 99.375000    
2018-10-27 01:07:01,202 - Test: [   40/   39]    Loss 0.693830    Top1 90.130000    Top5 99.430000    
2018-10-27 01:07:01,230 - ==> Top1: 90.130    Top5: 99.430    Loss: 0.694

2018-10-27 01:07:01,231 - Testing sensitivity of module.layer3.1.conv2.weight [80.0% sparsity]
2018-10-27 01:07:01,234 - --- test ---------------------
2018-10-27 01:07:01,234 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:01,671 - Test: [   10/   39]    Loss 0.721456    Top1 89.687500    Top5 99.062500    
2018-10-27 01:07:01,774 - Test: [   20/   39]    Loss 0.740842    Top1 89.511719    Top5 99.042969    
2018-10-27 01:07:01,874 - Test: [   30/   39]    Loss 0.739388    Top1 89.531250    Top5 99.127604    
2018-10-27 01:07:01,966 - Test: [   40/   39]    Loss 0.731628    Top1 89.610000    Top5 99.220000    
2018-10-27 01:07:01,994 - ==> Top1: 89.610    Top5: 99.220    Loss: 0.732

2018-10-27 01:07:01,995 - Testing sensitivity of module.layer3.1.conv2.weight [85.0% sparsity]
2018-10-27 01:07:01,998 - --- test ---------------------
2018-10-27 01:07:01,999 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:02,451 - Test: [   10/   39]    Loss 0.934307    Top1 87.539062    Top5 98.593750    
2018-10-27 01:07:02,555 - Test: [   20/   39]    Loss 0.953185    Top1 87.519531    Top5 98.632812    
2018-10-27 01:07:02,655 - Test: [   30/   39]    Loss 0.953695    Top1 87.434896    Top5 98.750000    
2018-10-27 01:07:02,747 - Test: [   40/   39]    Loss 0.935124    Top1 87.550000    Top5 98.820000    
2018-10-27 01:07:02,775 - ==> Top1: 87.550    Top5: 98.820    Loss: 0.935

2018-10-27 01:07:02,775 - Testing sensitivity of module.layer3.1.conv2.weight [90.0% sparsity]
2018-10-27 01:07:02,778 - --- test ---------------------
2018-10-27 01:07:02,778 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:03,221 - Test: [   10/   39]    Loss 1.162421    Top1 84.882812    Top5 98.281250    
2018-10-27 01:07:03,324 - Test: [   20/   39]    Loss 1.173670    Top1 84.882812    Top5 98.007812    
2018-10-27 01:07:03,425 - Test: [   30/   39]    Loss 1.179024    Top1 84.726562    Top5 98.151042    
2018-10-27 01:07:03,517 - Test: [   40/   39]    Loss 1.153622    Top1 84.690000    Top5 98.090000    
2018-10-27 01:07:03,545 - ==> Top1: 84.690    Top5: 98.090    Loss: 1.154

2018-10-27 01:07:03,560 - Testing sensitivity of module.layer3.2.conv1.weight [0.0% sparsity]
2018-10-27 01:07:03,563 - --- test ---------------------
2018-10-27 01:07:03,563 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:04,001 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:07:04,104 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:07:04,204 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:07:04,296 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:07:04,323 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:07:04,324 - Testing sensitivity of module.layer3.2.conv1.weight [5.0% sparsity]
2018-10-27 01:07:04,326 - --- test ---------------------
2018-10-27 01:07:04,326 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:04,771 - Test: [   10/   39]    Loss 0.551884    Top1 91.250000    Top5 99.687500    
2018-10-27 01:07:04,875 - Test: [   20/   39]    Loss 0.552988    Top1 91.484375    Top5 99.570312    
2018-10-27 01:07:04,975 - Test: [   30/   39]    Loss 0.546363    Top1 91.484375    Top5 99.661458    
2018-10-27 01:07:05,067 - Test: [   40/   39]    Loss 0.541719    Top1 91.490000    Top5 99.640000    
2018-10-27 01:07:05,095 - ==> Top1: 91.490    Top5: 99.640    Loss: 0.542

2018-10-27 01:07:05,095 - Testing sensitivity of module.layer3.2.conv1.weight [10.0% sparsity]
2018-10-27 01:07:05,097 - --- test ---------------------
2018-10-27 01:07:05,097 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:05,557 - Test: [   10/   39]    Loss 0.551798    Top1 91.250000    Top5 99.687500    
2018-10-27 01:07:05,659 - Test: [   20/   39]    Loss 0.552801    Top1 91.503906    Top5 99.570312    
2018-10-27 01:07:05,760 - Test: [   30/   39]    Loss 0.546312    Top1 91.484375    Top5 99.661458    
2018-10-27 01:07:05,853 - Test: [   40/   39]    Loss 0.541211    Top1 91.500000    Top5 99.640000    
2018-10-27 01:07:05,880 - ==> Top1: 91.500    Top5: 99.640    Loss: 0.541

2018-10-27 01:07:05,881 - Testing sensitivity of module.layer3.2.conv1.weight [15.0% sparsity]
2018-10-27 01:07:05,884 - --- test ---------------------
2018-10-27 01:07:05,884 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:06,334 - Test: [   10/   39]    Loss 0.551648    Top1 91.171875    Top5 99.687500    
2018-10-27 01:07:06,446 - Test: [   20/   39]    Loss 0.552110    Top1 91.445312    Top5 99.589844    
2018-10-27 01:07:06,556 - Test: [   30/   39]    Loss 0.545192    Top1 91.484375    Top5 99.674479    
2018-10-27 01:07:06,656 - Test: [   40/   39]    Loss 0.540030    Top1 91.530000    Top5 99.650000    
2018-10-27 01:07:06,685 - ==> Top1: 91.530    Top5: 99.650    Loss: 0.540

2018-10-27 01:07:06,686 - Testing sensitivity of module.layer3.2.conv1.weight [20.0% sparsity]
2018-10-27 01:07:06,689 - --- test ---------------------
2018-10-27 01:07:06,690 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:07,138 - Test: [   10/   39]    Loss 0.550490    Top1 91.210938    Top5 99.687500    
2018-10-27 01:07:07,243 - Test: [   20/   39]    Loss 0.550659    Top1 91.464844    Top5 99.589844    
2018-10-27 01:07:07,342 - Test: [   30/   39]    Loss 0.543946    Top1 91.510417    Top5 99.674479    
2018-10-27 01:07:07,434 - Test: [   40/   39]    Loss 0.539096    Top1 91.540000    Top5 99.660000    
2018-10-27 01:07:07,462 - ==> Top1: 91.540    Top5: 99.660    Loss: 0.539

2018-10-27 01:07:07,463 - Testing sensitivity of module.layer3.2.conv1.weight [25.0% sparsity]
2018-10-27 01:07:07,466 - --- test ---------------------
2018-10-27 01:07:07,466 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:07,914 - Test: [   10/   39]    Loss 0.552868    Top1 91.132812    Top5 99.687500    
2018-10-27 01:07:08,018 - Test: [   20/   39]    Loss 0.553522    Top1 91.406250    Top5 99.589844    
2018-10-27 01:07:08,119 - Test: [   30/   39]    Loss 0.546340    Top1 91.523438    Top5 99.661458    
2018-10-27 01:07:08,211 - Test: [   40/   39]    Loss 0.541835    Top1 91.580000    Top5 99.640000    
2018-10-27 01:07:08,239 - ==> Top1: 91.580    Top5: 99.640    Loss: 0.542

2018-10-27 01:07:08,240 - Testing sensitivity of module.layer3.2.conv1.weight [30.0% sparsity]
2018-10-27 01:07:08,242 - --- test ---------------------
2018-10-27 01:07:08,242 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:08,683 - Test: [   10/   39]    Loss 0.552480    Top1 91.054688    Top5 99.687500    
2018-10-27 01:07:08,787 - Test: [   20/   39]    Loss 0.555037    Top1 91.308594    Top5 99.589844    
2018-10-27 01:07:08,887 - Test: [   30/   39]    Loss 0.548043    Top1 91.406250    Top5 99.648438    
2018-10-27 01:07:08,978 - Test: [   40/   39]    Loss 0.544621    Top1 91.430000    Top5 99.630000    
2018-10-27 01:07:09,006 - ==> Top1: 91.430    Top5: 99.630    Loss: 0.545

2018-10-27 01:07:09,007 - Testing sensitivity of module.layer3.2.conv1.weight [35.0% sparsity]
2018-10-27 01:07:09,010 - --- test ---------------------
2018-10-27 01:07:09,011 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:09,438 - Test: [   10/   39]    Loss 0.551352    Top1 91.132812    Top5 99.648438    
2018-10-27 01:07:09,543 - Test: [   20/   39]    Loss 0.555736    Top1 91.328125    Top5 99.570312    
2018-10-27 01:07:09,645 - Test: [   30/   39]    Loss 0.549577    Top1 91.380208    Top5 99.622396    
2018-10-27 01:07:09,739 - Test: [   40/   39]    Loss 0.546225    Top1 91.420000    Top5 99.580000    
2018-10-27 01:07:09,767 - ==> Top1: 91.420    Top5: 99.580    Loss: 0.546

2018-10-27 01:07:09,768 - Testing sensitivity of module.layer3.2.conv1.weight [40.0% sparsity]
2018-10-27 01:07:09,771 - --- test ---------------------
2018-10-27 01:07:09,771 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:10,224 - Test: [   10/   39]    Loss 0.556320    Top1 90.976562    Top5 99.531250    
2018-10-27 01:07:10,327 - Test: [   20/   39]    Loss 0.559501    Top1 91.191406    Top5 99.492188    
2018-10-27 01:07:10,427 - Test: [   30/   39]    Loss 0.553688    Top1 91.276042    Top5 99.570312    
2018-10-27 01:07:10,519 - Test: [   40/   39]    Loss 0.550328    Top1 91.340000    Top5 99.550000    
2018-10-27 01:07:10,546 - ==> Top1: 91.340    Top5: 99.550    Loss: 0.550

2018-10-27 01:07:10,547 - Testing sensitivity of module.layer3.2.conv1.weight [45.0% sparsity]
2018-10-27 01:07:10,550 - --- test ---------------------
2018-10-27 01:07:10,551 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:10,990 - Test: [   10/   39]    Loss 0.559655    Top1 90.820312    Top5 99.570312    
2018-10-27 01:07:11,098 - Test: [   20/   39]    Loss 0.562943    Top1 91.074219    Top5 99.511719    
2018-10-27 01:07:11,206 - Test: [   30/   39]    Loss 0.555487    Top1 91.171875    Top5 99.570312    
2018-10-27 01:07:11,305 - Test: [   40/   39]    Loss 0.552824    Top1 91.190000    Top5 99.540000    
2018-10-27 01:07:11,343 - ==> Top1: 91.190    Top5: 99.540    Loss: 0.553

2018-10-27 01:07:11,344 - Testing sensitivity of module.layer3.2.conv1.weight [50.0% sparsity]
2018-10-27 01:07:11,347 - --- test ---------------------
2018-10-27 01:07:11,347 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:11,805 - Test: [   10/   39]    Loss 0.559013    Top1 90.781250    Top5 99.609375    
2018-10-27 01:07:11,910 - Test: [   20/   39]    Loss 0.563632    Top1 90.937500    Top5 99.550781    
2018-10-27 01:07:12,010 - Test: [   30/   39]    Loss 0.555227    Top1 91.054688    Top5 99.609375    
2018-10-27 01:07:12,103 - Test: [   40/   39]    Loss 0.553040    Top1 91.110000    Top5 99.580000    
2018-10-27 01:07:12,130 - ==> Top1: 91.110    Top5: 99.580    Loss: 0.553

2018-10-27 01:07:12,131 - Testing sensitivity of module.layer3.2.conv1.weight [55.0% sparsity]
2018-10-27 01:07:12,134 - --- test ---------------------
2018-10-27 01:07:12,135 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:12,574 - Test: [   10/   39]    Loss 0.578963    Top1 90.507812    Top5 99.570312    
2018-10-27 01:07:12,678 - Test: [   20/   39]    Loss 0.581728    Top1 90.625000    Top5 99.492188    
2018-10-27 01:07:12,778 - Test: [   30/   39]    Loss 0.571788    Top1 90.820312    Top5 99.557292    
2018-10-27 01:07:12,875 - Test: [   40/   39]    Loss 0.570517    Top1 90.860000    Top5 99.530000    
2018-10-27 01:07:12,903 - ==> Top1: 90.860    Top5: 99.530    Loss: 0.571

2018-10-27 01:07:12,904 - Testing sensitivity of module.layer3.2.conv1.weight [60.0% sparsity]
2018-10-27 01:07:12,907 - --- test ---------------------
2018-10-27 01:07:12,907 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:13,368 - Test: [   10/   39]    Loss 0.582350    Top1 90.351562    Top5 99.453125    
2018-10-27 01:07:13,473 - Test: [   20/   39]    Loss 0.586994    Top1 90.546875    Top5 99.414062    
2018-10-27 01:07:13,575 - Test: [   30/   39]    Loss 0.580307    Top1 90.664062    Top5 99.492188    
2018-10-27 01:07:13,668 - Test: [   40/   39]    Loss 0.576658    Top1 90.800000    Top5 99.470000    
2018-10-27 01:07:13,695 - ==> Top1: 90.800    Top5: 99.470    Loss: 0.577

2018-10-27 01:07:13,696 - Testing sensitivity of module.layer3.2.conv1.weight [65.0% sparsity]
2018-10-27 01:07:13,699 - --- test ---------------------
2018-10-27 01:07:13,699 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:14,174 - Test: [   10/   39]    Loss 0.605743    Top1 89.921875    Top5 99.453125    
2018-10-27 01:07:14,277 - Test: [   20/   39]    Loss 0.608979    Top1 90.312500    Top5 99.414062    
2018-10-27 01:07:14,379 - Test: [   30/   39]    Loss 0.603040    Top1 90.468750    Top5 99.453125    
2018-10-27 01:07:14,478 - Test: [   40/   39]    Loss 0.597291    Top1 90.620000    Top5 99.420000    
2018-10-27 01:07:14,516 - ==> Top1: 90.620    Top5: 99.420    Loss: 0.597

2018-10-27 01:07:14,517 - Testing sensitivity of module.layer3.2.conv1.weight [70.0% sparsity]
2018-10-27 01:07:14,519 - --- test ---------------------
2018-10-27 01:07:14,519 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:14,955 - Test: [   10/   39]    Loss 0.608688    Top1 90.000000    Top5 99.453125    
2018-10-27 01:07:15,060 - Test: [   20/   39]    Loss 0.610541    Top1 90.195312    Top5 99.316406    
2018-10-27 01:07:15,160 - Test: [   30/   39]    Loss 0.608526    Top1 90.104167    Top5 99.348958    
2018-10-27 01:07:15,252 - Test: [   40/   39]    Loss 0.599698    Top1 90.360000    Top5 99.320000    
2018-10-27 01:07:15,290 - ==> Top1: 90.360    Top5: 99.320    Loss: 0.600

2018-10-27 01:07:15,291 - Testing sensitivity of module.layer3.2.conv1.weight [75.0% sparsity]
2018-10-27 01:07:15,293 - --- test ---------------------
2018-10-27 01:07:15,293 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:15,722 - Test: [   10/   39]    Loss 0.656666    Top1 88.750000    Top5 99.179688    
2018-10-27 01:07:15,828 - Test: [   20/   39]    Loss 0.656010    Top1 89.140625    Top5 99.082031    
2018-10-27 01:07:15,928 - Test: [   30/   39]    Loss 0.657255    Top1 89.140625    Top5 99.114583    
2018-10-27 01:07:16,020 - Test: [   40/   39]    Loss 0.649090    Top1 89.300000    Top5 99.090000    
2018-10-27 01:07:16,049 - ==> Top1: 89.300    Top5: 99.090    Loss: 0.649

2018-10-27 01:07:16,050 - Testing sensitivity of module.layer3.2.conv1.weight [80.0% sparsity]
2018-10-27 01:07:16,053 - --- test ---------------------
2018-10-27 01:07:16,053 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:16,489 - Test: [   10/   39]    Loss 0.771211    Top1 86.718750    Top5 98.593750    
2018-10-27 01:07:16,591 - Test: [   20/   39]    Loss 0.761390    Top1 87.187500    Top5 98.652344    
2018-10-27 01:07:16,691 - Test: [   30/   39]    Loss 0.758194    Top1 87.096354    Top5 98.671875    
2018-10-27 01:07:16,783 - Test: [   40/   39]    Loss 0.747355    Top1 87.270000    Top5 98.640000    
2018-10-27 01:07:16,811 - ==> Top1: 87.270    Top5: 98.640    Loss: 0.747

2018-10-27 01:07:16,812 - Testing sensitivity of module.layer3.2.conv1.weight [85.0% sparsity]
2018-10-27 01:07:16,815 - --- test ---------------------
2018-10-27 01:07:16,815 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:17,250 - Test: [   10/   39]    Loss 0.947379    Top1 84.218750    Top5 97.343750    
2018-10-27 01:07:17,352 - Test: [   20/   39]    Loss 0.940061    Top1 84.492188    Top5 97.519531    
2018-10-27 01:07:17,453 - Test: [   30/   39]    Loss 0.936173    Top1 84.661458    Top5 97.591146    
2018-10-27 01:07:17,546 - Test: [   40/   39]    Loss 0.924307    Top1 84.900000    Top5 97.690000    
2018-10-27 01:07:17,576 - ==> Top1: 84.900    Top5: 97.690    Loss: 0.924

2018-10-27 01:07:17,577 - Testing sensitivity of module.layer3.2.conv1.weight [90.0% sparsity]
2018-10-27 01:07:17,580 - --- test ---------------------
2018-10-27 01:07:17,581 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:18,014 - Test: [   10/   39]    Loss 1.304796    Top1 77.656250    Top5 95.742188    
2018-10-27 01:07:18,117 - Test: [   20/   39]    Loss 1.298894    Top1 77.441406    Top5 96.210938    
2018-10-27 01:07:18,218 - Test: [   30/   39]    Loss 1.300279    Top1 77.460938    Top5 96.171875    
2018-10-27 01:07:18,310 - Test: [   40/   39]    Loss 1.289059    Top1 77.690000    Top5 96.250000    
2018-10-27 01:07:18,337 - ==> Top1: 77.690    Top5: 96.250    Loss: 1.289

2018-10-27 01:07:18,352 - Testing sensitivity of module.layer3.2.conv2.weight [0.0% sparsity]
2018-10-27 01:07:18,356 - --- test ---------------------
2018-10-27 01:07:18,356 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:18,766 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:07:18,870 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:07:18,970 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:07:19,062 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:07:19,090 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:07:19,091 - Testing sensitivity of module.layer3.2.conv2.weight [5.0% sparsity]
2018-10-27 01:07:19,094 - --- test ---------------------
2018-10-27 01:07:19,094 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:19,533 - Test: [   10/   39]    Loss 0.551923    Top1 91.289062    Top5 99.687500    
2018-10-27 01:07:19,636 - Test: [   20/   39]    Loss 0.552994    Top1 91.542969    Top5 99.570312    
2018-10-27 01:07:19,736 - Test: [   30/   39]    Loss 0.546340    Top1 91.523438    Top5 99.661458    
2018-10-27 01:07:19,829 - Test: [   40/   39]    Loss 0.541609    Top1 91.530000    Top5 99.640000    
2018-10-27 01:07:19,858 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:07:19,858 - Testing sensitivity of module.layer3.2.conv2.weight [10.0% sparsity]
2018-10-27 01:07:19,861 - --- test ---------------------
2018-10-27 01:07:19,862 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:20,295 - Test: [   10/   39]    Loss 0.551975    Top1 91.289062    Top5 99.687500    
2018-10-27 01:07:20,399 - Test: [   20/   39]    Loss 0.553033    Top1 91.542969    Top5 99.570312    
2018-10-27 01:07:20,500 - Test: [   30/   39]    Loss 0.546401    Top1 91.523438    Top5 99.661458    
2018-10-27 01:07:20,591 - Test: [   40/   39]    Loss 0.541655    Top1 91.520000    Top5 99.640000    
2018-10-27 01:07:20,618 - ==> Top1: 91.520    Top5: 99.640    Loss: 0.542

2018-10-27 01:07:20,619 - Testing sensitivity of module.layer3.2.conv2.weight [15.0% sparsity]
2018-10-27 01:07:20,623 - --- test ---------------------
2018-10-27 01:07:20,623 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:21,065 - Test: [   10/   39]    Loss 0.551958    Top1 91.289062    Top5 99.687500    
2018-10-27 01:07:21,169 - Test: [   20/   39]    Loss 0.552948    Top1 91.542969    Top5 99.570312    
2018-10-27 01:07:21,277 - Test: [   30/   39]    Loss 0.546224    Top1 91.523438    Top5 99.661458    
2018-10-27 01:07:21,373 - Test: [   40/   39]    Loss 0.541491    Top1 91.520000    Top5 99.640000    
2018-10-27 01:07:21,401 - ==> Top1: 91.520    Top5: 99.640    Loss: 0.541

2018-10-27 01:07:21,402 - Testing sensitivity of module.layer3.2.conv2.weight [20.0% sparsity]
2018-10-27 01:07:21,405 - --- test ---------------------
2018-10-27 01:07:21,405 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:21,854 - Test: [   10/   39]    Loss 0.551817    Top1 91.250000    Top5 99.687500    
2018-10-27 01:07:21,958 - Test: [   20/   39]    Loss 0.552932    Top1 91.523438    Top5 99.589844    
2018-10-27 01:07:22,058 - Test: [   30/   39]    Loss 0.546164    Top1 91.510417    Top5 99.674479    
2018-10-27 01:07:22,150 - Test: [   40/   39]    Loss 0.541584    Top1 91.510000    Top5 99.650000    
2018-10-27 01:07:22,177 - ==> Top1: 91.510    Top5: 99.650    Loss: 0.542

2018-10-27 01:07:22,179 - Testing sensitivity of module.layer3.2.conv2.weight [25.0% sparsity]
2018-10-27 01:07:22,182 - --- test ---------------------
2018-10-27 01:07:22,183 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:22,615 - Test: [   10/   39]    Loss 0.551875    Top1 91.250000    Top5 99.687500    
2018-10-27 01:07:22,718 - Test: [   20/   39]    Loss 0.553136    Top1 91.523438    Top5 99.589844    
2018-10-27 01:07:22,818 - Test: [   30/   39]    Loss 0.546346    Top1 91.510417    Top5 99.674479    
2018-10-27 01:07:22,910 - Test: [   40/   39]    Loss 0.541790    Top1 91.500000    Top5 99.650000    
2018-10-27 01:07:22,938 - ==> Top1: 91.500    Top5: 99.650    Loss: 0.542

2018-10-27 01:07:22,939 - Testing sensitivity of module.layer3.2.conv2.weight [30.0% sparsity]
2018-10-27 01:07:22,942 - --- test ---------------------
2018-10-27 01:07:22,942 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:23,385 - Test: [   10/   39]    Loss 0.551260    Top1 91.210938    Top5 99.687500    
2018-10-27 01:07:23,489 - Test: [   20/   39]    Loss 0.552424    Top1 91.503906    Top5 99.570312    
2018-10-27 01:07:23,598 - Test: [   30/   39]    Loss 0.545525    Top1 91.510417    Top5 99.661458    
2018-10-27 01:07:23,700 - Test: [   40/   39]    Loss 0.541024    Top1 91.520000    Top5 99.640000    
2018-10-27 01:07:23,728 - ==> Top1: 91.520    Top5: 99.640    Loss: 0.541

2018-10-27 01:07:23,728 - Testing sensitivity of module.layer3.2.conv2.weight [35.0% sparsity]
2018-10-27 01:07:23,731 - --- test ---------------------
2018-10-27 01:07:23,732 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:24,170 - Test: [   10/   39]    Loss 0.549862    Top1 91.171875    Top5 99.687500    
2018-10-27 01:07:24,275 - Test: [   20/   39]    Loss 0.551232    Top1 91.464844    Top5 99.570312    
2018-10-27 01:07:24,375 - Test: [   30/   39]    Loss 0.544250    Top1 91.497396    Top5 99.661458    
2018-10-27 01:07:24,467 - Test: [   40/   39]    Loss 0.539648    Top1 91.500000    Top5 99.640000    
2018-10-27 01:07:24,495 - ==> Top1: 91.500    Top5: 99.640    Loss: 0.540

2018-10-27 01:07:24,496 - Testing sensitivity of module.layer3.2.conv2.weight [40.0% sparsity]
2018-10-27 01:07:24,499 - --- test ---------------------
2018-10-27 01:07:24,499 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:24,948 - Test: [   10/   39]    Loss 0.548975    Top1 91.289062    Top5 99.648438    
2018-10-27 01:07:25,051 - Test: [   20/   39]    Loss 0.550298    Top1 91.523438    Top5 99.550781    
2018-10-27 01:07:25,152 - Test: [   30/   39]    Loss 0.543099    Top1 91.510417    Top5 99.648438    
2018-10-27 01:07:25,244 - Test: [   40/   39]    Loss 0.538187    Top1 91.510000    Top5 99.630000    
2018-10-27 01:07:25,272 - ==> Top1: 91.510    Top5: 99.630    Loss: 0.538

2018-10-27 01:07:25,273 - Testing sensitivity of module.layer3.2.conv2.weight [45.0% sparsity]
2018-10-27 01:07:25,277 - --- test ---------------------
2018-10-27 01:07:25,278 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:25,723 - Test: [   10/   39]    Loss 0.547801    Top1 91.210938    Top5 99.648438    
2018-10-27 01:07:25,827 - Test: [   20/   39]    Loss 0.548790    Top1 91.503906    Top5 99.550781    
2018-10-27 01:07:25,928 - Test: [   30/   39]    Loss 0.541369    Top1 91.458333    Top5 99.648438    
2018-10-27 01:07:26,020 - Test: [   40/   39]    Loss 0.536214    Top1 91.440000    Top5 99.630000    
2018-10-27 01:07:26,048 - ==> Top1: 91.440    Top5: 99.630    Loss: 0.536

2018-10-27 01:07:26,049 - Testing sensitivity of module.layer3.2.conv2.weight [50.0% sparsity]
2018-10-27 01:07:26,052 - --- test ---------------------
2018-10-27 01:07:26,052 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:26,498 - Test: [   10/   39]    Loss 0.546347    Top1 91.132812    Top5 99.648438    
2018-10-27 01:07:26,603 - Test: [   20/   39]    Loss 0.547258    Top1 91.425781    Top5 99.550781    
2018-10-27 01:07:26,703 - Test: [   30/   39]    Loss 0.539878    Top1 91.432292    Top5 99.648438    
2018-10-27 01:07:26,800 - Test: [   40/   39]    Loss 0.534597    Top1 91.420000    Top5 99.630000    
2018-10-27 01:07:26,843 - ==> Top1: 91.420    Top5: 99.630    Loss: 0.535

2018-10-27 01:07:26,845 - Testing sensitivity of module.layer3.2.conv2.weight [55.0% sparsity]
2018-10-27 01:07:26,848 - --- test ---------------------
2018-10-27 01:07:26,848 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:27,287 - Test: [   10/   39]    Loss 0.543338    Top1 91.132812    Top5 99.648438    
2018-10-27 01:07:27,393 - Test: [   20/   39]    Loss 0.544270    Top1 91.425781    Top5 99.550781    
2018-10-27 01:07:27,493 - Test: [   30/   39]    Loss 0.536545    Top1 91.432292    Top5 99.648438    
2018-10-27 01:07:27,586 - Test: [   40/   39]    Loss 0.530950    Top1 91.420000    Top5 99.630000    
2018-10-27 01:07:27,614 - ==> Top1: 91.420    Top5: 99.630    Loss: 0.531

2018-10-27 01:07:27,615 - Testing sensitivity of module.layer3.2.conv2.weight [60.0% sparsity]
2018-10-27 01:07:27,618 - --- test ---------------------
2018-10-27 01:07:27,619 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:28,064 - Test: [   10/   39]    Loss 0.538922    Top1 91.093750    Top5 99.687500    
2018-10-27 01:07:28,169 - Test: [   20/   39]    Loss 0.539996    Top1 91.406250    Top5 99.609375    
2018-10-27 01:07:28,269 - Test: [   30/   39]    Loss 0.532544    Top1 91.354167    Top5 99.687500    
2018-10-27 01:07:28,361 - Test: [   40/   39]    Loss 0.526580    Top1 91.340000    Top5 99.660000    
2018-10-27 01:07:28,389 - ==> Top1: 91.340    Top5: 99.660    Loss: 0.527

2018-10-27 01:07:28,390 - Testing sensitivity of module.layer3.2.conv2.weight [65.0% sparsity]
2018-10-27 01:07:28,393 - --- test ---------------------
2018-10-27 01:07:28,394 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:28,833 - Test: [   10/   39]    Loss 0.535000    Top1 91.054688    Top5 99.687500    
2018-10-27 01:07:28,937 - Test: [   20/   39]    Loss 0.536063    Top1 91.406250    Top5 99.609375    
2018-10-27 01:07:29,037 - Test: [   30/   39]    Loss 0.528558    Top1 91.341146    Top5 99.674479    
2018-10-27 01:07:29,129 - Test: [   40/   39]    Loss 0.522547    Top1 91.300000    Top5 99.650000    
2018-10-27 01:07:29,158 - ==> Top1: 91.300    Top5: 99.650    Loss: 0.523

2018-10-27 01:07:29,159 - Testing sensitivity of module.layer3.2.conv2.weight [70.0% sparsity]
2018-10-27 01:07:29,162 - --- test ---------------------
2018-10-27 01:07:29,163 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:29,606 - Test: [   10/   39]    Loss 0.527977    Top1 90.976562    Top5 99.687500    
2018-10-27 01:07:29,710 - Test: [   20/   39]    Loss 0.529001    Top1 91.367188    Top5 99.609375    
2018-10-27 01:07:29,810 - Test: [   30/   39]    Loss 0.521099    Top1 91.341146    Top5 99.674479    
2018-10-27 01:07:29,903 - Test: [   40/   39]    Loss 0.515142    Top1 91.310000    Top5 99.660000    
2018-10-27 01:07:29,936 - ==> Top1: 91.310    Top5: 99.660    Loss: 0.515

2018-10-27 01:07:29,937 - Testing sensitivity of module.layer3.2.conv2.weight [75.0% sparsity]
2018-10-27 01:07:29,939 - --- test ---------------------
2018-10-27 01:07:29,940 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:30,431 - Test: [   10/   39]    Loss 0.517881    Top1 91.054688    Top5 99.570312    
2018-10-27 01:07:30,538 - Test: [   20/   39]    Loss 0.518096    Top1 91.367188    Top5 99.550781    
2018-10-27 01:07:30,643 - Test: [   30/   39]    Loss 0.511623    Top1 91.315104    Top5 99.635417    
2018-10-27 01:07:30,740 - Test: [   40/   39]    Loss 0.504990    Top1 91.300000    Top5 99.630000    
2018-10-27 01:07:30,768 - ==> Top1: 91.300    Top5: 99.630    Loss: 0.505

2018-10-27 01:07:30,769 - Testing sensitivity of module.layer3.2.conv2.weight [80.0% sparsity]
2018-10-27 01:07:30,771 - --- test ---------------------
2018-10-27 01:07:30,772 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:31,220 - Test: [   10/   39]    Loss 0.503168    Top1 90.898438    Top5 99.570312    
2018-10-27 01:07:31,373 - Test: [   20/   39]    Loss 0.505687    Top1 91.269531    Top5 99.550781    
2018-10-27 01:07:31,483 - Test: [   30/   39]    Loss 0.499800    Top1 91.223958    Top5 99.648438    
2018-10-27 01:07:31,584 - Test: [   40/   39]    Loss 0.492872    Top1 91.180000    Top5 99.620000    
2018-10-27 01:07:31,613 - ==> Top1: 91.180    Top5: 99.620    Loss: 0.493

2018-10-27 01:07:31,613 - Testing sensitivity of module.layer3.2.conv2.weight [85.0% sparsity]
2018-10-27 01:07:31,617 - --- test ---------------------
2018-10-27 01:07:31,617 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:32,093 - Test: [   10/   39]    Loss 0.486891    Top1 90.820312    Top5 99.648438    
2018-10-27 01:07:32,205 - Test: [   20/   39]    Loss 0.492018    Top1 91.074219    Top5 99.531250    
2018-10-27 01:07:32,314 - Test: [   30/   39]    Loss 0.486419    Top1 90.950521    Top5 99.622396    
2018-10-27 01:07:32,415 - Test: [   40/   39]    Loss 0.479313    Top1 90.890000    Top5 99.580000    
2018-10-27 01:07:32,448 - ==> Top1: 90.890    Top5: 99.580    Loss: 0.479

2018-10-27 01:07:32,450 - Testing sensitivity of module.layer3.2.conv2.weight [90.0% sparsity]
2018-10-27 01:07:32,453 - --- test ---------------------
2018-10-27 01:07:32,453 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:32,920 - Test: [   10/   39]    Loss 0.462028    Top1 90.390625    Top5 99.453125    
2018-10-27 01:07:33,032 - Test: [   20/   39]    Loss 0.472656    Top1 90.585938    Top5 99.492188    
2018-10-27 01:07:33,141 - Test: [   30/   39]    Loss 0.468852    Top1 90.507812    Top5 99.531250    
2018-10-27 01:07:33,242 - Test: [   40/   39]    Loss 0.463962    Top1 90.460000    Top5 99.500000    
2018-10-27 01:07:33,285 - ==> Top1: 90.460    Top5: 99.500    Loss: 0.464

2018-10-27 01:07:33,295 - Testing sensitivity of module.fc.weight [0.0% sparsity]
2018-10-27 01:07:33,298 - --- test ---------------------
2018-10-27 01:07:33,298 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:33,699 - Test: [   10/   39]    Loss 0.551863    Top1 91.289062    Top5 99.687500    
2018-10-27 01:07:33,802 - Test: [   20/   39]    Loss 0.552944    Top1 91.542969    Top5 99.570312    
2018-10-27 01:07:33,902 - Test: [   30/   39]    Loss 0.546295    Top1 91.523438    Top5 99.661458    
2018-10-27 01:07:33,994 - Test: [   40/   39]    Loss 0.541567    Top1 91.530000    Top5 99.640000    
2018-10-27 01:07:34,022 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.542

2018-10-27 01:07:34,023 - Testing sensitivity of module.fc.weight [5.0% sparsity]
2018-10-27 01:07:34,025 - --- test ---------------------
2018-10-27 01:07:34,026 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:34,472 - Test: [   10/   39]    Loss 0.552207    Top1 91.250000    Top5 99.687500    
2018-10-27 01:07:34,575 - Test: [   20/   39]    Loss 0.553405    Top1 91.503906    Top5 99.570312    
2018-10-27 01:07:34,676 - Test: [   30/   39]    Loss 0.546818    Top1 91.497396    Top5 99.661458    
2018-10-27 01:07:34,768 - Test: [   40/   39]    Loss 0.542230    Top1 91.510000    Top5 99.640000    
2018-10-27 01:07:34,796 - ==> Top1: 91.510    Top5: 99.640    Loss: 0.542

2018-10-27 01:07:34,796 - Testing sensitivity of module.fc.weight [10.0% sparsity]
2018-10-27 01:07:34,798 - --- test ---------------------
2018-10-27 01:07:34,798 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:35,235 - Test: [   10/   39]    Loss 0.552658    Top1 91.210938    Top5 99.687500    
2018-10-27 01:07:35,338 - Test: [   20/   39]    Loss 0.553972    Top1 91.464844    Top5 99.589844    
2018-10-27 01:07:35,438 - Test: [   30/   39]    Loss 0.547254    Top1 91.484375    Top5 99.674479    
2018-10-27 01:07:35,531 - Test: [   40/   39]    Loss 0.542955    Top1 91.500000    Top5 99.660000    
2018-10-27 01:07:35,558 - ==> Top1: 91.500    Top5: 99.660    Loss: 0.543

2018-10-27 01:07:35,559 - Testing sensitivity of module.fc.weight [15.0% sparsity]
2018-10-27 01:07:35,562 - --- test ---------------------
2018-10-27 01:07:35,563 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:36,018 - Test: [   10/   39]    Loss 0.553128    Top1 91.289062    Top5 99.687500    
2018-10-27 01:07:36,125 - Test: [   20/   39]    Loss 0.554082    Top1 91.503906    Top5 99.589844    
2018-10-27 01:07:36,229 - Test: [   30/   39]    Loss 0.547221    Top1 91.523438    Top5 99.674479    
2018-10-27 01:07:36,325 - Test: [   40/   39]    Loss 0.542942    Top1 91.510000    Top5 99.660000    
2018-10-27 01:07:36,353 - ==> Top1: 91.510    Top5: 99.660    Loss: 0.543

2018-10-27 01:07:36,353 - Testing sensitivity of module.fc.weight [20.0% sparsity]
2018-10-27 01:07:36,356 - --- test ---------------------
2018-10-27 01:07:36,356 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:36,803 - Test: [   10/   39]    Loss 0.552501    Top1 91.328125    Top5 99.687500    
2018-10-27 01:07:36,905 - Test: [   20/   39]    Loss 0.553016    Top1 91.503906    Top5 99.570312    
2018-10-27 01:07:37,005 - Test: [   30/   39]    Loss 0.546422    Top1 91.471354    Top5 99.661458    
2018-10-27 01:07:37,097 - Test: [   40/   39]    Loss 0.542009    Top1 91.460000    Top5 99.640000    
2018-10-27 01:07:37,125 - ==> Top1: 91.460    Top5: 99.640    Loss: 0.542

2018-10-27 01:07:37,126 - Testing sensitivity of module.fc.weight [25.0% sparsity]
2018-10-27 01:07:37,127 - --- test ---------------------
2018-10-27 01:07:37,128 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:37,574 - Test: [   10/   39]    Loss 0.551294    Top1 91.328125    Top5 99.687500    
2018-10-27 01:07:37,678 - Test: [   20/   39]    Loss 0.551385    Top1 91.523438    Top5 99.570312    
2018-10-27 01:07:37,778 - Test: [   30/   39]    Loss 0.544449    Top1 91.549479    Top5 99.661458    
2018-10-27 01:07:37,870 - Test: [   40/   39]    Loss 0.540225    Top1 91.530000    Top5 99.640000    
2018-10-27 01:07:37,898 - ==> Top1: 91.530    Top5: 99.640    Loss: 0.540

2018-10-27 01:07:37,909 - Testing sensitivity of module.fc.weight [30.0% sparsity]
2018-10-27 01:07:37,911 - --- test ---------------------
2018-10-27 01:07:37,911 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:38,347 - Test: [   10/   39]    Loss 0.551521    Top1 91.250000    Top5 99.687500    
2018-10-27 01:07:38,451 - Test: [   20/   39]    Loss 0.550788    Top1 91.464844    Top5 99.570312    
2018-10-27 01:07:38,552 - Test: [   30/   39]    Loss 0.543639    Top1 91.523438    Top5 99.661458    
2018-10-27 01:07:38,645 - Test: [   40/   39]    Loss 0.538792    Top1 91.500000    Top5 99.640000    
2018-10-27 01:07:38,673 - ==> Top1: 91.500    Top5: 99.640    Loss: 0.539

2018-10-27 01:07:38,674 - Testing sensitivity of module.fc.weight [35.0% sparsity]
2018-10-27 01:07:38,677 - --- test ---------------------
2018-10-27 01:07:38,677 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:39,124 - Test: [   10/   39]    Loss 0.548656    Top1 91.367188    Top5 99.648438    
2018-10-27 01:07:39,227 - Test: [   20/   39]    Loss 0.547433    Top1 91.503906    Top5 99.492188    
2018-10-27 01:07:39,328 - Test: [   30/   39]    Loss 0.540675    Top1 91.523438    Top5 99.596354    
2018-10-27 01:07:39,420 - Test: [   40/   39]    Loss 0.536115    Top1 91.510000    Top5 99.590000    
2018-10-27 01:07:39,448 - ==> Top1: 91.510    Top5: 99.590    Loss: 0.536

2018-10-27 01:07:39,449 - Testing sensitivity of module.fc.weight [40.0% sparsity]
2018-10-27 01:07:39,451 - --- test ---------------------
2018-10-27 01:07:39,452 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:39,895 - Test: [   10/   39]    Loss 0.545701    Top1 91.250000    Top5 99.531250    
2018-10-27 01:07:40,001 - Test: [   20/   39]    Loss 0.544080    Top1 91.464844    Top5 99.414062    
2018-10-27 01:07:40,106 - Test: [   30/   39]    Loss 0.538344    Top1 91.484375    Top5 99.544271    
2018-10-27 01:07:40,202 - Test: [   40/   39]    Loss 0.533498    Top1 91.490000    Top5 99.540000    
2018-10-27 01:07:40,230 - ==> Top1: 91.490    Top5: 99.540    Loss: 0.533

2018-10-27 01:07:40,231 - Testing sensitivity of module.fc.weight [45.0% sparsity]
2018-10-27 01:07:40,234 - --- test ---------------------
2018-10-27 01:07:40,234 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:40,767 - Test: [   10/   39]    Loss 0.545720    Top1 91.328125    Top5 99.492188    
2018-10-27 01:07:40,879 - Test: [   20/   39]    Loss 0.543296    Top1 91.464844    Top5 99.453125    
2018-10-27 01:07:40,987 - Test: [   30/   39]    Loss 0.536680    Top1 91.497396    Top5 99.570312    
2018-10-27 01:07:41,085 - Test: [   40/   39]    Loss 0.532391    Top1 91.470000    Top5 99.560000    
2018-10-27 01:07:41,125 - ==> Top1: 91.470    Top5: 99.560    Loss: 0.532

2018-10-27 01:07:41,125 - Testing sensitivity of module.fc.weight [50.0% sparsity]
2018-10-27 01:07:41,128 - --- test ---------------------
2018-10-27 01:07:41,128 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:41,566 - Test: [   10/   39]    Loss 0.542470    Top1 91.250000    Top5 99.492188    
2018-10-27 01:07:41,670 - Test: [   20/   39]    Loss 0.539562    Top1 91.445312    Top5 99.414062    
2018-10-27 01:07:41,770 - Test: [   30/   39]    Loss 0.532978    Top1 91.497396    Top5 99.531250    
2018-10-27 01:07:41,867 - Test: [   40/   39]    Loss 0.527718    Top1 91.500000    Top5 99.520000    
2018-10-27 01:07:41,900 - ==> Top1: 91.500    Top5: 99.520    Loss: 0.528

2018-10-27 01:07:41,901 - Testing sensitivity of module.fc.weight [55.0% sparsity]
2018-10-27 01:07:41,904 - --- test ---------------------
2018-10-27 01:07:41,905 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:42,373 - Test: [   10/   39]    Loss 0.534821    Top1 91.171875    Top5 99.414062    
2018-10-27 01:07:42,484 - Test: [   20/   39]    Loss 0.532297    Top1 91.464844    Top5 99.335938    
2018-10-27 01:07:42,594 - Test: [   30/   39]    Loss 0.526957    Top1 91.484375    Top5 99.505208    
2018-10-27 01:07:42,693 - Test: [   40/   39]    Loss 0.521728    Top1 91.540000    Top5 99.510000    
2018-10-27 01:07:42,721 - ==> Top1: 91.540    Top5: 99.510    Loss: 0.522

2018-10-27 01:07:42,721 - Testing sensitivity of module.fc.weight [60.0% sparsity]
2018-10-27 01:07:42,724 - --- test ---------------------
2018-10-27 01:07:42,724 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:43,157 - Test: [   10/   39]    Loss 0.528587    Top1 91.328125    Top5 99.414062    
2018-10-27 01:07:43,260 - Test: [   20/   39]    Loss 0.526272    Top1 91.464844    Top5 99.335938    
2018-10-27 01:07:43,360 - Test: [   30/   39]    Loss 0.519711    Top1 91.536458    Top5 99.492188    
2018-10-27 01:07:43,452 - Test: [   40/   39]    Loss 0.516182    Top1 91.550000    Top5 99.490000    
2018-10-27 01:07:43,481 - ==> Top1: 91.550    Top5: 99.490    Loss: 0.516

2018-10-27 01:07:43,481 - Testing sensitivity of module.fc.weight [65.0% sparsity]
2018-10-27 01:07:43,484 - --- test ---------------------
2018-10-27 01:07:43,484 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:43,916 - Test: [   10/   39]    Loss 0.520322    Top1 91.289062    Top5 99.375000    
2018-10-27 01:07:44,021 - Test: [   20/   39]    Loss 0.519874    Top1 91.386719    Top5 99.335938    
2018-10-27 01:07:44,122 - Test: [   30/   39]    Loss 0.513876    Top1 91.406250    Top5 99.505208    
2018-10-27 01:07:44,214 - Test: [   40/   39]    Loss 0.511248    Top1 91.410000    Top5 99.510000    
2018-10-27 01:07:44,241 - ==> Top1: 91.410    Top5: 99.510    Loss: 0.511

2018-10-27 01:07:44,242 - Testing sensitivity of module.fc.weight [70.0% sparsity]
2018-10-27 01:07:44,244 - --- test ---------------------
2018-10-27 01:07:44,244 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:44,683 - Test: [   10/   39]    Loss 0.519501    Top1 91.171875    Top5 99.257812    
2018-10-27 01:07:44,788 - Test: [   20/   39]    Loss 0.517990    Top1 91.308594    Top5 99.238281    
2018-10-27 01:07:44,888 - Test: [   30/   39]    Loss 0.512503    Top1 91.354167    Top5 99.375000    
2018-10-27 01:07:44,985 - Test: [   40/   39]    Loss 0.508297    Top1 91.360000    Top5 99.390000    
2018-10-27 01:07:45,014 - ==> Top1: 91.360    Top5: 99.390    Loss: 0.508

2018-10-27 01:07:45,015 - Testing sensitivity of module.fc.weight [75.0% sparsity]
2018-10-27 01:07:45,017 - --- test ---------------------
2018-10-27 01:07:45,017 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:45,467 - Test: [   10/   39]    Loss 0.507411    Top1 91.250000    Top5 99.335938    
2018-10-27 01:07:45,573 - Test: [   20/   39]    Loss 0.507058    Top1 91.347656    Top5 99.296875    
2018-10-27 01:07:45,677 - Test: [   30/   39]    Loss 0.502227    Top1 91.341146    Top5 99.401042    
2018-10-27 01:07:45,773 - Test: [   40/   39]    Loss 0.499353    Top1 91.410000    Top5 99.420000    
2018-10-27 01:07:45,804 - ==> Top1: 91.410    Top5: 99.420    Loss: 0.499

2018-10-27 01:07:45,805 - Testing sensitivity of module.fc.weight [80.0% sparsity]
2018-10-27 01:07:45,808 - --- test ---------------------
2018-10-27 01:07:45,808 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:46,244 - Test: [   10/   39]    Loss 0.497851    Top1 90.937500    Top5 99.296875    
2018-10-27 01:07:46,347 - Test: [   20/   39]    Loss 0.496413    Top1 90.996094    Top5 99.296875    
2018-10-27 01:07:46,447 - Test: [   30/   39]    Loss 0.491973    Top1 91.093750    Top5 99.414062    
2018-10-27 01:07:46,539 - Test: [   40/   39]    Loss 0.486488    Top1 91.200000    Top5 99.450000    
2018-10-27 01:07:46,567 - ==> Top1: 91.200    Top5: 99.450    Loss: 0.486

2018-10-27 01:07:46,568 - Testing sensitivity of module.fc.weight [85.0% sparsity]
2018-10-27 01:07:46,571 - --- test ---------------------
2018-10-27 01:07:46,571 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:47,014 - Test: [   10/   39]    Loss 0.476116    Top1 90.664062    Top5 99.414062    
2018-10-27 01:07:47,117 - Test: [   20/   39]    Loss 0.473834    Top1 91.035156    Top5 99.375000    
2018-10-27 01:07:47,217 - Test: [   30/   39]    Loss 0.467615    Top1 91.093750    Top5 99.453125    
2018-10-27 01:07:47,310 - Test: [   40/   39]    Loss 0.463044    Top1 91.120000    Top5 99.470000    
2018-10-27 01:07:47,338 - ==> Top1: 91.120    Top5: 99.470    Loss: 0.463

2018-10-27 01:07:47,339 - Testing sensitivity of module.fc.weight [90.0% sparsity]
2018-10-27 01:07:47,342 - --- test ---------------------
2018-10-27 01:07:47,342 - 10000 samples (256 per mini-batch)
2018-10-27 01:07:47,780 - Test: [   10/   39]    Loss 0.482963    Top1 89.804688    Top5 99.140625    
2018-10-27 01:07:47,884 - Test: [   20/   39]    Loss 0.483942    Top1 89.765625    Top5 99.179688    
2018-10-27 01:07:47,984 - Test: [   30/   39]    Loss 0.474477    Top1 89.934896    Top5 99.309896    
2018-10-27 01:07:48,076 - Test: [   40/   39]    Loss 0.471949    Top1 90.140000    Top5 99.340000    
2018-10-27 01:07:48,104 - ==> Top1: 90.140    Top5: 99.340    Loss: 0.472

2018-10-27 01:07:48,222 - Generating sensitivity graph
2018-10-27 01:07:48,437 - 
2018-10-27 01:07:48,437 - Log file for this run: /home/ccma/Chilung/1022/distiller/examples/classifier_compression/logs/2018.10.27-010213/2018.10.27-010213.log
