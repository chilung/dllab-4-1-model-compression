2018-10-28 00:24:06,852 - Log file for this run: /home/ccma/Chilung/1022/distiller/examples/classifier_compression/logs/2018.10.28-002406/2018.10.28-002406.log
2018-10-28 00:24:06,852 - Number of CPUs: 8
2018-10-28 00:24:06,862 - Number of GPUs: 1
2018-10-28 00:24:06,862 - CUDA version: 8.0.61
2018-10-28 00:24:06,862 - CUDNN version: 7102
2018-10-28 00:24:06,862 - Kernel: 4.13.0-38-generic
2018-10-28 00:24:06,862 - Python: 3.5.2 (default, Nov 23 2017, 16:37:01) 
[GCC 5.4.0 20160609]
2018-10-28 00:24:06,862 - PyTorch: 0.4.0
2018-10-28 00:24:06,862 - Numpy: 1.14.3
2018-10-28 00:24:06,873 - Git is dirty
2018-10-28 00:24:06,873 - Active Git branch: master
2018-10-28 00:24:06,877 - Git commit: 8bf95d12172fb6e82a00ce40007953e23d9648c7
2018-10-28 00:24:06,877 - App args: ['compress_classifier.py', '--arch', 'resnet20_cifar', '--lr', '0.3', '-p', '50', '../../../data.cifar10', '-b', '128', '-j', '1', '--vs', '0', '--deterministic', '--epochs', '300', '--compress=../../../resnet20_cifar_baseline/resnet20_cifar_baseline.yaml']
2018-10-28 00:24:06,878 - ==> using cifar10 dataset
2018-10-28 00:24:06,878 - => creating resnet20_cifar model for CIFAR10
2018-10-28 00:24:09,267 - Optimizer Type: <class 'torch.optim.sgd.SGD'>
2018-10-28 00:24:09,267 - Optimizer Args: {'nesterov': False, 'weight_decay': 0.0001, 'momentum': 0.9, 'dampening': 0, 'lr': 0.3}
2018-10-28 00:24:10,497 - Dataset sizes:
	training=50000
	validation=10000
	test=10000
2018-10-28 00:24:10,498 - Reading compression schedule from: ../../../resnet20_cifar_baseline/resnet20_cifar_baseline.yaml
2018-10-28 00:24:10,539 - Schedule contents:
{
  "pruners": {
    "pruner1": {
      "class": "SensitivityPruner",
      "sensitivities": {
        "module.conv1.weight": 0.4,
        "module.layer1.0.conv1.weight": 0.35,
        "module.layer1.0.conv2.weight": 0.3,
        "module.layer1.1.conv1.weight": 0.2,
        "module.layer1.1.conv2.weight": 0.5,
        "module.layer1.2.conv1.weight": 0.25,
        "module.layer1.2.conv2.weight": 0.45,
        "module.layer2.0.conv1.weight": 0.2,
        "module.layer2.0.conv2.weight": 0.1,
        "module.layer2.0.downsample.0.weight": 0.25,
        "module.layer2.1.conv1.weight": 0.25,
        "module.layer2.1.conv2.weight": 0.3,
        "module.layer2.2.conv1.weight": 0.2,
        "module.layer2.2.conv2.weight": 0.15,
        "module.layer3.0.conv1.weight": 0.35,
        "module.layer3.0.conv2.weight": 0.15,
        "module.layer3.0.downsample.0.weight": 0.3,
        "module.layer3.1.conv1.weight": 0.25,
        "module.layer3.1.conv2.weight": 0.15,
        "module.layer3.2.conv1.weight": 0.25,
        "module.layer3.2.conv2.weight": 0.4,
        "module.fc.weight": 0.6
      }
    }
  },
  "lr_schedulers": {
    "training_lr": {
      "class": "MultiStepMultiGammaLR",
      "milestones": [
        100,
        200,
        250
      ],
      "gammas": [
        0.1,
        0.1,
        0.5
      ]
    }
  },
  "policies": [
    {
      "pruner": {
        "instance_name": "pruner1"
      },
      "starting_epoch": 0,
      "ending_epoch": 38,
      "frequency": 2
    },
    {
      "lr_scheduler": {
        "instance_name": "training_lr"
      },
      "starting_epoch": 0,
      "ending_epoch": 301,
      "frequency": 1
    }
  ]
}
2018-10-28 00:24:10,540 - 

2018-10-28 00:24:10,545 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:24:11,719 - Epoch: [0][   50/  391]    Overall Loss 2.162933    Objective Loss 2.162933    Top1 20.312500    Top5 72.265625    LR 0.300000    Time 0.023446    
2018-10-28 00:24:12,830 - Epoch: [0][  100/  391]    Overall Loss 2.034101    Objective Loss 2.034101    Top1 23.359375    Top5 77.695312    LR 0.300000    Time 0.022820    
2018-10-28 00:24:13,943 - Epoch: [0][  150/  391]    Overall Loss 1.946634    Objective Loss 1.946634    Top1 26.229167    Top5 80.614583    LR 0.300000    Time 0.022627    
2018-10-28 00:24:15,058 - Epoch: [0][  200/  391]    Overall Loss 1.894574    Objective Loss 1.894574    Top1 27.945312    Top5 82.406250    LR 0.300000    Time 0.022536    
2018-10-28 00:24:16,170 - Epoch: [0][  250/  391]    Overall Loss 1.845000    Objective Loss 1.845000    Top1 30.100000    Top5 83.734375    LR 0.300000    Time 0.022475    
2018-10-28 00:24:17,289 - Epoch: [0][  300/  391]    Overall Loss 1.798600    Objective Loss 1.798600    Top1 31.976563    Top5 84.945312    LR 0.300000    Time 0.022452    
2018-10-28 00:24:18,408 - Epoch: [0][  350/  391]    Overall Loss 1.757612    Objective Loss 1.757612    Top1 33.667411    Top5 85.928571    LR 0.300000    Time 0.022440    
2018-10-28 00:24:19,407 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            311 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   28.00926 | 0.41555 |  0.00987 |    0.28252 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |           1672 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   27.43056 | 0.15937 | -0.00658 |    0.10759 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |           1752 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   23.95833 | 0.15315 |  0.00078 |    0.10734 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |           1984 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   13.88889 | 0.15706 | -0.00028 |    0.11445 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |           1432 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   37.84722 | 0.13619 | -0.00425 |    0.08734 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |           1827 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   20.70312 | 0.13597 |  0.00167 |    0.09754 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |           1515 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   34.24479 | 0.12686 | -0.00451 |    0.08462 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           3861 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   16.21094 | 0.10038 | -0.00804 |    0.07471 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           8504 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    7.72569 | 0.09294 | -0.00307 |    0.07158 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            413 |    0.00000 |    0.00000 |  0.00000 | 19.33594 |  0.00000 |   19.33594 | 0.26466 | -0.00152 |    0.19511 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           7328 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   20.48611 | 0.09348 | -0.00527 |    0.06742 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           7031 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   23.70877 | 0.09063 | -0.00310 |    0.06502 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           7755 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   15.85286 | 0.08823 | -0.00144 |    0.06587 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           8114 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   11.95747 | 0.08722 | -0.00313 |    0.06648 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |          13418 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   27.20269 | 0.06532 | -0.00113 |    0.04571 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          32448 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   11.97917 | 0.06282 | -0.00227 |    0.04745 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |           1546 |    0.00000 |    0.00000 |  0.00000 | 24.51172 |  0.00000 |   24.51172 | 0.18545 | -0.00874 |    0.13369 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          29520 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   19.92188 | 0.06035 |  0.00048 |    0.04435 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          32483 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   11.88422 | 0.06181 | -0.00020 |    0.04679 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          29577 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   19.76725 | 0.05618 |  0.00201 |    0.04191 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |          25457 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   30.94347 | 0.05686 | -0.00844 |    0.04016 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            419 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   34.53125 | 0.31327 | -0.03865 |    0.19639 |
| 22 | Total sparsity:                     | -              |        270896 |         218367 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   19.39084 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:24:19,407 - Total sparsity: 19.39

2018-10-28 00:24:19,407 - --- validate (epoch=0)-----------
2018-10-28 00:24:19,408 - 10000 samples (128 per mini-batch)
2018-10-28 00:24:20,179 - Epoch: [0][   50/   78]    Loss 1.507260    Top1 45.109375    Top5 92.453125    
2018-10-28 00:24:20,573 - ==> Top1: 44.410    Top5: 92.320    Loss: 1.512

2018-10-28 00:24:20,574 - ==> Best Top1: 44.410   On Epoch: 0

2018-10-28 00:24:20,574 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:24:20,643 - 

2018-10-28 00:24:20,643 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:24:21,771 - Epoch: [1][   50/  391]    Overall Loss 1.420911    Objective Loss 1.420911    Top1 48.312500    Top5 92.765625    LR 0.300000    Time 0.022519    
2018-10-28 00:24:22,867 - Epoch: [1][  100/  391]    Overall Loss 1.365012    Objective Loss 1.365012    Top1 50.296875    Top5 93.484375    LR 0.300000    Time 0.022212    
2018-10-28 00:24:23,965 - Epoch: [1][  150/  391]    Overall Loss 1.327529    Objective Loss 1.327529    Top1 51.901042    Top5 93.932292    LR 0.300000    Time 0.022118    
2018-10-28 00:24:25,063 - Epoch: [1][  200/  391]    Overall Loss 1.303306    Objective Loss 1.303306    Top1 52.769531    Top5 94.191406    LR 0.300000    Time 0.022070    
2018-10-28 00:24:26,162 - Epoch: [1][  250/  391]    Overall Loss 1.271224    Objective Loss 1.271224    Top1 54.018750    Top5 94.525000    LR 0.300000    Time 0.022047    
2018-10-28 00:24:27,263 - Epoch: [1][  300/  391]    Overall Loss 1.247215    Objective Loss 1.247215    Top1 54.992188    Top5 94.718750    LR 0.300000    Time 0.022039    
2018-10-28 00:24:28,365 - Epoch: [1][  350/  391]    Overall Loss 1.222303    Objective Loss 1.222303    Top1 55.955357    Top5 94.881696    LR 0.300000    Time 0.022035    
2018-10-28 00:24:29,347 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            311 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   28.00926 | 0.50625 | -0.00171 |    0.33552 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |           1672 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   27.43056 | 0.17994 | -0.00993 |    0.11793 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |           1752 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   23.95833 | 0.17553 | -0.00216 |    0.11810 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |           1984 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   13.88889 | 0.17949 | -0.01075 |    0.12666 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |           1432 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   37.84722 | 0.15546 | -0.01170 |    0.09635 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |           1827 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   20.70312 | 0.15586 | -0.00619 |    0.10658 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |           1515 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   34.24479 | 0.13950 | -0.01221 |    0.09011 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           3861 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   16.21094 | 0.12356 | -0.01145 |    0.09013 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           8504 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |    7.72569 | 0.10758 | -0.00682 |    0.08144 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            413 |    0.00000 |    0.00000 |  0.00000 | 19.33594 |  0.00000 |   19.33594 | 0.30237 | -0.00068 |    0.22164 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           7328 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   20.48611 | 0.10502 | -0.00793 |    0.07444 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           7031 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   23.70877 | 0.09897 | -0.00369 |    0.06934 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           7755 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   15.85286 | 0.09632 | -0.00305 |    0.07066 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           8114 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   11.95747 | 0.09269 | -0.00484 |    0.06982 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |          13418 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   27.20269 | 0.07832 | -0.00393 |    0.05345 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          32448 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   11.97917 | 0.07018 | -0.00436 |    0.05172 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |           1546 |    0.00000 |    0.00000 |  0.00000 | 24.51172 |  0.00000 |   24.51172 | 0.19240 | -0.01472 |    0.13489 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          29520 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   19.92188 | 0.06620 | -0.00080 |    0.04713 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          32483 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   11.88422 | 0.06499 |  0.00026 |    0.04806 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          29577 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   19.76725 | 0.05395 |  0.00219 |    0.03953 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |          25457 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   30.94347 | 0.05397 | -0.00871 |    0.03756 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            419 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   34.53125 | 0.41456 | -0.04855 |    0.27058 |
| 22 | Total sparsity:                     | -              |        270896 |         218367 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   19.39084 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:24:29,347 - Total sparsity: 19.39

2018-10-28 00:24:29,347 - --- validate (epoch=1)-----------
2018-10-28 00:24:29,347 - 10000 samples (128 per mini-batch)
2018-10-28 00:24:30,066 - Epoch: [1][   50/   78]    Loss 1.248342    Top1 58.437500    Top5 94.359375    
2018-10-28 00:24:30,456 - ==> Top1: 58.310    Top5: 94.390    Loss: 1.244

2018-10-28 00:24:30,456 - ==> Best Top1: 58.310   On Epoch: 1

2018-10-28 00:24:30,456 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:24:30,474 - 

2018-10-28 00:24:30,476 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:24:31,635 - Epoch: [2][   50/  391]    Overall Loss 1.036464    Objective Loss 1.036464    Top1 63.171875    Top5 96.484375    LR 0.300000    Time 0.023136    
2018-10-28 00:24:32,759 - Epoch: [2][  100/  391]    Overall Loss 1.023976    Objective Loss 1.023976    Top1 63.421875    Top5 96.523438    LR 0.300000    Time 0.022801    
2018-10-28 00:24:33,887 - Epoch: [2][  150/  391]    Overall Loss 1.003990    Objective Loss 1.003990    Top1 64.161458    Top5 96.692708    LR 0.300000    Time 0.022710    
2018-10-28 00:24:35,014 - Epoch: [2][  200/  391]    Overall Loss 0.991452    Objective Loss 0.991452    Top1 64.722656    Top5 96.750000    LR 0.300000    Time 0.022661    
2018-10-28 00:24:36,138 - Epoch: [2][  250/  391]    Overall Loss 0.978985    Objective Loss 0.978985    Top1 65.128125    Top5 96.828125    LR 0.300000    Time 0.022622    
2018-10-28 00:24:37,265 - Epoch: [2][  300/  391]    Overall Loss 0.967048    Objective Loss 0.967048    Top1 65.606771    Top5 96.895833    LR 0.300000    Time 0.022603    
2018-10-28 00:24:38,389 - Epoch: [2][  350/  391]    Overall Loss 0.955535    Objective Loss 0.955535    Top1 66.051339    Top5 96.995536    LR 0.300000    Time 0.022584    
2018-10-28 00:24:39,391 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            296 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   31.48148 | 0.54065 | -0.00228 |    0.34634 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |           1399 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   39.27951 | 0.18796 | -0.01316 |    0.11517 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |           1490 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   35.32986 | 0.18581 | -0.00376 |    0.11793 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |           1763 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   23.48090 | 0.18930 | -0.01316 |    0.12711 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |           1097 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   52.38715 | 0.16274 | -0.01224 |    0.09179 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |           1557 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   32.42188 | 0.16507 | -0.00493 |    0.10565 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |           1147 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   50.21701 | 0.14386 | -0.01284 |    0.08471 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           3480 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   24.47917 | 0.14107 | -0.01004 |    0.09864 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           7968 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   13.54167 | 0.12102 | -0.00907 |    0.08956 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            362 |    0.00000 |    0.00000 |  0.00000 | 29.29688 |  0.00000 |   29.29688 | 0.32331 | -0.00443 |    0.22630 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           6271 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   31.95530 | 0.11243 | -0.00954 |    0.07543 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           5830 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   36.74045 | 0.10398 | -0.00268 |    0.06813 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           6771 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   26.52995 | 0.10219 | -0.00441 |    0.07112 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           7323 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   20.54036 | 0.09643 | -0.00555 |    0.06989 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |          11038 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   40.11502 | 0.08970 | -0.00488 |    0.05689 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          29343 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   20.40202 | 0.07859 | -0.00583 |    0.05530 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |           1265 |    0.00000 |    0.00000 |  0.00000 | 38.23242 |  0.00000 |   38.23242 | 0.19513 | -0.01718 |    0.12831 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          25127 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   31.83865 | 0.07243 | -0.00202 |    0.04814 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          29138 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   20.95812 | 0.06825 |  0.00064 |    0.04818 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          24709 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   32.97255 | 0.05182 |  0.00194 |    0.03598 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |          19628 |    0.00000 |    0.00000 |  0.00000 |  0.14648 |  0.00000 |   46.75564 | 0.05086 | -0.00830 |    0.03295 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            393 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   38.59375 | 0.45623 | -0.05061 |    0.29795 |
| 22 | Total sparsity:                     | -              |        270896 |         187395 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   30.82401 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:24:39,391 - Total sparsity: 30.82

2018-10-28 00:24:39,391 - --- validate (epoch=2)-----------
2018-10-28 00:24:39,391 - 10000 samples (128 per mini-batch)
2018-10-28 00:24:40,108 - Epoch: [2][   50/   78]    Loss 0.989186    Top1 65.734375    Top5 97.609375    
2018-10-28 00:24:40,498 - ==> Top1: 66.150    Top5: 97.840    Loss: 0.968

2018-10-28 00:24:40,499 - ==> Best Top1: 66.150   On Epoch: 2

2018-10-28 00:24:40,499 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:24:40,521 - 

2018-10-28 00:24:40,521 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:24:41,654 - Epoch: [3][   50/  391]    Overall Loss 0.849294    Objective Loss 0.849294    Top1 70.078125    Top5 97.984375    LR 0.300000    Time 0.022623    
2018-10-28 00:24:42,757 - Epoch: [3][  100/  391]    Overall Loss 0.849244    Objective Loss 0.849244    Top1 70.078125    Top5 97.812500    LR 0.300000    Time 0.022326    
2018-10-28 00:24:43,862 - Epoch: [3][  150/  391]    Overall Loss 0.841942    Objective Loss 0.841942    Top1 70.395833    Top5 97.765625    LR 0.300000    Time 0.022241    
2018-10-28 00:24:44,967 - Epoch: [3][  200/  391]    Overall Loss 0.837459    Objective Loss 0.837459    Top1 70.496094    Top5 97.863281    LR 0.300000    Time 0.022198    
2018-10-28 00:24:46,072 - Epoch: [3][  250/  391]    Overall Loss 0.830318    Objective Loss 0.830318    Top1 70.796875    Top5 97.878125    LR 0.300000    Time 0.022173    
2018-10-28 00:24:47,175 - Epoch: [3][  300/  391]    Overall Loss 0.822574    Objective Loss 0.822574    Top1 71.114583    Top5 97.932292    LR 0.300000    Time 0.022151    
2018-10-28 00:24:48,279 - Epoch: [3][  350/  391]    Overall Loss 0.814992    Objective Loss 0.814992    Top1 71.390625    Top5 97.977679    LR 0.300000    Time 0.022137    
2018-10-28 00:24:49,264 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            296 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   31.48148 | 0.56153 |  0.00723 |    0.35608 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |           1399 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   39.27951 | 0.19336 | -0.01022 |    0.11505 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |           1490 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   35.32986 | 0.19280 | -0.00459 |    0.11812 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |           1763 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   23.48090 | 0.19671 | -0.01467 |    0.12920 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |           1097 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   52.38715 | 0.16824 | -0.01193 |    0.09203 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |           1557 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   32.42188 | 0.16977 | -0.00744 |    0.10680 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |           1147 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   50.21701 | 0.14748 | -0.00990 |    0.08397 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           3480 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   24.47917 | 0.15298 | -0.01042 |    0.10592 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           7968 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   13.54167 | 0.13167 | -0.00994 |    0.09644 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            362 |    0.00000 |    0.00000 |  0.00000 | 29.29688 |  0.00000 |   29.29688 | 0.33750 | -0.00159 |    0.23485 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           6271 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   31.95530 | 0.11883 | -0.01026 |    0.07834 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           5830 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   36.74045 | 0.10872 | -0.00199 |    0.06983 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           6771 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   26.52995 | 0.10825 | -0.00660 |    0.07360 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           7323 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   20.54036 | 0.09993 | -0.00584 |    0.07160 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |          11038 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   40.11502 | 0.10037 | -0.00574 |    0.06234 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          29343 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   20.40202 | 0.08717 | -0.00644 |    0.06025 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |           1265 |    0.00000 |    0.00000 |  0.00000 | 38.23242 |  0.00000 |   38.23242 | 0.19799 | -0.01664 |    0.12612 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          25127 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   31.83865 | 0.07964 | -0.00289 |    0.05160 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          29138 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   20.95812 | 0.07151 |  0.00104 |    0.04943 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          24709 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   32.97255 | 0.05059 |  0.00169 |    0.03427 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |          19628 |    0.00000 |    0.00000 |  0.00000 |  0.14648 |  0.00000 |   46.75564 | 0.04880 | -0.00810 |    0.03074 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            393 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   38.59375 | 0.48338 | -0.04909 |    0.31714 |
| 22 | Total sparsity:                     | -              |        270896 |         187395 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   30.82401 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:24:49,265 - Total sparsity: 30.82

2018-10-28 00:24:49,265 - --- validate (epoch=3)-----------
2018-10-28 00:24:49,265 - 10000 samples (128 per mini-batch)
2018-10-28 00:24:49,987 - Epoch: [3][   50/   78]    Loss 0.791446    Top1 73.281250    Top5 98.156250    
2018-10-28 00:24:50,377 - ==> Top1: 73.090    Top5: 98.230    Loss: 0.791

2018-10-28 00:24:50,378 - ==> Best Top1: 73.090   On Epoch: 3

2018-10-28 00:24:50,378 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:24:50,400 - 

2018-10-28 00:24:50,401 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:24:51,560 - Epoch: [4][   50/  391]    Overall Loss 0.743263    Objective Loss 0.743263    Top1 74.046875    Top5 98.203125    LR 0.300000    Time 0.023139    
2018-10-28 00:24:52,689 - Epoch: [4][  100/  391]    Overall Loss 0.731665    Objective Loss 0.731665    Top1 74.343750    Top5 98.328125    LR 0.300000    Time 0.022849    
2018-10-28 00:24:53,819 - Epoch: [4][  150/  391]    Overall Loss 0.722831    Objective Loss 0.722831    Top1 74.869792    Top5 98.395833    LR 0.300000    Time 0.022753    
2018-10-28 00:24:54,946 - Epoch: [4][  200/  391]    Overall Loss 0.719439    Objective Loss 0.719439    Top1 75.050781    Top5 98.355469    LR 0.300000    Time 0.022696    
2018-10-28 00:24:56,078 - Epoch: [4][  250/  391]    Overall Loss 0.715382    Objective Loss 0.715382    Top1 75.078125    Top5 98.446875    LR 0.300000    Time 0.022677    
2018-10-28 00:24:57,205 - Epoch: [4][  300/  391]    Overall Loss 0.716282    Objective Loss 0.716282    Top1 75.039062    Top5 98.385417    LR 0.300000    Time 0.022653    
2018-10-28 00:24:58,335 - Epoch: [4][  350/  391]    Overall Loss 0.711137    Objective Loss 0.711137    Top1 75.283482    Top5 98.412946    LR 0.300000    Time 0.022640    
2018-10-28 00:24:59,337 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            281 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   34.95370 | 0.56896 |  0.00827 |    0.35400 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |           1246 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   45.92014 | 0.19408 | -0.01029 |    0.11043 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |           1322 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   42.62153 | 0.19454 | -0.00489 |    0.11473 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |           1620 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   29.68750 | 0.19913 | -0.01522 |    0.12621 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            908 |    0.00000 |    0.00000 |  0.00000 |  1.56250 |  0.00000 |   60.59028 | 0.16762 | -0.01209 |    0.08623 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |           1369 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   40.58160 | 0.17405 | -0.00657 |    0.10468 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            949 |    0.00000 |    0.00000 |  0.00000 |  1.56250 |  0.00000 |   58.81076 | 0.14744 | -0.00982 |    0.07866 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           3233 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   29.83941 | 0.15941 | -0.00859 |    0.10746 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           7611 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   17.41536 | 0.13751 | -0.01023 |    0.09883 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            327 |    0.00000 |    0.00000 |  0.00000 | 36.13281 |  0.00000 |   36.13281 | 0.33879 | -0.00128 |    0.23203 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           5586 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   39.38802 | 0.12211 | -0.01124 |    0.07780 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           5064 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   45.05208 | 0.11081 | -0.00160 |    0.06791 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           6076 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   34.07118 | 0.11187 | -0.00667 |    0.07326 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           6762 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   26.62760 | 0.10112 | -0.00504 |    0.07042 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           9705 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   47.34701 | 0.10614 | -0.00578 |    0.06326 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          27091 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   26.51096 | 0.09351 | -0.00693 |    0.06272 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |           1085 |    0.00000 |    0.00000 |  0.00000 | 47.02148 |  0.00000 |   47.02148 | 0.19593 | -0.01503 |    0.11778 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          22061 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   40.15571 | 0.08493 | -0.00383 |    0.05247 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          26574 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   27.91341 | 0.07345 |  0.00102 |    0.04894 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          21283 |    0.00000 |    0.00000 |  0.00000 |  0.02441 |  0.00000 |   42.26617 | 0.04929 |  0.00182 |    0.03167 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |          16502 |    0.00000 |    0.00000 |  0.00000 |  0.51270 |  0.00000 |   55.23546 | 0.04678 | -0.00758 |    0.02767 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            376 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   41.25000 | 0.49424 | -0.04717 |    0.32285 |
| 22 | Total sparsity:                     | -              |        270896 |         167031 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   38.34128 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:24:59,338 - Total sparsity: 38.34

2018-10-28 00:24:59,338 - --- validate (epoch=4)-----------
2018-10-28 00:24:59,338 - 10000 samples (128 per mini-batch)
2018-10-28 00:25:00,055 - Epoch: [4][   50/   78]    Loss 0.808362    Top1 72.203125    Top5 98.156250    
2018-10-28 00:25:00,444 - ==> Top1: 72.260    Top5: 98.060    Loss: 0.809

2018-10-28 00:25:00,445 - ==> Best Top1: 73.090   On Epoch: 3

2018-10-28 00:25:00,445 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:25:00,459 - 

2018-10-28 00:25:00,459 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:25:01,620 - Epoch: [5][   50/  391]    Overall Loss 0.674104    Objective Loss 0.674104    Top1 76.781250    Top5 98.578125    LR 0.300000    Time 0.023191    
2018-10-28 00:25:02,723 - Epoch: [5][  100/  391]    Overall Loss 0.676666    Objective Loss 0.676666    Top1 76.460938    Top5 98.601562    LR 0.300000    Time 0.022610    
2018-10-28 00:25:03,827 - Epoch: [5][  150/  391]    Overall Loss 0.664372    Objective Loss 0.664372    Top1 76.765625    Top5 98.645833    LR 0.300000    Time 0.022423    
2018-10-28 00:25:04,931 - Epoch: [5][  200/  391]    Overall Loss 0.668524    Objective Loss 0.668524    Top1 76.570312    Top5 98.589844    LR 0.300000    Time 0.022329    
2018-10-28 00:25:06,036 - Epoch: [5][  250/  391]    Overall Loss 0.667479    Objective Loss 0.667479    Top1 76.518750    Top5 98.606250    LR 0.300000    Time 0.022279    
2018-10-28 00:25:07,140 - Epoch: [5][  300/  391]    Overall Loss 0.670004    Objective Loss 0.670004    Top1 76.533854    Top5 98.575521    LR 0.300000    Time 0.022243    
2018-10-28 00:25:08,244 - Epoch: [5][  350/  391]    Overall Loss 0.671158    Objective Loss 0.671158    Top1 76.491071    Top5 98.560268    LR 0.300000    Time 0.022215    
2018-10-28 00:25:09,228 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            281 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   34.95370 | 0.57535 |  0.00664 |    0.35603 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |           1246 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   45.92014 | 0.19443 | -0.01064 |    0.10947 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |           1322 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   42.62153 | 0.19550 | -0.00608 |    0.11312 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |           1620 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   29.68750 | 0.20103 | -0.01289 |    0.12527 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            908 |    0.00000 |    0.00000 |  0.00000 |  1.56250 |  0.00000 |   60.59028 | 0.16788 | -0.01127 |    0.08474 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |           1369 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   40.58160 | 0.17899 | -0.00946 |    0.10628 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            949 |    0.00000 |    0.00000 |  0.00000 |  1.56250 |  0.00000 |   58.81076 | 0.14864 | -0.00822 |    0.07734 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           3233 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   29.83941 | 0.16381 | -0.00962 |    0.10959 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           7611 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   17.41536 | 0.14188 | -0.01088 |    0.10167 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            327 |    0.00000 |    0.00000 |  0.00000 | 36.13281 |  0.00000 |   36.13281 | 0.33891 | -0.00678 |    0.22854 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           5586 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   39.38802 | 0.12565 | -0.01253 |    0.07903 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           5064 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   45.05208 | 0.11299 | -0.00140 |    0.06821 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           6076 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   34.07118 | 0.11505 | -0.00702 |    0.07412 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           6762 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   26.62760 | 0.10214 | -0.00473 |    0.07008 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           9705 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   47.34701 | 0.11089 | -0.00628 |    0.06532 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          27091 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   26.51096 | 0.09848 | -0.00720 |    0.06554 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |           1085 |    0.00000 |    0.00000 |  0.00000 | 47.02148 |  0.00000 |   47.02148 | 0.19344 | -0.01432 |    0.11472 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          22061 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   40.15571 | 0.08962 | -0.00477 |    0.05485 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          26574 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   27.91341 | 0.07536 |  0.00101 |    0.04944 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          21283 |    0.00000 |    0.00000 |  0.00000 |  0.02441 |  0.00000 |   42.26617 | 0.04845 |  0.00133 |    0.03038 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |          16502 |    0.00000 |    0.00000 |  0.00000 |  0.51270 |  0.00000 |   55.23546 | 0.04537 | -0.00720 |    0.02607 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            376 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   41.25000 | 0.50312 | -0.04499 |    0.32590 |
| 22 | Total sparsity:                     | -              |        270896 |         167031 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   38.34128 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:25:09,229 - Total sparsity: 38.34

2018-10-28 00:25:09,229 - --- validate (epoch=5)-----------
2018-10-28 00:25:09,229 - 10000 samples (128 per mini-batch)
2018-10-28 00:25:09,948 - Epoch: [5][   50/   78]    Loss 0.906308    Top1 70.375000    Top5 96.656250    
2018-10-28 00:25:10,334 - ==> Top1: 70.390    Top5: 96.780    Loss: 0.905

2018-10-28 00:25:10,335 - ==> Best Top1: 73.090   On Epoch: 3

2018-10-28 00:25:10,335 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:25:10,346 - 

2018-10-28 00:25:10,348 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:25:11,535 - Epoch: [6][   50/  391]    Overall Loss 0.596870    Objective Loss 0.596870    Top1 79.515625    Top5 98.843750    LR 0.300000    Time 0.023717    
2018-10-28 00:25:12,663 - Epoch: [6][  100/  391]    Overall Loss 0.615045    Objective Loss 0.615045    Top1 78.804688    Top5 98.820312    LR 0.300000    Time 0.023117    
2018-10-28 00:25:13,791 - Epoch: [6][  150/  391]    Overall Loss 0.620506    Objective Loss 0.620506    Top1 78.578125    Top5 98.843750    LR 0.300000    Time 0.022921    
2018-10-28 00:25:14,918 - Epoch: [6][  200/  391]    Overall Loss 0.619957    Objective Loss 0.619957    Top1 78.535156    Top5 98.816406    LR 0.300000    Time 0.022822    
2018-10-28 00:25:16,045 - Epoch: [6][  250/  391]    Overall Loss 0.624682    Objective Loss 0.624682    Top1 78.356250    Top5 98.812500    LR 0.300000    Time 0.022760    
2018-10-28 00:25:17,172 - Epoch: [6][  300/  391]    Overall Loss 0.623548    Objective Loss 0.623548    Top1 78.226562    Top5 98.812500    LR 0.300000    Time 0.022718    
2018-10-28 00:25:18,303 - Epoch: [6][  350/  391]    Overall Loss 0.619740    Objective Loss 0.619740    Top1 78.372768    Top5 98.808036    LR 0.300000    Time 0.022702    
2018-10-28 00:25:19,306 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            269 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   37.73148 | 0.57593 |  0.00412 |    0.34737 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |           1119 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   51.43229 | 0.19408 | -0.00788 |    0.10617 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |           1186 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   48.52431 | 0.19552 | -0.00712 |    0.10936 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |           1487 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   35.46007 | 0.20072 | -0.01161 |    0.12186 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            803 |    0.00000 |    0.00000 |  0.00000 |  5.07812 |  0.00000 |   65.14757 | 0.16678 | -0.01006 |    0.08045 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |           1229 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   46.65799 | 0.18081 | -0.00532 |    0.10354 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            810 |    0.00000 |    0.00000 |  0.00000 |  3.12500 |  0.00000 |   64.84375 | 0.14792 | -0.00922 |    0.07293 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           3022 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   34.41840 | 0.16644 | -0.00746 |    0.10878 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           7291 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   20.88759 | 0.14442 | -0.01190 |    0.10193 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            295 |    0.00000 |    0.00000 |  0.00000 | 42.38281 |  0.00000 |   42.38281 | 0.33602 | -0.01027 |    0.22008 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           5092 |    0.00000 |    0.00000 |  0.00000 |  0.09766 |  0.00000 |   44.74826 | 0.12810 | -0.01169 |    0.07796 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           4523 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   50.92231 | 0.11401 | -0.00146 |    0.06602 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           5568 |    0.00000 |    0.00000 |  0.00000 |  0.09766 |  0.00000 |   39.58333 | 0.11746 | -0.00777 |    0.07366 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           6264 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   32.03125 | 0.10272 | -0.00437 |    0.06844 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           8812 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   52.19184 | 0.11304 | -0.00635 |    0.06447 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          25339 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   31.26356 | 0.10174 | -0.00657 |    0.06646 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            952 |    0.00000 |    0.00000 |  0.00000 | 53.51562 |  0.00000 |   53.51562 | 0.18928 | -0.01206 |    0.10725 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          19899 |    0.00000 |    0.00000 |  0.00000 |  0.12207 |  0.00000 |   46.02051 | 0.09303 | -0.00533 |    0.05521 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          24513 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   33.50423 | 0.07644 |  0.00142 |    0.04867 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          18567 |    0.00000 |    0.00000 |  0.00000 |  0.24414 |  0.00000 |   49.63379 | 0.04762 |  0.00102 |    0.02852 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |          14072 |    0.00000 |    0.00000 |  0.00000 |  1.83105 |  0.00000 |   61.82726 | 0.04378 | -0.00673 |    0.02365 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            366 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   42.81250 | 0.51359 | -0.03937 |    0.33007 |
| 22 | Total sparsity:                     | -              |        270896 |         151478 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   44.08260 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:25:19,306 - Total sparsity: 44.08

2018-10-28 00:25:19,306 - --- validate (epoch=6)-----------
2018-10-28 00:25:19,307 - 10000 samples (128 per mini-batch)
2018-10-28 00:25:20,019 - Epoch: [6][   50/   78]    Loss 0.664898    Top1 77.484375    Top5 98.515625    
2018-10-28 00:25:20,404 - ==> Top1: 77.300    Top5: 98.590    Loss: 0.663

2018-10-28 00:25:20,405 - ==> Best Top1: 77.300   On Epoch: 6

2018-10-28 00:25:20,405 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:25:20,419 - 

2018-10-28 00:25:20,419 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:25:21,583 - Epoch: [7][   50/  391]    Overall Loss 0.575466    Objective Loss 0.575466    Top1 79.593750    Top5 99.078125    LR 0.300000    Time 0.023235    
2018-10-28 00:25:22,690 - Epoch: [7][  100/  391]    Overall Loss 0.579765    Objective Loss 0.579765    Top1 79.546875    Top5 98.984375    LR 0.300000    Time 0.022671    
2018-10-28 00:25:23,795 - Epoch: [7][  150/  391]    Overall Loss 0.592421    Objective Loss 0.592421    Top1 79.260417    Top5 98.828125    LR 0.300000    Time 0.022476    
2018-10-28 00:25:24,901 - Epoch: [7][  200/  391]    Overall Loss 0.592228    Objective Loss 0.592228    Top1 79.437500    Top5 98.863281    LR 0.300000    Time 0.022381    
2018-10-28 00:25:26,010 - Epoch: [7][  250/  391]    Overall Loss 0.592880    Objective Loss 0.592880    Top1 79.362500    Top5 98.837500    LR 0.300000    Time 0.022334    
2018-10-28 00:25:27,115 - Epoch: [7][  300/  391]    Overall Loss 0.594443    Objective Loss 0.594443    Top1 79.320312    Top5 98.833333    LR 0.300000    Time 0.022293    
2018-10-28 00:25:28,221 - Epoch: [7][  350/  391]    Overall Loss 0.593743    Objective Loss 0.593743    Top1 79.337054    Top5 98.843750    LR 0.300000    Time 0.022263    
2018-10-28 00:25:29,206 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            269 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   37.73148 | 0.58185 | -0.00143 |    0.34842 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |           1119 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   51.43229 | 0.19327 | -0.00620 |    0.10337 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |           1186 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   48.52431 | 0.19526 | -0.00535 |    0.10785 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |           1487 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   35.46007 | 0.20075 | -0.01237 |    0.11987 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            803 |    0.00000 |    0.00000 |  0.00000 |  5.07812 |  0.00000 |   65.14757 | 0.16608 | -0.01042 |    0.07828 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |           1229 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   46.65799 | 0.18450 | -0.00828 |    0.10545 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            810 |    0.00000 |    0.00000 |  0.00000 |  3.12500 |  0.00000 |   64.84375 | 0.14865 | -0.00682 |    0.07161 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           3022 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   34.41840 | 0.16968 | -0.00670 |    0.11033 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           7291 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   20.88759 | 0.14762 | -0.01073 |    0.10420 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            295 |    0.00000 |    0.00000 |  0.00000 | 42.38281 |  0.00000 |   42.38281 | 0.33452 | -0.01373 |    0.21587 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           5092 |    0.00000 |    0.00000 |  0.00000 |  0.09766 |  0.00000 |   44.74826 | 0.13032 | -0.01217 |    0.07847 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           4523 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   50.92231 | 0.11555 | -0.00030 |    0.06618 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           5568 |    0.00000 |    0.00000 |  0.00000 |  0.09766 |  0.00000 |   39.58333 | 0.11982 | -0.00784 |    0.07430 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           6264 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   32.03125 | 0.10394 | -0.00491 |    0.06847 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           8812 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   52.19184 | 0.11551 | -0.00753 |    0.06529 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          25339 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   31.26356 | 0.10497 | -0.00738 |    0.06839 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            952 |    0.00000 |    0.00000 |  0.00000 | 53.51562 |  0.00000 |   53.51562 | 0.18774 | -0.01349 |    0.10488 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          19899 |    0.00000 |    0.00000 |  0.00000 |  0.12207 |  0.00000 |   46.02051 | 0.09653 | -0.00613 |    0.05682 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          24513 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   33.50423 | 0.07776 |  0.00090 |    0.04891 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          18567 |    0.00000 |    0.00000 |  0.00000 |  0.24414 |  0.00000 |   49.63379 | 0.04717 |  0.00080 |    0.02750 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |          14072 |    0.00000 |    0.00000 |  0.00000 |  1.83105 |  0.00000 |   61.82726 | 0.04278 | -0.00617 |    0.02240 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            366 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   42.81250 | 0.52144 | -0.03663 |    0.33623 |
| 22 | Total sparsity:                     | -              |        270896 |         151478 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   44.08260 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:25:29,206 - Total sparsity: 44.08

2018-10-28 00:25:29,206 - --- validate (epoch=7)-----------
2018-10-28 00:25:29,206 - 10000 samples (128 per mini-batch)
2018-10-28 00:25:29,932 - Epoch: [7][   50/   78]    Loss 0.648007    Top1 78.000000    Top5 98.796875    
2018-10-28 00:25:30,323 - ==> Top1: 78.260    Top5: 98.800    Loss: 0.636

2018-10-28 00:25:30,324 - ==> Best Top1: 78.260   On Epoch: 7

2018-10-28 00:25:30,324 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:25:30,346 - 

2018-10-28 00:25:30,351 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:25:31,526 - Epoch: [8][   50/  391]    Overall Loss 0.569419    Objective Loss 0.569419    Top1 79.765625    Top5 98.968750    LR 0.300000    Time 0.023467    
2018-10-28 00:25:32,655 - Epoch: [8][  100/  391]    Overall Loss 0.574101    Objective Loss 0.574101    Top1 79.937500    Top5 98.953125    LR 0.300000    Time 0.023010    
2018-10-28 00:25:33,785 - Epoch: [8][  150/  391]    Overall Loss 0.573162    Objective Loss 0.573162    Top1 79.984375    Top5 98.979167    LR 0.300000    Time 0.022864    
2018-10-28 00:25:34,914 - Epoch: [8][  200/  391]    Overall Loss 0.572024    Objective Loss 0.572024    Top1 80.054688    Top5 98.996094    LR 0.300000    Time 0.022787    
2018-10-28 00:25:36,043 - Epoch: [8][  250/  391]    Overall Loss 0.570125    Objective Loss 0.570125    Top1 80.181250    Top5 99.000000    LR 0.300000    Time 0.022740    
2018-10-28 00:25:37,172 - Epoch: [8][  300/  391]    Overall Loss 0.569786    Objective Loss 0.569786    Top1 80.080729    Top5 98.986979    LR 0.300000    Time 0.022710    
2018-10-28 00:25:38,303 - Epoch: [8][  350/  391]    Overall Loss 0.568536    Objective Loss 0.568536    Top1 80.180804    Top5 98.964286    LR 0.300000    Time 0.022692    
2018-10-28 00:25:39,307 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            256 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   40.74074 | 0.58615 |  0.00926 |    0.34820 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |           1006 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   56.33681 | 0.19251 | -0.00662 |    0.09980 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |           1066 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   53.73264 | 0.19398 | -0.00662 |    0.10465 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |           1380 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   40.10417 | 0.20000 | -0.01109 |    0.11596 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            693 |    0.00000 |    0.00000 |  0.00000 |  8.98438 |  0.00000 |   69.92188 | 0.16384 | -0.00908 |    0.07438 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |           1123 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   51.25868 | 0.18568 | -0.00837 |    0.10263 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            698 |    0.00000 |    0.00000 |  0.00000 |  7.03125 |  0.00000 |   69.70486 | 0.14844 | -0.00637 |    0.06835 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2836 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   38.45486 | 0.17226 | -0.00736 |    0.10946 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           6991 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   24.14280 | 0.14991 | -0.00995 |    0.10375 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            273 |    0.00000 |    0.00000 |  0.00000 | 46.67969 |  0.00000 |   46.67969 | 0.32990 | -0.01047 |    0.20714 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           4671 |    0.00000 |    0.00000 |  0.00000 |  0.29297 |  0.00000 |   49.31641 | 0.13072 | -0.01234 |    0.07682 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           4101 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   55.50130 | 0.11544 |  0.00030 |    0.06390 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           5147 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   44.15148 | 0.12120 | -0.00821 |    0.07320 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           5803 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   37.03342 | 0.10429 | -0.00403 |    0.06703 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           8115 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   55.97331 | 0.11722 | -0.00699 |    0.06446 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          23949 |    0.00000 |    0.00000 |  0.00000 |  0.02441 |  0.00000 |   35.03418 | 0.10770 | -0.00744 |    0.06903 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            847 |    0.00000 |    0.00000 |  0.00000 | 58.64258 |  0.00000 |   58.64258 | 0.18491 | -0.01272 |    0.09951 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          18290 |    0.00000 |    0.00000 |  0.00000 |  0.29297 |  0.00000 |   50.38520 | 0.09931 | -0.00623 |    0.05699 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          22664 |    0.00000 |    0.00000 |  0.00000 |  0.02441 |  0.00000 |   38.51997 | 0.07874 |  0.00142 |    0.04806 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          16290 |    0.00000 |    0.00000 |  0.00000 |  0.87891 |  0.00000 |   55.81055 | 0.04673 |  0.00060 |    0.02596 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |          11927 |    0.00000 |    0.00000 |  0.00000 |  4.10156 |  0.00000 |   67.64594 | 0.04184 | -0.00583 |    0.02051 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            361 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   43.59375 | 0.52317 | -0.03631 |    0.33420 |
| 22 | Total sparsity:                     | -              |        270896 |         138487 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   48.87817 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:25:39,307 - Total sparsity: 48.88

2018-10-28 00:25:39,307 - --- validate (epoch=8)-----------
2018-10-28 00:25:39,307 - 10000 samples (128 per mini-batch)
2018-10-28 00:25:40,027 - Epoch: [8][   50/   78]    Loss 0.669755    Top1 77.156250    Top5 98.906250    
2018-10-28 00:25:40,417 - ==> Top1: 77.000    Top5: 98.960    Loss: 0.669

2018-10-28 00:25:40,418 - ==> Best Top1: 78.260   On Epoch: 7

2018-10-28 00:25:40,418 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:25:40,432 - 

2018-10-28 00:25:40,432 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:25:41,598 - Epoch: [9][   50/  391]    Overall Loss 0.537108    Objective Loss 0.537108    Top1 81.781250    Top5 99.031250    LR 0.300000    Time 0.023287    
2018-10-28 00:25:42,703 - Epoch: [9][  100/  391]    Overall Loss 0.546612    Objective Loss 0.546612    Top1 81.304688    Top5 99.007812    LR 0.300000    Time 0.022680    
2018-10-28 00:25:43,808 - Epoch: [9][  150/  391]    Overall Loss 0.549894    Objective Loss 0.549894    Top1 81.114583    Top5 98.989583    LR 0.300000    Time 0.022475    
2018-10-28 00:25:44,915 - Epoch: [9][  200/  391]    Overall Loss 0.558031    Objective Loss 0.558031    Top1 80.960938    Top5 98.949219    LR 0.300000    Time 0.022382    
2018-10-28 00:25:46,020 - Epoch: [9][  250/  391]    Overall Loss 0.557196    Objective Loss 0.557196    Top1 80.943750    Top5 98.968750    LR 0.300000    Time 0.022321    
2018-10-28 00:25:47,123 - Epoch: [9][  300/  391]    Overall Loss 0.554500    Objective Loss 0.554500    Top1 80.906250    Top5 99.002604    LR 0.300000    Time 0.022272    
2018-10-28 00:25:48,230 - Epoch: [9][  350/  391]    Overall Loss 0.554337    Objective Loss 0.554337    Top1 80.921875    Top5 98.997768    LR 0.300000    Time 0.022251    
2018-10-28 00:25:49,214 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            256 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   40.74074 | 0.58880 |  0.00717 |    0.34552 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |           1006 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   56.33681 | 0.19207 | -0.00520 |    0.09840 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |           1066 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   53.73264 | 0.19330 | -0.00532 |    0.10372 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |           1380 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   40.10417 | 0.19939 | -0.00962 |    0.11390 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            693 |    0.00000 |    0.00000 |  0.00000 |  8.98438 |  0.00000 |   69.92188 | 0.16217 | -0.01035 |    0.07246 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |           1123 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   51.25868 | 0.18785 | -0.00711 |    0.10318 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            698 |    0.00000 |    0.00000 |  0.00000 |  7.03125 |  0.00000 |   69.70486 | 0.14945 | -0.00617 |    0.06690 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2836 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   38.45486 | 0.17389 | -0.00556 |    0.11017 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           6991 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   24.14280 | 0.15228 | -0.01091 |    0.10501 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            273 |    0.00000 |    0.00000 |  0.00000 | 46.67969 |  0.00000 |   46.67969 | 0.32727 | -0.01137 |    0.20573 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           4671 |    0.00000 |    0.00000 |  0.00000 |  0.29297 |  0.00000 |   49.31641 | 0.13166 | -0.01213 |    0.07614 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           4101 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   55.50130 | 0.11562 |  0.00041 |    0.06298 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           5147 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   44.15148 | 0.12313 | -0.00966 |    0.07431 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           5803 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   37.03342 | 0.10507 | -0.00406 |    0.06690 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           8115 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   55.97331 | 0.11826 | -0.00720 |    0.06447 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          23949 |    0.00000 |    0.00000 |  0.00000 |  0.02441 |  0.00000 |   35.03418 | 0.10967 | -0.00772 |    0.06987 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            847 |    0.00000 |    0.00000 |  0.00000 | 58.64258 |  0.00000 |   58.64258 | 0.18297 | -0.01109 |    0.09649 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          18290 |    0.00000 |    0.00000 |  0.00000 |  0.29297 |  0.00000 |   50.38520 | 0.10169 | -0.00653 |    0.05791 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          22664 |    0.00000 |    0.00000 |  0.00000 |  0.02441 |  0.00000 |   38.51997 | 0.07987 |  0.00118 |    0.04826 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          16290 |    0.00000 |    0.00000 |  0.00000 |  0.87891 |  0.00000 |   55.81055 | 0.04671 |  0.00074 |    0.02541 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |          11927 |    0.00000 |    0.00000 |  0.00000 |  4.10156 |  0.00000 |   67.64594 | 0.04159 | -0.00551 |    0.01984 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            361 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   43.59375 | 0.52526 | -0.03856 |    0.33422 |
| 22 | Total sparsity:                     | -              |        270896 |         138487 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   48.87817 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:25:49,214 - Total sparsity: 48.88

2018-10-28 00:25:49,215 - --- validate (epoch=9)-----------
2018-10-28 00:25:49,215 - 10000 samples (128 per mini-batch)
2018-10-28 00:25:49,932 - Epoch: [9][   50/   78]    Loss 0.675638    Top1 77.156250    Top5 98.531250    
2018-10-28 00:25:50,321 - ==> Top1: 77.490    Top5: 98.590    Loss: 0.668

2018-10-28 00:25:50,321 - ==> Best Top1: 78.260   On Epoch: 7

2018-10-28 00:25:50,322 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:25:50,333 - 

2018-10-28 00:25:50,335 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:25:51,525 - Epoch: [10][   50/  391]    Overall Loss 0.524721    Objective Loss 0.524721    Top1 81.843750    Top5 99.000000    LR 0.300000    Time 0.023769    
2018-10-28 00:25:52,655 - Epoch: [10][  100/  391]    Overall Loss 0.531260    Objective Loss 0.531260    Top1 81.375000    Top5 99.093750    LR 0.300000    Time 0.023167    
2018-10-28 00:25:53,785 - Epoch: [10][  150/  391]    Overall Loss 0.536501    Objective Loss 0.536501    Top1 81.239583    Top5 99.093750    LR 0.300000    Time 0.022972    
2018-10-28 00:25:54,915 - Epoch: [10][  200/  391]    Overall Loss 0.533414    Objective Loss 0.533414    Top1 81.402344    Top5 99.101562    LR 0.300000    Time 0.022872    
2018-10-28 00:25:56,047 - Epoch: [10][  250/  391]    Overall Loss 0.537628    Objective Loss 0.537628    Top1 81.181250    Top5 99.106250    LR 0.300000    Time 0.022818    
2018-10-28 00:25:57,176 - Epoch: [10][  300/  391]    Overall Loss 0.535016    Objective Loss 0.535016    Top1 81.322917    Top5 99.119792    LR 0.300000    Time 0.022775    
2018-10-28 00:25:58,306 - Epoch: [10][  350/  391]    Overall Loss 0.532696    Objective Loss 0.532696    Top1 81.464286    Top5 99.111607    LR 0.300000    Time 0.022746    
2018-10-28 00:25:59,309 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            243 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   43.75000 | 0.58761 |  0.00645 |    0.34409 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            923 |    0.00000 |    0.00000 |  0.00000 |  1.17188 |  0.00000 |   59.93924 | 0.19269 | -0.00549 |    0.09636 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            998 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   56.68403 | 0.19379 | -0.00479 |    0.10128 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |           1269 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   44.92188 | 0.19820 | -0.00906 |    0.11083 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            622 |    0.00000 |    0.00000 |  0.00000 | 10.93750 |  0.00000 |   73.00347 | 0.16081 | -0.00857 |    0.06919 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |           1034 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   55.12153 | 0.18931 | -0.00734 |    0.10156 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            620 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   73.09028 | 0.14760 | -0.00670 |    0.06406 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2700 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   41.40625 | 0.17445 | -0.00664 |    0.10854 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           6731 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   26.96398 | 0.15392 | -0.01033 |    0.10511 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            263 |    0.00000 |    0.00000 |  0.00000 | 48.63281 |  0.00000 |   48.63281 | 0.32535 | -0.01477 |    0.20163 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           4323 |    0.00000 |    0.00000 |  0.00000 |  0.48828 |  0.00000 |   53.09245 | 0.13226 | -0.01207 |    0.07510 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           3737 |    0.00000 |    0.00000 |  0.00000 |  1.17188 |  0.00000 |   59.45095 | 0.11559 |  0.00106 |    0.06138 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           4796 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   47.96007 | 0.12426 | -0.00944 |    0.07351 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           5388 |    0.00000 |    0.00000 |  0.00000 |  0.09766 |  0.00000 |   41.53646 | 0.10544 | -0.00490 |    0.06581 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           7478 |    0.00000 |    0.00000 |  0.00000 |  1.41602 |  0.00000 |   59.42925 | 0.11880 | -0.00687 |    0.06308 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          22744 |    0.00000 |    0.00000 |  0.00000 |  0.07324 |  0.00000 |   38.30295 | 0.11115 | -0.00811 |    0.06986 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            754 |    0.00000 |    0.00000 |  0.00000 | 63.18359 |  0.00000 |   63.18359 | 0.17910 | -0.01002 |    0.09134 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          17052 |    0.00000 |    0.00000 |  0.00000 |  0.56152 |  0.00000 |   53.74349 | 0.10328 | -0.00624 |    0.05748 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          21068 |    0.00000 |    0.00000 |  0.00000 |  0.12207 |  0.00000 |   42.84939 | 0.08041 |  0.00102 |    0.04762 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          14335 |    0.00000 |    0.00000 |  0.00000 |  2.22168 |  0.00000 |   61.11382 | 0.04629 |  0.00028 |    0.02411 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |          10097 |    0.00000 |    0.00000 |  0.00000 |  7.88574 |  0.00000 |   72.61013 | 0.04088 | -0.00500 |    0.01838 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            355 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   44.53125 | 0.52791 | -0.03535 |    0.33448 |
| 22 | Total sparsity:                     | -              |        270896 |         127530 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   52.92289 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:25:59,309 - Total sparsity: 52.92

2018-10-28 00:25:59,309 - --- validate (epoch=10)-----------
2018-10-28 00:25:59,309 - 10000 samples (128 per mini-batch)
2018-10-28 00:26:00,031 - Epoch: [10][   50/   78]    Loss 0.792684    Top1 74.859375    Top5 98.312500    
2018-10-28 00:26:00,420 - ==> Top1: 74.610    Top5: 98.320    Loss: 0.794

2018-10-28 00:26:00,421 - ==> Best Top1: 78.260   On Epoch: 7

2018-10-28 00:26:00,421 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:26:00,435 - 

2018-10-28 00:26:00,435 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:26:01,599 - Epoch: [11][   50/  391]    Overall Loss 0.508579    Objective Loss 0.508579    Top1 82.359375    Top5 99.093750    LR 0.300000    Time 0.023247    
2018-10-28 00:26:02,705 - Epoch: [11][  100/  391]    Overall Loss 0.515192    Objective Loss 0.515192    Top1 82.039062    Top5 99.109375    LR 0.300000    Time 0.022670    
2018-10-28 00:26:03,811 - Epoch: [11][  150/  391]    Overall Loss 0.519288    Objective Loss 0.519288    Top1 82.036458    Top5 99.088542    LR 0.300000    Time 0.022475    
2018-10-28 00:26:04,914 - Epoch: [11][  200/  391]    Overall Loss 0.524680    Objective Loss 0.524680    Top1 81.980469    Top5 99.097656    LR 0.300000    Time 0.022348    
2018-10-28 00:26:06,019 - Epoch: [11][  250/  391]    Overall Loss 0.524223    Objective Loss 0.524223    Top1 81.853125    Top5 99.153125    LR 0.300000    Time 0.022293    
2018-10-28 00:26:07,122 - Epoch: [11][  300/  391]    Overall Loss 0.524952    Objective Loss 0.524952    Top1 81.778646    Top5 99.127604    LR 0.300000    Time 0.022250    
2018-10-28 00:26:08,228 - Epoch: [11][  350/  391]    Overall Loss 0.523017    Objective Loss 0.523017    Top1 81.868304    Top5 99.107143    LR 0.300000    Time 0.022228    
2018-10-28 00:26:09,213 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            243 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   43.75000 | 0.58423 | -0.00367 |    0.33845 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            923 |    0.00000 |    0.00000 |  0.00000 |  1.17188 |  0.00000 |   59.93924 | 0.19182 | -0.00516 |    0.09317 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            998 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   56.68403 | 0.19281 | -0.00506 |    0.09929 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |           1269 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   44.92188 | 0.19901 | -0.01161 |    0.11122 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            622 |    0.00000 |    0.00000 |  0.00000 | 10.93750 |  0.00000 |   73.00347 | 0.16113 | -0.01059 |    0.06838 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |           1034 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   55.12153 | 0.19043 | -0.00704 |    0.10076 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            620 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   73.09028 | 0.14792 | -0.00582 |    0.06301 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2700 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   41.40625 | 0.17660 | -0.00473 |    0.10917 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           6731 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   26.96398 | 0.15600 | -0.01080 |    0.10610 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            263 |    0.00000 |    0.00000 |  0.00000 | 48.63281 |  0.00000 |   48.63281 | 0.32292 | -0.01521 |    0.19782 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           4323 |    0.00000 |    0.00000 |  0.00000 |  0.48828 |  0.00000 |   53.09245 | 0.13361 | -0.01164 |    0.07505 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           3737 |    0.00000 |    0.00000 |  0.00000 |  1.17188 |  0.00000 |   59.45095 | 0.11565 |  0.00065 |    0.06032 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           4796 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   47.96007 | 0.12572 | -0.00991 |    0.07346 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           5388 |    0.00000 |    0.00000 |  0.00000 |  0.09766 |  0.00000 |   41.53646 | 0.10631 | -0.00533 |    0.06581 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           7478 |    0.00000 |    0.00000 |  0.00000 |  1.41602 |  0.00000 |   59.42925 | 0.11986 | -0.00685 |    0.06317 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          22744 |    0.00000 |    0.00000 |  0.00000 |  0.07324 |  0.00000 |   38.30295 | 0.11314 | -0.00760 |    0.07088 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            754 |    0.00000 |    0.00000 |  0.00000 | 63.18359 |  0.00000 |   63.18359 | 0.17676 | -0.00973 |    0.08965 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          17052 |    0.00000 |    0.00000 |  0.00000 |  0.56152 |  0.00000 |   53.74349 | 0.10540 | -0.00700 |    0.05834 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          21068 |    0.00000 |    0.00000 |  0.00000 |  0.12207 |  0.00000 |   42.84939 | 0.08141 |  0.00108 |    0.04780 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          14335 |    0.00000 |    0.00000 |  0.00000 |  2.22168 |  0.00000 |   61.11382 | 0.04664 |  0.00038 |    0.02381 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |          10097 |    0.00000 |    0.00000 |  0.00000 |  7.88574 |  0.00000 |   72.61013 | 0.04090 | -0.00477 |    0.01802 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            355 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   44.53125 | 0.52453 | -0.03602 |    0.33071 |
| 22 | Total sparsity:                     | -              |        270896 |         127530 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   52.92289 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:26:09,213 - Total sparsity: 52.92

2018-10-28 00:26:09,213 - --- validate (epoch=11)-----------
2018-10-28 00:26:09,214 - 10000 samples (128 per mini-batch)
2018-10-28 00:26:09,931 - Epoch: [11][   50/   78]    Loss 0.888507    Top1 73.203125    Top5 97.734375    
2018-10-28 00:26:10,321 - ==> Top1: 73.150    Top5: 97.840    Loss: 0.885

2018-10-28 00:26:10,322 - ==> Best Top1: 78.260   On Epoch: 7

2018-10-28 00:26:10,322 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:26:10,334 - 

2018-10-28 00:26:10,335 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:26:11,525 - Epoch: [12][   50/  391]    Overall Loss 0.494308    Objective Loss 0.494308    Top1 83.500000    Top5 99.359375    LR 0.300000    Time 0.023754    
2018-10-28 00:26:12,653 - Epoch: [12][  100/  391]    Overall Loss 0.494387    Objective Loss 0.494387    Top1 83.109375    Top5 99.328125    LR 0.300000    Time 0.023142    
2018-10-28 00:26:13,782 - Epoch: [12][  150/  391]    Overall Loss 0.508825    Objective Loss 0.508825    Top1 82.526042    Top5 99.239583    LR 0.300000    Time 0.022947    
2018-10-28 00:26:14,911 - Epoch: [12][  200/  391]    Overall Loss 0.511262    Objective Loss 0.511262    Top1 82.468750    Top5 99.210938    LR 0.300000    Time 0.022848    
2018-10-28 00:26:16,038 - Epoch: [12][  250/  391]    Overall Loss 0.514082    Objective Loss 0.514082    Top1 82.368750    Top5 99.200000    LR 0.300000    Time 0.022782    
2018-10-28 00:26:17,166 - Epoch: [12][  300/  391]    Overall Loss 0.515360    Objective Loss 0.515360    Top1 82.294271    Top5 99.195312    LR 0.300000    Time 0.022740    
2018-10-28 00:26:18,296 - Epoch: [12][  350/  391]    Overall Loss 0.513322    Objective Loss 0.513322    Top1 82.283482    Top5 99.218750    LR 0.300000    Time 0.022716    
2018-10-28 00:26:19,297 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            233 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   46.06481 | 0.58291 | -0.00102 |    0.33546 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            814 |    0.00000 |    0.00000 |  0.00000 |  3.51562 |  0.00000 |   64.67014 | 0.19013 | -0.00447 |    0.09103 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            929 |    0.00000 |    0.00000 |  0.00000 |  1.56250 |  0.00000 |   59.67882 | 0.19274 | -0.00631 |    0.09731 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |           1194 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   48.17708 | 0.19817 | -0.01111 |    0.10896 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            556 |    0.00000 |    0.00000 |  0.00000 | 13.28125 |  0.00000 |   75.86806 | 0.16002 | -0.00985 |    0.06597 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            957 |    0.00000 |    0.00000 |  0.00000 |  1.17188 |  0.00000 |   58.46354 | 0.19119 | -0.00759 |    0.09958 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            568 |    0.00000 |    0.00000 |  0.00000 | 13.28125 |  0.00000 |   75.34722 | 0.14761 | -0.00542 |    0.06104 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2568 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   44.27083 | 0.17735 | -0.00734 |    0.10766 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           6494 |    0.00000 |    0.00000 |  0.00000 |  0.09766 |  0.00000 |   29.53559 | 0.15714 | -0.00988 |    0.10594 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            250 |    0.00000 |    0.00000 |  0.00000 | 51.17188 |  0.00000 |   51.17188 | 0.31928 | -0.01258 |    0.19204 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           4028 |    0.00000 |    0.00000 |  0.00000 |  1.17188 |  0.00000 |   56.29340 | 0.13403 | -0.01105 |    0.07359 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           3407 |    0.00000 |    0.00000 |  0.00000 |  2.05078 |  0.00000 |   63.03168 | 0.11514 |  0.00084 |    0.05847 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           4477 |    0.00000 |    0.00000 |  0.00000 |  0.29297 |  0.00000 |   51.42144 | 0.12639 | -0.01004 |    0.07231 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           5060 |    0.00000 |    0.00000 |  0.00000 |  0.09766 |  0.00000 |   45.09549 | 0.10661 | -0.00516 |    0.06463 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           6990 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   62.07682 | 0.12019 | -0.00696 |    0.06184 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          21696 |    0.00000 |    0.00000 |  0.00000 |  0.09766 |  0.00000 |   41.14583 | 0.11414 | -0.00760 |    0.07050 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            690 |    0.00000 |    0.00000 |  0.00000 | 66.30859 |  0.00000 |   66.30859 | 0.17481 | -0.00996 |    0.08556 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          15949 |    0.00000 |    0.00000 |  0.00000 |  1.00098 |  0.00000 |   56.73557 | 0.10659 | -0.00635 |    0.05767 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          19774 |    0.00000 |    0.00000 |  0.00000 |  0.26855 |  0.00000 |   46.35959 | 0.08180 |  0.00163 |    0.04705 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          12835 |    0.00000 |    0.00000 |  0.00000 |  3.49121 |  0.00000 |   65.18283 | 0.04673 |  0.00038 |    0.02293 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           8752 |    0.00000 |    0.00000 |  0.00000 | 12.74414 |  0.00000 |   76.25868 | 0.04068 | -0.00443 |    0.01697 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            350 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   45.31250 | 0.52658 | -0.03589 |    0.33139 |
| 22 | Total sparsity:                     | -              |        270896 |         118571 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   56.23007 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:26:19,298 - Total sparsity: 56.23

2018-10-28 00:26:19,298 - --- validate (epoch=12)-----------
2018-10-28 00:26:19,298 - 10000 samples (128 per mini-batch)
2018-10-28 00:26:20,017 - Epoch: [12][   50/   78]    Loss 0.653929    Top1 78.671875    Top5 98.718750    
2018-10-28 00:26:20,409 - ==> Top1: 78.530    Top5: 98.740    Loss: 0.659

2018-10-28 00:26:20,409 - ==> Best Top1: 78.530   On Epoch: 12

2018-10-28 00:26:20,409 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:26:20,423 - 

2018-10-28 00:26:20,424 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:26:21,588 - Epoch: [13][   50/  391]    Overall Loss 0.481241    Objective Loss 0.481241    Top1 83.156250    Top5 99.359375    LR 0.300000    Time 0.023254    
2018-10-28 00:26:22,692 - Epoch: [13][  100/  391]    Overall Loss 0.493041    Objective Loss 0.493041    Top1 83.078125    Top5 99.289062    LR 0.300000    Time 0.022654    
2018-10-28 00:26:23,796 - Epoch: [13][  150/  391]    Overall Loss 0.495832    Objective Loss 0.495832    Top1 82.807292    Top5 99.276042    LR 0.300000    Time 0.022453    
2018-10-28 00:26:24,899 - Epoch: [13][  200/  391]    Overall Loss 0.498344    Objective Loss 0.498344    Top1 82.843750    Top5 99.234375    LR 0.300000    Time 0.022349    
2018-10-28 00:26:26,003 - Epoch: [13][  250/  391]    Overall Loss 0.498247    Objective Loss 0.498247    Top1 82.890625    Top5 99.246875    LR 0.300000    Time 0.022289    
2018-10-28 00:26:27,107 - Epoch: [13][  300/  391]    Overall Loss 0.497184    Objective Loss 0.497184    Top1 82.986979    Top5 99.192708    LR 0.300000    Time 0.022253    
2018-10-28 00:26:28,215 - Epoch: [13][  350/  391]    Overall Loss 0.501374    Objective Loss 0.501374    Top1 82.848214    Top5 99.162946    LR 0.300000    Time 0.022236    
2018-10-28 00:26:29,205 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            233 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   46.06481 | 0.58195 |  0.00494 |    0.33274 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            814 |    0.00000 |    0.00000 |  0.00000 |  3.51562 |  0.00000 |   64.67014 | 0.18881 | -0.00433 |    0.08921 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            929 |    0.00000 |    0.00000 |  0.00000 |  1.56250 |  0.00000 |   59.67882 | 0.19259 | -0.00521 |    0.09676 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |           1194 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   48.17708 | 0.19709 | -0.01208 |    0.10719 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            556 |    0.00000 |    0.00000 |  0.00000 | 13.28125 |  0.00000 |   75.86806 | 0.15835 | -0.00940 |    0.06465 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            957 |    0.00000 |    0.00000 |  0.00000 |  1.17188 |  0.00000 |   58.46354 | 0.19235 | -0.00701 |    0.09839 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            568 |    0.00000 |    0.00000 |  0.00000 | 13.28125 |  0.00000 |   75.34722 | 0.14706 | -0.00375 |    0.05992 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2568 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   44.27083 | 0.17789 | -0.00682 |    0.10664 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           6494 |    0.00000 |    0.00000 |  0.00000 |  0.09766 |  0.00000 |   29.53559 | 0.15778 | -0.01080 |    0.10615 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            250 |    0.00000 |    0.00000 |  0.00000 | 51.17188 |  0.00000 |   51.17188 | 0.31506 | -0.01471 |    0.19050 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           4028 |    0.00000 |    0.00000 |  0.00000 |  1.17188 |  0.00000 |   56.29340 | 0.13413 | -0.01167 |    0.07293 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           3407 |    0.00000 |    0.00000 |  0.00000 |  2.05078 |  0.00000 |   63.03168 | 0.11501 |  0.00082 |    0.05797 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           4477 |    0.00000 |    0.00000 |  0.00000 |  0.29297 |  0.00000 |   51.42144 | 0.12703 | -0.00968 |    0.07256 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           5060 |    0.00000 |    0.00000 |  0.00000 |  0.09766 |  0.00000 |   45.09549 | 0.10687 | -0.00599 |    0.06416 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           6990 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   62.07682 | 0.12086 | -0.00670 |    0.06193 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          21696 |    0.00000 |    0.00000 |  0.00000 |  0.09766 |  0.00000 |   41.14583 | 0.11520 | -0.00746 |    0.07087 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            690 |    0.00000 |    0.00000 |  0.00000 | 66.30859 |  0.00000 |   66.30859 | 0.17230 | -0.01076 |    0.08384 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          15949 |    0.00000 |    0.00000 |  0.00000 |  1.00098 |  0.00000 |   56.73557 | 0.10770 | -0.00634 |    0.05789 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          19774 |    0.00000 |    0.00000 |  0.00000 |  0.26855 |  0.00000 |   46.35959 | 0.08230 |  0.00128 |    0.04700 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          12835 |    0.00000 |    0.00000 |  0.00000 |  3.49121 |  0.00000 |   65.18283 | 0.04741 | -0.00002 |    0.02288 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           8752 |    0.00000 |    0.00000 |  0.00000 | 12.74414 |  0.00000 |   76.25868 | 0.04106 | -0.00444 |    0.01682 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            350 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   45.31250 | 0.52930 | -0.03360 |    0.33246 |
| 22 | Total sparsity:                     | -              |        270896 |         118571 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   56.23007 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:26:29,205 - Total sparsity: 56.23

2018-10-28 00:26:29,205 - --- validate (epoch=13)-----------
2018-10-28 00:26:29,205 - 10000 samples (128 per mini-batch)
2018-10-28 00:26:29,926 - Epoch: [13][   50/   78]    Loss 0.659205    Top1 78.125000    Top5 98.640625    
2018-10-28 00:26:30,317 - ==> Top1: 77.810    Top5: 98.720    Loss: 0.665

2018-10-28 00:26:30,318 - ==> Best Top1: 78.530   On Epoch: 12

2018-10-28 00:26:30,318 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:26:30,330 - 

2018-10-28 00:26:30,332 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:26:31,526 - Epoch: [14][   50/  391]    Overall Loss 0.472986    Objective Loss 0.472986    Top1 83.453125    Top5 99.406250    LR 0.300000    Time 0.023854    
2018-10-28 00:26:32,662 - Epoch: [14][  100/  391]    Overall Loss 0.493567    Objective Loss 0.493567    Top1 82.765625    Top5 99.265625    LR 0.300000    Time 0.023276    
2018-10-28 00:26:33,797 - Epoch: [14][  150/  391]    Overall Loss 0.498256    Objective Loss 0.498256    Top1 82.453125    Top5 99.276042    LR 0.300000    Time 0.023074    
2018-10-28 00:26:34,932 - Epoch: [14][  200/  391]    Overall Loss 0.486678    Objective Loss 0.486678    Top1 83.007812    Top5 99.324219    LR 0.300000    Time 0.022972    
2018-10-28 00:26:36,068 - Epoch: [14][  250/  391]    Overall Loss 0.490769    Objective Loss 0.490769    Top1 82.831250    Top5 99.312500    LR 0.300000    Time 0.022915    
2018-10-28 00:26:37,201 - Epoch: [14][  300/  391]    Overall Loss 0.492325    Objective Loss 0.492325    Top1 82.888021    Top5 99.333333    LR 0.300000    Time 0.022869    
2018-10-28 00:26:38,335 - Epoch: [14][  350/  391]    Overall Loss 0.489411    Objective Loss 0.489411    Top1 83.091518    Top5 99.337054    LR 0.300000    Time 0.022837    
2018-10-28 00:26:39,342 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            223 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   48.37963 | 0.57807 | -0.00438 |    0.32938 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            752 |    0.00000 |    0.00000 |  0.00000 |  5.07812 |  0.00000 |   67.36111 | 0.18853 | -0.00475 |    0.08674 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            864 |    0.00000 |    0.00000 |  0.00000 |  3.12500 |  0.00000 |   62.50000 | 0.19104 | -0.00743 |    0.09443 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |           1106 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.99653 | 0.19589 | -0.01405 |    0.10441 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            518 |    0.00000 |    0.00000 |  0.00000 | 16.40625 |  0.00000 |   77.51736 | 0.15739 | -0.00982 |    0.06236 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            904 |    0.00000 |    0.00000 |  0.00000 |  2.34375 |  0.00000 |   60.76389 | 0.19188 | -0.00966 |    0.09696 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            514 |    0.00000 |    0.00000 |  0.00000 | 16.79688 |  0.00000 |   77.69097 | 0.14511 | -0.00418 |    0.05700 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2428 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   47.30903 | 0.17768 | -0.00699 |    0.10543 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           6281 |    0.00000 |    0.00000 |  0.00000 |  0.09766 |  0.00000 |   31.84679 | 0.15810 | -0.01020 |    0.10495 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            234 |    0.00000 |    0.00000 |  0.00000 | 54.29688 |  0.00000 |   54.29688 | 0.30877 | -0.01839 |    0.18275 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           3779 |    0.00000 |    0.00000 |  0.00000 |  1.95312 |  0.00000 |   58.99523 | 0.13373 | -0.01137 |    0.07134 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           3135 |    0.00000 |    0.00000 |  0.00000 |  3.22266 |  0.00000 |   65.98307 | 0.11429 |  0.00090 |    0.05626 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           4248 |    0.00000 |    0.00000 |  0.00000 |  0.68359 |  0.00000 |   53.90625 | 0.12698 | -0.01033 |    0.07106 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           4759 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   48.36155 | 0.10643 | -0.00563 |    0.06269 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           6574 |    0.00000 |    0.00000 |  0.00000 |  3.12500 |  0.00000 |   64.33377 | 0.12036 | -0.00623 |    0.06051 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          20756 |    0.00000 |    0.00000 |  0.00000 |  0.12207 |  0.00000 |   43.69575 | 0.11560 | -0.00740 |    0.07024 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            631 |    0.00000 |    0.00000 |  0.00000 | 69.18945 |  0.00000 |   69.18945 | 0.16876 | -0.01037 |    0.08033 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          15017 |    0.00000 |    0.00000 |  0.00000 |  1.31836 |  0.00000 |   59.26378 | 0.10817 | -0.00665 |    0.05715 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          18634 |    0.00000 |    0.00000 |  0.00000 |  0.46387 |  0.00000 |   49.45204 | 0.08250 |  0.00145 |    0.04635 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          11522 |    0.00000 |    0.00000 |  0.00000 |  5.90820 |  0.00000 |   68.74457 | 0.04753 | -0.00012 |    0.02221 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           7640 |    0.00000 |    0.00000 |  0.00000 | 18.31055 |  0.00000 |   79.27517 | 0.04110 | -0.00409 |    0.01603 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            347 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   45.78125 | 0.53565 | -0.03523 |    0.33544 |
| 22 | Total sparsity:                     | -              |        270896 |         110866 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   59.07433 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:26:39,343 - Total sparsity: 59.07

2018-10-28 00:26:39,343 - --- validate (epoch=14)-----------
2018-10-28 00:26:39,343 - 10000 samples (128 per mini-batch)
2018-10-28 00:26:40,067 - Epoch: [14][   50/   78]    Loss 0.737050    Top1 76.843750    Top5 98.375000    
2018-10-28 00:26:40,457 - ==> Top1: 77.000    Top5: 98.330    Loss: 0.733

2018-10-28 00:26:40,457 - ==> Best Top1: 78.530   On Epoch: 12

2018-10-28 00:26:40,457 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:26:40,468 - 

2018-10-28 00:26:40,468 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:26:41,637 - Epoch: [15][   50/  391]    Overall Loss 0.485017    Objective Loss 0.485017    Top1 82.812500    Top5 99.312500    LR 0.300000    Time 0.023333    
2018-10-28 00:26:42,748 - Epoch: [15][  100/  391]    Overall Loss 0.478594    Objective Loss 0.478594    Top1 83.156250    Top5 99.359375    LR 0.300000    Time 0.022765    
2018-10-28 00:26:43,860 - Epoch: [15][  150/  391]    Overall Loss 0.488319    Objective Loss 0.488319    Top1 82.906250    Top5 99.276042    LR 0.300000    Time 0.022582    
2018-10-28 00:26:44,971 - Epoch: [15][  200/  391]    Overall Loss 0.482523    Objective Loss 0.482523    Top1 83.125000    Top5 99.332031    LR 0.300000    Time 0.022486    
2018-10-28 00:26:46,084 - Epoch: [15][  250/  391]    Overall Loss 0.484041    Objective Loss 0.484041    Top1 83.125000    Top5 99.284375    LR 0.300000    Time 0.022436    
2018-10-28 00:26:47,196 - Epoch: [15][  300/  391]    Overall Loss 0.487976    Objective Loss 0.487976    Top1 83.007812    Top5 99.247396    LR 0.300000    Time 0.022398    
2018-10-28 00:26:48,307 - Epoch: [15][  350/  391]    Overall Loss 0.487641    Objective Loss 0.487641    Top1 83.046875    Top5 99.265625    LR 0.300000    Time 0.022371    
2018-10-28 00:26:49,297 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            223 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   48.37963 | 0.57811 |  0.00861 |    0.32768 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            752 |    0.00000 |    0.00000 |  0.00000 |  5.07812 |  0.00000 |   67.36111 | 0.18894 | -0.00665 |    0.08602 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            864 |    0.00000 |    0.00000 |  0.00000 |  3.12500 |  0.00000 |   62.50000 | 0.19241 | -0.00578 |    0.09370 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |           1106 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.99653 | 0.19522 | -0.01290 |    0.10336 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            518 |    0.00000 |    0.00000 |  0.00000 | 16.40625 |  0.00000 |   77.51736 | 0.15700 | -0.00973 |    0.06127 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            904 |    0.00000 |    0.00000 |  0.00000 |  2.34375 |  0.00000 |   60.76389 | 0.19324 | -0.00969 |    0.09650 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            514 |    0.00000 |    0.00000 |  0.00000 | 16.79688 |  0.00000 |   77.69097 | 0.14418 | -0.00317 |    0.05599 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2428 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   47.30903 | 0.17810 | -0.00670 |    0.10472 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           6281 |    0.00000 |    0.00000 |  0.00000 |  0.09766 |  0.00000 |   31.84679 | 0.15884 | -0.01002 |    0.10536 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            234 |    0.00000 |    0.00000 |  0.00000 | 54.29688 |  0.00000 |   54.29688 | 0.30517 | -0.01710 |    0.17860 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           3779 |    0.00000 |    0.00000 |  0.00000 |  1.95312 |  0.00000 |   58.99523 | 0.13427 | -0.01119 |    0.07120 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           3135 |    0.00000 |    0.00000 |  0.00000 |  3.22266 |  0.00000 |   65.98307 | 0.11419 |  0.00040 |    0.05571 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           4248 |    0.00000 |    0.00000 |  0.00000 |  0.68359 |  0.00000 |   53.90625 | 0.12855 | -0.00992 |    0.07114 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           4759 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   48.36155 | 0.10789 | -0.00504 |    0.06350 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           6574 |    0.00000 |    0.00000 |  0.00000 |  3.12500 |  0.00000 |   64.33377 | 0.12075 | -0.00581 |    0.06020 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          20756 |    0.00000 |    0.00000 |  0.00000 |  0.12207 |  0.00000 |   43.69575 | 0.11650 | -0.00727 |    0.07049 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            631 |    0.00000 |    0.00000 |  0.00000 | 69.18945 |  0.00000 |   69.18945 | 0.16644 | -0.00891 |    0.07815 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          15017 |    0.00000 |    0.00000 |  0.00000 |  1.31836 |  0.00000 |   59.26378 | 0.10906 | -0.00652 |    0.05708 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          18634 |    0.00000 |    0.00000 |  0.00000 |  0.46387 |  0.00000 |   49.45204 | 0.08291 |  0.00176 |    0.04632 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          11522 |    0.00000 |    0.00000 |  0.00000 |  5.90820 |  0.00000 |   68.74457 | 0.04807 | -0.00020 |    0.02218 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           7640 |    0.00000 |    0.00000 |  0.00000 | 18.31055 |  0.00000 |   79.27517 | 0.04145 | -0.00398 |    0.01590 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            347 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   45.78125 | 0.53219 | -0.03539 |    0.33129 |
| 22 | Total sparsity:                     | -              |        270896 |         110866 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   59.07433 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:26:49,297 - Total sparsity: 59.07

2018-10-28 00:26:49,297 - --- validate (epoch=15)-----------
2018-10-28 00:26:49,297 - 10000 samples (128 per mini-batch)
2018-10-28 00:26:50,014 - Epoch: [15][   50/   78]    Loss 0.615173    Top1 80.234375    Top5 98.750000    
2018-10-28 00:26:50,408 - ==> Top1: 79.690    Top5: 98.760    Loss: 0.627

2018-10-28 00:26:50,409 - ==> Best Top1: 79.690   On Epoch: 15

2018-10-28 00:26:50,409 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:26:50,429 - 

2018-10-28 00:26:50,431 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:26:51,600 - Epoch: [16][   50/  391]    Overall Loss 0.469087    Objective Loss 0.469087    Top1 83.828125    Top5 99.312500    LR 0.300000    Time 0.023351    
2018-10-28 00:26:52,739 - Epoch: [16][  100/  391]    Overall Loss 0.475476    Objective Loss 0.475476    Top1 83.593750    Top5 99.382812    LR 0.300000    Time 0.023058    
2018-10-28 00:26:53,879 - Epoch: [16][  150/  391]    Overall Loss 0.466590    Objective Loss 0.466590    Top1 83.989583    Top5 99.375000    LR 0.300000    Time 0.022957    
2018-10-28 00:26:55,017 - Epoch: [16][  200/  391]    Overall Loss 0.471231    Objective Loss 0.471231    Top1 83.859375    Top5 99.312500    LR 0.300000    Time 0.022887    
2018-10-28 00:26:56,156 - Epoch: [16][  250/  391]    Overall Loss 0.471984    Objective Loss 0.471984    Top1 83.753125    Top5 99.328125    LR 0.300000    Time 0.022859    
2018-10-28 00:26:57,292 - Epoch: [16][  300/  391]    Overall Loss 0.472385    Objective Loss 0.472385    Top1 83.778646    Top5 99.309896    LR 0.300000    Time 0.022833    
2018-10-28 00:26:58,427 - Epoch: [16][  350/  391]    Overall Loss 0.473413    Objective Loss 0.473413    Top1 83.750000    Top5 99.290179    LR 0.300000    Time 0.022810    
2018-10-28 00:26:59,439 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            218 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   49.53704 | 0.57419 |  0.00674 |    0.32590 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            685 |    0.00000 |    0.00000 |  0.00000 |  8.98438 |  0.00000 |   70.26910 | 0.18916 | -0.00447 |    0.08471 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            811 |    0.00000 |    0.00000 |  0.00000 |  4.29688 |  0.00000 |   64.80035 | 0.19212 | -0.00571 |    0.09187 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |           1037 |    0.00000 |    0.00000 |  0.00000 |  1.17188 |  0.00000 |   54.99132 | 0.19384 | -0.01203 |    0.10185 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            486 |    0.00000 |    0.00000 |  0.00000 | 19.53125 |  0.00000 |   78.90625 | 0.15624 | -0.00913 |    0.05955 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            854 |    0.00000 |    0.00000 |  0.00000 |  3.51562 |  0.00000 |   62.93403 | 0.19291 | -0.00784 |    0.09513 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            477 |    0.00000 |    0.00000 |  0.00000 | 19.53125 |  0.00000 |   79.29688 | 0.14305 | -0.00235 |    0.05383 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2322 |    0.00000 |    0.00000 |  0.00000 |  1.17188 |  0.00000 |   49.60938 | 0.17704 | -0.00685 |    0.10301 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           6099 |    0.00000 |    0.00000 |  0.00000 |  0.09766 |  0.00000 |   33.82161 | 0.15898 | -0.00907 |    0.10475 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            224 |    0.00000 |    0.00000 |  0.00000 | 56.25000 |  0.00000 |   56.25000 | 0.30329 | -0.01846 |    0.17452 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           3573 |    0.00000 |    0.00000 |  0.00000 |  2.73438 |  0.00000 |   61.23047 | 0.13442 | -0.01183 |    0.07026 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           2922 |    0.00000 |    0.00000 |  0.00000 |  4.10156 |  0.00000 |   68.29427 | 0.11380 |  0.00100 |    0.05425 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           3995 |    0.00000 |    0.00000 |  0.00000 |  1.17188 |  0.00000 |   56.65148 | 0.12894 | -0.01004 |    0.07020 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           4501 |    0.00000 |    0.00000 |  0.00000 |  0.58594 |  0.00000 |   51.16102 | 0.10800 | -0.00464 |    0.06236 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           6186 |    0.00000 |    0.00000 |  0.00000 |  4.73633 |  0.00000 |   66.43880 | 0.12028 | -0.00608 |    0.05904 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          19853 |    0.00000 |    0.00000 |  0.00000 |  0.14648 |  0.00000 |   46.14529 | 0.11663 | -0.00689 |    0.06968 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            591 |    0.00000 |    0.00000 |  0.00000 | 71.14258 |  0.00000 |   71.14258 | 0.16334 | -0.00927 |    0.07516 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          14241 |    0.00000 |    0.00000 |  0.00000 |  2.00195 |  0.00000 |   61.36882 | 0.10927 | -0.00658 |    0.05631 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          17611 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   52.22711 | 0.08285 |  0.00120 |    0.04549 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          10589 |    0.00000 |    0.00000 |  0.00000 |  7.95898 |  0.00000 |   71.27550 | 0.04841 | -0.00006 |    0.02166 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           6806 |    0.00000 |    0.00000 |  0.00000 | 23.46191 |  0.00000 |   81.53754 | 0.04131 | -0.00363 |    0.01530 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            341 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   46.71875 | 0.54183 | -0.03168 |    0.33695 |
| 22 | Total sparsity:                     | -              |        270896 |         104422 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   61.45310 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:26:59,439 - Total sparsity: 61.45

2018-10-28 00:26:59,440 - --- validate (epoch=16)-----------
2018-10-28 00:26:59,440 - 10000 samples (128 per mini-batch)
2018-10-28 00:27:00,162 - Epoch: [16][   50/   78]    Loss 0.640221    Top1 79.421875    Top5 98.687500    
2018-10-28 00:27:00,556 - ==> Top1: 79.200    Top5: 98.780    Loss: 0.638

2018-10-28 00:27:00,557 - ==> Best Top1: 79.690   On Epoch: 15

2018-10-28 00:27:00,557 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:27:00,567 - 

2018-10-28 00:27:00,567 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:27:01,734 - Epoch: [17][   50/  391]    Overall Loss 0.476401    Objective Loss 0.476401    Top1 83.781250    Top5 99.109375    LR 0.300000    Time 0.023313    
2018-10-28 00:27:02,844 - Epoch: [17][  100/  391]    Overall Loss 0.476862    Objective Loss 0.476862    Top1 83.492188    Top5 99.203125    LR 0.300000    Time 0.022738    
2018-10-28 00:27:03,954 - Epoch: [17][  150/  391]    Overall Loss 0.472368    Objective Loss 0.472368    Top1 83.645833    Top5 99.203125    LR 0.300000    Time 0.022550    
2018-10-28 00:27:05,063 - Epoch: [17][  200/  391]    Overall Loss 0.472114    Objective Loss 0.472114    Top1 83.750000    Top5 99.230469    LR 0.300000    Time 0.022454    
2018-10-28 00:27:06,173 - Epoch: [17][  250/  391]    Overall Loss 0.473938    Objective Loss 0.473938    Top1 83.690625    Top5 99.234375    LR 0.300000    Time 0.022400    
2018-10-28 00:27:07,285 - Epoch: [17][  300/  391]    Overall Loss 0.475484    Objective Loss 0.475484    Top1 83.604167    Top5 99.250000    LR 0.300000    Time 0.022368    
2018-10-28 00:27:08,396 - Epoch: [17][  350/  391]    Overall Loss 0.477911    Objective Loss 0.477911    Top1 83.595982    Top5 99.218750    LR 0.300000    Time 0.022344    
2018-10-28 00:27:09,387 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            218 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   49.53704 | 0.57605 |  0.00585 |    0.32623 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            685 |    0.00000 |    0.00000 |  0.00000 |  8.98438 |  0.00000 |   70.26910 | 0.19013 | -0.00544 |    0.08356 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            811 |    0.00000 |    0.00000 |  0.00000 |  4.29688 |  0.00000 |   64.80035 | 0.19243 | -0.00610 |    0.09002 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |           1037 |    0.00000 |    0.00000 |  0.00000 |  1.17188 |  0.00000 |   54.99132 | 0.19321 | -0.01158 |    0.10041 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            486 |    0.00000 |    0.00000 |  0.00000 | 19.53125 |  0.00000 |   78.90625 | 0.15553 | -0.00755 |    0.05909 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            854 |    0.00000 |    0.00000 |  0.00000 |  3.51562 |  0.00000 |   62.93403 | 0.19419 | -0.00657 |    0.09507 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            477 |    0.00000 |    0.00000 |  0.00000 | 19.53125 |  0.00000 |   79.29688 | 0.14458 | -0.00147 |    0.05347 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2322 |    0.00000 |    0.00000 |  0.00000 |  1.17188 |  0.00000 |   49.60938 | 0.17820 | -0.00753 |    0.10322 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           6099 |    0.00000 |    0.00000 |  0.00000 |  0.09766 |  0.00000 |   33.82161 | 0.16006 | -0.00935 |    0.10525 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            224 |    0.00000 |    0.00000 |  0.00000 | 56.25000 |  0.00000 |   56.25000 | 0.30423 | -0.02114 |    0.17686 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           3573 |    0.00000 |    0.00000 |  0.00000 |  2.73438 |  0.00000 |   61.23047 | 0.13532 | -0.01132 |    0.06996 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           2922 |    0.00000 |    0.00000 |  0.00000 |  4.10156 |  0.00000 |   68.29427 | 0.11437 |  0.00180 |    0.05408 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           3995 |    0.00000 |    0.00000 |  0.00000 |  1.17188 |  0.00000 |   56.65148 | 0.12999 | -0.01092 |    0.07032 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           4501 |    0.00000 |    0.00000 |  0.00000 |  0.58594 |  0.00000 |   51.16102 | 0.10854 | -0.00412 |    0.06212 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           6186 |    0.00000 |    0.00000 |  0.00000 |  4.73633 |  0.00000 |   66.43880 | 0.12099 | -0.00610 |    0.05887 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          19853 |    0.00000 |    0.00000 |  0.00000 |  0.14648 |  0.00000 |   46.14529 | 0.11761 | -0.00696 |    0.06984 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            591 |    0.00000 |    0.00000 |  0.00000 | 71.14258 |  0.00000 |   71.14258 | 0.16332 | -0.00807 |    0.07481 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          14241 |    0.00000 |    0.00000 |  0.00000 |  2.00195 |  0.00000 |   61.36882 | 0.11045 | -0.00636 |    0.05656 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          17611 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   52.22711 | 0.08384 |  0.00124 |    0.04575 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          10589 |    0.00000 |    0.00000 |  0.00000 |  7.95898 |  0.00000 |   71.27550 | 0.04938 | -0.00021 |    0.02185 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           6806 |    0.00000 |    0.00000 |  0.00000 | 23.46191 |  0.00000 |   81.53754 | 0.04208 | -0.00363 |    0.01539 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            341 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   46.71875 | 0.54141 | -0.03401 |    0.33720 |
| 22 | Total sparsity:                     | -              |        270896 |         104422 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   61.45310 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:27:09,387 - Total sparsity: 61.45

2018-10-28 00:27:09,387 - --- validate (epoch=17)-----------
2018-10-28 00:27:09,388 - 10000 samples (128 per mini-batch)
2018-10-28 00:27:10,107 - Epoch: [17][   50/   78]    Loss 0.615703    Top1 79.421875    Top5 98.734375    
2018-10-28 00:27:10,497 - ==> Top1: 79.470    Top5: 98.750    Loss: 0.616

2018-10-28 00:27:10,498 - ==> Best Top1: 79.690   On Epoch: 15

2018-10-28 00:27:10,498 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:27:10,508 - 

2018-10-28 00:27:10,510 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:27:11,704 - Epoch: [18][   50/  391]    Overall Loss 0.450402    Objective Loss 0.450402    Top1 84.515625    Top5 99.562500    LR 0.300000    Time 0.023853    
2018-10-28 00:27:12,839 - Epoch: [18][  100/  391]    Overall Loss 0.466128    Objective Loss 0.466128    Top1 84.195312    Top5 99.406250    LR 0.300000    Time 0.023265    
2018-10-28 00:27:13,978 - Epoch: [18][  150/  391]    Overall Loss 0.476781    Objective Loss 0.476781    Top1 83.703125    Top5 99.395833    LR 0.300000    Time 0.023094    
2018-10-28 00:27:15,114 - Epoch: [18][  200/  391]    Overall Loss 0.475747    Objective Loss 0.475747    Top1 83.636719    Top5 99.375000    LR 0.300000    Time 0.022991    
2018-10-28 00:27:16,248 - Epoch: [18][  250/  391]    Overall Loss 0.467048    Objective Loss 0.467048    Top1 83.850000    Top5 99.371875    LR 0.300000    Time 0.022925    
2018-10-28 00:27:17,383 - Epoch: [18][  300/  391]    Overall Loss 0.465585    Objective Loss 0.465585    Top1 83.828125    Top5 99.354167    LR 0.300000    Time 0.022883    
2018-10-28 00:27:18,519 - Epoch: [18][  350/  391]    Overall Loss 0.462802    Objective Loss 0.462802    Top1 83.899554    Top5 99.348214    LR 0.300000    Time 0.022855    
2018-10-28 00:27:19,527 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            211 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.15741 | 0.57383 | -0.00129 |    0.32046 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            640 |    0.00000 |    0.00000 |  0.00000 | 11.71875 |  0.00000 |   72.22222 | 0.18998 | -0.00676 |    0.08181 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            750 |    0.00000 |    0.00000 |  0.00000 |  5.85938 |  0.00000 |   67.44792 | 0.19081 | -0.00720 |    0.08787 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            978 |    0.00000 |    0.00000 |  0.00000 |  1.56250 |  0.00000 |   57.55208 | 0.19047 | -0.01204 |    0.09822 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            444 |    0.00000 |    0.00000 |  0.00000 | 23.04688 |  0.00000 |   80.72917 | 0.15350 | -0.00804 |    0.05653 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            818 |    0.00000 |    0.00000 |  0.00000 |  4.29688 |  0.00000 |   64.49653 | 0.19334 | -0.01057 |    0.09374 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            443 |    0.00000 |    0.00000 |  0.00000 | 21.87500 |  0.00000 |   80.77257 | 0.14329 | -0.00175 |    0.05192 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2224 |    0.00000 |    0.00000 |  0.00000 |  1.36719 |  0.00000 |   51.73611 | 0.17774 | -0.00916 |    0.10105 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           5905 |    0.00000 |    0.00000 |  0.00000 |  0.09766 |  0.00000 |   35.92665 | 0.15988 | -0.00931 |    0.10375 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            219 |    0.00000 |    0.00000 |  0.00000 | 57.22656 |  0.00000 |   57.22656 | 0.30317 | -0.01464 |    0.17470 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           3374 |    0.00000 |    0.00000 |  0.00000 |  3.61328 |  0.00000 |   63.38976 | 0.13485 | -0.01041 |    0.06840 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           2741 |    0.00000 |    0.00000 |  0.00000 |  5.85938 |  0.00000 |   70.25825 | 0.11341 |  0.00179 |    0.05275 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           3784 |    0.00000 |    0.00000 |  0.00000 |  1.75781 |  0.00000 |   58.94097 | 0.13047 | -0.01107 |    0.06949 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           4274 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   53.62413 | 0.10854 | -0.00450 |    0.06083 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           5873 |    0.00000 |    0.00000 |  0.00000 |  6.29883 |  0.00000 |   68.13694 | 0.12084 | -0.00622 |    0.05784 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          19074 |    0.00000 |    0.00000 |  0.00000 |  0.24414 |  0.00000 |   48.25846 | 0.11779 | -0.00716 |    0.06906 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            560 |    0.00000 |    0.00000 |  0.00000 | 72.65625 |  0.00000 |   72.65625 | 0.16159 | -0.00797 |    0.07274 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          13532 |    0.00000 |    0.00000 |  0.00000 |  2.75879 |  0.00000 |   63.29210 | 0.11058 | -0.00692 |    0.05564 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          16688 |    0.00000 |    0.00000 |  0.00000 |  1.34277 |  0.00000 |   54.73090 | 0.08400 |  0.00076 |    0.04522 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           9716 |    0.00000 |    0.00000 |  0.00000 | 10.83984 |  0.00000 |   73.64366 | 0.04955 | -0.00043 |    0.02129 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           6204 |    0.00000 |    0.00000 |  0.00000 | 27.88086 |  0.00000 |   83.17057 | 0.04207 | -0.00341 |    0.01488 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            338 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   47.18750 | 0.54128 | -0.03246 |    0.33643 |
| 22 | Total sparsity:                     | -              |        270896 |          98790 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   63.53213 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:27:19,527 - Total sparsity: 63.53

2018-10-28 00:27:19,527 - --- validate (epoch=18)-----------
2018-10-28 00:27:19,527 - 10000 samples (128 per mini-batch)
2018-10-28 00:27:20,247 - Epoch: [18][   50/   78]    Loss 0.608649    Top1 79.953125    Top5 98.765625    
2018-10-28 00:27:20,638 - ==> Top1: 80.420    Top5: 98.870    Loss: 0.596

2018-10-28 00:27:20,639 - ==> Best Top1: 80.420   On Epoch: 18

2018-10-28 00:27:20,639 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:27:20,653 - 

2018-10-28 00:27:20,653 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:27:21,820 - Epoch: [19][   50/  391]    Overall Loss 0.436013    Objective Loss 0.436013    Top1 84.765625    Top5 99.609375    LR 0.300000    Time 0.023299    
2018-10-28 00:27:22,929 - Epoch: [19][  100/  391]    Overall Loss 0.446453    Objective Loss 0.446453    Top1 84.585938    Top5 99.429688    LR 0.300000    Time 0.022731    
2018-10-28 00:27:24,037 - Epoch: [19][  150/  391]    Overall Loss 0.455855    Objective Loss 0.455855    Top1 84.296875    Top5 99.348958    LR 0.300000    Time 0.022532    
2018-10-28 00:27:25,148 - Epoch: [19][  200/  391]    Overall Loss 0.456935    Objective Loss 0.456935    Top1 84.242188    Top5 99.312500    LR 0.300000    Time 0.022444    
2018-10-28 00:27:26,257 - Epoch: [19][  250/  391]    Overall Loss 0.459824    Objective Loss 0.459824    Top1 84.181250    Top5 99.340625    LR 0.300000    Time 0.022389    
2018-10-28 00:27:27,366 - Epoch: [19][  300/  391]    Overall Loss 0.462063    Objective Loss 0.462063    Top1 84.065104    Top5 99.351562    LR 0.300000    Time 0.022349    
2018-10-28 00:27:28,476 - Epoch: [19][  350/  391]    Overall Loss 0.461538    Objective Loss 0.461538    Top1 84.073661    Top5 99.341518    LR 0.300000    Time 0.022325    
2018-10-28 00:27:29,463 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            211 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.15741 | 0.57173 |  0.00954 |    0.32022 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            640 |    0.00000 |    0.00000 |  0.00000 | 11.71875 |  0.00000 |   72.22222 | 0.18958 | -0.00549 |    0.08134 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            750 |    0.00000 |    0.00000 |  0.00000 |  5.85938 |  0.00000 |   67.44792 | 0.19031 | -0.00608 |    0.08700 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            978 |    0.00000 |    0.00000 |  0.00000 |  1.56250 |  0.00000 |   57.55208 | 0.18934 | -0.01388 |    0.09748 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            444 |    0.00000 |    0.00000 |  0.00000 | 23.04688 |  0.00000 |   80.72917 | 0.15232 | -0.00964 |    0.05612 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            818 |    0.00000 |    0.00000 |  0.00000 |  4.29688 |  0.00000 |   64.49653 | 0.19419 | -0.01109 |    0.09390 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            443 |    0.00000 |    0.00000 |  0.00000 | 21.87500 |  0.00000 |   80.77257 | 0.14309 | -0.00115 |    0.05165 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2224 |    0.00000 |    0.00000 |  0.00000 |  1.36719 |  0.00000 |   51.73611 | 0.17803 | -0.00812 |    0.10148 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           5905 |    0.00000 |    0.00000 |  0.00000 |  0.09766 |  0.00000 |   35.92665 | 0.15978 | -0.01080 |    0.10370 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            219 |    0.00000 |    0.00000 |  0.00000 | 57.22656 |  0.00000 |   57.22656 | 0.30008 | -0.01367 |    0.17315 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           3374 |    0.00000 |    0.00000 |  0.00000 |  3.61328 |  0.00000 |   63.38976 | 0.13506 | -0.01009 |    0.06820 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           2741 |    0.00000 |    0.00000 |  0.00000 |  5.85938 |  0.00000 |   70.25825 | 0.11373 |  0.00159 |    0.05257 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           3784 |    0.00000 |    0.00000 |  0.00000 |  1.75781 |  0.00000 |   58.94097 | 0.13123 | -0.01072 |    0.06965 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           4274 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   53.62413 | 0.10893 | -0.00452 |    0.06064 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           5873 |    0.00000 |    0.00000 |  0.00000 |  6.29883 |  0.00000 |   68.13694 | 0.12088 | -0.00633 |    0.05758 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          19074 |    0.00000 |    0.00000 |  0.00000 |  0.24414 |  0.00000 |   48.25846 | 0.11837 | -0.00711 |    0.06925 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            560 |    0.00000 |    0.00000 |  0.00000 | 72.65625 |  0.00000 |   72.65625 | 0.16092 | -0.00715 |    0.07204 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          13532 |    0.00000 |    0.00000 |  0.00000 |  2.75879 |  0.00000 |   63.29210 | 0.11121 | -0.00648 |    0.05558 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          16688 |    0.00000 |    0.00000 |  0.00000 |  1.34277 |  0.00000 |   54.73090 | 0.08451 |  0.00103 |    0.04527 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           9716 |    0.00000 |    0.00000 |  0.00000 | 10.83984 |  0.00000 |   73.64366 | 0.05035 | -0.00027 |    0.02136 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           6204 |    0.00000 |    0.00000 |  0.00000 | 27.88086 |  0.00000 |   83.17057 | 0.04264 | -0.00324 |    0.01487 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            338 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   47.18750 | 0.53940 | -0.03350 |    0.33477 |
| 22 | Total sparsity:                     | -              |        270896 |          98790 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   63.53213 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:27:29,463 - Total sparsity: 63.53

2018-10-28 00:27:29,464 - --- validate (epoch=19)-----------
2018-10-28 00:27:29,464 - 10000 samples (128 per mini-batch)
2018-10-28 00:27:30,185 - Epoch: [19][   50/   78]    Loss 0.561604    Top1 81.718750    Top5 98.765625    
2018-10-28 00:27:30,573 - ==> Top1: 81.500    Top5: 98.910    Loss: 0.560

2018-10-28 00:27:30,574 - ==> Best Top1: 81.500   On Epoch: 19

2018-10-28 00:27:30,575 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:27:30,599 - 

2018-10-28 00:27:30,601 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:27:31,768 - Epoch: [20][   50/  391]    Overall Loss 0.421488    Objective Loss 0.421488    Top1 85.453125    Top5 99.578125    LR 0.300000    Time 0.023306    
2018-10-28 00:27:32,902 - Epoch: [20][  100/  391]    Overall Loss 0.435737    Objective Loss 0.435737    Top1 84.750000    Top5 99.484375    LR 0.300000    Time 0.022988    
2018-10-28 00:27:34,036 - Epoch: [20][  150/  391]    Overall Loss 0.445492    Objective Loss 0.445492    Top1 84.468750    Top5 99.406250    LR 0.300000    Time 0.022873    
2018-10-28 00:27:35,172 - Epoch: [20][  200/  391]    Overall Loss 0.449716    Objective Loss 0.449716    Top1 84.457031    Top5 99.375000    LR 0.300000    Time 0.022826    
2018-10-28 00:27:36,305 - Epoch: [20][  250/  391]    Overall Loss 0.450097    Objective Loss 0.450097    Top1 84.428125    Top5 99.378125    LR 0.300000    Time 0.022791    
2018-10-28 00:27:37,440 - Epoch: [20][  300/  391]    Overall Loss 0.450666    Objective Loss 0.450666    Top1 84.348958    Top5 99.382812    LR 0.300000    Time 0.022769    
2018-10-28 00:27:38,574 - Epoch: [20][  350/  391]    Overall Loss 0.449861    Objective Loss 0.449861    Top1 84.370536    Top5 99.395089    LR 0.300000    Time 0.022754    
2018-10-28 00:27:39,593 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            202 |    0.00000 |    0.00000 |  0.00000 |  2.08333 |  0.00000 |   53.24074 | 0.57093 |  0.00563 |    0.31826 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            604 |    0.00000 |    0.00000 |  6.25000 | 14.06250 |  0.00000 |   73.78472 | 0.18825 | -0.00402 |    0.07983 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            710 |    0.00000 |    0.00000 |  0.00000 |  8.98438 |  0.00000 |   69.18403 | 0.18895 | -0.00646 |    0.08449 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            935 |    0.00000 |    0.00000 |  0.00000 |  1.95312 |  0.00000 |   59.41840 | 0.18700 | -0.01117 |    0.09534 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            414 |    0.00000 |    0.00000 |  0.00000 | 25.39062 |  0.00000 |   82.03125 | 0.15034 | -0.00994 |    0.05425 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            774 |    0.00000 |    0.00000 |  0.00000 |  5.46875 |  0.00000 |   66.40625 | 0.19436 | -0.01048 |    0.09243 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            410 |    0.00000 |    0.00000 |  0.00000 | 23.04688 |  0.00000 |   82.20486 | 0.14240 | -0.00151 |    0.05020 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2125 |    0.00000 |    0.00000 |  0.00000 |  2.14844 |  0.00000 |   53.88455 | 0.17849 | -0.00860 |    0.10042 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           5723 |    0.00000 |    0.00000 |  0.00000 |  0.09766 |  0.00000 |   37.90148 | 0.16062 | -0.01029 |    0.10315 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            214 |    0.00000 |    0.00000 |  0.00000 | 58.20312 |  0.00000 |   58.20312 | 0.29607 | -0.01501 |    0.16942 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           3205 |    0.00000 |    0.00000 |  0.00000 |  4.58984 |  0.00000 |   65.22352 | 0.13449 | -0.01007 |    0.06701 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           2591 |    0.00000 |    0.00000 |  0.00000 |  7.32422 |  0.00000 |   71.88585 | 0.11308 |  0.00138 |    0.05117 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           3610 |    0.00000 |    0.00000 |  0.00000 |  2.05078 |  0.00000 |   60.82899 | 0.13117 | -0.01030 |    0.06847 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           4066 |    0.00000 |    0.00000 |  0.00000 |  0.97656 |  0.00000 |   55.88108 | 0.10902 | -0.00374 |    0.05974 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           5573 |    0.00000 |    0.00000 |  0.00000 |  7.42188 |  0.00000 |   69.76454 | 0.12067 | -0.00544 |    0.05643 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          18350 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   50.22244 | 0.11844 | -0.00746 |    0.06842 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            522 |    0.00000 |    0.00000 |  0.00000 | 74.51172 |  0.00000 |   74.51172 | 0.15917 | -0.00667 |    0.06940 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          12871 |    0.00000 |    0.00000 |  0.00000 |  3.58887 |  0.00000 |   65.08518 | 0.11107 | -0.00607 |    0.05470 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          15867 |    0.00000 |    0.00000 |  0.00000 |  1.73340 |  0.00000 |   56.95801 | 0.08436 |  0.00102 |    0.04461 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           9005 |    0.00000 |    0.00000 |  0.00000 | 13.74512 |  0.00000 |   75.57237 | 0.05053 | -0.00022 |    0.02099 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           5689 |    0.00000 |    0.00000 |  0.00000 | 31.98242 |  0.00000 |   84.56760 | 0.04254 | -0.00305 |    0.01440 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            332 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   48.12500 | 0.54171 | -0.03314 |    0.33572 |
| 22 | Total sparsity:                     | -              |        270896 |          93792 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   65.37712 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:27:39,594 - Total sparsity: 65.38

2018-10-28 00:27:39,594 - --- validate (epoch=20)-----------
2018-10-28 00:27:39,594 - 10000 samples (128 per mini-batch)
2018-10-28 00:27:40,315 - Epoch: [20][   50/   78]    Loss 0.605093    Top1 79.984375    Top5 98.875000    
2018-10-28 00:27:40,708 - ==> Top1: 80.030    Top5: 99.020    Loss: 0.593

2018-10-28 00:27:40,708 - ==> Best Top1: 81.500   On Epoch: 19

2018-10-28 00:27:40,708 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:27:40,719 - 

2018-10-28 00:27:40,719 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:27:41,889 - Epoch: [21][   50/  391]    Overall Loss 0.452630    Objective Loss 0.452630    Top1 84.546875    Top5 99.437500    LR 0.300000    Time 0.023352    
2018-10-28 00:27:42,999 - Epoch: [21][  100/  391]    Overall Loss 0.454007    Objective Loss 0.454007    Top1 84.476562    Top5 99.367188    LR 0.300000    Time 0.022768    
2018-10-28 00:27:44,111 - Epoch: [21][  150/  391]    Overall Loss 0.458771    Objective Loss 0.458771    Top1 84.333333    Top5 99.364583    LR 0.300000    Time 0.022579    
2018-10-28 00:27:45,221 - Epoch: [21][  200/  391]    Overall Loss 0.460095    Objective Loss 0.460095    Top1 84.285156    Top5 99.359375    LR 0.300000    Time 0.022481    
2018-10-28 00:27:46,332 - Epoch: [21][  250/  391]    Overall Loss 0.463104    Objective Loss 0.463104    Top1 84.165625    Top5 99.368750    LR 0.300000    Time 0.022423    
2018-10-28 00:27:47,443 - Epoch: [21][  300/  391]    Overall Loss 0.464340    Objective Loss 0.464340    Top1 84.151042    Top5 99.354167    LR 0.300000    Time 0.022385    
2018-10-28 00:27:48,554 - Epoch: [21][  350/  391]    Overall Loss 0.459224    Objective Loss 0.459224    Top1 84.261161    Top5 99.361607    LR 0.300000    Time 0.022358    
2018-10-28 00:27:49,544 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            202 |    0.00000 |    0.00000 |  0.00000 |  2.08333 |  0.00000 |   53.24074 | 0.57349 |  0.00249 |    0.31798 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            604 |    0.00000 |    0.00000 |  6.25000 | 14.06250 |  0.00000 |   73.78472 | 0.18893 | -0.00439 |    0.07916 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            710 |    0.00000 |    0.00000 |  0.00000 |  8.98438 |  0.00000 |   69.18403 | 0.18901 | -0.00625 |    0.08358 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            935 |    0.00000 |    0.00000 |  0.00000 |  1.95312 |  0.00000 |   59.41840 | 0.18614 | -0.01144 |    0.09418 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            414 |    0.00000 |    0.00000 |  0.00000 | 25.39062 |  0.00000 |   82.03125 | 0.15032 | -0.01001 |    0.05344 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            774 |    0.00000 |    0.00000 |  0.00000 |  5.46875 |  0.00000 |   66.40625 | 0.19549 | -0.00832 |    0.09250 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            410 |    0.00000 |    0.00000 |  0.00000 | 23.04688 |  0.00000 |   82.20486 | 0.14287 | -0.00225 |    0.04935 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2125 |    0.00000 |    0.00000 |  0.00000 |  2.14844 |  0.00000 |   53.88455 | 0.17917 | -0.00812 |    0.10053 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           5723 |    0.00000 |    0.00000 |  0.00000 |  0.09766 |  0.00000 |   37.90148 | 0.16127 | -0.00945 |    0.10329 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            214 |    0.00000 |    0.00000 |  0.00000 | 58.20312 |  0.00000 |   58.20312 | 0.29375 | -0.01098 |    0.16668 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           3205 |    0.00000 |    0.00000 |  0.00000 |  4.58984 |  0.00000 |   65.22352 | 0.13410 | -0.01004 |    0.06605 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           2591 |    0.00000 |    0.00000 |  0.00000 |  7.32422 |  0.00000 |   71.88585 | 0.11286 |  0.00263 |    0.05082 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           3610 |    0.00000 |    0.00000 |  0.00000 |  2.05078 |  0.00000 |   60.82899 | 0.13141 | -0.01084 |    0.06831 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           4066 |    0.00000 |    0.00000 |  0.00000 |  0.97656 |  0.00000 |   55.88108 | 0.10902 | -0.00371 |    0.05943 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           5573 |    0.00000 |    0.00000 |  0.00000 |  7.42188 |  0.00000 |   69.76454 | 0.12046 | -0.00680 |    0.05593 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          18350 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   50.22244 | 0.11854 | -0.00706 |    0.06819 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            522 |    0.00000 |    0.00000 |  0.00000 | 74.51172 |  0.00000 |   74.51172 | 0.15845 | -0.00601 |    0.06820 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          12871 |    0.00000 |    0.00000 |  0.00000 |  3.58887 |  0.00000 |   65.08518 | 0.11104 | -0.00587 |    0.05441 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          15867 |    0.00000 |    0.00000 |  0.00000 |  1.73340 |  0.00000 |   56.95801 | 0.08467 |  0.00108 |    0.04460 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           9005 |    0.00000 |    0.00000 |  0.00000 | 13.74512 |  0.00000 |   75.57237 | 0.05117 | -0.00030 |    0.02100 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           5689 |    0.00000 |    0.00000 |  0.00000 | 31.98242 |  0.00000 |   84.56760 | 0.04290 | -0.00308 |    0.01432 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            332 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   48.12500 | 0.54479 | -0.03102 |    0.33791 |
| 22 | Total sparsity:                     | -              |        270896 |          93792 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   65.37712 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:27:49,544 - Total sparsity: 65.38

2018-10-28 00:27:49,544 - --- validate (epoch=21)-----------
2018-10-28 00:27:49,544 - 10000 samples (128 per mini-batch)
2018-10-28 00:27:50,269 - Epoch: [21][   50/   78]    Loss 0.751702    Top1 77.687500    Top5 98.718750    
2018-10-28 00:27:50,658 - ==> Top1: 76.830    Top5: 98.790    Loss: 0.758

2018-10-28 00:27:50,659 - ==> Best Top1: 81.500   On Epoch: 19

2018-10-28 00:27:50,659 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:27:50,671 - 

2018-10-28 00:27:50,673 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:27:51,867 - Epoch: [22][   50/  391]    Overall Loss 0.457025    Objective Loss 0.457025    Top1 84.281250    Top5 99.375000    LR 0.300000    Time 0.023842    
2018-10-28 00:27:53,003 - Epoch: [22][  100/  391]    Overall Loss 0.446866    Objective Loss 0.446866    Top1 84.585938    Top5 99.320312    LR 0.300000    Time 0.023270    
2018-10-28 00:27:54,140 - Epoch: [22][  150/  391]    Overall Loss 0.448580    Objective Loss 0.448580    Top1 84.453125    Top5 99.354167    LR 0.300000    Time 0.023084    
2018-10-28 00:27:55,277 - Epoch: [22][  200/  391]    Overall Loss 0.446335    Objective Loss 0.446335    Top1 84.554688    Top5 99.371094    LR 0.300000    Time 0.022994    
2018-10-28 00:27:56,414 - Epoch: [22][  250/  391]    Overall Loss 0.447095    Objective Loss 0.447095    Top1 84.521875    Top5 99.353125    LR 0.300000    Time 0.022937    
2018-10-28 00:27:57,551 - Epoch: [22][  300/  391]    Overall Loss 0.448273    Objective Loss 0.448273    Top1 84.468750    Top5 99.346354    LR 0.300000    Time 0.022901    
2018-10-28 00:27:58,688 - Epoch: [22][  350/  391]    Overall Loss 0.449238    Objective Loss 0.449238    Top1 84.444196    Top5 99.339286    LR 0.300000    Time 0.022874    
2018-10-28 00:27:59,703 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            195 |    0.00000 |    0.00000 |  0.00000 |  4.16667 |  0.00000 |   54.86111 | 0.57284 |  0.00985 |    0.31647 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            573 |    0.00000 |    0.00000 |  6.25000 | 14.84375 |  0.00000 |   75.13021 | 0.18916 | -0.00516 |    0.07737 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            661 |    0.00000 |    0.00000 |  0.00000 |  9.76562 |  0.00000 |   71.31076 | 0.18825 | -0.00493 |    0.08196 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            891 |    0.00000 |    0.00000 |  0.00000 |  2.34375 |  0.00000 |   61.32812 | 0.18499 | -0.01163 |    0.09211 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            388 |    0.00000 |    0.00000 |  0.00000 | 27.73438 |  0.00000 |   83.15972 | 0.14980 | -0.00865 |    0.05244 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            735 |    0.00000 |    0.00000 |  0.00000 |  6.25000 |  0.00000 |   68.09896 | 0.19580 | -0.00747 |    0.09127 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            377 |    0.00000 |    0.00000 |  0.00000 | 26.95312 |  0.00000 |   83.63715 | 0.14239 | -0.00037 |    0.04841 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2054 |    0.00000 |    0.00000 |  0.00000 |  2.53906 |  0.00000 |   55.42535 | 0.17962 | -0.00884 |    0.10004 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           5577 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   39.48568 | 0.16138 | -0.00898 |    0.10263 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            207 |    0.00000 |    0.00000 |  0.00000 | 59.57031 |  0.00000 |   59.57031 | 0.28953 | -0.01384 |    0.16337 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           3041 |    0.00000 |    0.00000 |  0.00000 |  5.46875 |  0.00000 |   67.00304 | 0.13322 | -0.00989 |    0.06493 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           2462 |    0.00000 |    0.00000 |  0.00000 |  8.78906 |  0.00000 |   73.28559 | 0.11224 |  0.00246 |    0.04940 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           3444 |    0.00000 |    0.00000 |  0.00000 |  2.92969 |  0.00000 |   62.63021 | 0.13171 | -0.01079 |    0.06717 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           3867 |    0.00000 |    0.00000 |  0.00000 |  1.17188 |  0.00000 |   58.04036 | 0.10906 | -0.00335 |    0.05828 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           5295 |    0.00000 |    0.00000 |  0.00000 |  8.54492 |  0.00000 |   71.27279 | 0.12002 | -0.00604 |    0.05497 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          17670 |    0.00000 |    0.00000 |  0.00000 |  0.61035 |  0.00000 |   52.06706 | 0.11866 | -0.00697 |    0.06735 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            485 |    0.00000 |    0.00000 |  0.00000 | 76.31836 |  0.00000 |   76.31836 | 0.15556 | -0.00523 |    0.06586 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          12263 |    0.00000 |    0.00000 |  0.00000 |  4.34570 |  0.00000 |   66.73448 | 0.11095 | -0.00585 |    0.05356 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          15213 |    0.00000 |    0.00000 |  0.00000 |  2.39258 |  0.00000 |   58.73210 | 0.08466 |  0.00118 |    0.04397 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           8397 |    0.00000 |    0.00000 |  0.00000 | 16.43066 |  0.00000 |   77.22168 | 0.05178 | -0.00031 |    0.02073 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           5233 |    0.00000 |    0.00000 |  0.00000 | 35.69336 |  0.00000 |   85.80458 | 0.04302 | -0.00299 |    0.01399 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            329 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   48.59375 | 0.54378 | -0.03188 |    0.33620 |
| 22 | Total sparsity:                     | -              |        270896 |          89357 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   67.01428 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:27:59,703 - Total sparsity: 67.01

2018-10-28 00:27:59,703 - --- validate (epoch=22)-----------
2018-10-28 00:27:59,703 - 10000 samples (128 per mini-batch)
2018-10-28 00:28:00,427 - Epoch: [22][   50/   78]    Loss 0.650735    Top1 79.031250    Top5 98.718750    
2018-10-28 00:28:00,819 - ==> Top1: 79.240    Top5: 98.840    Loss: 0.646

2018-10-28 00:28:00,819 - ==> Best Top1: 81.500   On Epoch: 19

2018-10-28 00:28:00,820 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:28:00,829 - 

2018-10-28 00:28:00,829 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:28:02,000 - Epoch: [23][   50/  391]    Overall Loss 0.437043    Objective Loss 0.437043    Top1 84.421875    Top5 99.531250    LR 0.300000    Time 0.023372    
2018-10-28 00:28:03,111 - Epoch: [23][  100/  391]    Overall Loss 0.439508    Objective Loss 0.439508    Top1 84.421875    Top5 99.406250    LR 0.300000    Time 0.022779    
2018-10-28 00:28:04,219 - Epoch: [23][  150/  391]    Overall Loss 0.436459    Objective Loss 0.436459    Top1 84.755208    Top5 99.437500    LR 0.300000    Time 0.022569    
2018-10-28 00:28:05,328 - Epoch: [23][  200/  391]    Overall Loss 0.437277    Objective Loss 0.437277    Top1 84.933594    Top5 99.437500    LR 0.300000    Time 0.022465    
2018-10-28 00:28:06,437 - Epoch: [23][  250/  391]    Overall Loss 0.440135    Objective Loss 0.440135    Top1 84.868750    Top5 99.409375    LR 0.300000    Time 0.022402    
2018-10-28 00:28:07,547 - Epoch: [23][  300/  391]    Overall Loss 0.438852    Objective Loss 0.438852    Top1 84.838542    Top5 99.403646    LR 0.300000    Time 0.022363    
2018-10-28 00:28:08,655 - Epoch: [23][  350/  391]    Overall Loss 0.442419    Objective Loss 0.442419    Top1 84.785714    Top5 99.392857    LR 0.300000    Time 0.022332    
2018-10-28 00:28:09,645 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            195 |    0.00000 |    0.00000 |  0.00000 |  4.16667 |  0.00000 |   54.86111 | 0.57261 | -0.00726 |    0.31190 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            573 |    0.00000 |    0.00000 |  6.25000 | 14.84375 |  0.00000 |   75.13021 | 0.18750 | -0.00676 |    0.07671 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            661 |    0.00000 |    0.00000 |  0.00000 |  9.76562 |  0.00000 |   71.31076 | 0.18714 | -0.00526 |    0.08112 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            891 |    0.00000 |    0.00000 |  0.00000 |  2.34375 |  0.00000 |   61.32812 | 0.18418 | -0.01293 |    0.09080 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            388 |    0.00000 |    0.00000 |  0.00000 | 27.73438 |  0.00000 |   83.15972 | 0.14924 | -0.00785 |    0.05180 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            735 |    0.00000 |    0.00000 |  0.00000 |  6.25000 |  0.00000 |   68.09896 | 0.19556 | -0.00632 |    0.09030 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            377 |    0.00000 |    0.00000 |  0.00000 | 26.95312 |  0.00000 |   83.63715 | 0.14145 | -0.00014 |    0.04814 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2054 |    0.00000 |    0.00000 |  0.00000 |  2.53906 |  0.00000 |   55.42535 | 0.17957 | -0.00870 |    0.09948 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           5577 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   39.48568 | 0.16132 | -0.00818 |    0.10212 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            207 |    0.00000 |    0.00000 |  0.00000 | 59.57031 |  0.00000 |   59.57031 | 0.28902 | -0.00744 |    0.16288 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           3041 |    0.00000 |    0.00000 |  0.00000 |  5.46875 |  0.00000 |   67.00304 | 0.13311 | -0.01002 |    0.06424 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           2462 |    0.00000 |    0.00000 |  0.00000 |  8.78906 |  0.00000 |   73.28559 | 0.11208 |  0.00280 |    0.04907 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           3444 |    0.00000 |    0.00000 |  0.00000 |  2.92969 |  0.00000 |   62.63021 | 0.13264 | -0.01047 |    0.06721 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           3867 |    0.00000 |    0.00000 |  0.00000 |  1.17188 |  0.00000 |   58.04036 | 0.10961 | -0.00359 |    0.05831 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           5295 |    0.00000 |    0.00000 |  0.00000 |  8.54492 |  0.00000 |   71.27279 | 0.11986 | -0.00588 |    0.05460 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          17670 |    0.00000 |    0.00000 |  0.00000 |  0.61035 |  0.00000 |   52.06706 | 0.11873 | -0.00655 |    0.06711 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            485 |    0.00000 |    0.00000 |  0.00000 | 76.31836 |  0.00000 |   76.31836 | 0.15351 | -0.00541 |    0.06430 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          12263 |    0.00000 |    0.00000 |  0.00000 |  4.34570 |  0.00000 |   66.73448 | 0.11102 | -0.00598 |    0.05330 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          15213 |    0.00000 |    0.00000 |  0.00000 |  2.39258 |  0.00000 |   58.73210 | 0.08469 |  0.00125 |    0.04373 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           8397 |    0.00000 |    0.00000 |  0.00000 | 16.43066 |  0.00000 |   77.22168 | 0.05210 | -0.00035 |    0.02071 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           5233 |    0.00000 |    0.00000 |  0.00000 | 35.69336 |  0.00000 |   85.80458 | 0.04313 | -0.00283 |    0.01388 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            329 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   48.59375 | 0.54075 | -0.03330 |    0.33245 |
| 22 | Total sparsity:                     | -              |        270896 |          89357 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   67.01428 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:28:09,645 - Total sparsity: 67.01

2018-10-28 00:28:09,645 - --- validate (epoch=23)-----------
2018-10-28 00:28:09,646 - 10000 samples (128 per mini-batch)
2018-10-28 00:28:10,363 - Epoch: [23][   50/   78]    Loss 0.718200    Top1 78.031250    Top5 98.343750    
2018-10-28 00:28:10,755 - ==> Top1: 77.720    Top5: 98.440    Loss: 0.720

2018-10-28 00:28:10,756 - ==> Best Top1: 81.500   On Epoch: 19

2018-10-28 00:28:10,756 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:28:10,778 - 

2018-10-28 00:28:10,779 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:28:11,947 - Epoch: [24][   50/  391]    Overall Loss 0.457221    Objective Loss 0.457221    Top1 84.234375    Top5 99.312500    LR 0.300000    Time 0.023314    
2018-10-28 00:28:13,082 - Epoch: [24][  100/  391]    Overall Loss 0.449333    Objective Loss 0.449333    Top1 84.187500    Top5 99.382812    LR 0.300000    Time 0.022994    
2018-10-28 00:28:14,217 - Epoch: [24][  150/  391]    Overall Loss 0.442213    Objective Loss 0.442213    Top1 84.541667    Top5 99.359375    LR 0.300000    Time 0.022891    
2018-10-28 00:28:15,354 - Epoch: [24][  200/  391]    Overall Loss 0.438204    Objective Loss 0.438204    Top1 84.800781    Top5 99.402344    LR 0.300000    Time 0.022846    
2018-10-28 00:28:16,490 - Epoch: [24][  250/  391]    Overall Loss 0.443724    Objective Loss 0.443724    Top1 84.668750    Top5 99.378125    LR 0.300000    Time 0.022816    
2018-10-28 00:28:17,624 - Epoch: [24][  300/  391]    Overall Loss 0.445889    Objective Loss 0.445889    Top1 84.554688    Top5 99.361979    LR 0.300000    Time 0.022789    
2018-10-28 00:28:18,758 - Epoch: [24][  350/  391]    Overall Loss 0.446911    Objective Loss 0.446911    Top1 84.497768    Top5 99.345982    LR 0.300000    Time 0.022769    
2018-10-28 00:28:19,771 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            191 |    0.00000 |    0.00000 |  0.00000 |  6.25000 |  6.25000 |   55.78704 | 0.57521 |  0.00722 |    0.31140 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            549 |    0.00000 |    0.00000 |  6.25000 | 17.18750 |  0.00000 |   76.17188 | 0.18687 | -0.00550 |    0.07581 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            617 |    0.00000 |    0.00000 |  0.00000 | 12.50000 |  0.00000 |   73.22049 | 0.18702 | -0.00550 |    0.07949 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            850 |    0.00000 |    0.00000 |  0.00000 |  2.73438 |  0.00000 |   63.10764 | 0.18378 | -0.01193 |    0.08927 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            362 |    0.00000 |    0.00000 |  0.00000 | 31.25000 |  0.00000 |   84.28819 | 0.14793 | -0.00780 |    0.05048 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            692 |    0.00000 |    0.00000 |  0.00000 |  8.98438 |  0.00000 |   69.96528 | 0.19628 | -0.00927 |    0.08935 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            359 |    0.00000 |    0.00000 |  0.00000 | 28.12500 |  0.00000 |   84.41840 | 0.14133 | -0.00106 |    0.04733 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1984 |    0.00000 |    0.00000 |  0.00000 |  3.32031 |  0.00000 |   56.94444 | 0.18025 | -0.00882 |    0.09847 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           5415 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   41.24349 | 0.16152 | -0.00740 |    0.10134 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            203 |    0.00000 |    0.00000 |  0.00000 | 60.35156 |  0.00000 |   60.35156 | 0.28820 | -0.00918 |    0.16074 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2916 |    0.00000 |    0.00000 |  0.00000 |  6.25000 |  0.00000 |   68.35938 | 0.13318 | -0.01047 |    0.06324 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           2324 |    0.00000 |    0.00000 |  0.00000 | 10.35156 |  0.00000 |   74.78299 | 0.11174 |  0.00253 |    0.04824 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           3296 |    0.00000 |    0.00000 |  0.00000 |  3.80859 |  0.00000 |   64.23611 | 0.13337 | -0.01110 |    0.06701 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           3707 |    0.00000 |    0.00000 |  0.00000 |  1.56250 |  0.00000 |   59.77648 | 0.11052 | -0.00339 |    0.05782 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           5072 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   72.48264 | 0.12003 | -0.00588 |    0.05391 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          17035 |    0.00000 |    0.00000 |  0.00000 |  0.83008 |  0.00000 |   53.78961 | 0.11904 | -0.00670 |    0.06655 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            456 |    0.00000 |    0.00000 |  0.00000 | 77.73438 |  0.00000 |   77.73438 | 0.15317 | -0.00566 |    0.06331 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          11731 |    0.00000 |    0.00000 |  0.00000 |  5.17578 |  0.00000 |   68.17763 | 0.11117 | -0.00635 |    0.05266 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          14549 |    0.00000 |    0.00000 |  0.00000 |  3.22266 |  0.00000 |   60.53331 | 0.08493 |  0.00133 |    0.04321 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           7924 |    0.00000 |    0.00000 |  0.00000 | 18.50586 |  0.00000 |   78.50477 | 0.05281 | -0.00036 |    0.02057 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           4855 |    0.00000 |    0.00000 |  0.00000 | 38.57422 |  0.00000 |   86.82997 | 0.04356 | -0.00253 |    0.01369 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            323 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   49.53125 | 0.54399 | -0.03106 |    0.33443 |
| 22 | Total sparsity:                     | -              |        270896 |          85410 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   68.47130 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:28:19,772 - Total sparsity: 68.47

2018-10-28 00:28:19,772 - --- validate (epoch=24)-----------
2018-10-28 00:28:19,772 - 10000 samples (128 per mini-batch)
2018-10-28 00:28:20,497 - Epoch: [24][   50/   78]    Loss 0.568260    Top1 81.562500    Top5 99.000000    
2018-10-28 00:28:20,891 - ==> Top1: 81.490    Top5: 99.080    Loss: 0.562

2018-10-28 00:28:20,892 - ==> Best Top1: 81.500   On Epoch: 19

2018-10-28 00:28:20,892 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:28:20,908 - 

2018-10-28 00:28:20,908 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:28:22,051 - Epoch: [25][   50/  391]    Overall Loss 0.415981    Objective Loss 0.415981    Top1 85.343750    Top5 99.437500    LR 0.300000    Time 0.022826    
2018-10-28 00:28:23,161 - Epoch: [25][  100/  391]    Overall Loss 0.423104    Objective Loss 0.423104    Top1 85.242188    Top5 99.421875    LR 0.300000    Time 0.022495    
2018-10-28 00:28:24,272 - Epoch: [25][  150/  391]    Overall Loss 0.435492    Objective Loss 0.435492    Top1 84.869792    Top5 99.390625    LR 0.300000    Time 0.022395    
2018-10-28 00:28:25,383 - Epoch: [25][  200/  391]    Overall Loss 0.439435    Objective Loss 0.439435    Top1 84.781250    Top5 99.410156    LR 0.300000    Time 0.022347    
2018-10-28 00:28:26,495 - Epoch: [25][  250/  391]    Overall Loss 0.437780    Objective Loss 0.437780    Top1 84.862500    Top5 99.393750    LR 0.300000    Time 0.022318    
2018-10-28 00:28:27,608 - Epoch: [25][  300/  391]    Overall Loss 0.439564    Objective Loss 0.439564    Top1 84.773438    Top5 99.403646    LR 0.300000    Time 0.022303    
2018-10-28 00:28:28,718 - Epoch: [25][  350/  391]    Overall Loss 0.440859    Objective Loss 0.440859    Top1 84.738839    Top5 99.395089    LR 0.300000    Time 0.022285    
2018-10-28 00:28:29,707 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            191 |    0.00000 |    0.00000 |  0.00000 |  6.25000 |  6.25000 |   55.78704 | 0.57413 |  0.00357 |    0.31135 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            549 |    0.00000 |    0.00000 |  6.25000 | 17.18750 |  0.00000 |   76.17188 | 0.18816 | -0.00716 |    0.07527 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            617 |    0.00000 |    0.00000 |  0.00000 | 12.50000 |  0.00000 |   73.22049 | 0.18754 | -0.00465 |    0.07934 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            850 |    0.00000 |    0.00000 |  0.00000 |  2.73438 |  0.00000 |   63.10764 | 0.18349 | -0.01214 |    0.08910 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            362 |    0.00000 |    0.00000 |  0.00000 | 31.25000 |  0.00000 |   84.28819 | 0.14733 | -0.00728 |    0.04917 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            692 |    0.00000 |    0.00000 |  0.00000 |  8.98438 |  0.00000 |   69.96528 | 0.19621 | -0.00877 |    0.08856 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            359 |    0.00000 |    0.00000 |  0.00000 | 28.12500 |  0.00000 |   84.41840 | 0.14004 | -0.00055 |    0.04622 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1984 |    0.00000 |    0.00000 |  0.00000 |  3.32031 |  0.00000 |   56.94444 | 0.18053 | -0.00965 |    0.09846 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           5415 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   41.24349 | 0.16166 | -0.00770 |    0.10101 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            203 |    0.00000 |    0.00000 |  0.00000 | 60.35156 |  0.00000 |   60.35156 | 0.28516 | -0.01100 |    0.15682 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2916 |    0.00000 |    0.00000 |  0.00000 |  6.25000 |  0.00000 |   68.35938 | 0.13234 | -0.01004 |    0.06267 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           2324 |    0.00000 |    0.00000 |  0.00000 | 10.35156 |  0.00000 |   74.78299 | 0.11108 |  0.00250 |    0.04785 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           3296 |    0.00000 |    0.00000 |  0.00000 |  3.80859 |  0.00000 |   64.23611 | 0.13329 | -0.01069 |    0.06667 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           3707 |    0.00000 |    0.00000 |  0.00000 |  1.56250 |  0.00000 |   59.77648 | 0.11036 | -0.00327 |    0.05742 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           5072 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   72.48264 | 0.11985 | -0.00563 |    0.05346 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          17035 |    0.00000 |    0.00000 |  0.00000 |  0.83008 |  0.00000 |   53.78961 | 0.11910 | -0.00687 |    0.06631 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            456 |    0.00000 |    0.00000 |  0.00000 | 77.73438 |  0.00000 |   77.73438 | 0.15216 | -0.00652 |    0.06230 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          11731 |    0.00000 |    0.00000 |  0.00000 |  5.17578 |  0.00000 |   68.17763 | 0.11128 | -0.00617 |    0.05248 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          14549 |    0.00000 |    0.00000 |  0.00000 |  3.22266 |  0.00000 |   60.53331 | 0.08486 |  0.00108 |    0.04301 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           7924 |    0.00000 |    0.00000 |  0.00000 | 18.50586 |  0.00000 |   78.50477 | 0.05331 | -0.00046 |    0.02063 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           4855 |    0.00000 |    0.00000 |  0.00000 | 38.57422 |  0.00000 |   86.82997 | 0.04372 | -0.00256 |    0.01362 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            323 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   49.53125 | 0.54617 | -0.03126 |    0.33566 |
| 22 | Total sparsity:                     | -              |        270896 |          85410 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   68.47130 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:28:29,707 - Total sparsity: 68.47

2018-10-28 00:28:29,707 - --- validate (epoch=25)-----------
2018-10-28 00:28:29,707 - 10000 samples (128 per mini-batch)
2018-10-28 00:28:30,431 - Epoch: [25][   50/   78]    Loss 0.604130    Top1 80.703125    Top5 98.937500    
2018-10-28 00:28:30,819 - ==> Top1: 80.740    Top5: 99.010    Loss: 0.603

2018-10-28 00:28:30,820 - ==> Best Top1: 81.500   On Epoch: 19

2018-10-28 00:28:30,820 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:28:30,831 - 

2018-10-28 00:28:30,833 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:28:32,027 - Epoch: [26][   50/  391]    Overall Loss 0.437566    Objective Loss 0.437566    Top1 85.296875    Top5 99.390625    LR 0.300000    Time 0.023848    
2018-10-28 00:28:33,163 - Epoch: [26][  100/  391]    Overall Loss 0.425336    Objective Loss 0.425336    Top1 85.453125    Top5 99.437500    LR 0.300000    Time 0.023270    
2018-10-28 00:28:34,299 - Epoch: [26][  150/  391]    Overall Loss 0.431482    Objective Loss 0.431482    Top1 85.276042    Top5 99.395833    LR 0.300000    Time 0.023077    
2018-10-28 00:28:35,434 - Epoch: [26][  200/  391]    Overall Loss 0.428642    Objective Loss 0.428642    Top1 85.226562    Top5 99.421875    LR 0.300000    Time 0.022975    
2018-10-28 00:28:36,570 - Epoch: [26][  250/  391]    Overall Loss 0.430643    Objective Loss 0.430643    Top1 85.181250    Top5 99.409375    LR 0.300000    Time 0.022919    
2018-10-28 00:28:37,704 - Epoch: [26][  300/  391]    Overall Loss 0.433016    Objective Loss 0.433016    Top1 85.093750    Top5 99.395833    LR 0.300000    Time 0.022877    
2018-10-28 00:28:38,839 - Epoch: [26][  350/  391]    Overall Loss 0.434160    Objective Loss 0.434160    Top1 85.071429    Top5 99.397321    LR 0.300000    Time 0.022848    
2018-10-28 00:28:39,853 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            184 |    0.00000 |    0.00000 |  0.00000 |  8.33333 |  6.25000 |   57.40741 | 0.57558 |  0.00382 |    0.31208 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            522 |    0.00000 |    0.00000 |  6.25000 | 19.53125 |  0.00000 |   77.34375 | 0.18760 | -0.00620 |    0.07288 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            583 |    0.00000 |    0.00000 |  0.00000 | 14.45312 |  0.00000 |   74.69618 | 0.18732 | -0.00430 |    0.07777 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            823 |    0.00000 |    0.00000 |  0.00000 |  5.07812 |  0.00000 |   64.27951 | 0.18214 | -0.01231 |    0.08790 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            338 |    0.00000 |    0.00000 |  0.00000 | 33.59375 |  0.00000 |   85.32986 | 0.14517 | -0.00694 |    0.04770 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            671 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.87674 | 0.19630 | -0.00834 |    0.08770 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            334 |    0.00000 |    0.00000 |  0.00000 | 32.42188 |  0.00000 |   85.50347 | 0.13805 | -0.00023 |    0.04413 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1917 |    0.00000 |    0.00000 |  0.00000 |  3.90625 |  0.00000 |   58.39844 | 0.17993 | -0.00913 |    0.09688 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           5266 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   42.86024 | 0.16142 | -0.00765 |    0.09974 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            193 |    0.00000 |    0.00000 |  0.00000 | 62.30469 |  0.00000 |   62.30469 | 0.28286 | -0.01526 |    0.15410 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2792 |    0.00000 |    0.00000 |  0.00000 |  7.22656 |  0.00000 |   69.70486 | 0.13223 | -0.00901 |    0.06144 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           2206 |    0.00000 |    0.00000 |  0.00000 | 11.91406 |  0.00000 |   76.06337 | 0.11021 |  0.00182 |    0.04658 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           3185 |    0.00000 |    0.00000 |  0.00000 |  4.49219 |  0.00000 |   65.44054 | 0.13249 | -0.01116 |    0.06547 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           3535 |    0.00000 |    0.00000 |  0.00000 |  2.14844 |  0.00000 |   61.64280 | 0.10962 | -0.00340 |    0.05604 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           4853 |    0.00000 |    0.00000 |  0.00000 | 11.91406 |  0.00000 |   73.67079 | 0.11944 | -0.00566 |    0.05254 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          16437 |    0.00000 |    0.00000 |  0.00000 |  1.12305 |  0.00000 |   55.41178 | 0.11876 | -0.00653 |    0.06534 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            430 |    0.00000 |    0.00000 |  0.00000 | 79.00391 |  0.00000 |   79.00391 | 0.15016 | -0.00773 |    0.06086 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          11282 |    0.00000 |    0.00000 |  0.00000 |  5.95703 |  0.00000 |   69.39562 | 0.11102 | -0.00596 |    0.05168 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          13925 |    0.00000 |    0.00000 |  0.00000 |  3.78418 |  0.00000 |   62.22602 | 0.08482 |  0.00111 |    0.04256 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           7511 |    0.00000 |    0.00000 |  0.00000 | 20.67871 |  0.00000 |   79.62511 | 0.05361 | -0.00060 |    0.02035 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           4542 |    0.00000 |    0.00000 |  0.00000 | 41.28418 |  0.00000 |   87.67904 | 0.04373 | -0.00247 |    0.01324 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            323 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   49.53125 | 0.54295 | -0.03431 |    0.33272 |
| 22 | Total sparsity:                     | -              |        270896 |          81852 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   69.78471 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:28:39,853 - Total sparsity: 69.78

2018-10-28 00:28:39,853 - --- validate (epoch=26)-----------
2018-10-28 00:28:39,853 - 10000 samples (128 per mini-batch)
2018-10-28 00:28:40,574 - Epoch: [26][   50/   78]    Loss 0.565709    Top1 80.937500    Top5 99.171875    
2018-10-28 00:28:40,961 - ==> Top1: 81.030    Top5: 99.270    Loss: 0.568

2018-10-28 00:28:40,961 - ==> Best Top1: 81.500   On Epoch: 19

2018-10-28 00:28:40,962 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:28:40,972 - 

2018-10-28 00:28:40,973 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:28:42,142 - Epoch: [27][   50/  391]    Overall Loss 0.424913    Objective Loss 0.424913    Top1 85.093750    Top5 99.390625    LR 0.300000    Time 0.023361    
2018-10-28 00:28:43,254 - Epoch: [27][  100/  391]    Overall Loss 0.445130    Objective Loss 0.445130    Top1 84.593750    Top5 99.390625    LR 0.300000    Time 0.022784    
2018-10-28 00:28:44,365 - Epoch: [27][  150/  391]    Overall Loss 0.434613    Objective Loss 0.434613    Top1 85.067708    Top5 99.390625    LR 0.300000    Time 0.022588    
2018-10-28 00:28:45,475 - Epoch: [27][  200/  391]    Overall Loss 0.433484    Objective Loss 0.433484    Top1 85.128906    Top5 99.371094    LR 0.300000    Time 0.022485    
2018-10-28 00:28:46,585 - Epoch: [27][  250/  391]    Overall Loss 0.433824    Objective Loss 0.433824    Top1 85.068750    Top5 99.384375    LR 0.300000    Time 0.022408    
2018-10-28 00:28:47,699 - Epoch: [27][  300/  391]    Overall Loss 0.431181    Objective Loss 0.431181    Top1 85.070312    Top5 99.403646    LR 0.300000    Time 0.022381    
2018-10-28 00:28:48,811 - Epoch: [27][  350/  391]    Overall Loss 0.433202    Objective Loss 0.433202    Top1 84.955357    Top5 99.399554    LR 0.300000    Time 0.022359    
2018-10-28 00:28:49,800 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            184 |    0.00000 |    0.00000 |  0.00000 |  8.33333 |  6.25000 |   57.40741 | 0.57111 |  0.00324 |    0.30694 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            522 |    0.00000 |    0.00000 |  6.25000 | 19.53125 |  0.00000 |   77.34375 | 0.18760 | -0.00591 |    0.07319 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            583 |    0.00000 |    0.00000 |  0.00000 | 14.45312 |  0.00000 |   74.69618 | 0.18740 | -0.00478 |    0.07755 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            823 |    0.00000 |    0.00000 |  0.00000 |  5.07812 |  0.00000 |   64.27951 | 0.18230 | -0.01397 |    0.08755 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            338 |    0.00000 |    0.00000 |  0.00000 | 33.59375 |  0.00000 |   85.32986 | 0.14547 | -0.00733 |    0.04823 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            671 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.87674 | 0.19618 | -0.00828 |    0.08697 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            334 |    0.00000 |    0.00000 |  0.00000 | 32.42188 |  0.00000 |   85.50347 | 0.13717 | -0.00057 |    0.04433 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1917 |    0.00000 |    0.00000 |  0.00000 |  3.90625 |  0.00000 |   58.39844 | 0.17965 | -0.00880 |    0.09659 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           5266 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   42.86024 | 0.16088 | -0.00845 |    0.09914 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            193 |    0.00000 |    0.00000 |  0.00000 | 62.30469 |  0.00000 |   62.30469 | 0.28225 | -0.01399 |    0.15197 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2792 |    0.00000 |    0.00000 |  0.00000 |  7.22656 |  0.00000 |   69.70486 | 0.13187 | -0.00890 |    0.06064 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           2206 |    0.00000 |    0.00000 |  0.00000 | 11.91406 |  0.00000 |   76.06337 | 0.11006 |  0.00152 |    0.04631 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           3185 |    0.00000 |    0.00000 |  0.00000 |  4.49219 |  0.00000 |   65.44054 | 0.13274 | -0.01161 |    0.06554 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           3535 |    0.00000 |    0.00000 |  0.00000 |  2.14844 |  0.00000 |   61.64280 | 0.10966 | -0.00261 |    0.05585 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           4853 |    0.00000 |    0.00000 |  0.00000 | 11.91406 |  0.00000 |   73.67079 | 0.11933 | -0.00541 |    0.05224 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          16437 |    0.00000 |    0.00000 |  0.00000 |  1.12305 |  0.00000 |   55.41178 | 0.11898 | -0.00685 |    0.06524 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            430 |    0.00000 |    0.00000 |  0.00000 | 79.00391 |  0.00000 |   79.00391 | 0.14954 | -0.00587 |    0.06013 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          11282 |    0.00000 |    0.00000 |  0.00000 |  5.95703 |  0.00000 |   69.39562 | 0.11106 | -0.00622 |    0.05150 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          13925 |    0.00000 |    0.00000 |  0.00000 |  3.78418 |  0.00000 |   62.22602 | 0.08499 |  0.00082 |    0.04248 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           7511 |    0.00000 |    0.00000 |  0.00000 | 20.67871 |  0.00000 |   79.62511 | 0.05420 | -0.00077 |    0.02048 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           4542 |    0.00000 |    0.00000 |  0.00000 | 41.28418 |  0.00000 |   87.67904 | 0.04399 | -0.00238 |    0.01321 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            323 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   49.53125 | 0.54234 | -0.03469 |    0.33123 |
| 22 | Total sparsity:                     | -              |        270896 |          81852 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   69.78471 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:28:49,801 - Total sparsity: 69.78

2018-10-28 00:28:49,801 - --- validate (epoch=27)-----------
2018-10-28 00:28:49,801 - 10000 samples (128 per mini-batch)
2018-10-28 00:28:50,520 - Epoch: [27][   50/   78]    Loss 0.726252    Top1 77.421875    Top5 98.359375    
2018-10-28 00:28:50,908 - ==> Top1: 77.270    Top5: 98.420    Loss: 0.738

2018-10-28 00:28:50,909 - ==> Best Top1: 81.500   On Epoch: 19

2018-10-28 00:28:50,909 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:28:50,919 - 

2018-10-28 00:28:50,921 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:28:52,115 - Epoch: [28][   50/  391]    Overall Loss 0.431734    Objective Loss 0.431734    Top1 85.250000    Top5 99.515625    LR 0.300000    Time 0.023858    
2018-10-28 00:28:53,252 - Epoch: [28][  100/  391]    Overall Loss 0.429893    Objective Loss 0.429893    Top1 85.296875    Top5 99.445312    LR 0.300000    Time 0.023278    
2018-10-28 00:28:54,390 - Epoch: [28][  150/  391]    Overall Loss 0.423408    Objective Loss 0.423408    Top1 85.296875    Top5 99.401042    LR 0.300000    Time 0.023097    
2018-10-28 00:28:55,529 - Epoch: [28][  200/  391]    Overall Loss 0.423439    Objective Loss 0.423439    Top1 85.285156    Top5 99.406250    LR 0.300000    Time 0.023012    
2018-10-28 00:28:56,667 - Epoch: [28][  250/  391]    Overall Loss 0.430759    Objective Loss 0.430759    Top1 85.028125    Top5 99.371875    LR 0.300000    Time 0.022956    
2018-10-28 00:28:57,804 - Epoch: [28][  300/  391]    Overall Loss 0.428162    Objective Loss 0.428162    Top1 85.065104    Top5 99.385417    LR 0.300000    Time 0.022903    
2018-10-28 00:28:58,940 - Epoch: [28][  350/  391]    Overall Loss 0.426279    Objective Loss 0.426279    Top1 85.140625    Top5 99.408482    LR 0.300000    Time 0.022876    
2018-10-28 00:28:59,952 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            181 |    0.00000 |    0.00000 |  0.00000 |  8.33333 |  6.25000 |   58.10185 | 0.57287 |  0.00555 |    0.30454 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            490 |    0.00000 |    0.00000 |  6.25000 | 20.31250 |  0.00000 |   78.73264 | 0.18677 | -0.00450 |    0.07213 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            555 |    0.00000 |    0.00000 |  0.00000 | 16.01562 |  0.00000 |   75.91146 | 0.18693 | -0.00593 |    0.07679 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            792 |    0.00000 |    0.00000 |  0.00000 |  5.85938 |  0.00000 |   65.62500 | 0.18285 | -0.01403 |    0.08679 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            326 |    0.00000 |    0.00000 |  0.00000 | 34.76562 |  0.00000 |   85.85069 | 0.14508 | -0.00769 |    0.04778 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            636 |    0.00000 |    0.00000 |  0.00000 | 11.32812 |  0.00000 |   72.39583 | 0.19549 | -0.00729 |    0.08582 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            320 |    0.00000 |    0.00000 |  0.00000 | 33.98438 |  0.00000 |   86.11111 | 0.13610 | -0.00019 |    0.04260 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1856 |    0.00000 |    0.00000 |  0.00000 |  3.90625 |  0.00000 |   59.72222 | 0.17918 | -0.00937 |    0.09509 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           5133 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   44.30339 | 0.16113 | -0.00861 |    0.09840 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            185 |    0.00000 |    0.00000 |  0.00000 | 63.86719 |  0.00000 |   63.86719 | 0.27982 | -0.01462 |    0.14944 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2663 |    0.00000 |    0.00000 |  0.00000 |  8.00781 |  0.00000 |   71.10460 | 0.13184 | -0.00841 |    0.06004 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           2109 |    0.00000 |    0.00000 |  0.00000 | 13.57422 |  0.00000 |   77.11589 | 0.11001 |  0.00132 |    0.04532 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           3057 |    0.00000 |    0.00000 |  0.00000 |  5.46875 |  0.00000 |   66.82943 | 0.13312 | -0.01158 |    0.06480 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           3368 |    0.00000 |    0.00000 |  0.00000 |  3.02734 |  0.00000 |   63.45486 | 0.11006 | -0.00238 |    0.05497 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           4610 |    0.00000 |    0.00000 |  0.00000 | 13.72070 |  0.00000 |   74.98915 | 0.11884 | -0.00541 |    0.05113 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          15883 |    0.00000 |    0.00000 |  0.00000 |  1.31836 |  0.00000 |   56.91461 | 0.11910 | -0.00701 |    0.06460 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            412 |    0.00000 |    0.00000 |  0.00000 | 79.88281 |  0.00000 |   79.88281 | 0.14845 | -0.00599 |    0.05900 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          10881 |    0.00000 |    0.00000 |  0.00000 |  6.76270 |  0.00000 |   70.48340 | 0.11088 | -0.00596 |    0.05079 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          13383 |    0.00000 |    0.00000 |  0.00000 |  4.49219 |  0.00000 |   63.69629 | 0.08493 |  0.00091 |    0.04200 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           7155 |    0.00000 |    0.00000 |  0.00000 | 22.82715 |  0.00000 |   80.59082 | 0.05453 | -0.00069 |    0.02027 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           4254 |    0.00000 |    0.00000 |  0.00000 | 43.77441 |  0.00000 |   88.46029 | 0.04415 | -0.00233 |    0.01301 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            321 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   49.84375 | 0.54035 | -0.03421 |    0.33146 |
| 22 | Total sparsity:                     | -              |        270896 |          78570 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   70.99625 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:28:59,952 - Total sparsity: 71.00

2018-10-28 00:28:59,953 - --- validate (epoch=28)-----------
2018-10-28 00:28:59,953 - 10000 samples (128 per mini-batch)
2018-10-28 00:29:00,677 - Epoch: [28][   50/   78]    Loss 0.590609    Top1 80.515625    Top5 99.140625    
2018-10-28 00:29:01,073 - ==> Top1: 80.930    Top5: 99.200    Loss: 0.575

2018-10-28 00:29:01,074 - ==> Best Top1: 81.500   On Epoch: 19

2018-10-28 00:29:01,074 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:29:01,091 - 

2018-10-28 00:29:01,091 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:29:02,239 - Epoch: [29][   50/  391]    Overall Loss 0.438556    Objective Loss 0.438556    Top1 85.265625    Top5 99.421875    LR 0.300000    Time 0.022935    
2018-10-28 00:29:03,350 - Epoch: [29][  100/  391]    Overall Loss 0.420142    Objective Loss 0.420142    Top1 85.632812    Top5 99.476562    LR 0.300000    Time 0.022558    
2018-10-28 00:29:04,460 - Epoch: [29][  150/  391]    Overall Loss 0.414463    Objective Loss 0.414463    Top1 85.682292    Top5 99.505208    LR 0.300000    Time 0.022431    
2018-10-28 00:29:05,571 - Epoch: [29][  200/  391]    Overall Loss 0.417625    Objective Loss 0.417625    Top1 85.699219    Top5 99.492188    LR 0.300000    Time 0.022372    
2018-10-28 00:29:06,683 - Epoch: [29][  250/  391]    Overall Loss 0.419909    Objective Loss 0.419909    Top1 85.606250    Top5 99.484375    LR 0.300000    Time 0.022341    
2018-10-28 00:29:07,794 - Epoch: [29][  300/  391]    Overall Loss 0.421379    Objective Loss 0.421379    Top1 85.510417    Top5 99.492188    LR 0.300000    Time 0.022318    
2018-10-28 00:29:08,906 - Epoch: [29][  350/  391]    Overall Loss 0.422871    Objective Loss 0.422871    Top1 85.395089    Top5 99.484375    LR 0.300000    Time 0.022292    
2018-10-28 00:29:09,893 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            181 |    0.00000 |    0.00000 |  0.00000 |  8.33333 |  6.25000 |   58.10185 | 0.57201 | -0.00123 |    0.30374 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            490 |    0.00000 |    0.00000 |  6.25000 | 20.31250 |  0.00000 |   78.73264 | 0.18679 | -0.00509 |    0.07116 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            555 |    0.00000 |    0.00000 |  0.00000 | 16.01562 |  0.00000 |   75.91146 | 0.18644 | -0.00735 |    0.07608 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            792 |    0.00000 |    0.00000 |  0.00000 |  5.85938 |  0.00000 |   65.62500 | 0.18290 | -0.01230 |    0.08733 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            326 |    0.00000 |    0.00000 |  0.00000 | 34.76562 |  0.00000 |   85.85069 | 0.14428 | -0.00773 |    0.04709 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            636 |    0.00000 |    0.00000 |  0.00000 | 11.32812 |  0.00000 |   72.39583 | 0.19587 | -0.00897 |    0.08537 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            320 |    0.00000 |    0.00000 |  0.00000 | 33.98438 |  0.00000 |   86.11111 | 0.13551 | -0.00089 |    0.04227 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1856 |    0.00000 |    0.00000 |  0.00000 |  3.90625 |  0.00000 |   59.72222 | 0.17940 | -0.00979 |    0.09482 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           5133 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   44.30339 | 0.16116 | -0.00810 |    0.09825 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            185 |    0.00000 |    0.00000 |  0.00000 | 63.86719 |  0.00000 |   63.86719 | 0.27831 | -0.01640 |    0.14906 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2663 |    0.00000 |    0.00000 |  0.00000 |  8.00781 |  0.00000 |   71.10460 | 0.13220 | -0.00784 |    0.05975 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           2109 |    0.00000 |    0.00000 |  0.00000 | 13.57422 |  0.00000 |   77.11589 | 0.11012 |  0.00153 |    0.04491 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           3057 |    0.00000 |    0.00000 |  0.00000 |  5.46875 |  0.00000 |   66.82943 | 0.13284 | -0.01138 |    0.06461 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           3368 |    0.00000 |    0.00000 |  0.00000 |  3.02734 |  0.00000 |   63.45486 | 0.10979 | -0.00195 |    0.05482 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           4610 |    0.00000 |    0.00000 |  0.00000 | 13.72070 |  0.00000 |   74.98915 | 0.11852 | -0.00524 |    0.05083 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          15883 |    0.00000 |    0.00000 |  0.00000 |  1.31836 |  0.00000 |   56.91461 | 0.11925 | -0.00753 |    0.06433 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            412 |    0.00000 |    0.00000 |  0.00000 | 79.88281 |  0.00000 |   79.88281 | 0.14747 | -0.00430 |    0.05785 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          10881 |    0.00000 |    0.00000 |  0.00000 |  6.76270 |  0.00000 |   70.48340 | 0.11086 | -0.00571 |    0.05054 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          13383 |    0.00000 |    0.00000 |  0.00000 |  4.49219 |  0.00000 |   63.69629 | 0.08504 |  0.00047 |    0.04184 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           7155 |    0.00000 |    0.00000 |  0.00000 | 22.82715 |  0.00000 |   80.59082 | 0.05489 | -0.00077 |    0.02028 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           4254 |    0.00000 |    0.00000 |  0.00000 | 43.77441 |  0.00000 |   88.46029 | 0.04433 | -0.00227 |    0.01293 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            321 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   49.84375 | 0.53742 | -0.03524 |    0.32786 |
| 22 | Total sparsity:                     | -              |        270896 |          78570 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   70.99625 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:29:09,893 - Total sparsity: 71.00

2018-10-28 00:29:09,893 - --- validate (epoch=29)-----------
2018-10-28 00:29:09,894 - 10000 samples (128 per mini-batch)
2018-10-28 00:29:10,613 - Epoch: [29][   50/   78]    Loss 0.592886    Top1 80.671875    Top5 98.875000    
2018-10-28 00:29:11,004 - ==> Top1: 80.670    Top5: 99.000    Loss: 0.588

2018-10-28 00:29:11,005 - ==> Best Top1: 81.500   On Epoch: 19

2018-10-28 00:29:11,005 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:29:11,022 - 

2018-10-28 00:29:11,023 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:29:12,191 - Epoch: [30][   50/  391]    Overall Loss 0.405303    Objective Loss 0.405303    Top1 85.937500    Top5 99.593750    LR 0.300000    Time 0.023315    
2018-10-28 00:29:13,328 - Epoch: [30][  100/  391]    Overall Loss 0.408006    Objective Loss 0.408006    Top1 85.804688    Top5 99.476562    LR 0.300000    Time 0.023019    
2018-10-28 00:29:14,465 - Epoch: [30][  150/  391]    Overall Loss 0.413559    Objective Loss 0.413559    Top1 85.557292    Top5 99.479167    LR 0.300000    Time 0.022914    
2018-10-28 00:29:15,600 - Epoch: [30][  200/  391]    Overall Loss 0.415593    Objective Loss 0.415593    Top1 85.496094    Top5 99.492188    LR 0.300000    Time 0.022857    
2018-10-28 00:29:16,736 - Epoch: [30][  250/  391]    Overall Loss 0.420130    Objective Loss 0.420130    Top1 85.265625    Top5 99.462500    LR 0.300000    Time 0.022823    
2018-10-28 00:29:17,872 - Epoch: [30][  300/  391]    Overall Loss 0.419574    Objective Loss 0.419574    Top1 85.281250    Top5 99.476562    LR 0.300000    Time 0.022802    
2018-10-28 00:29:19,007 - Epoch: [30][  350/  391]    Overall Loss 0.420517    Objective Loss 0.420517    Top1 85.319196    Top5 99.450893    LR 0.300000    Time 0.022784    
2018-10-28 00:29:20,020 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            177 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   59.02778 | 0.57465 |  0.00487 |    0.30678 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            468 |    0.00000 |    0.00000 |  6.25000 | 22.65625 |  0.00000 |   79.68750 | 0.18732 | -0.00570 |    0.07104 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            528 |    0.00000 |    0.00000 |  0.00000 | 19.53125 |  0.00000 |   77.08333 | 0.18695 | -0.00559 |    0.07481 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            763 |    0.00000 |    0.00000 |  0.00000 |  7.42188 |  0.00000 |   66.88368 | 0.18283 | -0.01142 |    0.08557 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            314 |    0.00000 |    0.00000 |  0.00000 | 36.32812 |  0.00000 |   86.37153 | 0.14331 | -0.00764 |    0.04576 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            613 |    0.00000 |    0.00000 |  0.00000 | 14.06250 |  0.00000 |   73.39410 | 0.19588 | -0.00803 |    0.08529 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            309 |    0.00000 |    0.00000 |  0.00000 | 35.54688 |  0.00000 |   86.58854 | 0.13562 | -0.00025 |    0.04118 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1791 |    0.00000 |    0.00000 |  0.00000 |  4.49219 |  0.00000 |   61.13281 | 0.17883 | -0.00751 |    0.09386 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4992 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   45.83333 | 0.16110 | -0.00803 |    0.09744 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            181 |    0.00000 |    0.00000 |  0.00000 | 64.64844 |  0.00000 |   64.64844 | 0.27836 | -0.01566 |    0.14942 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2539 |    0.00000 |    0.00000 |  0.00000 |  9.57031 |  0.00000 |   72.45009 | 0.13093 | -0.00796 |    0.05841 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1993 |    0.00000 |    0.00000 |  0.00000 | 15.33203 |  0.00000 |   78.37457 | 0.10923 |  0.00210 |    0.04355 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2954 |    0.00000 |    0.00000 |  0.00000 |  5.95703 |  0.00000 |   67.94705 | 0.13275 | -0.01163 |    0.06379 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           3229 |    0.00000 |    0.00000 |  0.00000 |  3.61328 |  0.00000 |   64.96311 | 0.10959 | -0.00181 |    0.05389 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           4420 |    0.00000 |    0.00000 |  0.00000 | 14.94141 |  0.00000 |   76.01997 | 0.11853 | -0.00562 |    0.05003 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          15377 |    0.00000 |    0.00000 |  0.00000 |  1.46484 |  0.00000 |   58.28722 | 0.11964 | -0.00694 |    0.06396 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            398 |    0.00000 |    0.00000 |  0.00000 | 80.56641 |  0.00000 |   80.56641 | 0.14663 | -0.00353 |    0.05675 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          10480 |    0.00000 |    0.00000 |  0.00000 |  7.64160 |  0.00000 |   71.57118 | 0.11091 | -0.00590 |    0.05006 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          12878 |    0.00000 |    0.00000 |  0.00000 |  5.22461 |  0.00000 |   65.06619 | 0.08503 |  0.00109 |    0.04137 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           6796 |    0.00000 |    0.00000 |  0.00000 | 24.58496 |  0.00000 |   81.56467 | 0.05545 | -0.00070 |    0.02011 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           4003 |    0.00000 |    0.00000 |  0.00000 | 46.06934 |  0.00000 |   89.14117 | 0.04447 | -0.00216 |    0.01272 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            316 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   50.62500 | 0.54027 | -0.03365 |    0.32932 |
| 22 | Total sparsity:                     | -              |        270896 |          75519 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   72.12251 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:29:20,020 - Total sparsity: 72.12

2018-10-28 00:29:20,020 - --- validate (epoch=30)-----------
2018-10-28 00:29:20,020 - 10000 samples (128 per mini-batch)
2018-10-28 00:29:20,742 - Epoch: [30][   50/   78]    Loss 0.524900    Top1 83.562500    Top5 99.234375    
2018-10-28 00:29:21,133 - ==> Top1: 83.060    Top5: 99.310    Loss: 0.524

2018-10-28 00:29:21,134 - ==> Best Top1: 83.060   On Epoch: 30

2018-10-28 00:29:21,134 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:29:21,149 - 

2018-10-28 00:29:21,150 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:29:22,292 - Epoch: [31][   50/  391]    Overall Loss 0.400027    Objective Loss 0.400027    Top1 86.000000    Top5 99.453125    LR 0.300000    Time 0.022812    
2018-10-28 00:29:23,404 - Epoch: [31][  100/  391]    Overall Loss 0.421201    Objective Loss 0.421201    Top1 85.296875    Top5 99.414062    LR 0.300000    Time 0.022511    
2018-10-28 00:29:24,518 - Epoch: [31][  150/  391]    Overall Loss 0.418039    Objective Loss 0.418039    Top1 85.526042    Top5 99.447917    LR 0.300000    Time 0.022427    
2018-10-28 00:29:25,629 - Epoch: [31][  200/  391]    Overall Loss 0.419827    Objective Loss 0.419827    Top1 85.421875    Top5 99.445312    LR 0.300000    Time 0.022349    
2018-10-28 00:29:26,741 - Epoch: [31][  250/  391]    Overall Loss 0.420321    Objective Loss 0.420321    Top1 85.306250    Top5 99.450000    LR 0.300000    Time 0.022320    
2018-10-28 00:29:27,853 - Epoch: [31][  300/  391]    Overall Loss 0.422922    Objective Loss 0.422922    Top1 85.229167    Top5 99.453125    LR 0.300000    Time 0.022302    
2018-10-28 00:29:28,965 - Epoch: [31][  350/  391]    Overall Loss 0.420253    Objective Loss 0.420253    Top1 85.366071    Top5 99.450893    LR 0.300000    Time 0.022290    
2018-10-28 00:29:29,956 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            177 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   59.02778 | 0.57061 |  0.00926 |    0.30158 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            468 |    0.00000 |    0.00000 |  6.25000 | 22.65625 |  0.00000 |   79.68750 | 0.18697 | -0.00533 |    0.07024 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            528 |    0.00000 |    0.00000 |  0.00000 | 19.53125 |  0.00000 |   77.08333 | 0.18670 | -0.00787 |    0.07499 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            763 |    0.00000 |    0.00000 |  0.00000 |  7.42188 |  0.00000 |   66.88368 | 0.18298 | -0.01174 |    0.08528 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            314 |    0.00000 |    0.00000 |  0.00000 | 36.32812 |  0.00000 |   86.37153 | 0.14255 | -0.00730 |    0.04518 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            613 |    0.00000 |    0.00000 |  0.00000 | 14.06250 |  0.00000 |   73.39410 | 0.19707 | -0.00884 |    0.08559 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            309 |    0.00000 |    0.00000 |  0.00000 | 35.54688 |  0.00000 |   86.58854 | 0.13600 |  0.00091 |    0.04095 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1791 |    0.00000 |    0.00000 |  0.00000 |  4.49219 |  0.00000 |   61.13281 | 0.17937 | -0.00725 |    0.09379 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4992 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   45.83333 | 0.16140 | -0.00760 |    0.09744 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            181 |    0.00000 |    0.00000 |  0.00000 | 64.64844 |  0.00000 |   64.64844 | 0.27633 | -0.01501 |    0.14833 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2539 |    0.00000 |    0.00000 |  0.00000 |  9.57031 |  0.00000 |   72.45009 | 0.13130 | -0.00819 |    0.05781 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1993 |    0.00000 |    0.00000 |  0.00000 | 15.33203 |  0.00000 |   78.37457 | 0.10916 |  0.00202 |    0.04345 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2954 |    0.00000 |    0.00000 |  0.00000 |  5.95703 |  0.00000 |   67.94705 | 0.13271 | -0.01145 |    0.06341 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           3229 |    0.00000 |    0.00000 |  0.00000 |  3.61328 |  0.00000 |   64.96311 | 0.10978 | -0.00229 |    0.05375 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           4420 |    0.00000 |    0.00000 |  0.00000 | 14.94141 |  0.00000 |   76.01997 | 0.11847 | -0.00559 |    0.04971 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          15377 |    0.00000 |    0.00000 |  0.00000 |  1.46484 |  0.00000 |   58.28722 | 0.11930 | -0.00749 |    0.06363 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            398 |    0.00000 |    0.00000 |  0.00000 | 80.56641 |  0.00000 |   80.56641 | 0.14595 | -0.00477 |    0.05674 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          10480 |    0.00000 |    0.00000 |  0.00000 |  7.64160 |  0.00000 |   71.57118 | 0.11065 | -0.00647 |    0.04954 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          12878 |    0.00000 |    0.00000 |  0.00000 |  5.22461 |  0.00000 |   65.06619 | 0.08473 |  0.00081 |    0.04107 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           6796 |    0.00000 |    0.00000 |  0.00000 | 24.58496 |  0.00000 |   81.56467 | 0.05556 | -0.00073 |    0.02005 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           4003 |    0.00000 |    0.00000 |  0.00000 | 46.06934 |  0.00000 |   89.14117 | 0.04454 | -0.00208 |    0.01260 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            316 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   50.62500 | 0.53972 | -0.03381 |    0.32820 |
| 22 | Total sparsity:                     | -              |        270896 |          75519 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   72.12251 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:29:29,956 - Total sparsity: 72.12

2018-10-28 00:29:29,956 - --- validate (epoch=31)-----------
2018-10-28 00:29:29,956 - 10000 samples (128 per mini-batch)
2018-10-28 00:29:30,679 - Epoch: [31][   50/   78]    Loss 0.612830    Top1 80.296875    Top5 98.968750    
2018-10-28 00:29:31,070 - ==> Top1: 80.060    Top5: 98.970    Loss: 0.619

2018-10-28 00:29:31,071 - ==> Best Top1: 83.060   On Epoch: 30

2018-10-28 00:29:31,071 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:29:31,082 - 

2018-10-28 00:29:31,084 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:29:32,282 - Epoch: [32][   50/  391]    Overall Loss 0.425993    Objective Loss 0.425993    Top1 85.500000    Top5 99.343750    LR 0.300000    Time 0.023919    
2018-10-28 00:29:33,419 - Epoch: [32][  100/  391]    Overall Loss 0.431272    Objective Loss 0.431272    Top1 85.171875    Top5 99.343750    LR 0.300000    Time 0.023315    
2018-10-28 00:29:34,554 - Epoch: [32][  150/  391]    Overall Loss 0.425416    Objective Loss 0.425416    Top1 85.343750    Top5 99.354167    LR 0.300000    Time 0.023104    
2018-10-28 00:29:35,689 - Epoch: [32][  200/  391]    Overall Loss 0.425253    Objective Loss 0.425253    Top1 85.347656    Top5 99.312500    LR 0.300000    Time 0.022995    
2018-10-28 00:29:36,824 - Epoch: [32][  250/  391]    Overall Loss 0.425043    Objective Loss 0.425043    Top1 85.337500    Top5 99.334375    LR 0.300000    Time 0.022931    
2018-10-28 00:29:37,959 - Epoch: [32][  300/  391]    Overall Loss 0.423102    Objective Loss 0.423102    Top1 85.453125    Top5 99.351562    LR 0.300000    Time 0.022889    
2018-10-28 00:29:39,095 - Epoch: [32][  350/  391]    Overall Loss 0.425051    Objective Loss 0.425051    Top1 85.370536    Top5 99.370536    LR 0.300000    Time 0.022861    
2018-10-28 00:29:40,108 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            174 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   59.72222 | 0.57037 |  0.00344 |    0.30491 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            450 |    0.00000 |    0.00000 |  6.25000 | 23.82812 |  0.00000 |   80.46875 | 0.18613 | -0.00559 |    0.06873 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            508 |    0.00000 |    0.00000 |  0.00000 | 21.09375 |  0.00000 |   77.95139 | 0.18673 | -0.00608 |    0.07377 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            732 |    0.00000 |    0.00000 |  0.00000 |  9.37500 |  0.00000 |   68.22917 | 0.18192 | -0.01443 |    0.08379 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            296 |    0.00000 |    0.00000 |  0.00000 | 38.28125 |  0.00000 |   87.15278 | 0.14170 | -0.00518 |    0.04442 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            597 |    0.00000 |    0.00000 |  0.00000 | 15.23438 |  0.00000 |   74.08854 | 0.19827 | -0.01099 |    0.08518 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            286 |    0.00000 |    0.00000 |  0.00000 | 39.45312 |  0.00000 |   87.58681 | 0.13584 |  0.00065 |    0.04056 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1741 |    0.00000 |    0.00000 |  0.00000 |  5.07812 |  0.00000 |   62.21788 | 0.17865 | -0.00699 |    0.09242 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4879 |    0.00000 |    0.00000 |  0.00000 |  0.68359 |  0.00000 |   47.05946 | 0.16181 | -0.00704 |    0.09675 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            175 |    0.00000 |    0.00000 |  0.00000 | 65.82031 |  0.00000 |   65.82031 | 0.27516 | -0.01367 |    0.14512 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2407 |    0.00000 |    0.00000 |  0.00000 | 11.42578 |  0.00000 |   73.88238 | 0.13075 | -0.00772 |    0.05679 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1907 |    0.00000 |    0.00000 |  0.00000 | 16.11328 |  0.00000 |   79.30773 | 0.10871 |  0.00143 |    0.04271 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2850 |    0.00000 |    0.00000 |  0.00000 |  6.73828 |  0.00000 |   69.07552 | 0.13238 | -0.01104 |    0.06242 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           3100 |    0.00000 |    0.00000 |  0.00000 |  4.00391 |  0.00000 |   66.36285 | 0.10949 | -0.00205 |    0.05276 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           4250 |    0.00000 |    0.00000 |  0.00000 | 16.60156 |  0.00000 |   76.94227 | 0.11836 | -0.00487 |    0.04910 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14941 |    0.00000 |    0.00000 |  0.00000 |  1.68457 |  0.00000 |   59.46994 | 0.11935 | -0.00744 |    0.06301 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            385 |    0.00000 |    0.00000 |  0.00000 | 81.20117 |  0.00000 |   81.20117 | 0.14558 | -0.00444 |    0.05584 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          10096 |    0.00000 |    0.00000 |  0.00000 |  8.81348 |  0.00000 |   72.61285 | 0.11066 | -0.00601 |    0.04899 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          12430 |    0.00000 |    0.00000 |  0.00000 |  6.03027 |  0.00000 |   66.28147 | 0.08486 |  0.00092 |    0.04059 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           6476 |    0.00000 |    0.00000 |  0.00000 | 26.58691 |  0.00000 |   82.43273 | 0.05586 | -0.00090 |    0.01988 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3774 |    0.00000 |    0.00000 |  0.00000 | 48.58398 |  0.00000 |   89.76237 | 0.04469 | -0.00208 |    0.01243 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53712 | -0.03623 |    0.32599 |
| 22 | Total sparsity:                     | -              |        270896 |          72767 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   73.13840 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:29:40,108 - Total sparsity: 73.14

2018-10-28 00:29:40,108 - --- validate (epoch=32)-----------
2018-10-28 00:29:40,108 - 10000 samples (128 per mini-batch)
2018-10-28 00:29:40,834 - Epoch: [32][   50/   78]    Loss 0.697880    Top1 77.750000    Top5 98.859375    
2018-10-28 00:29:41,229 - ==> Top1: 77.180    Top5: 98.840    Loss: 0.717

2018-10-28 00:29:41,230 - ==> Best Top1: 83.060   On Epoch: 30

2018-10-28 00:29:41,230 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:29:41,239 - 

2018-10-28 00:29:41,240 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:29:42,412 - Epoch: [33][   50/  391]    Overall Loss 0.383611    Objective Loss 0.383611    Top1 86.593750    Top5 99.453125    LR 0.300000    Time 0.023398    
2018-10-28 00:29:43,524 - Epoch: [33][  100/  391]    Overall Loss 0.400426    Objective Loss 0.400426    Top1 86.085938    Top5 99.468750    LR 0.300000    Time 0.022812    
2018-10-28 00:29:44,638 - Epoch: [33][  150/  391]    Overall Loss 0.407128    Objective Loss 0.407128    Top1 85.796875    Top5 99.479167    LR 0.300000    Time 0.022624    
2018-10-28 00:29:45,750 - Epoch: [33][  200/  391]    Overall Loss 0.414357    Objective Loss 0.414357    Top1 85.519531    Top5 99.468750    LR 0.300000    Time 0.022521    
2018-10-28 00:29:46,861 - Epoch: [33][  250/  391]    Overall Loss 0.412524    Objective Loss 0.412524    Top1 85.565625    Top5 99.487500    LR 0.300000    Time 0.022457    
2018-10-28 00:29:47,973 - Epoch: [33][  300/  391]    Overall Loss 0.414912    Objective Loss 0.414912    Top1 85.437500    Top5 99.494792    LR 0.300000    Time 0.022416    
2018-10-28 00:29:49,083 - Epoch: [33][  350/  391]    Overall Loss 0.414851    Objective Loss 0.414851    Top1 85.444196    Top5 99.468750    LR 0.300000    Time 0.022381    
2018-10-28 00:29:50,074 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            174 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   59.72222 | 0.57085 | -0.00064 |    0.30169 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            450 |    0.00000 |    0.00000 |  6.25000 | 23.82812 |  0.00000 |   80.46875 | 0.18449 | -0.00533 |    0.06782 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            508 |    0.00000 |    0.00000 |  0.00000 | 21.09375 |  0.00000 |   77.95139 | 0.18514 | -0.00525 |    0.07256 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            732 |    0.00000 |    0.00000 |  0.00000 |  9.37500 |  0.00000 |   68.22917 | 0.18055 | -0.01315 |    0.08260 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            296 |    0.00000 |    0.00000 |  0.00000 | 38.28125 |  0.00000 |   87.15278 | 0.14083 | -0.00702 |    0.04419 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            597 |    0.00000 |    0.00000 |  0.00000 | 15.23438 |  0.00000 |   74.08854 | 0.19702 | -0.00992 |    0.08451 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            286 |    0.00000 |    0.00000 |  0.00000 | 39.45312 |  0.00000 |   87.58681 | 0.13452 |  0.00045 |    0.03991 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1741 |    0.00000 |    0.00000 |  0.00000 |  5.07812 |  0.00000 |   62.21788 | 0.17848 | -0.00789 |    0.09200 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4879 |    0.00000 |    0.00000 |  0.00000 |  0.68359 |  0.00000 |   47.05946 | 0.16150 | -0.00669 |    0.09633 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            175 |    0.00000 |    0.00000 |  0.00000 | 65.82031 |  0.00000 |   65.82031 | 0.27149 | -0.01413 |    0.14272 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2407 |    0.00000 |    0.00000 |  0.00000 | 11.42578 |  0.00000 |   73.88238 | 0.12989 | -0.00763 |    0.05600 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1907 |    0.00000 |    0.00000 |  0.00000 | 16.11328 |  0.00000 |   79.30773 | 0.10812 |  0.00171 |    0.04223 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2850 |    0.00000 |    0.00000 |  0.00000 |  6.73828 |  0.00000 |   69.07552 | 0.13183 | -0.01061 |    0.06191 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           3100 |    0.00000 |    0.00000 |  0.00000 |  4.00391 |  0.00000 |   66.36285 | 0.10896 | -0.00212 |    0.05226 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           4250 |    0.00000 |    0.00000 |  0.00000 | 16.60156 |  0.00000 |   76.94227 | 0.11783 | -0.00473 |    0.04863 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14941 |    0.00000 |    0.00000 |  0.00000 |  1.68457 |  0.00000 |   59.46994 | 0.11891 | -0.00723 |    0.06268 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            385 |    0.00000 |    0.00000 |  0.00000 | 81.20117 |  0.00000 |   81.20117 | 0.14558 | -0.00483 |    0.05561 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          10096 |    0.00000 |    0.00000 |  0.00000 |  8.81348 |  0.00000 |   72.61285 | 0.11038 | -0.00604 |    0.04872 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          12430 |    0.00000 |    0.00000 |  0.00000 |  6.03027 |  0.00000 |   66.28147 | 0.08487 |  0.00066 |    0.04059 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           6476 |    0.00000 |    0.00000 |  0.00000 | 26.58691 |  0.00000 |   82.43273 | 0.05615 | -0.00090 |    0.01995 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3774 |    0.00000 |    0.00000 |  0.00000 | 48.58398 |  0.00000 |   89.76237 | 0.04470 | -0.00197 |    0.01235 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.54129 | -0.03468 |    0.32905 |
| 22 | Total sparsity:                     | -              |        270896 |          72767 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   73.13840 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:29:50,075 - Total sparsity: 73.14

2018-10-28 00:29:50,075 - --- validate (epoch=33)-----------
2018-10-28 00:29:50,075 - 10000 samples (128 per mini-batch)
2018-10-28 00:29:50,797 - Epoch: [33][   50/   78]    Loss 0.575896    Top1 81.984375    Top5 99.093750    
2018-10-28 00:29:51,189 - ==> Top1: 81.630    Top5: 99.110    Loss: 0.588

2018-10-28 00:29:51,190 - ==> Best Top1: 83.060   On Epoch: 30

2018-10-28 00:29:51,190 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:29:51,208 - 

2018-10-28 00:29:51,209 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:29:52,373 - Epoch: [34][   50/  391]    Overall Loss 0.401830    Objective Loss 0.401830    Top1 86.281250    Top5 99.515625    LR 0.300000    Time 0.023240    
2018-10-28 00:29:53,507 - Epoch: [34][  100/  391]    Overall Loss 0.410124    Objective Loss 0.410124    Top1 85.781250    Top5 99.437500    LR 0.300000    Time 0.022946    
2018-10-28 00:29:54,642 - Epoch: [34][  150/  391]    Overall Loss 0.407801    Objective Loss 0.407801    Top1 85.932292    Top5 99.401042    LR 0.300000    Time 0.022853    
2018-10-28 00:29:55,778 - Epoch: [34][  200/  391]    Overall Loss 0.412061    Objective Loss 0.412061    Top1 85.722656    Top5 99.425781    LR 0.300000    Time 0.022814    
2018-10-28 00:29:56,915 - Epoch: [34][  250/  391]    Overall Loss 0.416804    Objective Loss 0.416804    Top1 85.559375    Top5 99.431250    LR 0.300000    Time 0.022793    
2018-10-28 00:29:58,050 - Epoch: [34][  300/  391]    Overall Loss 0.416741    Objective Loss 0.416741    Top1 85.627604    Top5 99.403646    LR 0.300000    Time 0.022776    
2018-10-28 00:29:59,188 - Epoch: [34][  350/  391]    Overall Loss 0.414927    Objective Loss 0.414927    Top1 85.754464    Top5 99.415179    LR 0.300000    Time 0.022769    
2018-10-28 00:30:00,197 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            173 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   59.95370 | 0.57434 |  0.00219 |    0.30426 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            439 |    0.00000 |    0.00000 |  6.25000 | 25.00000 |  0.00000 |   80.94618 | 0.18528 | -0.00672 |    0.06801 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            488 |    0.00000 |    0.00000 |  0.00000 | 21.48438 |  0.00000 |   78.81944 | 0.18534 | -0.00471 |    0.07110 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            706 |    0.00000 |    0.00000 |  0.00000 |  9.76562 |  0.00000 |   69.35764 | 0.17962 | -0.01469 |    0.08105 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            288 |    0.00000 |    0.00000 |  0.00000 | 39.45312 |  0.00000 |   87.50000 | 0.14007 | -0.00575 |    0.04366 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            569 |    0.00000 |    0.00000 |  0.00000 | 16.40625 |  0.00000 |   75.30382 | 0.19565 | -0.00846 |    0.08276 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            274 |    0.00000 |    0.00000 |  0.00000 | 41.01562 |  0.00000 |   88.10764 | 0.13307 |  0.00119 |    0.03867 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1683 |    0.00000 |    0.00000 |  0.00000 |  5.66406 |  0.00000 |   63.47656 | 0.17866 | -0.00786 |    0.09156 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4761 |    0.00000 |    0.00000 |  0.00000 |  0.68359 |  0.00000 |   48.33984 | 0.16186 | -0.00701 |    0.09630 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            173 |    0.00000 |    0.00000 |  0.00000 | 66.21094 |  0.00000 |   66.21094 | 0.26924 | -0.00771 |    0.14164 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2310 |    0.00000 |    0.00000 |  0.00000 | 12.10938 |  0.00000 |   74.93490 | 0.13007 | -0.00835 |    0.05538 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1827 |    0.00000 |    0.00000 |  0.00000 | 17.28516 |  0.00000 |   80.17578 | 0.10822 |  0.00170 |    0.04157 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2736 |    0.00000 |    0.00000 |  0.00000 |  7.42188 |  0.00000 |   70.31250 | 0.13180 | -0.01066 |    0.06112 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2962 |    0.00000 |    0.00000 |  0.00000 |  5.56641 |  0.00000 |   67.86024 | 0.10877 | -0.00144 |    0.05135 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           4105 |    0.00000 |    0.00000 |  0.00000 | 18.06641 |  0.00000 |   77.72895 | 0.11739 | -0.00453 |    0.04792 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14523 |    0.00000 |    0.00000 |  0.00000 |  2.00195 |  0.00000 |   60.60384 | 0.11874 | -0.00685 |    0.06185 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            370 |    0.00000 |    0.00000 |  0.00000 | 81.93359 |  0.00000 |   81.93359 | 0.14545 | -0.00463 |    0.05450 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9764 |    0.00000 |    0.00000 |  0.00000 |  9.54590 |  0.00000 |   73.51345 | 0.11009 | -0.00608 |    0.04800 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11991 |    0.00000 |    0.00000 |  0.00000 |  6.81152 |  0.00000 |   67.47233 | 0.08477 |  0.00073 |    0.04004 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           6208 |    0.00000 |    0.00000 |  0.00000 | 28.54004 |  0.00000 |   83.15972 | 0.05626 | -0.00076 |    0.01971 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3597 |    0.00000 |    0.00000 |  0.00000 | 50.39062 |  0.00000 |   90.24251 | 0.04477 | -0.00180 |    0.01217 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.54172 | -0.03449 |    0.32658 |
| 22 | Total sparsity:                     | -              |        270896 |          70260 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.06385 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:30:00,197 - Total sparsity: 74.06

2018-10-28 00:30:00,197 - --- validate (epoch=34)-----------
2018-10-28 00:30:00,197 - 10000 samples (128 per mini-batch)
2018-10-28 00:30:00,917 - Epoch: [34][   50/   78]    Loss 0.600888    Top1 80.953125    Top5 98.328125    
2018-10-28 00:30:01,308 - ==> Top1: 80.980    Top5: 98.370    Loss: 0.608

2018-10-28 00:30:01,309 - ==> Best Top1: 83.060   On Epoch: 30

2018-10-28 00:30:01,309 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:30:01,319 - 

2018-10-28 00:30:01,320 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:30:02,488 - Epoch: [35][   50/  391]    Overall Loss 0.392442    Objective Loss 0.392442    Top1 86.718750    Top5 99.593750    LR 0.300000    Time 0.023339    
2018-10-28 00:30:03,601 - Epoch: [35][  100/  391]    Overall Loss 0.396834    Objective Loss 0.396834    Top1 86.476562    Top5 99.468750    LR 0.300000    Time 0.022776    
2018-10-28 00:30:04,714 - Epoch: [35][  150/  391]    Overall Loss 0.400143    Objective Loss 0.400143    Top1 86.250000    Top5 99.479167    LR 0.300000    Time 0.022599    
2018-10-28 00:30:05,827 - Epoch: [35][  200/  391]    Overall Loss 0.410735    Objective Loss 0.410735    Top1 85.902344    Top5 99.457031    LR 0.300000    Time 0.022509    
2018-10-28 00:30:06,939 - Epoch: [35][  250/  391]    Overall Loss 0.412122    Objective Loss 0.412122    Top1 85.850000    Top5 99.443750    LR 0.300000    Time 0.022448    
2018-10-28 00:30:08,049 - Epoch: [35][  300/  391]    Overall Loss 0.416048    Objective Loss 0.416048    Top1 85.674479    Top5 99.453125    LR 0.300000    Time 0.022403    
2018-10-28 00:30:09,159 - Epoch: [35][  350/  391]    Overall Loss 0.414815    Objective Loss 0.414815    Top1 85.718750    Top5 99.446429    LR 0.300000    Time 0.022371    
2018-10-28 00:30:10,148 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            173 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   59.95370 | 0.57054 |  0.00429 |    0.30338 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            439 |    0.00000 |    0.00000 |  6.25000 | 25.00000 |  0.00000 |   80.94618 | 0.18494 | -0.00737 |    0.06683 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            488 |    0.00000 |    0.00000 |  0.00000 | 21.48438 |  0.00000 |   78.81944 | 0.18375 | -0.00491 |    0.07037 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            706 |    0.00000 |    0.00000 |  0.00000 |  9.76562 |  0.00000 |   69.35764 | 0.17940 | -0.01324 |    0.08121 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            288 |    0.00000 |    0.00000 |  0.00000 | 39.45312 |  0.00000 |   87.50000 | 0.13928 | -0.00823 |    0.04348 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            569 |    0.00000 |    0.00000 |  0.00000 | 16.40625 |  0.00000 |   75.30382 | 0.19589 | -0.01039 |    0.08279 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            274 |    0.00000 |    0.00000 |  0.00000 | 41.01562 |  0.00000 |   88.10764 | 0.13312 |  0.00194 |    0.03822 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1683 |    0.00000 |    0.00000 |  0.00000 |  5.66406 |  0.00000 |   63.47656 | 0.17871 | -0.00881 |    0.09131 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4761 |    0.00000 |    0.00000 |  0.00000 |  0.68359 |  0.00000 |   48.33984 | 0.16201 | -0.00663 |    0.09571 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            173 |    0.00000 |    0.00000 |  0.00000 | 66.21094 |  0.00000 |   66.21094 | 0.26861 | -0.00987 |    0.13991 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2310 |    0.00000 |    0.00000 |  0.00000 | 12.10938 |  0.00000 |   74.93490 | 0.12994 | -0.00783 |    0.05512 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1827 |    0.00000 |    0.00000 |  0.00000 | 17.28516 |  0.00000 |   80.17578 | 0.10793 |  0.00179 |    0.04115 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2736 |    0.00000 |    0.00000 |  0.00000 |  7.42188 |  0.00000 |   70.31250 | 0.13284 | -0.01055 |    0.06135 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2962 |    0.00000 |    0.00000 |  0.00000 |  5.56641 |  0.00000 |   67.86024 | 0.10931 | -0.00088 |    0.05154 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           4105 |    0.00000 |    0.00000 |  0.00000 | 18.06641 |  0.00000 |   77.72895 | 0.11710 | -0.00488 |    0.04774 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14523 |    0.00000 |    0.00000 |  0.00000 |  2.00195 |  0.00000 |   60.60384 | 0.11880 | -0.00714 |    0.06180 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            370 |    0.00000 |    0.00000 |  0.00000 | 81.93359 |  0.00000 |   81.93359 | 0.14485 | -0.00477 |    0.05437 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9764 |    0.00000 |    0.00000 |  0.00000 |  9.54590 |  0.00000 |   73.51345 | 0.11020 | -0.00561 |    0.04782 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11991 |    0.00000 |    0.00000 |  0.00000 |  6.81152 |  0.00000 |   67.47233 | 0.08503 |  0.00082 |    0.04000 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           6208 |    0.00000 |    0.00000 |  0.00000 | 28.54004 |  0.00000 |   83.15972 | 0.05667 | -0.00092 |    0.01971 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3597 |    0.00000 |    0.00000 |  0.00000 | 50.39062 |  0.00000 |   90.24251 | 0.04503 | -0.00189 |    0.01216 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53585 | -0.03386 |    0.32308 |
| 22 | Total sparsity:                     | -              |        270896 |          70260 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.06385 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:30:10,148 - Total sparsity: 74.06

2018-10-28 00:30:10,148 - --- validate (epoch=35)-----------
2018-10-28 00:30:10,148 - 10000 samples (128 per mini-batch)
2018-10-28 00:30:10,870 - Epoch: [35][   50/   78]    Loss 0.590823    Top1 80.562500    Top5 98.906250    
2018-10-28 00:30:11,260 - ==> Top1: 80.390    Top5: 98.900    Loss: 0.598

2018-10-28 00:30:11,261 - ==> Best Top1: 83.060   On Epoch: 30

2018-10-28 00:30:11,261 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:30:11,277 - 

2018-10-28 00:30:11,279 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:30:12,448 - Epoch: [36][   50/  391]    Overall Loss 0.398348    Objective Loss 0.398348    Top1 86.593750    Top5 99.437500    LR 0.300000    Time 0.023345    
2018-10-28 00:30:13,583 - Epoch: [36][  100/  391]    Overall Loss 0.405741    Objective Loss 0.405741    Top1 86.078125    Top5 99.390625    LR 0.300000    Time 0.023013    
2018-10-28 00:30:14,718 - Epoch: [36][  150/  391]    Overall Loss 0.401902    Objective Loss 0.401902    Top1 86.229167    Top5 99.406250    LR 0.300000    Time 0.022895    
2018-10-28 00:30:15,853 - Epoch: [36][  200/  391]    Overall Loss 0.401669    Objective Loss 0.401669    Top1 86.199219    Top5 99.449219    LR 0.300000    Time 0.022843    
2018-10-28 00:30:16,988 - Epoch: [36][  250/  391]    Overall Loss 0.402140    Objective Loss 0.402140    Top1 86.200000    Top5 99.440625    LR 0.300000    Time 0.022805    
2018-10-28 00:30:18,123 - Epoch: [36][  300/  391]    Overall Loss 0.405188    Objective Loss 0.405188    Top1 86.028646    Top5 99.440104    LR 0.300000    Time 0.022785    
2018-10-28 00:30:19,260 - Epoch: [36][  350/  391]    Overall Loss 0.408358    Objective Loss 0.408358    Top1 85.845982    Top5 99.444196    LR 0.300000    Time 0.022774    
2018-10-28 00:30:20,269 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57376 | -0.00082 |    0.30330 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18602 | -0.00538 |    0.06653 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18400 | -0.00477 |    0.07023 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.17889 | -0.01431 |    0.07976 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13938 | -0.00705 |    0.04257 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19486 | -0.00934 |    0.08087 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13276 |  0.00125 |    0.03728 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.17822 | -0.00855 |    0.09026 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16193 | -0.00707 |    0.09528 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.26724 | -0.00724 |    0.13821 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.12900 | -0.00712 |    0.05408 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10718 |  0.00151 |    0.04008 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13280 | -0.01088 |    0.06064 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10930 | -0.00154 |    0.05071 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11664 | -0.00509 |    0.04695 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11869 | -0.00665 |    0.06106 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.14400 | -0.00368 |    0.05332 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11017 | -0.00564 |    0.04728 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08508 |  0.00070 |    0.03955 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05689 | -0.00072 |    0.01942 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04516 | -0.00190 |    0.01198 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53301 | -0.03552 |    0.32117 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:30:20,269 - Total sparsity: 74.94

2018-10-28 00:30:20,269 - --- validate (epoch=36)-----------
2018-10-28 00:30:20,269 - 10000 samples (128 per mini-batch)
2018-10-28 00:30:20,993 - Epoch: [36][   50/   78]    Loss 0.501799    Top1 83.484375    Top5 98.968750    
2018-10-28 00:30:21,383 - ==> Top1: 83.720    Top5: 99.110    Loss: 0.491

2018-10-28 00:30:21,384 - ==> Best Top1: 83.720   On Epoch: 36

2018-10-28 00:30:21,384 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:30:21,400 - 

2018-10-28 00:30:21,400 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:30:22,543 - Epoch: [37][   50/  391]    Overall Loss 0.416766    Objective Loss 0.416766    Top1 85.546875    Top5 99.375000    LR 0.300000    Time 0.022821    
2018-10-28 00:30:23,656 - Epoch: [37][  100/  391]    Overall Loss 0.416916    Objective Loss 0.416916    Top1 85.507812    Top5 99.484375    LR 0.300000    Time 0.022530    
2018-10-28 00:30:24,771 - Epoch: [37][  150/  391]    Overall Loss 0.414095    Objective Loss 0.414095    Top1 85.661458    Top5 99.510417    LR 0.300000    Time 0.022438    
2018-10-28 00:30:25,885 - Epoch: [37][  200/  391]    Overall Loss 0.418467    Objective Loss 0.418467    Top1 85.433594    Top5 99.500000    LR 0.300000    Time 0.022376    
2018-10-28 00:30:26,996 - Epoch: [37][  250/  391]    Overall Loss 0.414011    Objective Loss 0.414011    Top1 85.518750    Top5 99.509375    LR 0.300000    Time 0.022340    
2018-10-28 00:30:28,109 - Epoch: [37][  300/  391]    Overall Loss 0.413594    Objective Loss 0.413594    Top1 85.541667    Top5 99.479167    LR 0.300000    Time 0.022320    
2018-10-28 00:30:29,222 - Epoch: [37][  350/  391]    Overall Loss 0.410792    Objective Loss 0.410792    Top1 85.660714    Top5 99.491071    LR 0.300000    Time 0.022309    
2018-10-28 00:30:30,218 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57164 |  0.00006 |    0.30034 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18379 | -0.00643 |    0.06580 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18244 | -0.00627 |    0.06984 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.17828 | -0.01256 |    0.07986 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13856 | -0.00717 |    0.04228 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19340 | -0.00774 |    0.08052 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13148 |  0.00158 |    0.03671 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.17760 | -0.00784 |    0.08971 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16179 | -0.00715 |    0.09478 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.26524 | -0.01120 |    0.13670 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.12847 | -0.00702 |    0.05352 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10687 |  0.00183 |    0.03979 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13250 | -0.01052 |    0.06034 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10885 | -0.00126 |    0.05039 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11620 | -0.00496 |    0.04663 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11842 | -0.00659 |    0.06060 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.14245 | -0.00348 |    0.05286 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11004 | -0.00584 |    0.04708 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08491 |  0.00068 |    0.03933 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05707 | -0.00074 |    0.01943 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04508 | -0.00167 |    0.01186 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.54177 | -0.03662 |    0.32600 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:30:30,218 - Total sparsity: 74.94

2018-10-28 00:30:30,219 - --- validate (epoch=37)-----------
2018-10-28 00:30:30,219 - 10000 samples (128 per mini-batch)
2018-10-28 00:30:30,939 - Epoch: [37][   50/   78]    Loss 0.645940    Top1 80.562500    Top5 98.546875    
2018-10-28 00:30:31,329 - ==> Top1: 80.630    Top5: 98.670    Loss: 0.641

2018-10-28 00:30:31,330 - ==> Best Top1: 83.720   On Epoch: 36

2018-10-28 00:30:31,330 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:30:31,341 - 

2018-10-28 00:30:31,342 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:30:32,533 - Epoch: [38][   50/  391]    Overall Loss 0.421338    Objective Loss 0.421338    Top1 85.187500    Top5 99.546875    LR 0.300000    Time 0.023782    
2018-10-28 00:30:33,653 - Epoch: [38][  100/  391]    Overall Loss 0.418051    Objective Loss 0.418051    Top1 85.648438    Top5 99.492188    LR 0.300000    Time 0.023084    
2018-10-28 00:30:34,772 - Epoch: [38][  150/  391]    Overall Loss 0.413604    Objective Loss 0.413604    Top1 85.760417    Top5 99.494792    LR 0.300000    Time 0.022836    
2018-10-28 00:30:35,886 - Epoch: [38][  200/  391]    Overall Loss 0.410932    Objective Loss 0.410932    Top1 85.835938    Top5 99.511719    LR 0.300000    Time 0.022690    
2018-10-28 00:30:36,998 - Epoch: [38][  250/  391]    Overall Loss 0.406986    Objective Loss 0.406986    Top1 85.887500    Top5 99.534375    LR 0.300000    Time 0.022594    
2018-10-28 00:30:38,151 - Epoch: [38][  300/  391]    Overall Loss 0.407618    Objective Loss 0.407618    Top1 85.890625    Top5 99.528646    LR 0.300000    Time 0.022667    
2018-10-28 00:30:39,301 - Epoch: [38][  350/  391]    Overall Loss 0.413619    Objective Loss 0.413619    Top1 85.631696    Top5 99.522321    LR 0.300000    Time 0.022712    
2018-10-28 00:30:40,314 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57512 | -0.00292 |    0.30227 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18532 | -0.00710 |    0.06628 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18313 | -0.00472 |    0.06917 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.17992 | -0.01270 |    0.08033 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13835 | -0.00683 |    0.04214 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19432 | -0.01029 |    0.08019 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13222 |  0.00178 |    0.03701 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.17762 | -0.00759 |    0.08866 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16207 | -0.00809 |    0.09477 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.26395 | -0.00790 |    0.13539 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.12860 | -0.00732 |    0.05347 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10722 |  0.00158 |    0.03993 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13305 | -0.01073 |    0.06042 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10926 | -0.00148 |    0.05058 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11656 | -0.00455 |    0.04663 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11890 | -0.00686 |    0.06082 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.14152 | -0.00469 |    0.05243 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11018 | -0.00613 |    0.04710 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08517 |  0.00041 |    0.03949 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05739 | -0.00086 |    0.01941 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04533 | -0.00159 |    0.01187 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53586 | -0.03692 |    0.32339 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:30:40,314 - Total sparsity: 74.94

2018-10-28 00:30:40,314 - --- validate (epoch=38)-----------
2018-10-28 00:30:40,314 - 10000 samples (128 per mini-batch)
2018-10-28 00:30:41,036 - Epoch: [38][   50/   78]    Loss 0.693024    Top1 78.937500    Top5 98.187500    
2018-10-28 00:30:41,425 - ==> Top1: 78.940    Top5: 98.330    Loss: 0.676

2018-10-28 00:30:41,426 - ==> Best Top1: 83.720   On Epoch: 36

2018-10-28 00:30:41,426 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:30:41,440 - 

2018-10-28 00:30:41,440 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:30:42,635 - Epoch: [39][   50/  391]    Overall Loss 0.402545    Objective Loss 0.402545    Top1 86.234375    Top5 99.515625    LR 0.300000    Time 0.023863    
2018-10-28 00:30:43,762 - Epoch: [39][  100/  391]    Overall Loss 0.410191    Objective Loss 0.410191    Top1 86.007812    Top5 99.398438    LR 0.300000    Time 0.023180    
2018-10-28 00:30:44,910 - Epoch: [39][  150/  391]    Overall Loss 0.408800    Objective Loss 0.408800    Top1 85.994792    Top5 99.442708    LR 0.300000    Time 0.023100    
2018-10-28 00:30:46,034 - Epoch: [39][  200/  391]    Overall Loss 0.409188    Objective Loss 0.409188    Top1 86.046875    Top5 99.406250    LR 0.300000    Time 0.022936    
2018-10-28 00:30:47,173 - Epoch: [39][  250/  391]    Overall Loss 0.408232    Objective Loss 0.408232    Top1 86.043750    Top5 99.406250    LR 0.300000    Time 0.022901    
2018-10-28 00:30:48,311 - Epoch: [39][  300/  391]    Overall Loss 0.410005    Objective Loss 0.410005    Top1 85.921875    Top5 99.429688    LR 0.300000    Time 0.022874    
2018-10-28 00:30:49,435 - Epoch: [39][  350/  391]    Overall Loss 0.411939    Objective Loss 0.411939    Top1 85.839286    Top5 99.426339    LR 0.300000    Time 0.022815    
2018-10-28 00:30:50,453 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57118 |  0.00387 |    0.30052 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18608 | -0.00592 |    0.06593 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18318 | -0.00616 |    0.06918 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.17897 | -0.01315 |    0.07994 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13822 | -0.00731 |    0.04222 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19426 | -0.00825 |    0.07969 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13232 | -0.00021 |    0.03699 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.17801 | -0.00836 |    0.08816 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16240 | -0.00694 |    0.09469 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.26019 | -0.00888 |    0.13392 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.12812 | -0.00770 |    0.05292 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10670 |  0.00137 |    0.03978 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13305 | -0.01072 |    0.06033 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10895 | -0.00097 |    0.05000 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11656 | -0.00462 |    0.04655 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11889 | -0.00695 |    0.06056 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.14071 | -0.00416 |    0.05218 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11002 | -0.00581 |    0.04695 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08529 |  0.00073 |    0.03931 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05763 | -0.00059 |    0.01940 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04553 | -0.00166 |    0.01187 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53184 | -0.03639 |    0.32081 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:30:50,454 - Total sparsity: 74.94

2018-10-28 00:30:50,454 - --- validate (epoch=39)-----------
2018-10-28 00:30:50,454 - 10000 samples (128 per mini-batch)
2018-10-28 00:30:51,181 - Epoch: [39][   50/   78]    Loss 0.570902    Top1 81.953125    Top5 98.968750    
2018-10-28 00:30:51,582 - ==> Top1: 81.720    Top5: 99.000    Loss: 0.573

2018-10-28 00:30:51,583 - ==> Best Top1: 83.720   On Epoch: 36

2018-10-28 00:30:51,583 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:30:51,600 - 

2018-10-28 00:30:51,601 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:30:52,790 - Epoch: [40][   50/  391]    Overall Loss 0.375314    Objective Loss 0.375314    Top1 86.906250    Top5 99.578125    LR 0.300000    Time 0.023746    
2018-10-28 00:30:53,897 - Epoch: [40][  100/  391]    Overall Loss 0.393045    Objective Loss 0.393045    Top1 86.398438    Top5 99.546875    LR 0.300000    Time 0.022928    
2018-10-28 00:30:55,005 - Epoch: [40][  150/  391]    Overall Loss 0.397650    Objective Loss 0.397650    Top1 86.338542    Top5 99.531250    LR 0.300000    Time 0.022664    
2018-10-28 00:30:56,112 - Epoch: [40][  200/  391]    Overall Loss 0.404091    Objective Loss 0.404091    Top1 86.136719    Top5 99.511719    LR 0.300000    Time 0.022530    
2018-10-28 00:30:57,221 - Epoch: [40][  250/  391]    Overall Loss 0.409891    Objective Loss 0.409891    Top1 85.856250    Top5 99.487500    LR 0.300000    Time 0.022453    
2018-10-28 00:30:58,329 - Epoch: [40][  300/  391]    Overall Loss 0.413150    Objective Loss 0.413150    Top1 85.739583    Top5 99.489583    LR 0.300000    Time 0.022400    
2018-10-28 00:30:59,437 - Epoch: [40][  350/  391]    Overall Loss 0.412019    Objective Loss 0.412019    Top1 85.741071    Top5 99.488839    LR 0.300000    Time 0.022363    
2018-10-28 00:31:00,424 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57444 |  0.00390 |    0.30435 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18586 | -0.00624 |    0.06550 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18299 | -0.00546 |    0.06881 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.17993 | -0.01374 |    0.07918 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13881 | -0.00744 |    0.04230 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19360 | -0.00914 |    0.07955 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13228 |  0.00077 |    0.03697 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.17822 | -0.00864 |    0.08840 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16256 | -0.00708 |    0.09443 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25764 | -0.01180 |    0.13084 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.12765 | -0.00719 |    0.05254 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10660 |  0.00133 |    0.03967 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13264 | -0.01035 |    0.05998 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10853 | -0.00191 |    0.04968 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11653 | -0.00481 |    0.04653 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11879 | -0.00658 |    0.06061 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.14031 | -0.00437 |    0.05169 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.10986 | -0.00594 |    0.04678 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08516 |  0.00062 |    0.03915 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05762 | -0.00084 |    0.01932 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04558 | -0.00162 |    0.01186 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53262 | -0.03626 |    0.31949 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:31:00,424 - Total sparsity: 74.94

2018-10-28 00:31:00,424 - --- validate (epoch=40)-----------
2018-10-28 00:31:00,424 - 10000 samples (128 per mini-batch)
2018-10-28 00:31:01,154 - Epoch: [40][   50/   78]    Loss 0.652275    Top1 79.468750    Top5 99.078125    
2018-10-28 00:31:01,555 - ==> Top1: 79.420    Top5: 99.050    Loss: 0.647

2018-10-28 00:31:01,556 - ==> Best Top1: 83.720   On Epoch: 36

2018-10-28 00:31:01,556 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:31:01,567 - 

2018-10-28 00:31:01,567 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:31:02,766 - Epoch: [41][   50/  391]    Overall Loss 0.424173    Objective Loss 0.424173    Top1 85.015625    Top5 99.375000    LR 0.300000    Time 0.023951    
2018-10-28 00:31:03,907 - Epoch: [41][  100/  391]    Overall Loss 0.418616    Objective Loss 0.418616    Top1 85.757812    Top5 99.421875    LR 0.300000    Time 0.023368    
2018-10-28 00:31:05,025 - Epoch: [41][  150/  391]    Overall Loss 0.409061    Objective Loss 0.409061    Top1 85.812500    Top5 99.494792    LR 0.300000    Time 0.023022    
2018-10-28 00:31:06,135 - Epoch: [41][  200/  391]    Overall Loss 0.413077    Objective Loss 0.413077    Top1 85.726562    Top5 99.488281    LR 0.300000    Time 0.022791    
2018-10-28 00:31:07,286 - Epoch: [41][  250/  391]    Overall Loss 0.407833    Objective Loss 0.407833    Top1 85.825000    Top5 99.490625    LR 0.300000    Time 0.022831    
2018-10-28 00:31:08,401 - Epoch: [41][  300/  391]    Overall Loss 0.414458    Objective Loss 0.414458    Top1 85.593750    Top5 99.479167    LR 0.300000    Time 0.022738    
2018-10-28 00:31:09,544 - Epoch: [41][  350/  391]    Overall Loss 0.414752    Objective Loss 0.414752    Top1 85.573661    Top5 99.462054    LR 0.300000    Time 0.022753    
2018-10-28 00:31:10,547 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57497 | -0.00318 |    0.30146 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18603 | -0.00650 |    0.06598 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18323 | -0.00302 |    0.06869 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18056 | -0.01172 |    0.08030 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13836 | -0.00677 |    0.04187 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19322 | -0.00827 |    0.07901 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13157 |  0.00042 |    0.03635 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.17873 | -0.00936 |    0.08877 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16234 | -0.00726 |    0.09462 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25701 | -0.01269 |    0.13102 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.12762 | -0.00644 |    0.05263 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10670 |  0.00091 |    0.03974 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13287 | -0.00998 |    0.05997 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10880 | -0.00166 |    0.04974 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11669 | -0.00421 |    0.04643 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11882 | -0.00697 |    0.06061 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.14073 | -0.00532 |    0.05198 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.10990 | -0.00571 |    0.04677 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08510 |  0.00059 |    0.03911 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05778 | -0.00079 |    0.01937 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04573 | -0.00162 |    0.01187 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53794 | -0.03326 |    0.32250 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:31:10,548 - Total sparsity: 74.94

2018-10-28 00:31:10,548 - --- validate (epoch=41)-----------
2018-10-28 00:31:10,548 - 10000 samples (128 per mini-batch)
2018-10-28 00:31:11,270 - Epoch: [41][   50/   78]    Loss 0.775559    Top1 76.000000    Top5 98.906250    
2018-10-28 00:31:11,661 - ==> Top1: 76.130    Top5: 98.810    Loss: 0.767

2018-10-28 00:31:11,662 - ==> Best Top1: 83.720   On Epoch: 36

2018-10-28 00:31:11,662 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:31:11,677 - 

2018-10-28 00:31:11,677 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:31:12,847 - Epoch: [42][   50/  391]    Overall Loss 0.381432    Objective Loss 0.381432    Top1 86.609375    Top5 99.546875    LR 0.300000    Time 0.023348    
2018-10-28 00:31:13,955 - Epoch: [42][  100/  391]    Overall Loss 0.403374    Objective Loss 0.403374    Top1 85.960938    Top5 99.492188    LR 0.300000    Time 0.022747    
2018-10-28 00:31:15,065 - Epoch: [42][  150/  391]    Overall Loss 0.404176    Objective Loss 0.404176    Top1 85.854167    Top5 99.489583    LR 0.300000    Time 0.022555    
2018-10-28 00:31:16,174 - Epoch: [42][  200/  391]    Overall Loss 0.408086    Objective Loss 0.408086    Top1 85.703125    Top5 99.468750    LR 0.300000    Time 0.022455    
2018-10-28 00:31:17,283 - Epoch: [42][  250/  391]    Overall Loss 0.409474    Objective Loss 0.409474    Top1 85.696875    Top5 99.475000    LR 0.300000    Time 0.022393    
2018-10-28 00:31:18,392 - Epoch: [42][  300/  391]    Overall Loss 0.406366    Objective Loss 0.406366    Top1 85.773438    Top5 99.492188    LR 0.300000    Time 0.022354    
2018-10-28 00:31:19,502 - Epoch: [42][  350/  391]    Overall Loss 0.410609    Objective Loss 0.410609    Top1 85.671875    Top5 99.466518    LR 0.300000    Time 0.022328    
2018-10-28 00:31:20,492 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57264 |  0.00429 |    0.30048 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18584 | -0.00699 |    0.06571 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18347 | -0.00389 |    0.06947 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18152 | -0.01078 |    0.07976 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13870 | -0.00714 |    0.04209 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19564 | -0.00739 |    0.08030 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13253 |  0.00007 |    0.03679 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.17912 | -0.00927 |    0.08914 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16252 | -0.00856 |    0.09496 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25780 | -0.00945 |    0.13105 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.12839 | -0.00729 |    0.05290 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10719 |  0.00099 |    0.03972 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13360 | -0.00968 |    0.06010 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10908 | -0.00093 |    0.04994 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11697 | -0.00436 |    0.04669 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11913 | -0.00702 |    0.06062 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.14093 | -0.00620 |    0.05183 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11000 | -0.00546 |    0.04680 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08529 |  0.00059 |    0.03908 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05822 | -0.00072 |    0.01946 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04603 | -0.00168 |    0.01188 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53466 | -0.03596 |    0.32154 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:31:20,493 - Total sparsity: 74.94

2018-10-28 00:31:20,493 - --- validate (epoch=42)-----------
2018-10-28 00:31:20,493 - 10000 samples (128 per mini-batch)
2018-10-28 00:31:21,225 - Epoch: [42][   50/   78]    Loss 0.752473    Top1 77.078125    Top5 97.984375    
2018-10-28 00:31:21,617 - ==> Top1: 76.910    Top5: 98.200    Loss: 0.732

2018-10-28 00:31:21,617 - ==> Best Top1: 83.720   On Epoch: 36

2018-10-28 00:31:21,618 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:31:21,627 - 

2018-10-28 00:31:21,627 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:31:22,794 - Epoch: [43][   50/  391]    Overall Loss 0.387112    Objective Loss 0.387112    Top1 86.906250    Top5 99.359375    LR 0.300000    Time 0.023293    
2018-10-28 00:31:23,902 - Epoch: [43][  100/  391]    Overall Loss 0.392874    Objective Loss 0.392874    Top1 86.531250    Top5 99.507812    LR 0.300000    Time 0.022714    
2018-10-28 00:31:25,010 - Epoch: [43][  150/  391]    Overall Loss 0.408675    Objective Loss 0.408675    Top1 85.989583    Top5 99.505208    LR 0.300000    Time 0.022526    
2018-10-28 00:31:26,119 - Epoch: [43][  200/  391]    Overall Loss 0.411595    Objective Loss 0.411595    Top1 85.898438    Top5 99.484375    LR 0.300000    Time 0.022430    
2018-10-28 00:31:27,228 - Epoch: [43][  250/  391]    Overall Loss 0.409182    Objective Loss 0.409182    Top1 85.950000    Top5 99.487500    LR 0.300000    Time 0.022375    
2018-10-28 00:31:28,338 - Epoch: [43][  300/  391]    Overall Loss 0.413013    Objective Loss 0.413013    Top1 85.809896    Top5 99.450521    LR 0.300000    Time 0.022340    
2018-10-28 00:31:29,446 - Epoch: [43][  350/  391]    Overall Loss 0.414011    Objective Loss 0.414011    Top1 85.738839    Top5 99.457589    LR 0.300000    Time 0.022311    
2018-10-28 00:31:30,438 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57505 | -0.00255 |    0.30106 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18737 | -0.00625 |    0.06565 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18479 | -0.00436 |    0.06943 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18099 | -0.01196 |    0.07945 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13885 | -0.00684 |    0.04149 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19506 | -0.00935 |    0.08041 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13197 | -0.00043 |    0.03703 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.17897 | -0.00854 |    0.08857 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16253 | -0.00723 |    0.09453 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25843 | -0.00579 |    0.13334 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.12827 | -0.00717 |    0.05279 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10677 |  0.00109 |    0.03952 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13419 | -0.01018 |    0.06032 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10975 | -0.00185 |    0.05024 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11716 | -0.00436 |    0.04654 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11912 | -0.00648 |    0.06076 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.14035 | -0.00565 |    0.05162 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.10988 | -0.00544 |    0.04670 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08538 |  0.00086 |    0.03902 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05847 | -0.00069 |    0.01946 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04611 | -0.00174 |    0.01187 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53523 | -0.03841 |    0.32215 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:31:30,438 - Total sparsity: 74.94

2018-10-28 00:31:30,438 - --- validate (epoch=43)-----------
2018-10-28 00:31:30,438 - 10000 samples (128 per mini-batch)
2018-10-28 00:31:31,161 - Epoch: [43][   50/   78]    Loss 0.664934    Top1 79.343750    Top5 98.953125    
2018-10-28 00:31:31,554 - ==> Top1: 79.130    Top5: 98.980    Loss: 0.666

2018-10-28 00:31:31,555 - ==> Best Top1: 83.720   On Epoch: 36

2018-10-28 00:31:31,555 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:31:31,564 - 

2018-10-28 00:31:31,565 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:31:32,733 - Epoch: [44][   50/  391]    Overall Loss 0.395360    Objective Loss 0.395360    Top1 86.093750    Top5 99.546875    LR 0.300000    Time 0.023319    
2018-10-28 00:31:33,842 - Epoch: [44][  100/  391]    Overall Loss 0.399021    Objective Loss 0.399021    Top1 85.921875    Top5 99.546875    LR 0.300000    Time 0.022739    
2018-10-28 00:31:34,951 - Epoch: [44][  150/  391]    Overall Loss 0.404432    Objective Loss 0.404432    Top1 86.031250    Top5 99.484375    LR 0.300000    Time 0.022543    
2018-10-28 00:31:36,059 - Epoch: [44][  200/  391]    Overall Loss 0.404966    Objective Loss 0.404966    Top1 86.046875    Top5 99.484375    LR 0.300000    Time 0.022441    
2018-10-28 00:31:37,168 - Epoch: [44][  250/  391]    Overall Loss 0.405343    Objective Loss 0.405343    Top1 86.078125    Top5 99.493750    LR 0.300000    Time 0.022383    
2018-10-28 00:31:38,276 - Epoch: [44][  300/  391]    Overall Loss 0.408931    Objective Loss 0.408931    Top1 85.992188    Top5 99.481771    LR 0.300000    Time 0.022341    
2018-10-28 00:31:39,383 - Epoch: [44][  350/  391]    Overall Loss 0.408436    Objective Loss 0.408436    Top1 85.970982    Top5 99.464286    LR 0.300000    Time 0.022310    
2018-10-28 00:31:40,371 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57291 | -0.00004 |    0.29978 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18558 | -0.00653 |    0.06470 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18331 | -0.00589 |    0.06871 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18121 | -0.01144 |    0.07968 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13918 | -0.00775 |    0.04169 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19493 | -0.00852 |    0.08001 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13154 | -0.00012 |    0.03667 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.17832 | -0.00943 |    0.08850 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16238 | -0.00771 |    0.09430 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25647 | -0.00951 |    0.13125 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.12817 | -0.00725 |    0.05257 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10648 |  0.00118 |    0.03947 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13386 | -0.01016 |    0.06025 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10972 | -0.00168 |    0.05001 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11699 | -0.00467 |    0.04654 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11882 | -0.00683 |    0.06047 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.14072 | -0.00607 |    0.05124 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.10973 | -0.00554 |    0.04658 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08547 |  0.00072 |    0.03899 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05873 | -0.00071 |    0.01949 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04623 | -0.00176 |    0.01185 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53255 | -0.03730 |    0.31958 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:31:40,371 - Total sparsity: 74.94

2018-10-28 00:31:40,371 - --- validate (epoch=44)-----------
2018-10-28 00:31:40,371 - 10000 samples (128 per mini-batch)
2018-10-28 00:31:41,087 - Epoch: [44][   50/   78]    Loss 0.676103    Top1 79.890625    Top5 98.437500    
2018-10-28 00:31:41,478 - ==> Top1: 79.920    Top5: 98.650    Loss: 0.652

2018-10-28 00:31:41,479 - ==> Best Top1: 83.720   On Epoch: 36

2018-10-28 00:31:41,479 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:31:41,493 - 

2018-10-28 00:31:41,493 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:31:42,663 - Epoch: [45][   50/  391]    Overall Loss 0.417990    Objective Loss 0.417990    Top1 85.687500    Top5 99.359375    LR 0.300000    Time 0.023350    
2018-10-28 00:31:43,772 - Epoch: [45][  100/  391]    Overall Loss 0.409534    Objective Loss 0.409534    Top1 85.960938    Top5 99.351562    LR 0.300000    Time 0.022754    
2018-10-28 00:31:44,881 - Epoch: [45][  150/  391]    Overall Loss 0.397796    Objective Loss 0.397796    Top1 86.322917    Top5 99.463542    LR 0.300000    Time 0.022555    
2018-10-28 00:31:45,989 - Epoch: [45][  200/  391]    Overall Loss 0.403453    Objective Loss 0.403453    Top1 86.136719    Top5 99.480469    LR 0.300000    Time 0.022451    
2018-10-28 00:31:47,098 - Epoch: [45][  250/  391]    Overall Loss 0.406789    Objective Loss 0.406789    Top1 85.959375    Top5 99.475000    LR 0.300000    Time 0.022389    
2018-10-28 00:31:48,205 - Epoch: [45][  300/  391]    Overall Loss 0.408480    Objective Loss 0.408480    Top1 85.843750    Top5 99.479167    LR 0.300000    Time 0.022345    
2018-10-28 00:31:49,313 - Epoch: [45][  350/  391]    Overall Loss 0.408353    Objective Loss 0.408353    Top1 85.854911    Top5 99.470982    LR 0.300000    Time 0.022315    
2018-10-28 00:31:50,300 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57291 |  0.00181 |    0.29848 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18636 | -0.00670 |    0.06570 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18427 | -0.00596 |    0.06963 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.17976 | -0.01145 |    0.07830 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13847 | -0.00636 |    0.04145 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19436 | -0.00839 |    0.07905 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13223 | -0.00041 |    0.03702 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.17878 | -0.00841 |    0.08793 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16311 | -0.00643 |    0.09461 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25681 | -0.01106 |    0.13075 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.12879 | -0.00711 |    0.05286 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10661 |  0.00163 |    0.03945 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13446 | -0.01081 |    0.06033 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11026 | -0.00279 |    0.05010 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11689 | -0.00433 |    0.04645 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11885 | -0.00649 |    0.06025 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13973 | -0.00655 |    0.05098 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.10955 | -0.00557 |    0.04633 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08562 |  0.00047 |    0.03909 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05871 | -0.00077 |    0.01951 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04622 | -0.00162 |    0.01189 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53501 | -0.03593 |    0.31962 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:31:50,300 - Total sparsity: 74.94

2018-10-28 00:31:50,300 - --- validate (epoch=45)-----------
2018-10-28 00:31:50,300 - 10000 samples (128 per mini-batch)
2018-10-28 00:31:51,021 - Epoch: [45][   50/   78]    Loss 0.692880    Top1 78.515625    Top5 98.671875    
2018-10-28 00:31:51,413 - ==> Top1: 78.350    Top5: 98.770    Loss: 0.702

2018-10-28 00:31:51,413 - ==> Best Top1: 83.720   On Epoch: 36

2018-10-28 00:31:51,413 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:31:51,425 - 

2018-10-28 00:31:51,426 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:31:52,590 - Epoch: [46][   50/  391]    Overall Loss 0.389371    Objective Loss 0.389371    Top1 86.640625    Top5 99.500000    LR 0.300000    Time 0.023255    
2018-10-28 00:31:53,697 - Epoch: [46][  100/  391]    Overall Loss 0.392093    Objective Loss 0.392093    Top1 86.492188    Top5 99.531250    LR 0.300000    Time 0.022685    
2018-10-28 00:31:54,804 - Epoch: [46][  150/  391]    Overall Loss 0.402479    Objective Loss 0.402479    Top1 86.072917    Top5 99.500000    LR 0.300000    Time 0.022490    
2018-10-28 00:31:55,912 - Epoch: [46][  200/  391]    Overall Loss 0.406449    Objective Loss 0.406449    Top1 85.835938    Top5 99.496094    LR 0.300000    Time 0.022402    
2018-10-28 00:31:57,022 - Epoch: [46][  250/  391]    Overall Loss 0.413285    Objective Loss 0.413285    Top1 85.578125    Top5 99.459375    LR 0.300000    Time 0.022359    
2018-10-28 00:31:58,134 - Epoch: [46][  300/  391]    Overall Loss 0.410711    Objective Loss 0.410711    Top1 85.697917    Top5 99.460938    LR 0.300000    Time 0.022334    
2018-10-28 00:31:59,246 - Epoch: [46][  350/  391]    Overall Loss 0.408815    Objective Loss 0.408815    Top1 85.801339    Top5 99.470982    LR 0.300000    Time 0.022315    
2018-10-28 00:32:00,235 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57074 |  0.00320 |    0.29643 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18659 | -0.00727 |    0.06563 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18323 | -0.00447 |    0.06850 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18034 | -0.01154 |    0.07932 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13880 | -0.00679 |    0.04141 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19453 | -0.00828 |    0.07908 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13243 |  0.00052 |    0.03694 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.17796 | -0.00878 |    0.08805 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16297 | -0.00789 |    0.09470 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25757 | -0.01086 |    0.13164 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.12945 | -0.00773 |    0.05324 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10716 |  0.00109 |    0.03971 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13496 | -0.00999 |    0.06071 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11025 | -0.00208 |    0.05019 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11689 | -0.00435 |    0.04624 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11916 | -0.00649 |    0.06054 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13959 | -0.00743 |    0.05117 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.10991 | -0.00552 |    0.04646 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08577 |  0.00050 |    0.03908 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05889 | -0.00091 |    0.01953 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04642 | -0.00168 |    0.01190 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53333 | -0.03613 |    0.31867 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:32:00,235 - Total sparsity: 74.94

2018-10-28 00:32:00,235 - --- validate (epoch=46)-----------
2018-10-28 00:32:00,235 - 10000 samples (128 per mini-batch)
2018-10-28 00:32:00,963 - Epoch: [46][   50/   78]    Loss 0.906255    Top1 73.546875    Top5 98.531250    
2018-10-28 00:32:01,360 - ==> Top1: 73.750    Top5: 98.670    Loss: 0.884

2018-10-28 00:32:01,361 - ==> Best Top1: 83.720   On Epoch: 36

2018-10-28 00:32:01,361 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:32:01,372 - 

2018-10-28 00:32:01,372 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:32:02,543 - Epoch: [47][   50/  391]    Overall Loss 0.412838    Objective Loss 0.412838    Top1 85.875000    Top5 99.468750    LR 0.300000    Time 0.023387    
2018-10-28 00:32:03,653 - Epoch: [47][  100/  391]    Overall Loss 0.417062    Objective Loss 0.417062    Top1 85.664062    Top5 99.492188    LR 0.300000    Time 0.022773    
2018-10-28 00:32:04,763 - Epoch: [47][  150/  391]    Overall Loss 0.417467    Objective Loss 0.417467    Top1 85.526042    Top5 99.505208    LR 0.300000    Time 0.022579    
2018-10-28 00:32:05,873 - Epoch: [47][  200/  391]    Overall Loss 0.415956    Objective Loss 0.415956    Top1 85.554688    Top5 99.500000    LR 0.300000    Time 0.022474    
2018-10-28 00:32:06,982 - Epoch: [47][  250/  391]    Overall Loss 0.417212    Objective Loss 0.417212    Top1 85.531250    Top5 99.500000    LR 0.300000    Time 0.022397    
2018-10-28 00:32:08,093 - Epoch: [47][  300/  391]    Overall Loss 0.414936    Objective Loss 0.414936    Top1 85.640625    Top5 99.518229    LR 0.300000    Time 0.022363    
2018-10-28 00:32:09,204 - Epoch: [47][  350/  391]    Overall Loss 0.415467    Objective Loss 0.415467    Top1 85.631696    Top5 99.493304    LR 0.300000    Time 0.022339    
2018-10-28 00:32:10,192 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.56890 |  0.00057 |    0.29639 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18561 | -0.00718 |    0.06526 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18167 | -0.00467 |    0.06838 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18011 | -0.01189 |    0.07884 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13945 | -0.00704 |    0.04144 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19469 | -0.00711 |    0.07880 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13263 |  0.00085 |    0.03682 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.17811 | -0.00921 |    0.08840 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16332 | -0.00721 |    0.09510 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25715 | -0.00925 |    0.13111 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13014 | -0.00738 |    0.05371 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10722 |  0.00116 |    0.03956 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13489 | -0.01058 |    0.06088 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11030 | -0.00176 |    0.04992 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11696 | -0.00422 |    0.04630 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11927 | -0.00646 |    0.06062 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13945 | -0.00723 |    0.05098 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11009 | -0.00576 |    0.04649 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08580 |  0.00023 |    0.03911 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05897 | -0.00088 |    0.01948 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04660 | -0.00153 |    0.01187 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53061 | -0.03397 |    0.31643 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:32:10,192 - Total sparsity: 74.94

2018-10-28 00:32:10,192 - --- validate (epoch=47)-----------
2018-10-28 00:32:10,193 - 10000 samples (128 per mini-batch)
2018-10-28 00:32:10,913 - Epoch: [47][   50/   78]    Loss 0.580706    Top1 80.437500    Top5 99.140625    
2018-10-28 00:32:11,321 - ==> Top1: 80.280    Top5: 99.200    Loss: 0.582

2018-10-28 00:32:11,321 - ==> Best Top1: 83.720   On Epoch: 36

2018-10-28 00:32:11,322 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:32:11,331 - 

2018-10-28 00:32:11,332 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:32:12,498 - Epoch: [48][   50/  391]    Overall Loss 0.407897    Objective Loss 0.407897    Top1 86.218750    Top5 99.437500    LR 0.300000    Time 0.023298    
2018-10-28 00:32:13,609 - Epoch: [48][  100/  391]    Overall Loss 0.399103    Objective Loss 0.399103    Top1 86.250000    Top5 99.445312    LR 0.300000    Time 0.022745    
2018-10-28 00:32:14,718 - Epoch: [48][  150/  391]    Overall Loss 0.403233    Objective Loss 0.403233    Top1 85.994792    Top5 99.489583    LR 0.300000    Time 0.022543    
2018-10-28 00:32:15,830 - Epoch: [48][  200/  391]    Overall Loss 0.401769    Objective Loss 0.401769    Top1 86.109375    Top5 99.492188    LR 0.300000    Time 0.022461    
2018-10-28 00:32:16,939 - Epoch: [48][  250/  391]    Overall Loss 0.407015    Objective Loss 0.407015    Top1 85.996875    Top5 99.440625    LR 0.300000    Time 0.022402    
2018-10-28 00:32:18,050 - Epoch: [48][  300/  391]    Overall Loss 0.409134    Objective Loss 0.409134    Top1 86.002604    Top5 99.450521    LR 0.300000    Time 0.022354    
2018-10-28 00:32:19,162 - Epoch: [48][  350/  391]    Overall Loss 0.410184    Objective Loss 0.410184    Top1 85.968750    Top5 99.448661    LR 0.300000    Time 0.022335    
2018-10-28 00:32:20,149 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.56710 |  0.00822 |    0.29569 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18618 | -0.00536 |    0.06494 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18187 | -0.00587 |    0.06764 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18009 | -0.01239 |    0.07903 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13939 | -0.00628 |    0.04158 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19498 | -0.00741 |    0.07910 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13234 |  0.00134 |    0.03646 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.17849 | -0.00922 |    0.08826 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16317 | -0.00779 |    0.09517 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25545 | -0.01152 |    0.13101 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.12967 | -0.00691 |    0.05346 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10703 |  0.00120 |    0.03937 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13471 | -0.01087 |    0.06070 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10992 | -0.00159 |    0.04997 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11689 | -0.00490 |    0.04633 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11915 | -0.00641 |    0.06065 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13900 | -0.00696 |    0.05078 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11004 | -0.00573 |    0.04643 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08567 |  0.00043 |    0.03901 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05889 | -0.00102 |    0.01947 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04648 | -0.00152 |    0.01181 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53436 | -0.03293 |    0.31873 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:32:20,149 - Total sparsity: 74.94

2018-10-28 00:32:20,149 - --- validate (epoch=48)-----------
2018-10-28 00:32:20,150 - 10000 samples (128 per mini-batch)
2018-10-28 00:32:20,871 - Epoch: [48][   50/   78]    Loss 0.576213    Top1 81.171875    Top5 98.859375    
2018-10-28 00:32:21,263 - ==> Top1: 80.820    Top5: 98.920    Loss: 0.586

2018-10-28 00:32:21,264 - ==> Best Top1: 83.720   On Epoch: 36

2018-10-28 00:32:21,264 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:32:21,280 - 

2018-10-28 00:32:21,280 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:32:22,420 - Epoch: [49][   50/  391]    Overall Loss 0.414468    Objective Loss 0.414468    Top1 85.312500    Top5 99.468750    LR 0.300000    Time 0.022771    
2018-10-28 00:32:23,531 - Epoch: [49][  100/  391]    Overall Loss 0.406556    Objective Loss 0.406556    Top1 85.671875    Top5 99.453125    LR 0.300000    Time 0.022475    
2018-10-28 00:32:24,640 - Epoch: [49][  150/  391]    Overall Loss 0.411433    Objective Loss 0.411433    Top1 85.588542    Top5 99.463542    LR 0.300000    Time 0.022370    
2018-10-28 00:32:25,749 - Epoch: [49][  200/  391]    Overall Loss 0.409770    Objective Loss 0.409770    Top1 85.632812    Top5 99.480469    LR 0.300000    Time 0.022318    
2018-10-28 00:32:26,858 - Epoch: [49][  250/  391]    Overall Loss 0.409481    Objective Loss 0.409481    Top1 85.709375    Top5 99.471875    LR 0.300000    Time 0.022284    
2018-10-28 00:32:27,966 - Epoch: [49][  300/  391]    Overall Loss 0.411170    Objective Loss 0.411170    Top1 85.690104    Top5 99.450521    LR 0.300000    Time 0.022261    
2018-10-28 00:32:29,075 - Epoch: [49][  350/  391]    Overall Loss 0.410184    Objective Loss 0.410184    Top1 85.736607    Top5 99.455357    LR 0.300000    Time 0.022233    
2018-10-28 00:32:30,063 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57440 |  0.00328 |    0.29629 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18696 | -0.00551 |    0.06491 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18243 | -0.00526 |    0.06771 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.17958 | -0.01288 |    0.07910 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13870 | -0.00732 |    0.04132 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19677 | -0.00639 |    0.08012 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13359 |  0.00105 |    0.03662 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.17888 | -0.00856 |    0.08823 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16344 | -0.00736 |    0.09497 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25505 | -0.01188 |    0.12981 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.12994 | -0.00662 |    0.05327 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10757 |  0.00151 |    0.03945 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13496 | -0.01048 |    0.06066 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10972 | -0.00160 |    0.04988 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11656 | -0.00445 |    0.04614 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11881 | -0.00683 |    0.06040 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13895 | -0.00684 |    0.05108 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.10980 | -0.00576 |    0.04637 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08559 |  0.00026 |    0.03901 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05910 | -0.00101 |    0.01961 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04646 | -0.00153 |    0.01183 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53234 | -0.03116 |    0.31866 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:32:30,064 - Total sparsity: 74.94

2018-10-28 00:32:30,064 - --- validate (epoch=49)-----------
2018-10-28 00:32:30,064 - 10000 samples (128 per mini-batch)
2018-10-28 00:32:30,783 - Epoch: [49][   50/   78]    Loss 0.595433    Top1 80.609375    Top5 98.828125    
2018-10-28 00:32:31,171 - ==> Top1: 80.500    Top5: 98.770    Loss: 0.592

2018-10-28 00:32:31,172 - ==> Best Top1: 83.720   On Epoch: 36

2018-10-28 00:32:31,172 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:32:31,193 - 

2018-10-28 00:32:31,193 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:32:32,334 - Epoch: [50][   50/  391]    Overall Loss 0.387037    Objective Loss 0.387037    Top1 86.156250    Top5 99.546875    LR 0.300000    Time 0.022777    
2018-10-28 00:32:33,444 - Epoch: [50][  100/  391]    Overall Loss 0.400164    Objective Loss 0.400164    Top1 85.921875    Top5 99.546875    LR 0.300000    Time 0.022477    
2018-10-28 00:32:34,556 - Epoch: [50][  150/  391]    Overall Loss 0.399050    Objective Loss 0.399050    Top1 85.927083    Top5 99.520833    LR 0.300000    Time 0.022391    
2018-10-28 00:32:35,668 - Epoch: [50][  200/  391]    Overall Loss 0.401179    Objective Loss 0.401179    Top1 85.875000    Top5 99.496094    LR 0.300000    Time 0.022343    
2018-10-28 00:32:36,780 - Epoch: [50][  250/  391]    Overall Loss 0.404763    Objective Loss 0.404763    Top1 85.831250    Top5 99.487500    LR 0.300000    Time 0.022317    
2018-10-28 00:32:37,893 - Epoch: [50][  300/  391]    Overall Loss 0.407718    Objective Loss 0.407718    Top1 85.690104    Top5 99.479167    LR 0.300000    Time 0.022303    
2018-10-28 00:32:39,004 - Epoch: [50][  350/  391]    Overall Loss 0.411447    Objective Loss 0.411447    Top1 85.604911    Top5 99.466518    LR 0.300000    Time 0.022289    
2018-10-28 00:32:39,994 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58138 | -0.00033 |    0.30278 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18754 | -0.00485 |    0.06567 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18342 | -0.00511 |    0.06771 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.17911 | -0.01224 |    0.07873 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13825 | -0.00735 |    0.04115 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19804 | -0.00673 |    0.08084 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13373 |  0.00042 |    0.03733 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18008 | -0.00782 |    0.08834 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16390 | -0.00766 |    0.09515 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25698 | -0.00633 |    0.13086 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13014 | -0.00686 |    0.05297 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10773 |  0.00120 |    0.03957 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13551 | -0.01019 |    0.06090 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11024 | -0.00222 |    0.04998 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11656 | -0.00440 |    0.04599 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11883 | -0.00676 |    0.06041 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13963 | -0.00539 |    0.05087 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.10991 | -0.00568 |    0.04637 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08559 |  0.00035 |    0.03903 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05919 | -0.00079 |    0.01959 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04660 | -0.00145 |    0.01180 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53419 | -0.03067 |    0.31972 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:32:39,994 - Total sparsity: 74.94

2018-10-28 00:32:39,994 - --- validate (epoch=50)-----------
2018-10-28 00:32:39,994 - 10000 samples (128 per mini-batch)
2018-10-28 00:32:40,715 - Epoch: [50][   50/   78]    Loss 0.716414    Top1 78.359375    Top5 98.671875    
2018-10-28 00:32:41,105 - ==> Top1: 78.320    Top5: 98.690    Loss: 0.709

2018-10-28 00:32:41,106 - ==> Best Top1: 83.720   On Epoch: 36

2018-10-28 00:32:41,106 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:32:41,116 - 

2018-10-28 00:32:41,117 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:32:42,285 - Epoch: [51][   50/  391]    Overall Loss 0.399537    Objective Loss 0.399537    Top1 86.375000    Top5 99.562500    LR 0.300000    Time 0.023331    
2018-10-28 00:32:43,393 - Epoch: [51][  100/  391]    Overall Loss 0.401143    Objective Loss 0.401143    Top1 86.031250    Top5 99.515625    LR 0.300000    Time 0.022731    
2018-10-28 00:32:44,502 - Epoch: [51][  150/  391]    Overall Loss 0.400864    Objective Loss 0.400864    Top1 86.109375    Top5 99.494792    LR 0.300000    Time 0.022540    
2018-10-28 00:32:45,611 - Epoch: [51][  200/  391]    Overall Loss 0.406714    Objective Loss 0.406714    Top1 85.984375    Top5 99.417969    LR 0.300000    Time 0.022442    
2018-10-28 00:32:46,720 - Epoch: [51][  250/  391]    Overall Loss 0.410139    Objective Loss 0.410139    Top1 85.837500    Top5 99.434375    LR 0.300000    Time 0.022387    
2018-10-28 00:32:47,830 - Epoch: [51][  300/  391]    Overall Loss 0.413847    Objective Loss 0.413847    Top1 85.721354    Top5 99.395833    LR 0.300000    Time 0.022350    
2018-10-28 00:32:48,938 - Epoch: [51][  350/  391]    Overall Loss 0.417596    Objective Loss 0.417596    Top1 85.558036    Top5 99.392857    LR 0.300000    Time 0.022321    
2018-10-28 00:32:49,932 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57980 |  0.00546 |    0.29954 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18715 | -0.00577 |    0.06426 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18332 | -0.00469 |    0.06783 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.17920 | -0.01323 |    0.07926 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13910 | -0.00739 |    0.04172 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19792 | -0.00769 |    0.08040 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13348 |  0.00156 |    0.03630 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18088 | -0.00696 |    0.08876 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16434 | -0.00760 |    0.09543 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25719 | -0.00937 |    0.13080 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13015 | -0.00708 |    0.05325 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10778 |  0.00135 |    0.03942 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13552 | -0.01134 |    0.06064 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11029 | -0.00132 |    0.05002 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11708 | -0.00471 |    0.04629 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11928 | -0.00662 |    0.06043 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.14045 | -0.00603 |    0.05165 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11038 | -0.00546 |    0.04644 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08601 |  0.00099 |    0.03910 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05974 | -0.00092 |    0.01979 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04695 | -0.00159 |    0.01188 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53167 | -0.03340 |    0.31842 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:32:49,932 - Total sparsity: 74.94

2018-10-28 00:32:49,932 - --- validate (epoch=51)-----------
2018-10-28 00:32:49,933 - 10000 samples (128 per mini-batch)
2018-10-28 00:32:50,651 - Epoch: [51][   50/   78]    Loss 0.588821    Top1 80.921875    Top5 99.000000    
2018-10-28 00:32:51,043 - ==> Top1: 80.950    Top5: 99.090    Loss: 0.590

2018-10-28 00:32:51,044 - ==> Best Top1: 83.720   On Epoch: 36

2018-10-28 00:32:51,044 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:32:51,055 - 

2018-10-28 00:32:51,055 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:32:52,222 - Epoch: [52][   50/  391]    Overall Loss 0.384536    Objective Loss 0.384536    Top1 86.359375    Top5 99.562500    LR 0.300000    Time 0.023299    
2018-10-28 00:32:53,330 - Epoch: [52][  100/  391]    Overall Loss 0.398929    Objective Loss 0.398929    Top1 86.078125    Top5 99.468750    LR 0.300000    Time 0.022710    
2018-10-28 00:32:54,439 - Epoch: [52][  150/  391]    Overall Loss 0.397854    Objective Loss 0.397854    Top1 86.213542    Top5 99.473958    LR 0.300000    Time 0.022524    
2018-10-28 00:32:55,547 - Epoch: [52][  200/  391]    Overall Loss 0.402679    Objective Loss 0.402679    Top1 86.078125    Top5 99.492188    LR 0.300000    Time 0.022428    
2018-10-28 00:32:56,655 - Epoch: [52][  250/  391]    Overall Loss 0.404839    Objective Loss 0.404839    Top1 86.028125    Top5 99.490625    LR 0.300000    Time 0.022356    
2018-10-28 00:32:57,764 - Epoch: [52][  300/  391]    Overall Loss 0.403044    Objective Loss 0.403044    Top1 86.117188    Top5 99.515625    LR 0.300000    Time 0.022322    
2018-10-28 00:32:58,873 - Epoch: [52][  350/  391]    Overall Loss 0.409030    Objective Loss 0.409030    Top1 85.886161    Top5 99.502232    LR 0.300000    Time 0.022300    
2018-10-28 00:32:59,866 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58011 |  0.00228 |    0.29679 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18839 | -0.00538 |    0.06466 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18429 | -0.00435 |    0.06796 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.17967 | -0.01328 |    0.07939 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13883 | -0.00808 |    0.04163 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19747 | -0.00699 |    0.08053 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13390 |  0.00068 |    0.03676 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18108 | -0.00636 |    0.08865 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16460 | -0.00727 |    0.09572 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25641 | -0.00478 |    0.13002 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13048 | -0.00702 |    0.05343 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10761 |  0.00117 |    0.03931 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13571 | -0.01066 |    0.06078 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11012 | -0.00216 |    0.04993 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11729 | -0.00436 |    0.04619 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11940 | -0.00655 |    0.06057 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.14013 | -0.00606 |    0.05104 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11048 | -0.00583 |    0.04650 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08598 |  0.00062 |    0.03916 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05999 | -0.00087 |    0.01980 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04706 | -0.00156 |    0.01193 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53529 | -0.03249 |    0.31781 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:32:59,866 - Total sparsity: 74.94

2018-10-28 00:32:59,866 - --- validate (epoch=52)-----------
2018-10-28 00:32:59,866 - 10000 samples (128 per mini-batch)
2018-10-28 00:33:00,587 - Epoch: [52][   50/   78]    Loss 0.572086    Top1 80.812500    Top5 98.968750    
2018-10-28 00:33:00,979 - ==> Top1: 81.100    Top5: 99.030    Loss: 0.561

2018-10-28 00:33:00,980 - ==> Best Top1: 83.720   On Epoch: 36

2018-10-28 00:33:00,980 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:33:00,994 - 

2018-10-28 00:33:00,994 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:33:02,165 - Epoch: [53][   50/  391]    Overall Loss 0.405051    Objective Loss 0.405051    Top1 85.546875    Top5 99.531250    LR 0.300000    Time 0.023373    
2018-10-28 00:33:03,275 - Epoch: [53][  100/  391]    Overall Loss 0.418999    Objective Loss 0.418999    Top1 85.343750    Top5 99.476562    LR 0.300000    Time 0.022774    
2018-10-28 00:33:04,389 - Epoch: [53][  150/  391]    Overall Loss 0.412027    Objective Loss 0.412027    Top1 85.697917    Top5 99.510417    LR 0.300000    Time 0.022603    
2018-10-28 00:33:05,497 - Epoch: [53][  200/  391]    Overall Loss 0.412119    Objective Loss 0.412119    Top1 85.671875    Top5 99.511719    LR 0.300000    Time 0.022487    
2018-10-28 00:33:06,608 - Epoch: [53][  250/  391]    Overall Loss 0.412326    Objective Loss 0.412326    Top1 85.668750    Top5 99.478125    LR 0.300000    Time 0.022426    
2018-10-28 00:33:07,718 - Epoch: [53][  300/  391]    Overall Loss 0.411790    Objective Loss 0.411790    Top1 85.718750    Top5 99.460938    LR 0.300000    Time 0.022371    
2018-10-28 00:33:08,827 - Epoch: [53][  350/  391]    Overall Loss 0.411319    Objective Loss 0.411319    Top1 85.736607    Top5 99.448661    LR 0.300000    Time 0.022342    
2018-10-28 00:33:09,818 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57972 |  0.00012 |    0.29787 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18812 | -0.00637 |    0.06488 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18443 | -0.00548 |    0.06779 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18105 | -0.01346 |    0.07934 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13956 | -0.00763 |    0.04125 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19774 | -0.00722 |    0.08066 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13352 |  0.00096 |    0.03651 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18050 | -0.00689 |    0.08849 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16472 | -0.00738 |    0.09545 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25488 | -0.00587 |    0.12972 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13018 | -0.00737 |    0.05335 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10709 |  0.00138 |    0.03920 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13508 | -0.01089 |    0.06034 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10986 | -0.00190 |    0.04991 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11721 | -0.00464 |    0.04627 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11929 | -0.00687 |    0.06041 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.14041 | -0.00568 |    0.05179 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11049 | -0.00532 |    0.04657 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08590 |  0.00069 |    0.03908 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05984 | -0.00080 |    0.01974 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04698 | -0.00160 |    0.01189 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53670 | -0.03366 |    0.31973 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:33:09,819 - Total sparsity: 74.94

2018-10-28 00:33:09,819 - --- validate (epoch=53)-----------
2018-10-28 00:33:09,819 - 10000 samples (128 per mini-batch)
2018-10-28 00:33:10,523 - Epoch: [53][   50/   78]    Loss 0.472627    Top1 84.281250    Top5 99.218750    
2018-10-28 00:33:10,904 - ==> Top1: 84.170    Top5: 99.300    Loss: 0.474

2018-10-28 00:33:10,905 - ==> Best Top1: 84.170   On Epoch: 53

2018-10-28 00:33:10,905 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:33:10,922 - 

2018-10-28 00:33:10,922 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:33:12,063 - Epoch: [54][   50/  391]    Overall Loss 0.398519    Objective Loss 0.398519    Top1 86.265625    Top5 99.531250    LR 0.300000    Time 0.022792    
2018-10-28 00:33:13,178 - Epoch: [54][  100/  391]    Overall Loss 0.409003    Objective Loss 0.409003    Top1 85.789062    Top5 99.531250    LR 0.300000    Time 0.022534    
2018-10-28 00:33:14,294 - Epoch: [54][  150/  391]    Overall Loss 0.414001    Objective Loss 0.414001    Top1 85.541667    Top5 99.494792    LR 0.300000    Time 0.022450    
2018-10-28 00:33:15,405 - Epoch: [54][  200/  391]    Overall Loss 0.408429    Objective Loss 0.408429    Top1 85.699219    Top5 99.523438    LR 0.300000    Time 0.022389    
2018-10-28 00:33:16,516 - Epoch: [54][  250/  391]    Overall Loss 0.408903    Objective Loss 0.408903    Top1 85.834375    Top5 99.496875    LR 0.300000    Time 0.022350    
2018-10-28 00:33:17,627 - Epoch: [54][  300/  391]    Overall Loss 0.410305    Objective Loss 0.410305    Top1 85.765625    Top5 99.460938    LR 0.300000    Time 0.022322    
2018-10-28 00:33:18,737 - Epoch: [54][  350/  391]    Overall Loss 0.411484    Objective Loss 0.411484    Top1 85.752232    Top5 99.479911    LR 0.300000    Time 0.022303    
2018-10-28 00:33:19,726 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57959 |  0.00579 |    0.29905 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18857 | -0.00555 |    0.06428 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18485 | -0.00350 |    0.06870 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18043 | -0.01408 |    0.07850 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13929 | -0.00834 |    0.04147 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19755 | -0.00794 |    0.08047 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13343 |  0.00077 |    0.03610 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18087 | -0.00846 |    0.08806 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16532 | -0.00822 |    0.09595 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25377 | -0.01121 |    0.12773 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13090 | -0.00726 |    0.05317 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10738 |  0.00118 |    0.03931 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13518 | -0.01086 |    0.06026 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10996 | -0.00213 |    0.05015 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11744 | -0.00459 |    0.04638 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11957 | -0.00679 |    0.06053 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.14034 | -0.00623 |    0.05174 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11066 | -0.00582 |    0.04654 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08584 |  0.00068 |    0.03907 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05977 | -0.00084 |    0.01965 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04694 | -0.00149 |    0.01188 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53246 | -0.03443 |    0.31772 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:33:19,726 - Total sparsity: 74.94

2018-10-28 00:33:19,726 - --- validate (epoch=54)-----------
2018-10-28 00:33:19,726 - 10000 samples (128 per mini-batch)
2018-10-28 00:33:20,451 - Epoch: [54][   50/   78]    Loss 0.632878    Top1 79.109375    Top5 98.921875    
2018-10-28 00:33:20,845 - ==> Top1: 78.610    Top5: 98.990    Loss: 0.641

2018-10-28 00:33:20,846 - ==> Best Top1: 84.170   On Epoch: 53

2018-10-28 00:33:20,846 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:33:20,857 - 

2018-10-28 00:33:20,857 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:33:22,024 - Epoch: [55][   50/  391]    Overall Loss 0.399208    Objective Loss 0.399208    Top1 86.437500    Top5 99.578125    LR 0.300000    Time 0.023312    
2018-10-28 00:33:23,133 - Epoch: [55][  100/  391]    Overall Loss 0.399332    Objective Loss 0.399332    Top1 86.218750    Top5 99.578125    LR 0.300000    Time 0.022730    
2018-10-28 00:33:24,242 - Epoch: [55][  150/  391]    Overall Loss 0.399024    Objective Loss 0.399024    Top1 86.343750    Top5 99.562500    LR 0.300000    Time 0.022539    
2018-10-28 00:33:25,351 - Epoch: [55][  200/  391]    Overall Loss 0.402487    Objective Loss 0.402487    Top1 86.148438    Top5 99.535156    LR 0.300000    Time 0.022441    
2018-10-28 00:33:26,461 - Epoch: [55][  250/  391]    Overall Loss 0.407486    Objective Loss 0.407486    Top1 85.996875    Top5 99.493750    LR 0.300000    Time 0.022386    
2018-10-28 00:33:27,570 - Epoch: [55][  300/  391]    Overall Loss 0.407340    Objective Loss 0.407340    Top1 86.033854    Top5 99.494792    LR 0.300000    Time 0.022348    
2018-10-28 00:33:28,678 - Epoch: [55][  350/  391]    Overall Loss 0.407802    Objective Loss 0.407802    Top1 86.040179    Top5 99.497768    LR 0.300000    Time 0.022319    
2018-10-28 00:33:29,666 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57919 |  0.00246 |    0.29665 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.19009 | -0.00593 |    0.06477 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18534 | -0.00328 |    0.06829 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18032 | -0.01181 |    0.07907 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13918 | -0.00694 |    0.04107 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19653 | -0.00816 |    0.08013 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13329 |  0.00158 |    0.03683 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18096 | -0.00836 |    0.08791 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16545 | -0.00810 |    0.09581 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25372 | -0.01468 |    0.12849 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13062 | -0.00716 |    0.05311 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10730 |  0.00153 |    0.03914 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13573 | -0.01082 |    0.06056 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11042 | -0.00086 |    0.05043 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11748 | -0.00486 |    0.04631 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11965 | -0.00643 |    0.06054 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13930 | -0.00667 |    0.05117 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11069 | -0.00576 |    0.04654 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08570 |  0.00068 |    0.03891 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05982 | -0.00069 |    0.01972 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04696 | -0.00145 |    0.01184 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.52914 | -0.03648 |    0.31680 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:33:29,667 - Total sparsity: 74.94

2018-10-28 00:33:29,667 - --- validate (epoch=55)-----------
2018-10-28 00:33:29,667 - 10000 samples (128 per mini-batch)
2018-10-28 00:33:30,393 - Epoch: [55][   50/   78]    Loss 0.564770    Top1 81.546875    Top5 98.968750    
2018-10-28 00:33:30,786 - ==> Top1: 81.450    Top5: 99.030    Loss: 0.570

2018-10-28 00:33:30,787 - ==> Best Top1: 84.170   On Epoch: 53

2018-10-28 00:33:30,787 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:33:30,799 - 

2018-10-28 00:33:30,799 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:33:31,968 - Epoch: [56][   50/  391]    Overall Loss 0.386939    Objective Loss 0.386939    Top1 86.546875    Top5 99.437500    LR 0.300000    Time 0.023350    
2018-10-28 00:33:33,080 - Epoch: [56][  100/  391]    Overall Loss 0.392779    Objective Loss 0.392779    Top1 86.414062    Top5 99.515625    LR 0.300000    Time 0.022776    
2018-10-28 00:33:34,192 - Epoch: [56][  150/  391]    Overall Loss 0.400365    Objective Loss 0.400365    Top1 86.197917    Top5 99.473958    LR 0.300000    Time 0.022592    
2018-10-28 00:33:35,305 - Epoch: [56][  200/  391]    Overall Loss 0.400666    Objective Loss 0.400666    Top1 86.167969    Top5 99.484375    LR 0.300000    Time 0.022502    
2018-10-28 00:33:36,417 - Epoch: [56][  250/  391]    Overall Loss 0.403107    Objective Loss 0.403107    Top1 86.009375    Top5 99.468750    LR 0.300000    Time 0.022444    
2018-10-28 00:33:37,529 - Epoch: [56][  300/  391]    Overall Loss 0.407062    Objective Loss 0.407062    Top1 85.893229    Top5 99.471354    LR 0.300000    Time 0.022405    
2018-10-28 00:33:38,642 - Epoch: [56][  350/  391]    Overall Loss 0.406453    Objective Loss 0.406453    Top1 85.937500    Top5 99.459821    LR 0.300000    Time 0.022370    
2018-10-28 00:33:39,630 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58006 |  0.00094 |    0.29977 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18998 | -0.00590 |    0.06452 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18477 | -0.00384 |    0.06789 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.17960 | -0.01337 |    0.07852 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13874 | -0.00607 |    0.04086 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19628 | -0.00695 |    0.08019 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13229 |  0.00076 |    0.03649 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18070 | -0.00753 |    0.08794 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16519 | -0.00787 |    0.09551 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25175 | -0.01163 |    0.12732 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13025 | -0.00667 |    0.05275 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10710 |  0.00130 |    0.03907 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13492 | -0.01036 |    0.06013 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10939 | -0.00065 |    0.04979 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11737 | -0.00418 |    0.04637 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11937 | -0.00669 |    0.06039 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13875 | -0.00558 |    0.05096 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11034 | -0.00557 |    0.04636 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08557 |  0.00076 |    0.03882 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06003 | -0.00086 |    0.01976 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04706 | -0.00148 |    0.01185 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53439 | -0.03369 |    0.31958 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:33:39,630 - Total sparsity: 74.94

2018-10-28 00:33:39,630 - --- validate (epoch=56)-----------
2018-10-28 00:33:39,630 - 10000 samples (128 per mini-batch)
2018-10-28 00:33:40,350 - Epoch: [56][   50/   78]    Loss 0.569687    Top1 81.187500    Top5 98.984375    
2018-10-28 00:33:40,742 - ==> Top1: 81.020    Top5: 99.050    Loss: 0.579

2018-10-28 00:33:40,743 - ==> Best Top1: 84.170   On Epoch: 53

2018-10-28 00:33:40,743 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:33:40,754 - 

2018-10-28 00:33:40,754 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:33:41,921 - Epoch: [57][   50/  391]    Overall Loss 0.400220    Objective Loss 0.400220    Top1 86.062500    Top5 99.484375    LR 0.300000    Time 0.023294    
2018-10-28 00:33:43,030 - Epoch: [57][  100/  391]    Overall Loss 0.404994    Objective Loss 0.404994    Top1 85.851562    Top5 99.453125    LR 0.300000    Time 0.022728    
2018-10-28 00:33:44,139 - Epoch: [57][  150/  391]    Overall Loss 0.401411    Objective Loss 0.401411    Top1 86.041667    Top5 99.453125    LR 0.300000    Time 0.022535    
2018-10-28 00:33:45,250 - Epoch: [57][  200/  391]    Overall Loss 0.407753    Objective Loss 0.407753    Top1 85.851562    Top5 99.417969    LR 0.300000    Time 0.022449    
2018-10-28 00:33:46,359 - Epoch: [57][  250/  391]    Overall Loss 0.409254    Objective Loss 0.409254    Top1 85.887500    Top5 99.421875    LR 0.300000    Time 0.022390    
2018-10-28 00:33:47,470 - Epoch: [57][  300/  391]    Overall Loss 0.404130    Objective Loss 0.404130    Top1 86.046875    Top5 99.408854    LR 0.300000    Time 0.022360    
2018-10-28 00:33:48,581 - Epoch: [57][  350/  391]    Overall Loss 0.404555    Objective Loss 0.404555    Top1 86.017857    Top5 99.406250    LR 0.300000    Time 0.022336    
2018-10-28 00:33:49,573 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57932 |  0.00480 |    0.30107 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18906 | -0.00652 |    0.06419 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18449 | -0.00432 |    0.06789 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.17996 | -0.01118 |    0.07860 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13837 | -0.00567 |    0.04044 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19727 | -0.00737 |    0.08048 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13280 |  0.00061 |    0.03645 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18008 | -0.00659 |    0.08730 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16452 | -0.00860 |    0.09521 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25290 | -0.00916 |    0.12839 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13052 | -0.00715 |    0.05302 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10699 |  0.00100 |    0.03884 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13464 | -0.01008 |    0.05985 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10925 | -0.00075 |    0.04990 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11728 | -0.00442 |    0.04613 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11934 | -0.00613 |    0.06021 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13897 | -0.00485 |    0.05112 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11024 | -0.00572 |    0.04634 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08585 |  0.00057 |    0.03903 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06055 | -0.00079 |    0.01993 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04738 | -0.00140 |    0.01191 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53568 | -0.03473 |    0.32047 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:33:49,573 - Total sparsity: 74.94

2018-10-28 00:33:49,573 - --- validate (epoch=57)-----------
2018-10-28 00:33:49,573 - 10000 samples (128 per mini-batch)
2018-10-28 00:33:50,291 - Epoch: [57][   50/   78]    Loss 0.550225    Top1 81.937500    Top5 98.828125    
2018-10-28 00:33:50,681 - ==> Top1: 81.590    Top5: 98.820    Loss: 0.552

2018-10-28 00:33:50,682 - ==> Best Top1: 84.170   On Epoch: 53

2018-10-28 00:33:50,682 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:33:50,697 - 

2018-10-28 00:33:50,697 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:33:51,866 - Epoch: [58][   50/  391]    Overall Loss 0.391235    Objective Loss 0.391235    Top1 86.296875    Top5 99.546875    LR 0.300000    Time 0.023342    
2018-10-28 00:33:52,979 - Epoch: [58][  100/  391]    Overall Loss 0.401019    Objective Loss 0.401019    Top1 85.859375    Top5 99.507812    LR 0.300000    Time 0.022787    
2018-10-28 00:33:54,088 - Epoch: [58][  150/  391]    Overall Loss 0.406235    Objective Loss 0.406235    Top1 85.828125    Top5 99.473958    LR 0.300000    Time 0.022578    
2018-10-28 00:33:55,197 - Epoch: [58][  200/  391]    Overall Loss 0.406123    Objective Loss 0.406123    Top1 85.820312    Top5 99.468750    LR 0.300000    Time 0.022470    
2018-10-28 00:33:56,305 - Epoch: [58][  250/  391]    Overall Loss 0.409125    Objective Loss 0.409125    Top1 85.675000    Top5 99.475000    LR 0.300000    Time 0.022404    
2018-10-28 00:33:57,413 - Epoch: [58][  300/  391]    Overall Loss 0.410452    Objective Loss 0.410452    Top1 85.682292    Top5 99.458333    LR 0.300000    Time 0.022359    
2018-10-28 00:33:58,522 - Epoch: [58][  350/  391]    Overall Loss 0.409604    Objective Loss 0.409604    Top1 85.736607    Top5 99.464286    LR 0.300000    Time 0.022329    
2018-10-28 00:33:59,510 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58062 |  0.00295 |    0.29996 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18905 | -0.00528 |    0.06425 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18465 | -0.00493 |    0.06747 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18005 | -0.01120 |    0.07874 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13817 | -0.00719 |    0.04037 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19710 | -0.00922 |    0.08003 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13258 |  0.00064 |    0.03627 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18123 | -0.00824 |    0.08846 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16523 | -0.00742 |    0.09550 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25383 | -0.01039 |    0.12650 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13105 | -0.00772 |    0.05330 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10715 |  0.00106 |    0.03890 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13514 | -0.01007 |    0.06020 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10983 | -0.00138 |    0.05014 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11759 | -0.00454 |    0.04638 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11955 | -0.00639 |    0.06026 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13923 | -0.00537 |    0.05153 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11062 | -0.00566 |    0.04645 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08631 |  0.00071 |    0.03909 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06099 | -0.00096 |    0.02004 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04754 | -0.00126 |    0.01188 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53070 | -0.03521 |    0.31701 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:33:59,511 - Total sparsity: 74.94

2018-10-28 00:33:59,511 - --- validate (epoch=58)-----------
2018-10-28 00:33:59,511 - 10000 samples (128 per mini-batch)
2018-10-28 00:34:00,229 - Epoch: [58][   50/   78]    Loss 0.612717    Top1 80.906250    Top5 98.859375    
2018-10-28 00:34:00,612 - ==> Top1: 80.470    Top5: 98.960    Loss: 0.620

2018-10-28 00:34:00,613 - ==> Best Top1: 84.170   On Epoch: 53

2018-10-28 00:34:00,613 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:34:00,623 - 

2018-10-28 00:34:00,623 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:34:01,791 - Epoch: [59][   50/  391]    Overall Loss 0.397699    Objective Loss 0.397699    Top1 86.140625    Top5 99.546875    LR 0.300000    Time 0.023324    
2018-10-28 00:34:02,900 - Epoch: [59][  100/  391]    Overall Loss 0.396044    Objective Loss 0.396044    Top1 86.062500    Top5 99.562500    LR 0.300000    Time 0.022741    
2018-10-28 00:34:04,009 - Epoch: [59][  150/  391]    Overall Loss 0.399785    Objective Loss 0.399785    Top1 85.973958    Top5 99.520833    LR 0.300000    Time 0.022543    
2018-10-28 00:34:05,119 - Epoch: [59][  200/  391]    Overall Loss 0.400965    Objective Loss 0.400965    Top1 85.921875    Top5 99.492188    LR 0.300000    Time 0.022453    
2018-10-28 00:34:06,230 - Epoch: [59][  250/  391]    Overall Loss 0.403922    Objective Loss 0.403922    Top1 85.893750    Top5 99.471875    LR 0.300000    Time 0.022400    
2018-10-28 00:34:07,342 - Epoch: [59][  300/  391]    Overall Loss 0.402484    Objective Loss 0.402484    Top1 85.960938    Top5 99.484375    LR 0.300000    Time 0.022369    
2018-10-28 00:34:08,455 - Epoch: [59][  350/  391]    Overall Loss 0.401293    Objective Loss 0.401293    Top1 85.948661    Top5 99.497768    LR 0.300000    Time 0.022350    
2018-10-28 00:34:09,443 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57922 | -0.00170 |    0.29841 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18859 | -0.00631 |    0.06380 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18544 | -0.00477 |    0.06828 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.17955 | -0.01310 |    0.07879 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13809 | -0.00684 |    0.04058 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19636 | -0.00829 |    0.07940 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13226 |  0.00085 |    0.03628 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18052 | -0.00822 |    0.08801 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16453 | -0.00839 |    0.09521 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25222 | -0.01252 |    0.12699 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13050 | -0.00723 |    0.05329 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10691 |  0.00110 |    0.03886 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13466 | -0.00996 |    0.05981 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10962 | -0.00121 |    0.05000 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11733 | -0.00472 |    0.04607 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11929 | -0.00688 |    0.06023 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13894 | -0.00513 |    0.05100 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11040 | -0.00599 |    0.04636 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08610 |  0.00050 |    0.03901 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06100 | -0.00092 |    0.02009 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04755 | -0.00133 |    0.01194 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53693 | -0.03629 |    0.32073 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:34:09,443 - Total sparsity: 74.94

2018-10-28 00:34:09,444 - --- validate (epoch=59)-----------
2018-10-28 00:34:09,444 - 10000 samples (128 per mini-batch)
2018-10-28 00:34:10,184 - Epoch: [59][   50/   78]    Loss 0.681357    Top1 78.390625    Top5 99.015625    
2018-10-28 00:34:10,573 - ==> Top1: 78.010    Top5: 99.150    Loss: 0.685

2018-10-28 00:34:10,574 - ==> Best Top1: 84.170   On Epoch: 53

2018-10-28 00:34:10,574 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:34:10,582 - 

2018-10-28 00:34:10,583 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:34:11,765 - Epoch: [60][   50/  391]    Overall Loss 0.398510    Objective Loss 0.398510    Top1 85.984375    Top5 99.515625    LR 0.300000    Time 0.023619    
2018-10-28 00:34:12,874 - Epoch: [60][  100/  391]    Overall Loss 0.402074    Objective Loss 0.402074    Top1 85.835938    Top5 99.523438    LR 0.300000    Time 0.022879    
2018-10-28 00:34:13,983 - Epoch: [60][  150/  391]    Overall Loss 0.402296    Objective Loss 0.402296    Top1 85.927083    Top5 99.489583    LR 0.300000    Time 0.022643    
2018-10-28 00:34:15,093 - Epoch: [60][  200/  391]    Overall Loss 0.409828    Objective Loss 0.409828    Top1 85.730469    Top5 99.464844    LR 0.300000    Time 0.022523    
2018-10-28 00:34:16,202 - Epoch: [60][  250/  391]    Overall Loss 0.409900    Objective Loss 0.409900    Top1 85.818750    Top5 99.471875    LR 0.300000    Time 0.022451    
2018-10-28 00:34:17,311 - Epoch: [60][  300/  391]    Overall Loss 0.410602    Objective Loss 0.410602    Top1 85.794271    Top5 99.479167    LR 0.300000    Time 0.022402    
2018-10-28 00:34:18,423 - Epoch: [60][  350/  391]    Overall Loss 0.410727    Objective Loss 0.410727    Top1 85.868304    Top5 99.468750    LR 0.300000    Time 0.022375    
2018-10-28 00:34:19,410 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57606 |  0.00288 |    0.29928 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18873 | -0.00690 |    0.06410 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18576 | -0.00467 |    0.06865 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18058 | -0.01329 |    0.07894 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13831 | -0.00734 |    0.04107 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19708 | -0.00811 |    0.07951 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13280 |  0.00043 |    0.03628 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18069 | -0.00631 |    0.08784 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16482 | -0.00748 |    0.09529 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25263 | -0.01561 |    0.12710 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13070 | -0.00738 |    0.05337 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10723 |  0.00144 |    0.03920 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13498 | -0.01018 |    0.05982 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10978 | -0.00154 |    0.04999 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11757 | -0.00473 |    0.04619 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11960 | -0.00689 |    0.06043 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13898 | -0.00497 |    0.05051 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11066 | -0.00561 |    0.04648 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08622 |  0.00041 |    0.03909 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06114 | -0.00094 |    0.02010 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04772 | -0.00148 |    0.01198 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53386 | -0.03584 |    0.31799 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:34:19,410 - Total sparsity: 74.94

2018-10-28 00:34:19,410 - --- validate (epoch=60)-----------
2018-10-28 00:34:19,411 - 10000 samples (128 per mini-batch)
2018-10-28 00:34:20,135 - Epoch: [60][   50/   78]    Loss 0.595398    Top1 80.781250    Top5 99.218750    
2018-10-28 00:34:20,528 - ==> Top1: 80.670    Top5: 99.280    Loss: 0.588

2018-10-28 00:34:20,529 - ==> Best Top1: 84.170   On Epoch: 53

2018-10-28 00:34:20,529 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:34:20,539 - 

2018-10-28 00:34:20,539 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:34:21,707 - Epoch: [61][   50/  391]    Overall Loss 0.401744    Objective Loss 0.401744    Top1 86.078125    Top5 99.562500    LR 0.300000    Time 0.023317    
2018-10-28 00:34:22,818 - Epoch: [61][  100/  391]    Overall Loss 0.401267    Objective Loss 0.401267    Top1 86.085938    Top5 99.601562    LR 0.300000    Time 0.022755    
2018-10-28 00:34:23,927 - Epoch: [61][  150/  391]    Overall Loss 0.407274    Objective Loss 0.407274    Top1 85.901042    Top5 99.546875    LR 0.300000    Time 0.022556    
2018-10-28 00:34:25,040 - Epoch: [61][  200/  391]    Overall Loss 0.407743    Objective Loss 0.407743    Top1 85.914062    Top5 99.511719    LR 0.300000    Time 0.022477    
2018-10-28 00:34:26,149 - Epoch: [61][  250/  391]    Overall Loss 0.412270    Objective Loss 0.412270    Top1 85.709375    Top5 99.475000    LR 0.300000    Time 0.022412    
2018-10-28 00:34:27,258 - Epoch: [61][  300/  391]    Overall Loss 0.409990    Objective Loss 0.409990    Top1 85.739583    Top5 99.466146    LR 0.300000    Time 0.022370    
2018-10-28 00:34:28,369 - Epoch: [61][  350/  391]    Overall Loss 0.409170    Objective Loss 0.409170    Top1 85.799107    Top5 99.477679    LR 0.300000    Time 0.022344    
2018-10-28 00:34:29,362 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57288 | -0.00019 |    0.29702 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18791 | -0.00705 |    0.06320 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18492 | -0.00484 |    0.06881 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18058 | -0.01085 |    0.07738 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13736 | -0.00687 |    0.04028 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19624 | -0.00733 |    0.07906 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13234 |  0.00097 |    0.03625 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18007 | -0.00796 |    0.08758 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16445 | -0.00878 |    0.09513 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25258 | -0.01210 |    0.12695 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13021 | -0.00781 |    0.05292 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10714 |  0.00135 |    0.03923 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13461 | -0.01051 |    0.05965 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10978 | -0.00130 |    0.04989 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11724 | -0.00448 |    0.04608 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11959 | -0.00660 |    0.06057 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13833 | -0.00569 |    0.05053 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11033 | -0.00567 |    0.04624 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08618 |  0.00047 |    0.03896 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06096 | -0.00119 |    0.01998 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04758 | -0.00133 |    0.01194 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.54009 | -0.03474 |    0.32179 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:34:29,362 - Total sparsity: 74.94

2018-10-28 00:34:29,362 - --- validate (epoch=61)-----------
2018-10-28 00:34:29,362 - 10000 samples (128 per mini-batch)
2018-10-28 00:34:30,083 - Epoch: [61][   50/   78]    Loss 0.491073    Top1 84.421875    Top5 98.937500    
2018-10-28 00:34:30,478 - ==> Top1: 84.520    Top5: 99.090    Loss: 0.489

2018-10-28 00:34:30,478 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:34:30,479 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:34:30,493 - 

2018-10-28 00:34:30,493 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:34:31,660 - Epoch: [62][   50/  391]    Overall Loss 0.405287    Objective Loss 0.405287    Top1 85.609375    Top5 99.468750    LR 0.300000    Time 0.023312    
2018-10-28 00:34:32,770 - Epoch: [62][  100/  391]    Overall Loss 0.401934    Objective Loss 0.401934    Top1 86.000000    Top5 99.460938    LR 0.300000    Time 0.022740    
2018-10-28 00:34:33,879 - Epoch: [62][  150/  391]    Overall Loss 0.405476    Objective Loss 0.405476    Top1 85.703125    Top5 99.427083    LR 0.300000    Time 0.022544    
2018-10-28 00:34:34,988 - Epoch: [62][  200/  391]    Overall Loss 0.408761    Objective Loss 0.408761    Top1 85.640625    Top5 99.445312    LR 0.300000    Time 0.022450    
2018-10-28 00:34:36,099 - Epoch: [62][  250/  391]    Overall Loss 0.407574    Objective Loss 0.407574    Top1 85.743750    Top5 99.425000    LR 0.300000    Time 0.022396    
2018-10-28 00:34:37,210 - Epoch: [62][  300/  391]    Overall Loss 0.409858    Objective Loss 0.409858    Top1 85.679688    Top5 99.458333    LR 0.300000    Time 0.022363    
2018-10-28 00:34:38,321 - Epoch: [62][  350/  391]    Overall Loss 0.411947    Objective Loss 0.411947    Top1 85.631696    Top5 99.450893    LR 0.300000    Time 0.022339    
2018-10-28 00:34:39,312 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58041 | -0.00018 |    0.29908 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18786 | -0.00506 |    0.06238 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18486 | -0.00473 |    0.06813 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18022 | -0.01194 |    0.07834 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13783 | -0.00736 |    0.04075 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19646 | -0.00811 |    0.07924 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13271 |  0.00136 |    0.03649 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18126 | -0.00869 |    0.08823 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16499 | -0.00787 |    0.09516 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25466 | -0.00988 |    0.12656 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13086 | -0.00722 |    0.05326 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10750 |  0.00150 |    0.03903 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13534 | -0.00994 |    0.05996 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11031 | -0.00145 |    0.05005 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11710 | -0.00456 |    0.04623 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11986 | -0.00667 |    0.06061 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13887 | -0.00414 |    0.05089 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11052 | -0.00579 |    0.04646 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08634 |  0.00065 |    0.03907 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06100 | -0.00086 |    0.02003 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04772 | -0.00145 |    0.01202 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53554 | -0.03379 |    0.31917 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:34:39,312 - Total sparsity: 74.94

2018-10-28 00:34:39,312 - --- validate (epoch=62)-----------
2018-10-28 00:34:39,312 - 10000 samples (128 per mini-batch)
2018-10-28 00:34:40,036 - Epoch: [62][   50/   78]    Loss 0.562869    Top1 82.468750    Top5 98.906250    
2018-10-28 00:34:40,428 - ==> Top1: 82.600    Top5: 99.050    Loss: 0.558

2018-10-28 00:34:40,429 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:34:40,429 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:34:40,440 - 

2018-10-28 00:34:40,441 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:34:41,610 - Epoch: [63][   50/  391]    Overall Loss 0.408348    Objective Loss 0.408348    Top1 86.234375    Top5 99.437500    LR 0.300000    Time 0.023341    
2018-10-28 00:34:42,720 - Epoch: [63][  100/  391]    Overall Loss 0.399569    Objective Loss 0.399569    Top1 86.398438    Top5 99.437500    LR 0.300000    Time 0.022765    
2018-10-28 00:34:43,831 - Epoch: [63][  150/  391]    Overall Loss 0.402350    Objective Loss 0.402350    Top1 86.359375    Top5 99.442708    LR 0.300000    Time 0.022570    
2018-10-28 00:34:44,942 - Epoch: [63][  200/  391]    Overall Loss 0.402766    Objective Loss 0.402766    Top1 86.386719    Top5 99.441406    LR 0.300000    Time 0.022461    
2018-10-28 00:34:46,053 - Epoch: [63][  250/  391]    Overall Loss 0.402004    Objective Loss 0.402004    Top1 86.381250    Top5 99.440625    LR 0.300000    Time 0.022407    
2018-10-28 00:34:47,164 - Epoch: [63][  300/  391]    Overall Loss 0.402499    Objective Loss 0.402499    Top1 86.372396    Top5 99.473958    LR 0.300000    Time 0.022370    
2018-10-28 00:34:48,275 - Epoch: [63][  350/  391]    Overall Loss 0.404704    Objective Loss 0.404704    Top1 86.194196    Top5 99.486607    LR 0.300000    Time 0.022345    
2018-10-28 00:34:49,264 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57883 |  0.00534 |    0.29958 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18687 | -0.00535 |    0.06251 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18357 | -0.00346 |    0.06778 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.17992 | -0.01135 |    0.07757 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13812 | -0.00827 |    0.04074 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19720 | -0.00727 |    0.07935 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13276 |  0.00201 |    0.03607 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18061 | -0.00851 |    0.08799 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16453 | -0.00817 |    0.09486 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25568 | -0.00723 |    0.12843 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13015 | -0.00676 |    0.05288 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10696 |  0.00139 |    0.03885 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13489 | -0.01039 |    0.05995 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11048 | -0.00249 |    0.05012 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11699 | -0.00481 |    0.04618 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11988 | -0.00682 |    0.06050 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13953 | -0.00484 |    0.05084 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11054 | -0.00534 |    0.04633 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08643 |  0.00054 |    0.03912 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06109 | -0.00106 |    0.02010 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04775 | -0.00141 |    0.01192 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53493 | -0.03626 |    0.31891 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:34:49,264 - Total sparsity: 74.94

2018-10-28 00:34:49,264 - --- validate (epoch=63)-----------
2018-10-28 00:34:49,265 - 10000 samples (128 per mini-batch)
2018-10-28 00:34:49,985 - Epoch: [63][   50/   78]    Loss 0.572084    Top1 82.125000    Top5 99.109375    
2018-10-28 00:34:50,376 - ==> Top1: 82.080    Top5: 99.170    Loss: 0.567

2018-10-28 00:34:50,376 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:34:50,377 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:34:50,388 - 

2018-10-28 00:34:50,388 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:34:51,557 - Epoch: [64][   50/  391]    Overall Loss 0.397527    Objective Loss 0.397527    Top1 86.125000    Top5 99.500000    LR 0.300000    Time 0.023345    
2018-10-28 00:34:52,666 - Epoch: [64][  100/  391]    Overall Loss 0.400414    Objective Loss 0.400414    Top1 85.937500    Top5 99.437500    LR 0.300000    Time 0.022743    
2018-10-28 00:34:53,778 - Epoch: [64][  150/  391]    Overall Loss 0.403779    Objective Loss 0.403779    Top1 85.911458    Top5 99.479167    LR 0.300000    Time 0.022567    
2018-10-28 00:34:54,889 - Epoch: [64][  200/  391]    Overall Loss 0.406943    Objective Loss 0.406943    Top1 85.824219    Top5 99.480469    LR 0.300000    Time 0.022473    
2018-10-28 00:34:56,002 - Epoch: [64][  250/  391]    Overall Loss 0.403404    Objective Loss 0.403404    Top1 85.915625    Top5 99.493750    LR 0.300000    Time 0.022426    
2018-10-28 00:34:57,112 - Epoch: [64][  300/  391]    Overall Loss 0.402613    Objective Loss 0.402613    Top1 86.002604    Top5 99.494792    LR 0.300000    Time 0.022386    
2018-10-28 00:34:58,223 - Epoch: [64][  350/  391]    Overall Loss 0.403758    Objective Loss 0.403758    Top1 85.957589    Top5 99.504464    LR 0.300000    Time 0.022358    
2018-10-28 00:34:59,210 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57975 | -0.00123 |    0.29975 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18865 | -0.00388 |    0.06278 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18362 | -0.00336 |    0.06709 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18048 | -0.01290 |    0.07756 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13816 | -0.00775 |    0.04107 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19939 | -0.00852 |    0.08042 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13374 |  0.00184 |    0.03588 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18133 | -0.00856 |    0.08779 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16521 | -0.00806 |    0.09519 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25523 | -0.00887 |    0.12918 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13100 | -0.00688 |    0.05302 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10729 |  0.00126 |    0.03901 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13498 | -0.01012 |    0.05996 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11036 | -0.00215 |    0.05046 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11722 | -0.00510 |    0.04629 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.12004 | -0.00652 |    0.06052 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13918 | -0.00555 |    0.05057 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11034 | -0.00544 |    0.04620 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08631 |  0.00073 |    0.03907 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06131 | -0.00111 |    0.02022 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04768 | -0.00138 |    0.01191 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53806 | -0.03569 |    0.32117 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:34:59,210 - Total sparsity: 74.94

2018-10-28 00:34:59,210 - --- validate (epoch=64)-----------
2018-10-28 00:34:59,211 - 10000 samples (128 per mini-batch)
2018-10-28 00:34:59,934 - Epoch: [64][   50/   78]    Loss 0.516123    Top1 82.875000    Top5 99.156250    
2018-10-28 00:35:00,326 - ==> Top1: 82.970    Top5: 99.240    Loss: 0.515

2018-10-28 00:35:00,327 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:35:00,327 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:35:00,336 - 

2018-10-28 00:35:00,337 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:35:01,506 - Epoch: [65][   50/  391]    Overall Loss 0.411909    Objective Loss 0.411909    Top1 85.828125    Top5 99.453125    LR 0.300000    Time 0.023345    
2018-10-28 00:35:02,617 - Epoch: [65][  100/  391]    Overall Loss 0.403510    Objective Loss 0.403510    Top1 86.078125    Top5 99.570312    LR 0.300000    Time 0.022773    
2018-10-28 00:35:03,726 - Epoch: [65][  150/  391]    Overall Loss 0.401548    Objective Loss 0.401548    Top1 86.119792    Top5 99.557292    LR 0.300000    Time 0.022566    
2018-10-28 00:35:04,837 - Epoch: [65][  200/  391]    Overall Loss 0.408848    Objective Loss 0.408848    Top1 85.738281    Top5 99.539062    LR 0.300000    Time 0.022474    
2018-10-28 00:35:05,950 - Epoch: [65][  250/  391]    Overall Loss 0.409601    Objective Loss 0.409601    Top1 85.759375    Top5 99.521875    LR 0.300000    Time 0.022424    
2018-10-28 00:35:07,061 - Epoch: [65][  300/  391]    Overall Loss 0.406812    Objective Loss 0.406812    Top1 85.893229    Top5 99.523438    LR 0.300000    Time 0.022385    
2018-10-28 00:35:08,173 - Epoch: [65][  350/  391]    Overall Loss 0.407323    Objective Loss 0.407323    Top1 85.861607    Top5 99.531250    LR 0.300000    Time 0.022360    
2018-10-28 00:35:09,164 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58727 | -0.00441 |    0.30319 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18871 | -0.00584 |    0.06276 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18295 | -0.00551 |    0.06672 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18024 | -0.01277 |    0.07778 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13767 | -0.00760 |    0.04022 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19879 | -0.00754 |    0.08012 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13392 |  0.00057 |    0.03594 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18069 | -0.00819 |    0.08739 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16469 | -0.00761 |    0.09492 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25663 | -0.01293 |    0.12962 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13068 | -0.00734 |    0.05311 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10733 |  0.00143 |    0.03887 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13525 | -0.00946 |    0.06010 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11010 | -0.00129 |    0.05003 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11713 | -0.00512 |    0.04616 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.12017 | -0.00672 |    0.06077 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13939 | -0.00577 |    0.05022 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11053 | -0.00537 |    0.04628 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08652 |  0.00084 |    0.03904 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06164 | -0.00110 |    0.02026 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04776 | -0.00137 |    0.01194 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53469 | -0.03697 |    0.31852 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:35:09,164 - Total sparsity: 74.94

2018-10-28 00:35:09,164 - --- validate (epoch=65)-----------
2018-10-28 00:35:09,164 - 10000 samples (128 per mini-batch)
2018-10-28 00:35:09,891 - Epoch: [65][   50/   78]    Loss 0.659471    Top1 79.609375    Top5 98.640625    
2018-10-28 00:35:10,284 - ==> Top1: 79.600    Top5: 98.750    Loss: 0.659

2018-10-28 00:35:10,285 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:35:10,285 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:35:10,303 - 

2018-10-28 00:35:10,303 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:35:11,443 - Epoch: [66][   50/  391]    Overall Loss 0.395333    Objective Loss 0.395333    Top1 85.796875    Top5 99.500000    LR 0.300000    Time 0.022774    
2018-10-28 00:35:12,553 - Epoch: [66][  100/  391]    Overall Loss 0.408706    Objective Loss 0.408706    Top1 85.570312    Top5 99.445312    LR 0.300000    Time 0.022475    
2018-10-28 00:35:13,663 - Epoch: [66][  150/  391]    Overall Loss 0.407233    Objective Loss 0.407233    Top1 85.843750    Top5 99.427083    LR 0.300000    Time 0.022372    
2018-10-28 00:35:14,773 - Epoch: [66][  200/  391]    Overall Loss 0.402386    Objective Loss 0.402386    Top1 86.035156    Top5 99.410156    LR 0.300000    Time 0.022325    
2018-10-28 00:35:15,882 - Epoch: [66][  250/  391]    Overall Loss 0.405636    Objective Loss 0.405636    Top1 85.959375    Top5 99.431250    LR 0.300000    Time 0.022292    
2018-10-28 00:35:16,993 - Epoch: [66][  300/  391]    Overall Loss 0.411405    Objective Loss 0.411405    Top1 85.763021    Top5 99.434896    LR 0.300000    Time 0.022273    
2018-10-28 00:35:18,104 - Epoch: [66][  350/  391]    Overall Loss 0.410527    Objective Loss 0.410527    Top1 85.765625    Top5 99.426339    LR 0.300000    Time 0.022264    
2018-10-28 00:35:19,096 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58574 |  0.00518 |    0.30234 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18868 | -0.00719 |    0.06307 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18264 | -0.00302 |    0.06615 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.17982 | -0.01145 |    0.07814 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13772 | -0.00614 |    0.04057 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19875 | -0.00795 |    0.08033 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13495 |  0.00192 |    0.03676 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18014 | -0.00833 |    0.08752 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16476 | -0.00696 |    0.09439 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25781 | -0.01514 |    0.13037 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13136 | -0.00716 |    0.05318 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10779 |  0.00167 |    0.03929 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13514 | -0.00975 |    0.06007 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11010 | -0.00129 |    0.04979 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11706 | -0.00476 |    0.04616 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.12004 | -0.00668 |    0.06083 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13994 | -0.00483 |    0.05122 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11060 | -0.00544 |    0.04624 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08678 |  0.00042 |    0.03917 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06191 | -0.00113 |    0.02029 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04787 | -0.00128 |    0.01192 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.52866 | -0.03831 |    0.31527 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:35:19,096 - Total sparsity: 74.94

2018-10-28 00:35:19,096 - --- validate (epoch=66)-----------
2018-10-28 00:35:19,097 - 10000 samples (128 per mini-batch)
2018-10-28 00:35:19,813 - Epoch: [66][   50/   78]    Loss 0.461831    Top1 84.968750    Top5 99.265625    
2018-10-28 00:35:20,203 - ==> Top1: 84.510    Top5: 99.340    Loss: 0.466

2018-10-28 00:35:20,203 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:35:20,204 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:35:20,221 - 

2018-10-28 00:35:20,221 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:35:21,362 - Epoch: [67][   50/  391]    Overall Loss 0.374984    Objective Loss 0.374984    Top1 87.328125    Top5 99.500000    LR 0.300000    Time 0.022798    
2018-10-28 00:35:22,471 - Epoch: [67][  100/  391]    Overall Loss 0.393116    Objective Loss 0.393116    Top1 86.703125    Top5 99.421875    LR 0.300000    Time 0.022473    
2018-10-28 00:35:23,579 - Epoch: [67][  150/  391]    Overall Loss 0.404374    Objective Loss 0.404374    Top1 86.130208    Top5 99.442708    LR 0.300000    Time 0.022360    
2018-10-28 00:35:24,687 - Epoch: [67][  200/  391]    Overall Loss 0.406557    Objective Loss 0.406557    Top1 85.949219    Top5 99.449219    LR 0.300000    Time 0.022303    
2018-10-28 00:35:25,794 - Epoch: [67][  250/  391]    Overall Loss 0.403004    Objective Loss 0.403004    Top1 86.040625    Top5 99.475000    LR 0.300000    Time 0.022268    
2018-10-28 00:35:26,903 - Epoch: [67][  300/  391]    Overall Loss 0.405506    Objective Loss 0.405506    Top1 85.981771    Top5 99.476562    LR 0.300000    Time 0.022247    
2018-10-28 00:35:28,012 - Epoch: [67][  350/  391]    Overall Loss 0.404555    Objective Loss 0.404555    Top1 86.044643    Top5 99.470982    LR 0.300000    Time 0.022233    
2018-10-28 00:35:29,001 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58811 | -0.00802 |    0.30489 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18813 | -0.00582 |    0.06345 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18236 | -0.00164 |    0.06613 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18033 | -0.01264 |    0.07792 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13873 | -0.00663 |    0.04030 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19955 | -0.00987 |    0.08008 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13489 |  0.00126 |    0.03649 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18022 | -0.00755 |    0.08740 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16495 | -0.00845 |    0.09457 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25654 | -0.01294 |    0.12972 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13116 | -0.00707 |    0.05314 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10744 |  0.00199 |    0.03927 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13571 | -0.00981 |    0.06028 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11073 | -0.00044 |    0.05004 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11719 | -0.00501 |    0.04635 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11991 | -0.00656 |    0.06070 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13945 | -0.00450 |    0.05045 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11057 | -0.00554 |    0.04630 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08663 |  0.00063 |    0.03898 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06177 | -0.00140 |    0.02025 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04777 | -0.00116 |    0.01192 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53520 | -0.03765 |    0.32044 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:35:29,001 - Total sparsity: 74.94

2018-10-28 00:35:29,001 - --- validate (epoch=67)-----------
2018-10-28 00:35:29,001 - 10000 samples (128 per mini-batch)
2018-10-28 00:35:29,723 - Epoch: [67][   50/   78]    Loss 0.668159    Top1 79.140625    Top5 99.218750    
2018-10-28 00:35:30,109 - ==> Top1: 78.710    Top5: 99.210    Loss: 0.658

2018-10-28 00:35:30,110 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:35:30,110 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:35:30,125 - 

2018-10-28 00:35:30,126 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:35:31,299 - Epoch: [68][   50/  391]    Overall Loss 0.421900    Objective Loss 0.421900    Top1 85.093750    Top5 99.687500    LR 0.300000    Time 0.023424    
2018-10-28 00:35:32,409 - Epoch: [68][  100/  391]    Overall Loss 0.415319    Objective Loss 0.415319    Top1 85.218750    Top5 99.609375    LR 0.300000    Time 0.022808    
2018-10-28 00:35:33,521 - Epoch: [68][  150/  391]    Overall Loss 0.407248    Objective Loss 0.407248    Top1 85.692708    Top5 99.567708    LR 0.300000    Time 0.022609    
2018-10-28 00:35:34,632 - Epoch: [68][  200/  391]    Overall Loss 0.404639    Objective Loss 0.404639    Top1 85.824219    Top5 99.578125    LR 0.300000    Time 0.022502    
2018-10-28 00:35:35,741 - Epoch: [68][  250/  391]    Overall Loss 0.407976    Objective Loss 0.407976    Top1 85.678125    Top5 99.553125    LR 0.300000    Time 0.022434    
2018-10-28 00:35:36,852 - Epoch: [68][  300/  391]    Overall Loss 0.409512    Objective Loss 0.409512    Top1 85.684896    Top5 99.546875    LR 0.300000    Time 0.022393    
2018-10-28 00:35:37,961 - Epoch: [68][  350/  391]    Overall Loss 0.409038    Objective Loss 0.409038    Top1 85.691964    Top5 99.535714    LR 0.300000    Time 0.022360    
2018-10-28 00:35:38,948 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58268 |  0.00402 |    0.29921 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18832 | -0.00478 |    0.06302 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18235 | -0.00288 |    0.06584 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18142 | -0.01388 |    0.07806 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13968 | -0.00695 |    0.04096 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.20037 | -0.00851 |    0.08082 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13476 |  0.00180 |    0.03687 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.17991 | -0.00833 |    0.08722 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16494 | -0.00844 |    0.09463 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25696 | -0.01252 |    0.12831 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13139 | -0.00736 |    0.05309 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10783 |  0.00187 |    0.03934 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13556 | -0.01012 |    0.06049 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11085 | -0.00235 |    0.05022 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11724 | -0.00455 |    0.04612 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11985 | -0.00626 |    0.06054 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.14002 | -0.00508 |    0.05151 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11046 | -0.00552 |    0.04626 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08654 |  0.00040 |    0.03900 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06186 | -0.00112 |    0.02018 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04777 | -0.00127 |    0.01195 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53636 | -0.03901 |    0.32048 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:35:38,949 - Total sparsity: 74.94

2018-10-28 00:35:38,949 - --- validate (epoch=68)-----------
2018-10-28 00:35:38,949 - 10000 samples (128 per mini-batch)
2018-10-28 00:35:39,674 - Epoch: [68][   50/   78]    Loss 0.743046    Top1 76.437500    Top5 99.000000    
2018-10-28 00:35:40,065 - ==> Top1: 76.340    Top5: 99.040    Loss: 0.739

2018-10-28 00:35:40,066 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:35:40,066 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:35:40,077 - 

2018-10-28 00:35:40,078 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:35:41,245 - Epoch: [69][   50/  391]    Overall Loss 0.386578    Objective Loss 0.386578    Top1 86.734375    Top5 99.515625    LR 0.300000    Time 0.023312    
2018-10-28 00:35:42,353 - Epoch: [69][  100/  391]    Overall Loss 0.389002    Objective Loss 0.389002    Top1 86.539062    Top5 99.562500    LR 0.300000    Time 0.022726    
2018-10-28 00:35:43,461 - Epoch: [69][  150/  391]    Overall Loss 0.395956    Objective Loss 0.395956    Top1 86.265625    Top5 99.536458    LR 0.300000    Time 0.022527    
2018-10-28 00:35:44,570 - Epoch: [69][  200/  391]    Overall Loss 0.400679    Objective Loss 0.400679    Top1 86.085938    Top5 99.531250    LR 0.300000    Time 0.022432    
2018-10-28 00:35:45,679 - Epoch: [69][  250/  391]    Overall Loss 0.405244    Objective Loss 0.405244    Top1 85.937500    Top5 99.515625    LR 0.300000    Time 0.022377    
2018-10-28 00:35:46,791 - Epoch: [69][  300/  391]    Overall Loss 0.408139    Objective Loss 0.408139    Top1 85.804688    Top5 99.523438    LR 0.300000    Time 0.022350    
2018-10-28 00:35:47,902 - Epoch: [69][  350/  391]    Overall Loss 0.410572    Objective Loss 0.410572    Top1 85.747768    Top5 99.495536    LR 0.300000    Time 0.022330    
2018-10-28 00:35:48,892 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58819 |  0.00063 |    0.30377 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18884 | -0.00589 |    0.06321 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18354 | -0.00260 |    0.06682 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18231 | -0.01307 |    0.07912 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.14022 | -0.00690 |    0.04084 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19921 | -0.00898 |    0.08090 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13444 |  0.00096 |    0.03631 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18108 | -0.00971 |    0.08748 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16568 | -0.00796 |    0.09532 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25513 | -0.01466 |    0.12849 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13150 | -0.00733 |    0.05340 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10773 |  0.00170 |    0.03938 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13525 | -0.01032 |    0.06040 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11064 | -0.00266 |    0.05036 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11728 | -0.00453 |    0.04602 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11985 | -0.00612 |    0.06049 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.14037 | -0.00580 |    0.05107 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11062 | -0.00583 |    0.04637 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08675 |  0.00054 |    0.03911 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06207 | -0.00103 |    0.02026 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04792 | -0.00136 |    0.01194 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53279 | -0.04243 |    0.31894 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:35:48,892 - Total sparsity: 74.94

2018-10-28 00:35:48,892 - --- validate (epoch=69)-----------
2018-10-28 00:35:48,892 - 10000 samples (128 per mini-batch)
2018-10-28 00:35:49,615 - Epoch: [69][   50/   78]    Loss 0.551877    Top1 81.218750    Top5 99.015625    
2018-10-28 00:35:50,017 - ==> Top1: 81.410    Top5: 99.020    Loss: 0.545

2018-10-28 00:35:50,018 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:35:50,018 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:35:50,032 - 

2018-10-28 00:35:50,032 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:35:51,204 - Epoch: [70][   50/  391]    Overall Loss 0.384351    Objective Loss 0.384351    Top1 86.703125    Top5 99.468750    LR 0.300000    Time 0.023398    
2018-10-28 00:35:52,314 - Epoch: [70][  100/  391]    Overall Loss 0.387505    Objective Loss 0.387505    Top1 86.390625    Top5 99.570312    LR 0.300000    Time 0.022791    
2018-10-28 00:35:53,425 - Epoch: [70][  150/  391]    Overall Loss 0.400565    Objective Loss 0.400565    Top1 85.968750    Top5 99.552083    LR 0.300000    Time 0.022590    
2018-10-28 00:35:54,536 - Epoch: [70][  200/  391]    Overall Loss 0.405266    Objective Loss 0.405266    Top1 85.835938    Top5 99.488281    LR 0.300000    Time 0.022489    
2018-10-28 00:35:55,646 - Epoch: [70][  250/  391]    Overall Loss 0.404608    Objective Loss 0.404608    Top1 85.900000    Top5 99.471875    LR 0.300000    Time 0.022427    
2018-10-28 00:35:56,755 - Epoch: [70][  300/  391]    Overall Loss 0.403726    Objective Loss 0.403726    Top1 85.981771    Top5 99.471354    LR 0.300000    Time 0.022369    
2018-10-28 00:35:57,865 - Epoch: [70][  350/  391]    Overall Loss 0.405697    Objective Loss 0.405697    Top1 85.883929    Top5 99.482143    LR 0.300000    Time 0.022342    
2018-10-28 00:35:58,852 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58638 |  0.00288 |    0.30194 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18821 | -0.00532 |    0.06284 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18303 | -0.00360 |    0.06639 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18075 | -0.01386 |    0.07793 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13966 | -0.00725 |    0.04101 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19777 | -0.00787 |    0.07970 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13361 |  0.00182 |    0.03675 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18117 | -0.00782 |    0.08774 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16544 | -0.00851 |    0.09559 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25461 | -0.01405 |    0.12939 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13102 | -0.00738 |    0.05299 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10770 |  0.00221 |    0.03898 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13519 | -0.00985 |    0.06022 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11084 | -0.00206 |    0.05042 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11742 | -0.00453 |    0.04597 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11954 | -0.00623 |    0.06028 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.14028 | -0.00649 |    0.05092 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11030 | -0.00548 |    0.04611 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08668 |  0.00046 |    0.03909 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06206 | -0.00117 |    0.02026 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04789 | -0.00137 |    0.01192 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53496 | -0.04085 |    0.31944 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:35:58,852 - Total sparsity: 74.94

2018-10-28 00:35:58,852 - --- validate (epoch=70)-----------
2018-10-28 00:35:58,852 - 10000 samples (128 per mini-batch)
2018-10-28 00:35:59,575 - Epoch: [70][   50/   78]    Loss 0.789545    Top1 75.796875    Top5 98.125000    
2018-10-28 00:35:59,967 - ==> Top1: 75.750    Top5: 98.230    Loss: 0.789

2018-10-28 00:35:59,967 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:35:59,967 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:35:59,979 - 

2018-10-28 00:35:59,979 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:36:01,150 - Epoch: [71][   50/  391]    Overall Loss 0.374189    Objective Loss 0.374189    Top1 86.968750    Top5 99.531250    LR 0.300000    Time 0.023392    
2018-10-28 00:36:02,262 - Epoch: [71][  100/  391]    Overall Loss 0.391359    Objective Loss 0.391359    Top1 86.570312    Top5 99.437500    LR 0.300000    Time 0.022800    
2018-10-28 00:36:03,374 - Epoch: [71][  150/  391]    Overall Loss 0.401573    Objective Loss 0.401573    Top1 86.322917    Top5 99.390625    LR 0.300000    Time 0.022605    
2018-10-28 00:36:04,486 - Epoch: [71][  200/  391]    Overall Loss 0.398229    Objective Loss 0.398229    Top1 86.390625    Top5 99.453125    LR 0.300000    Time 0.022508    
2018-10-28 00:36:05,598 - Epoch: [71][  250/  391]    Overall Loss 0.402266    Objective Loss 0.402266    Top1 86.168750    Top5 99.440625    LR 0.300000    Time 0.022450    
2018-10-28 00:36:06,710 - Epoch: [71][  300/  391]    Overall Loss 0.399992    Objective Loss 0.399992    Top1 86.270833    Top5 99.447917    LR 0.300000    Time 0.022410    
2018-10-28 00:36:07,823 - Epoch: [71][  350/  391]    Overall Loss 0.402653    Objective Loss 0.402653    Top1 86.189732    Top5 99.437500    LR 0.300000    Time 0.022373    
2018-10-28 00:36:08,812 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58499 |  0.00201 |    0.30094 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18740 | -0.00597 |    0.06276 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18300 | -0.00371 |    0.06667 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18097 | -0.01179 |    0.07762 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13913 | -0.00761 |    0.04115 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19898 | -0.00723 |    0.08101 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13419 |  0.00185 |    0.03675 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18078 | -0.00819 |    0.08784 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16553 | -0.00935 |    0.09532 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25420 | -0.01203 |    0.12932 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13067 | -0.00697 |    0.05290 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10718 |  0.00214 |    0.03900 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13495 | -0.00984 |    0.05988 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11079 | -0.00195 |    0.05046 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11727 | -0.00444 |    0.04612 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11950 | -0.00657 |    0.06031 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13984 | -0.00568 |    0.05096 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11035 | -0.00628 |    0.04621 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08664 |  0.00059 |    0.03895 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06208 | -0.00134 |    0.02033 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04793 | -0.00130 |    0.01191 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53212 | -0.03844 |    0.31654 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:36:08,813 - Total sparsity: 74.94

2018-10-28 00:36:08,813 - --- validate (epoch=71)-----------
2018-10-28 00:36:08,813 - 10000 samples (128 per mini-batch)
2018-10-28 00:36:09,545 - Epoch: [71][   50/   78]    Loss 0.601662    Top1 81.703125    Top5 98.562500    
2018-10-28 00:36:09,936 - ==> Top1: 81.310    Top5: 98.610    Loss: 0.603

2018-10-28 00:36:09,937 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:36:09,937 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:36:09,949 - 

2018-10-28 00:36:09,949 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:36:11,162 - Epoch: [72][   50/  391]    Overall Loss 0.402100    Objective Loss 0.402100    Top1 86.218750    Top5 99.562500    LR 0.300000    Time 0.024219    
2018-10-28 00:36:12,368 - Epoch: [72][  100/  391]    Overall Loss 0.396013    Objective Loss 0.396013    Top1 86.242188    Top5 99.531250    LR 0.300000    Time 0.024157    
2018-10-28 00:36:13,575 - Epoch: [72][  150/  391]    Overall Loss 0.399243    Objective Loss 0.399243    Top1 86.046875    Top5 99.572917    LR 0.300000    Time 0.024138    
2018-10-28 00:36:14,773 - Epoch: [72][  200/  391]    Overall Loss 0.403864    Objective Loss 0.403864    Top1 86.035156    Top5 99.527344    LR 0.300000    Time 0.024086    
2018-10-28 00:36:15,977 - Epoch: [72][  250/  391]    Overall Loss 0.400058    Objective Loss 0.400058    Top1 86.143750    Top5 99.537500    LR 0.300000    Time 0.024082    
2018-10-28 00:36:17,189 - Epoch: [72][  300/  391]    Overall Loss 0.396627    Objective Loss 0.396627    Top1 86.218750    Top5 99.541667    LR 0.300000    Time 0.024104    
2018-10-28 00:36:18,395 - Epoch: [72][  350/  391]    Overall Loss 0.399991    Objective Loss 0.399991    Top1 86.082589    Top5 99.531250    LR 0.300000    Time 0.024101    
2018-10-28 00:36:19,461 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58162 | -0.00386 |    0.29872 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18746 | -0.00577 |    0.06234 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18310 | -0.00350 |    0.06728 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18036 | -0.01195 |    0.07792 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13866 | -0.00725 |    0.04084 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19826 | -0.00916 |    0.08035 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13313 |  0.00215 |    0.03717 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18153 | -0.00829 |    0.08796 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16580 | -0.00947 |    0.09563 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25283 | -0.01272 |    0.12699 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13099 | -0.00743 |    0.05304 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10731 |  0.00195 |    0.03890 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13528 | -0.01005 |    0.06015 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11087 | -0.00172 |    0.05048 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11717 | -0.00429 |    0.04582 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11930 | -0.00676 |    0.06029 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13958 | -0.00500 |    0.05090 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11024 | -0.00545 |    0.04598 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08620 |  0.00057 |    0.03880 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06194 | -0.00115 |    0.02019 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04779 | -0.00139 |    0.01186 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53503 | -0.03804 |    0.31816 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:36:19,462 - Total sparsity: 74.94

2018-10-28 00:36:19,462 - --- validate (epoch=72)-----------
2018-10-28 00:36:19,462 - 10000 samples (128 per mini-batch)
2018-10-28 00:36:20,189 - Epoch: [72][   50/   78]    Loss 0.679569    Top1 79.234375    Top5 98.234375    
2018-10-28 00:36:20,584 - ==> Top1: 79.430    Top5: 98.440    Loss: 0.672

2018-10-28 00:36:20,585 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:36:20,585 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:36:20,596 - 

2018-10-28 00:36:20,596 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:36:21,895 - Epoch: [73][   50/  391]    Overall Loss 0.408714    Objective Loss 0.408714    Top1 85.546875    Top5 99.609375    LR 0.300000    Time 0.025932    
2018-10-28 00:36:23,026 - Epoch: [73][  100/  391]    Overall Loss 0.417779    Objective Loss 0.417779    Top1 85.281250    Top5 99.570312    LR 0.300000    Time 0.024262    
2018-10-28 00:36:24,136 - Epoch: [73][  150/  391]    Overall Loss 0.420408    Objective Loss 0.420408    Top1 85.307292    Top5 99.541667    LR 0.300000    Time 0.023565    
2018-10-28 00:36:25,249 - Epoch: [73][  200/  391]    Overall Loss 0.415631    Objective Loss 0.415631    Top1 85.441406    Top5 99.515625    LR 0.300000    Time 0.023233    
2018-10-28 00:36:26,358 - Epoch: [73][  250/  391]    Overall Loss 0.415068    Objective Loss 0.415068    Top1 85.500000    Top5 99.500000    LR 0.300000    Time 0.023020    
2018-10-28 00:36:27,468 - Epoch: [73][  300/  391]    Overall Loss 0.414127    Objective Loss 0.414127    Top1 85.513021    Top5 99.500000    LR 0.300000    Time 0.022878    
2018-10-28 00:36:28,577 - Epoch: [73][  350/  391]    Overall Loss 0.414235    Objective Loss 0.414235    Top1 85.502232    Top5 99.500000    LR 0.300000    Time 0.022775    
2018-10-28 00:36:29,565 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57984 |  0.00576 |    0.29936 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18724 | -0.00556 |    0.06261 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18266 | -0.00306 |    0.06685 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18031 | -0.01230 |    0.07808 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13922 | -0.00667 |    0.04076 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19823 | -0.01064 |    0.08024 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13313 |  0.00142 |    0.03703 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18238 | -0.00676 |    0.08859 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16628 | -0.00828 |    0.09539 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25294 | -0.01096 |    0.12628 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13123 | -0.00700 |    0.05342 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10793 |  0.00197 |    0.03923 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13560 | -0.01093 |    0.06028 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11075 | -0.00123 |    0.05037 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11756 | -0.00484 |    0.04602 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11949 | -0.00712 |    0.06032 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13917 | -0.00518 |    0.05072 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11024 | -0.00544 |    0.04618 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08644 |  0.00051 |    0.03881 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06217 | -0.00126 |    0.02031 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04800 | -0.00127 |    0.01188 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53399 | -0.03624 |    0.31809 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:36:29,566 - Total sparsity: 74.94

2018-10-28 00:36:29,566 - --- validate (epoch=73)-----------
2018-10-28 00:36:29,566 - 10000 samples (128 per mini-batch)
2018-10-28 00:36:30,291 - Epoch: [73][   50/   78]    Loss 0.616047    Top1 80.437500    Top5 98.718750    
2018-10-28 00:36:30,681 - ==> Top1: 80.680    Top5: 98.790    Loss: 0.610

2018-10-28 00:36:30,681 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:36:30,681 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:36:30,693 - 

2018-10-28 00:36:30,693 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:36:31,863 - Epoch: [74][   50/  391]    Overall Loss 0.368047    Objective Loss 0.368047    Top1 87.187500    Top5 99.468750    LR 0.300000    Time 0.023354    
2018-10-28 00:36:32,975 - Epoch: [74][  100/  391]    Overall Loss 0.391382    Objective Loss 0.391382    Top1 86.546875    Top5 99.445312    LR 0.300000    Time 0.022783    
2018-10-28 00:36:34,085 - Epoch: [74][  150/  391]    Overall Loss 0.390925    Objective Loss 0.390925    Top1 86.583333    Top5 99.447917    LR 0.300000    Time 0.022584    
2018-10-28 00:36:35,196 - Epoch: [74][  200/  391]    Overall Loss 0.394792    Objective Loss 0.394792    Top1 86.371094    Top5 99.472656    LR 0.300000    Time 0.022487    
2018-10-28 00:36:36,307 - Epoch: [74][  250/  391]    Overall Loss 0.400334    Objective Loss 0.400334    Top1 86.225000    Top5 99.471875    LR 0.300000    Time 0.022426    
2018-10-28 00:36:37,419 - Epoch: [74][  300/  391]    Overall Loss 0.403460    Objective Loss 0.403460    Top1 86.093750    Top5 99.476562    LR 0.300000    Time 0.022390    
2018-10-28 00:36:38,532 - Epoch: [74][  350/  391]    Overall Loss 0.408977    Objective Loss 0.408977    Top1 85.886161    Top5 99.482143    LR 0.300000    Time 0.022369    
2018-10-28 00:36:39,522 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58179 |  0.00426 |    0.30296 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18794 | -0.00369 |    0.06215 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18359 | -0.00282 |    0.06716 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18013 | -0.01218 |    0.07788 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13950 | -0.00681 |    0.04138 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19865 | -0.00867 |    0.08018 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13324 |  0.00093 |    0.03734 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18317 | -0.00639 |    0.08820 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16690 | -0.00785 |    0.09614 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25372 | -0.01084 |    0.12826 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13130 | -0.00670 |    0.05309 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10798 |  0.00152 |    0.03916 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13535 | -0.01027 |    0.06027 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11039 | -0.00097 |    0.05003 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11774 | -0.00434 |    0.04597 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11990 | -0.00689 |    0.06050 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13932 | -0.00615 |    0.05086 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11052 | -0.00582 |    0.04622 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08684 |  0.00016 |    0.03900 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06228 | -0.00111 |    0.02028 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04805 | -0.00133 |    0.01192 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.52895 | -0.03732 |    0.31462 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:36:39,522 - Total sparsity: 74.94

2018-10-28 00:36:39,522 - --- validate (epoch=74)-----------
2018-10-28 00:36:39,522 - 10000 samples (128 per mini-batch)
2018-10-28 00:36:40,237 - Epoch: [74][   50/   78]    Loss 0.485887    Top1 83.640625    Top5 99.203125    
2018-10-28 00:36:40,626 - ==> Top1: 83.760    Top5: 99.260    Loss: 0.483

2018-10-28 00:36:40,627 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:36:40,627 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:36:40,636 - 

2018-10-28 00:36:40,637 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:36:41,807 - Epoch: [75][   50/  391]    Overall Loss 0.391210    Objective Loss 0.391210    Top1 86.515625    Top5 99.625000    LR 0.300000    Time 0.023367    
2018-10-28 00:36:42,917 - Epoch: [75][  100/  391]    Overall Loss 0.401043    Objective Loss 0.401043    Top1 86.125000    Top5 99.562500    LR 0.300000    Time 0.022768    
2018-10-28 00:36:44,026 - Epoch: [75][  150/  391]    Overall Loss 0.403373    Objective Loss 0.403373    Top1 85.932292    Top5 99.515625    LR 0.300000    Time 0.022569    
2018-10-28 00:36:45,138 - Epoch: [75][  200/  391]    Overall Loss 0.404942    Objective Loss 0.404942    Top1 85.890625    Top5 99.503906    LR 0.300000    Time 0.022477    
2018-10-28 00:36:46,249 - Epoch: [75][  250/  391]    Overall Loss 0.405121    Objective Loss 0.405121    Top1 85.868750    Top5 99.496875    LR 0.300000    Time 0.022421    
2018-10-28 00:36:47,360 - Epoch: [75][  300/  391]    Overall Loss 0.407266    Objective Loss 0.407266    Top1 85.822917    Top5 99.466146    LR 0.300000    Time 0.022384    
2018-10-28 00:36:48,472 - Epoch: [75][  350/  391]    Overall Loss 0.409221    Objective Loss 0.409221    Top1 85.821429    Top5 99.473214    LR 0.300000    Time 0.022360    
2018-10-28 00:36:49,461 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58179 |  0.00313 |    0.30023 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18663 | -0.00398 |    0.06099 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18406 | -0.00556 |    0.06717 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18226 | -0.01241 |    0.07924 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.14042 | -0.00589 |    0.04167 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19868 | -0.00935 |    0.07994 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13335 |  0.00145 |    0.03657 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18306 | -0.00831 |    0.08835 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16653 | -0.00824 |    0.09594 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25520 | -0.01436 |    0.12924 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13107 | -0.00619 |    0.05289 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10761 |  0.00206 |    0.03909 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13470 | -0.01060 |    0.06023 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10969 | -0.00124 |    0.04984 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11756 | -0.00460 |    0.04597 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.12012 | -0.00688 |    0.06058 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13930 | -0.00547 |    0.05060 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11054 | -0.00559 |    0.04627 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08698 |  0.00079 |    0.03894 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06228 | -0.00109 |    0.02032 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04797 | -0.00129 |    0.01191 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53107 | -0.03564 |    0.31658 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:36:49,461 - Total sparsity: 74.94

2018-10-28 00:36:49,461 - --- validate (epoch=75)-----------
2018-10-28 00:36:49,461 - 10000 samples (128 per mini-batch)
2018-10-28 00:36:50,175 - Epoch: [75][   50/   78]    Loss 0.611148    Top1 80.546875    Top5 98.859375    
2018-10-28 00:36:50,566 - ==> Top1: 80.660    Top5: 98.770    Loss: 0.609

2018-10-28 00:36:50,567 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:36:50,567 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:36:50,579 - 

2018-10-28 00:36:50,579 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:36:51,754 - Epoch: [76][   50/  391]    Overall Loss 0.401097    Objective Loss 0.401097    Top1 85.921875    Top5 99.468750    LR 0.300000    Time 0.023468    
2018-10-28 00:36:52,867 - Epoch: [76][  100/  391]    Overall Loss 0.408164    Objective Loss 0.408164    Top1 85.765625    Top5 99.421875    LR 0.300000    Time 0.022843    
2018-10-28 00:36:53,982 - Epoch: [76][  150/  391]    Overall Loss 0.405294    Objective Loss 0.405294    Top1 86.020833    Top5 99.432292    LR 0.300000    Time 0.022657    
2018-10-28 00:36:55,094 - Epoch: [76][  200/  391]    Overall Loss 0.408286    Objective Loss 0.408286    Top1 85.894531    Top5 99.437500    LR 0.300000    Time 0.022545    
2018-10-28 00:36:56,206 - Epoch: [76][  250/  391]    Overall Loss 0.405887    Objective Loss 0.405887    Top1 85.931250    Top5 99.459375    LR 0.300000    Time 0.022478    
2018-10-28 00:36:57,318 - Epoch: [76][  300/  391]    Overall Loss 0.406252    Objective Loss 0.406252    Top1 85.903646    Top5 99.468750    LR 0.300000    Time 0.022433    
2018-10-28 00:36:58,430 - Epoch: [76][  350/  391]    Overall Loss 0.405821    Objective Loss 0.405821    Top1 85.886161    Top5 99.475446    LR 0.300000    Time 0.022403    
2018-10-28 00:36:59,421 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58593 |  0.01010 |    0.30110 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18712 | -0.00406 |    0.06183 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18434 | -0.00402 |    0.06724 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18086 | -0.01334 |    0.07751 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13879 | -0.00605 |    0.04130 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19812 | -0.00972 |    0.08051 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13380 |  0.00155 |    0.03670 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18291 | -0.00896 |    0.08914 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16643 | -0.00746 |    0.09558 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25481 | -0.01313 |    0.12866 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13103 | -0.00583 |    0.05288 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10786 |  0.00202 |    0.03904 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13468 | -0.01000 |    0.05989 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11000 | -0.00140 |    0.05001 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11737 | -0.00465 |    0.04592 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.12014 | -0.00704 |    0.06063 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13982 | -0.00619 |    0.05131 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11047 | -0.00573 |    0.04616 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08711 |  0.00048 |    0.03916 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06224 | -0.00119 |    0.02017 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04797 | -0.00127 |    0.01188 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53373 | -0.03566 |    0.31777 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:36:59,421 - Total sparsity: 74.94

2018-10-28 00:36:59,422 - --- validate (epoch=76)-----------
2018-10-28 00:36:59,422 - 10000 samples (128 per mini-batch)
2018-10-28 00:37:00,138 - Epoch: [76][   50/   78]    Loss 0.596455    Top1 80.578125    Top5 98.953125    
2018-10-28 00:37:00,518 - ==> Top1: 80.370    Top5: 99.080    Loss: 0.586

2018-10-28 00:37:00,519 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:37:00,519 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:37:00,530 - 

2018-10-28 00:37:00,530 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:37:01,703 - Epoch: [77][   50/  391]    Overall Loss 0.395396    Objective Loss 0.395396    Top1 86.718750    Top5 99.546875    LR 0.300000    Time 0.023417    
2018-10-28 00:37:02,816 - Epoch: [77][  100/  391]    Overall Loss 0.399916    Objective Loss 0.399916    Top1 86.304688    Top5 99.554688    LR 0.300000    Time 0.022825    
2018-10-28 00:37:03,927 - Epoch: [77][  150/  391]    Overall Loss 0.400236    Objective Loss 0.400236    Top1 86.244792    Top5 99.526042    LR 0.300000    Time 0.022620    
2018-10-28 00:37:05,037 - Epoch: [77][  200/  391]    Overall Loss 0.406975    Objective Loss 0.406975    Top1 85.937500    Top5 99.527344    LR 0.300000    Time 0.022508    
2018-10-28 00:37:06,149 - Epoch: [77][  250/  391]    Overall Loss 0.413093    Objective Loss 0.413093    Top1 85.640625    Top5 99.500000    LR 0.300000    Time 0.022449    
2018-10-28 00:37:07,261 - Epoch: [77][  300/  391]    Overall Loss 0.411865    Objective Loss 0.411865    Top1 85.697917    Top5 99.494792    LR 0.300000    Time 0.022407    
2018-10-28 00:37:08,374 - Epoch: [77][  350/  391]    Overall Loss 0.411866    Objective Loss 0.411866    Top1 85.694196    Top5 99.497768    LR 0.300000    Time 0.022384    
2018-10-28 00:37:09,362 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58387 |  0.00330 |    0.30095 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18767 | -0.00522 |    0.06192 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18556 | -0.00282 |    0.06768 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18026 | -0.01270 |    0.07744 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13817 | -0.00609 |    0.04039 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19801 | -0.00881 |    0.07986 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13418 |  0.00148 |    0.03652 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18274 | -0.00766 |    0.08869 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16610 | -0.00838 |    0.09542 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25572 | -0.00765 |    0.12781 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13100 | -0.00701 |    0.05271 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10803 |  0.00116 |    0.03928 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13499 | -0.00932 |    0.05978 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11031 | -0.00145 |    0.05036 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11713 | -0.00497 |    0.04590 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.12013 | -0.00673 |    0.06049 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13975 | -0.00604 |    0.05166 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11076 | -0.00568 |    0.04637 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08731 |  0.00072 |    0.03918 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06233 | -0.00096 |    0.02017 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04817 | -0.00121 |    0.01189 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53642 | -0.03684 |    0.31783 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:37:09,362 - Total sparsity: 74.94

2018-10-28 00:37:09,362 - --- validate (epoch=77)-----------
2018-10-28 00:37:09,363 - 10000 samples (128 per mini-batch)
2018-10-28 00:37:10,087 - Epoch: [77][   50/   78]    Loss 0.540129    Top1 82.781250    Top5 99.140625    
2018-10-28 00:37:10,479 - ==> Top1: 82.510    Top5: 99.180    Loss: 0.535

2018-10-28 00:37:10,480 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:37:10,480 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:37:10,492 - 

2018-10-28 00:37:10,492 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:37:11,663 - Epoch: [78][   50/  391]    Overall Loss 0.400053    Objective Loss 0.400053    Top1 86.031250    Top5 99.484375    LR 0.300000    Time 0.023385    
2018-10-28 00:37:12,774 - Epoch: [78][  100/  391]    Overall Loss 0.395443    Objective Loss 0.395443    Top1 86.320312    Top5 99.562500    LR 0.300000    Time 0.022792    
2018-10-28 00:37:13,887 - Epoch: [78][  150/  391]    Overall Loss 0.404814    Objective Loss 0.404814    Top1 85.958333    Top5 99.473958    LR 0.300000    Time 0.022606    
2018-10-28 00:37:15,001 - Epoch: [78][  200/  391]    Overall Loss 0.407733    Objective Loss 0.407733    Top1 85.882812    Top5 99.472656    LR 0.300000    Time 0.022496    
2018-10-28 00:37:16,114 - Epoch: [78][  250/  391]    Overall Loss 0.404141    Objective Loss 0.404141    Top1 86.046875    Top5 99.481250    LR 0.300000    Time 0.022444    
2018-10-28 00:37:17,226 - Epoch: [78][  300/  391]    Overall Loss 0.405594    Objective Loss 0.405594    Top1 86.039062    Top5 99.486979    LR 0.300000    Time 0.022406    
2018-10-28 00:37:18,335 - Epoch: [78][  350/  391]    Overall Loss 0.408312    Objective Loss 0.408312    Top1 85.852679    Top5 99.475446    LR 0.300000    Time 0.022371    
2018-10-28 00:37:19,326 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58496 | -0.00570 |    0.30143 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18721 | -0.00516 |    0.06179 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18533 | -0.00434 |    0.06757 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18110 | -0.01181 |    0.07783 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13791 | -0.00660 |    0.04026 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19815 | -0.00900 |    0.07998 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13409 |  0.00241 |    0.03683 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18256 | -0.00759 |    0.08796 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16649 | -0.00804 |    0.09591 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25439 | -0.00922 |    0.12697 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13117 | -0.00663 |    0.05268 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10819 |  0.00165 |    0.03921 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13493 | -0.00977 |    0.05989 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11065 | -0.00178 |    0.05038 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11733 | -0.00444 |    0.04578 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11999 | -0.00636 |    0.06038 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13964 | -0.00674 |    0.05138 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11049 | -0.00576 |    0.04621 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08719 |  0.00070 |    0.03913 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06241 | -0.00104 |    0.02020 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04825 | -0.00127 |    0.01194 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53469 | -0.03432 |    0.31617 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:37:19,326 - Total sparsity: 74.94

2018-10-28 00:37:19,326 - --- validate (epoch=78)-----------
2018-10-28 00:37:19,326 - 10000 samples (128 per mini-batch)
2018-10-28 00:37:20,077 - Epoch: [78][   50/   78]    Loss 0.561941    Top1 82.578125    Top5 99.093750    
2018-10-28 00:37:20,471 - ==> Top1: 82.860    Top5: 99.210    Loss: 0.544

2018-10-28 00:37:20,471 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:37:20,472 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:37:20,483 - 

2018-10-28 00:37:20,483 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:37:21,652 - Epoch: [79][   50/  391]    Overall Loss 0.398806    Objective Loss 0.398806    Top1 86.031250    Top5 99.593750    LR 0.300000    Time 0.023356    
2018-10-28 00:37:22,763 - Epoch: [79][  100/  391]    Overall Loss 0.395900    Objective Loss 0.395900    Top1 86.171875    Top5 99.460938    LR 0.300000    Time 0.022775    
2018-10-28 00:37:23,874 - Epoch: [79][  150/  391]    Overall Loss 0.397347    Objective Loss 0.397347    Top1 86.192708    Top5 99.437500    LR 0.300000    Time 0.022577    
2018-10-28 00:37:24,990 - Epoch: [79][  200/  391]    Overall Loss 0.395812    Objective Loss 0.395812    Top1 86.308594    Top5 99.460938    LR 0.300000    Time 0.022507    
2018-10-28 00:37:26,111 - Epoch: [79][  250/  391]    Overall Loss 0.399522    Objective Loss 0.399522    Top1 86.087500    Top5 99.475000    LR 0.300000    Time 0.022483    
2018-10-28 00:37:27,225 - Epoch: [79][  300/  391]    Overall Loss 0.406656    Objective Loss 0.406656    Top1 85.890625    Top5 99.455729    LR 0.300000    Time 0.022447    
2018-10-28 00:37:28,340 - Epoch: [79][  350/  391]    Overall Loss 0.407879    Objective Loss 0.407879    Top1 85.879464    Top5 99.444196    LR 0.300000    Time 0.022421    
2018-10-28 00:37:29,329 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58243 | -0.00078 |    0.29974 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18746 | -0.00539 |    0.06166 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18526 | -0.00386 |    0.06727 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18095 | -0.01262 |    0.07740 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13691 | -0.00637 |    0.03998 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19673 | -0.01067 |    0.07909 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13331 |  0.00212 |    0.03647 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18151 | -0.00742 |    0.08781 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16617 | -0.00770 |    0.09584 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25369 | -0.00553 |    0.12661 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13145 | -0.00655 |    0.05281 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10819 |  0.00161 |    0.03941 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13528 | -0.01042 |    0.06026 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11112 | -0.00131 |    0.05059 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11729 | -0.00441 |    0.04581 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11985 | -0.00666 |    0.06025 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13899 | -0.00512 |    0.05082 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11032 | -0.00582 |    0.04600 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08693 |  0.00051 |    0.03900 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06231 | -0.00117 |    0.02026 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04822 | -0.00115 |    0.01191 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53366 | -0.03525 |    0.31649 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:37:29,329 - Total sparsity: 74.94

2018-10-28 00:37:29,329 - --- validate (epoch=79)-----------
2018-10-28 00:37:29,329 - 10000 samples (128 per mini-batch)
2018-10-28 00:37:30,055 - Epoch: [79][   50/   78]    Loss 0.649033    Top1 79.046875    Top5 99.000000    
2018-10-28 00:37:30,450 - ==> Top1: 79.120    Top5: 98.970    Loss: 0.642

2018-10-28 00:37:30,451 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:37:30,451 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:37:30,460 - 

2018-10-28 00:37:30,460 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:37:31,627 - Epoch: [80][   50/  391]    Overall Loss 0.395077    Objective Loss 0.395077    Top1 86.187500    Top5 99.562500    LR 0.300000    Time 0.023307    
2018-10-28 00:37:32,739 - Epoch: [80][  100/  391]    Overall Loss 0.396824    Objective Loss 0.396824    Top1 86.062500    Top5 99.507812    LR 0.300000    Time 0.022754    
2018-10-28 00:37:33,849 - Epoch: [80][  150/  391]    Overall Loss 0.408391    Objective Loss 0.408391    Top1 85.687500    Top5 99.526042    LR 0.300000    Time 0.022567    
2018-10-28 00:37:34,959 - Epoch: [80][  200/  391]    Overall Loss 0.406141    Objective Loss 0.406141    Top1 85.753906    Top5 99.535156    LR 0.300000    Time 0.022468    
2018-10-28 00:37:36,067 - Epoch: [80][  250/  391]    Overall Loss 0.408622    Objective Loss 0.408622    Top1 85.668750    Top5 99.509375    LR 0.300000    Time 0.022400    
2018-10-28 00:37:37,178 - Epoch: [80][  300/  391]    Overall Loss 0.408064    Objective Loss 0.408064    Top1 85.619792    Top5 99.528646    LR 0.300000    Time 0.022366    
2018-10-28 00:37:38,290 - Epoch: [80][  350/  391]    Overall Loss 0.410270    Objective Loss 0.410270    Top1 85.555804    Top5 99.513393    LR 0.300000    Time 0.022345    
2018-10-28 00:37:39,280 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58304 |  0.00061 |    0.30074 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18814 | -0.00553 |    0.06178 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18707 | -0.00460 |    0.06771 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18071 | -0.01358 |    0.07771 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13699 | -0.00552 |    0.04032 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19751 | -0.01049 |    0.07935 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13391 |  0.00222 |    0.03629 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18199 | -0.00813 |    0.08827 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16646 | -0.00778 |    0.09606 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25351 | -0.00674 |    0.12670 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13150 | -0.00624 |    0.05291 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10821 |  0.00131 |    0.03930 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13545 | -0.01029 |    0.06032 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11129 | -0.00140 |    0.05083 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11755 | -0.00473 |    0.04600 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11995 | -0.00685 |    0.06039 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13921 | -0.00542 |    0.05083 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11054 | -0.00556 |    0.04607 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08717 |  0.00061 |    0.03915 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06259 | -0.00106 |    0.02027 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04840 | -0.00122 |    0.01193 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53720 | -0.03575 |    0.31804 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:37:39,280 - Total sparsity: 74.94

2018-10-28 00:37:39,281 - --- validate (epoch=80)-----------
2018-10-28 00:37:39,281 - 10000 samples (128 per mini-batch)
2018-10-28 00:37:40,007 - Epoch: [80][   50/   78]    Loss 0.485926    Top1 83.968750    Top5 99.171875    
2018-10-28 00:37:40,399 - ==> Top1: 83.620    Top5: 99.230    Loss: 0.491

2018-10-28 00:37:40,400 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:37:40,400 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:37:40,417 - 

2018-10-28 00:37:40,418 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:37:41,561 - Epoch: [81][   50/  391]    Overall Loss 0.395948    Objective Loss 0.395948    Top1 86.343750    Top5 99.609375    LR 0.300000    Time 0.022836    
2018-10-28 00:37:42,672 - Epoch: [81][  100/  391]    Overall Loss 0.391937    Objective Loss 0.391937    Top1 86.453125    Top5 99.554688    LR 0.300000    Time 0.022517    
2018-10-28 00:37:43,784 - Epoch: [81][  150/  391]    Overall Loss 0.392841    Objective Loss 0.392841    Top1 86.442708    Top5 99.546875    LR 0.300000    Time 0.022416    
2018-10-28 00:37:44,894 - Epoch: [81][  200/  391]    Overall Loss 0.392949    Objective Loss 0.392949    Top1 86.378906    Top5 99.542969    LR 0.300000    Time 0.022356    
2018-10-28 00:37:46,007 - Epoch: [81][  250/  391]    Overall Loss 0.398808    Objective Loss 0.398808    Top1 86.146875    Top5 99.503125    LR 0.300000    Time 0.022328    
2018-10-28 00:37:47,118 - Epoch: [81][  300/  391]    Overall Loss 0.401669    Objective Loss 0.401669    Top1 85.989583    Top5 99.471354    LR 0.300000    Time 0.022307    
2018-10-28 00:37:48,230 - Epoch: [81][  350/  391]    Overall Loss 0.406347    Objective Loss 0.406347    Top1 85.787946    Top5 99.470982    LR 0.300000    Time 0.022294    
2018-10-28 00:37:49,221 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58205 |  0.00190 |    0.29778 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18805 | -0.00539 |    0.06186 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18688 | -0.00420 |    0.06787 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18280 | -0.01265 |    0.07880 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13748 | -0.00747 |    0.04024 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19723 | -0.00729 |    0.07933 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13408 |  0.00189 |    0.03628 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18287 | -0.00719 |    0.08857 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16677 | -0.00889 |    0.09587 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25187 | -0.00884 |    0.12478 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13184 | -0.00619 |    0.05303 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10839 |  0.00150 |    0.03927 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13564 | -0.01053 |    0.06014 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11160 | -0.00078 |    0.05051 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11759 | -0.00441 |    0.04587 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.12014 | -0.00677 |    0.06054 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.14021 | -0.00533 |    0.05137 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11072 | -0.00575 |    0.04606 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08719 |  0.00049 |    0.03920 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06273 | -0.00083 |    0.02029 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04843 | -0.00124 |    0.01189 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53846 | -0.03774 |    0.31820 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:37:49,221 - Total sparsity: 74.94

2018-10-28 00:37:49,222 - --- validate (epoch=81)-----------
2018-10-28 00:37:49,222 - 10000 samples (128 per mini-batch)
2018-10-28 00:37:49,942 - Epoch: [81][   50/   78]    Loss 0.597385    Top1 80.890625    Top5 99.015625    
2018-10-28 00:37:50,333 - ==> Top1: 80.750    Top5: 99.100    Loss: 0.598

2018-10-28 00:37:50,334 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:37:50,334 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:37:50,344 - 

2018-10-28 00:37:50,344 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:37:51,515 - Epoch: [82][   50/  391]    Overall Loss 0.386854    Objective Loss 0.386854    Top1 86.296875    Top5 99.593750    LR 0.300000    Time 0.023379    
2018-10-28 00:37:52,626 - Epoch: [82][  100/  391]    Overall Loss 0.405752    Objective Loss 0.405752    Top1 85.781250    Top5 99.570312    LR 0.300000    Time 0.022786    
2018-10-28 00:37:53,738 - Epoch: [82][  150/  391]    Overall Loss 0.404557    Objective Loss 0.404557    Top1 86.020833    Top5 99.505208    LR 0.300000    Time 0.022599    
2018-10-28 00:37:54,850 - Epoch: [82][  200/  391]    Overall Loss 0.409577    Objective Loss 0.409577    Top1 85.984375    Top5 99.496094    LR 0.300000    Time 0.022502    
2018-10-28 00:37:55,962 - Epoch: [82][  250/  391]    Overall Loss 0.407173    Objective Loss 0.407173    Top1 86.009375    Top5 99.531250    LR 0.300000    Time 0.022446    
2018-10-28 00:37:57,075 - Epoch: [82][  300/  391]    Overall Loss 0.408115    Objective Loss 0.408115    Top1 85.903646    Top5 99.492188    LR 0.300000    Time 0.022408    
2018-10-28 00:37:58,185 - Epoch: [82][  350/  391]    Overall Loss 0.411114    Objective Loss 0.411114    Top1 85.863839    Top5 99.488839    LR 0.300000    Time 0.022376    
2018-10-28 00:37:59,176 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57946 | -0.00110 |    0.29695 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18878 | -0.00571 |    0.06166 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18669 | -0.00456 |    0.06764 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18350 | -0.01182 |    0.07834 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13823 | -0.00686 |    0.04045 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19708 | -0.00855 |    0.07947 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13372 |  0.00136 |    0.03596 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18306 | -0.00732 |    0.08885 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16658 | -0.00828 |    0.09576 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25161 | -0.00525 |    0.12482 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13218 | -0.00604 |    0.05340 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10844 |  0.00146 |    0.03930 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13588 | -0.01046 |    0.06024 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11123 | -0.00100 |    0.05046 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11751 | -0.00454 |    0.04586 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.12037 | -0.00657 |    0.06059 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13970 | -0.00679 |    0.05091 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11065 | -0.00589 |    0.04614 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08709 |  0.00096 |    0.03921 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06261 | -0.00116 |    0.02019 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04827 | -0.00119 |    0.01188 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53634 | -0.03869 |    0.31838 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:37:59,176 - Total sparsity: 74.94

2018-10-28 00:37:59,176 - --- validate (epoch=82)-----------
2018-10-28 00:37:59,177 - 10000 samples (128 per mini-batch)
2018-10-28 00:37:59,930 - Epoch: [82][   50/   78]    Loss 0.696232    Top1 79.281250    Top5 98.453125    
2018-10-28 00:38:00,320 - ==> Top1: 79.150    Top5: 98.630    Loss: 0.694

2018-10-28 00:38:00,321 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:38:00,321 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:38:00,332 - 

2018-10-28 00:38:00,333 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:38:01,502 - Epoch: [83][   50/  391]    Overall Loss 0.389706    Objective Loss 0.389706    Top1 86.437500    Top5 99.453125    LR 0.300000    Time 0.023354    
2018-10-28 00:38:02,613 - Epoch: [83][  100/  391]    Overall Loss 0.406552    Objective Loss 0.406552    Top1 85.804688    Top5 99.375000    LR 0.300000    Time 0.022769    
2018-10-28 00:38:03,722 - Epoch: [83][  150/  391]    Overall Loss 0.400813    Objective Loss 0.400813    Top1 85.963542    Top5 99.427083    LR 0.300000    Time 0.022564    
2018-10-28 00:38:04,834 - Epoch: [83][  200/  391]    Overall Loss 0.399983    Objective Loss 0.399983    Top1 86.031250    Top5 99.476562    LR 0.300000    Time 0.022459    
2018-10-28 00:38:05,946 - Epoch: [83][  250/  391]    Overall Loss 0.400557    Objective Loss 0.400557    Top1 86.021875    Top5 99.487500    LR 0.300000    Time 0.022412    
2018-10-28 00:38:07,059 - Epoch: [83][  300/  391]    Overall Loss 0.401064    Objective Loss 0.401064    Top1 86.044271    Top5 99.479167    LR 0.300000    Time 0.022381    
2018-10-28 00:38:08,172 - Epoch: [83][  350/  391]    Overall Loss 0.404646    Objective Loss 0.404646    Top1 85.935268    Top5 99.484375    LR 0.300000    Time 0.022360    
2018-10-28 00:38:09,163 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57572 | -0.00359 |    0.30026 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18731 | -0.00503 |    0.06152 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18587 | -0.00309 |    0.06687 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18144 | -0.01226 |    0.07853 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13857 | -0.00444 |    0.04069 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19750 | -0.00995 |    0.07971 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13463 |  0.00131 |    0.03625 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18305 | -0.00790 |    0.08811 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16653 | -0.00790 |    0.09592 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25177 | -0.00738 |    0.12549 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13160 | -0.00666 |    0.05311 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10869 |  0.00145 |    0.03939 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13562 | -0.01093 |    0.05990 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11089 | -0.00176 |    0.05029 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11770 | -0.00424 |    0.04592 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.12019 | -0.00701 |    0.06042 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13950 | -0.00568 |    0.05125 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11049 | -0.00544 |    0.04602 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08693 |  0.00064 |    0.03900 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06268 | -0.00105 |    0.02018 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04811 | -0.00115 |    0.01188 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53167 | -0.03804 |    0.31570 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:38:09,163 - Total sparsity: 74.94

2018-10-28 00:38:09,163 - --- validate (epoch=83)-----------
2018-10-28 00:38:09,163 - 10000 samples (128 per mini-batch)
2018-10-28 00:38:09,881 - Epoch: [83][   50/   78]    Loss 0.571025    Top1 81.937500    Top5 98.937500    
2018-10-28 00:38:10,265 - ==> Top1: 81.990    Top5: 99.030    Loss: 0.572

2018-10-28 00:38:10,266 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:38:10,266 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:38:10,282 - 

2018-10-28 00:38:10,282 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:38:11,452 - Epoch: [84][   50/  391]    Overall Loss 0.390991    Objective Loss 0.390991    Top1 85.984375    Top5 99.609375    LR 0.300000    Time 0.023358    
2018-10-28 00:38:12,561 - Epoch: [84][  100/  391]    Overall Loss 0.404728    Objective Loss 0.404728    Top1 85.773438    Top5 99.531250    LR 0.300000    Time 0.022751    
2018-10-28 00:38:13,673 - Epoch: [84][  150/  391]    Overall Loss 0.409438    Objective Loss 0.409438    Top1 85.640625    Top5 99.520833    LR 0.300000    Time 0.022574    
2018-10-28 00:38:14,788 - Epoch: [84][  200/  391]    Overall Loss 0.404971    Objective Loss 0.404971    Top1 85.808594    Top5 99.488281    LR 0.300000    Time 0.022481    
2018-10-28 00:38:15,898 - Epoch: [84][  250/  391]    Overall Loss 0.402690    Objective Loss 0.402690    Top1 85.990625    Top5 99.484375    LR 0.300000    Time 0.022422    
2018-10-28 00:38:17,011 - Epoch: [84][  300/  391]    Overall Loss 0.405851    Objective Loss 0.405851    Top1 85.867188    Top5 99.481771    LR 0.300000    Time 0.022388    
2018-10-28 00:38:18,123 - Epoch: [84][  350/  391]    Overall Loss 0.408815    Objective Loss 0.408815    Top1 85.770089    Top5 99.495536    LR 0.300000    Time 0.022363    
2018-10-28 00:38:19,115 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57376 |  0.00461 |    0.29656 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18812 | -0.00626 |    0.06189 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18508 | -0.00543 |    0.06619 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18014 | -0.01390 |    0.07810 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13879 | -0.00578 |    0.04114 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19722 | -0.00876 |    0.07982 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13541 |  0.00166 |    0.03692 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18314 | -0.00718 |    0.08825 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16649 | -0.00764 |    0.09602 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25327 | -0.00708 |    0.12534 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13130 | -0.00679 |    0.05325 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10821 |  0.00223 |    0.03906 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13550 | -0.01120 |    0.06001 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11061 | -0.00148 |    0.05016 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11765 | -0.00456 |    0.04585 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.12024 | -0.00649 |    0.06042 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.14000 | -0.00572 |    0.05118 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11057 | -0.00519 |    0.04599 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08709 |  0.00052 |    0.03912 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06273 | -0.00110 |    0.02026 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04826 | -0.00125 |    0.01184 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53415 | -0.03589 |    0.31547 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:38:19,115 - Total sparsity: 74.94

2018-10-28 00:38:19,115 - --- validate (epoch=84)-----------
2018-10-28 00:38:19,115 - 10000 samples (128 per mini-batch)
2018-10-28 00:38:19,839 - Epoch: [84][   50/   78]    Loss 0.579669    Top1 81.906250    Top5 98.812500    
2018-10-28 00:38:20,220 - ==> Top1: 81.890    Top5: 98.740    Loss: 0.577

2018-10-28 00:38:20,221 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:38:20,221 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:38:20,234 - 

2018-10-28 00:38:20,234 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:38:21,406 - Epoch: [85][   50/  391]    Overall Loss 0.382044    Objective Loss 0.382044    Top1 86.531250    Top5 99.578125    LR 0.300000    Time 0.023398    
2018-10-28 00:38:22,517 - Epoch: [85][  100/  391]    Overall Loss 0.398138    Objective Loss 0.398138    Top1 85.984375    Top5 99.546875    LR 0.300000    Time 0.022794    
2018-10-28 00:38:23,628 - Epoch: [85][  150/  391]    Overall Loss 0.399557    Objective Loss 0.399557    Top1 85.875000    Top5 99.578125    LR 0.300000    Time 0.022592    
2018-10-28 00:38:24,739 - Epoch: [85][  200/  391]    Overall Loss 0.397702    Objective Loss 0.397702    Top1 85.972656    Top5 99.578125    LR 0.300000    Time 0.022496    
2018-10-28 00:38:25,852 - Epoch: [85][  250/  391]    Overall Loss 0.396576    Objective Loss 0.396576    Top1 86.040625    Top5 99.568750    LR 0.300000    Time 0.022440    
2018-10-28 00:38:26,961 - Epoch: [85][  300/  391]    Overall Loss 0.397836    Objective Loss 0.397836    Top1 85.994792    Top5 99.562500    LR 0.300000    Time 0.022394    
2018-10-28 00:38:28,071 - Epoch: [85][  350/  391]    Overall Loss 0.399449    Objective Loss 0.399449    Top1 86.008929    Top5 99.520089    LR 0.300000    Time 0.022363    
2018-10-28 00:38:29,060 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57617 |  0.00403 |    0.29922 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18736 | -0.00686 |    0.06170 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18549 | -0.00293 |    0.06715 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18036 | -0.01248 |    0.07794 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13874 | -0.00557 |    0.04027 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19844 | -0.00753 |    0.08015 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13592 |  0.00228 |    0.03782 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18305 | -0.00801 |    0.08735 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16660 | -0.00811 |    0.09590 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25458 | -0.00918 |    0.12571 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13120 | -0.00705 |    0.05292 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10788 |  0.00185 |    0.03917 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13593 | -0.01056 |    0.06007 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11029 | -0.00172 |    0.05001 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11766 | -0.00445 |    0.04571 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.12013 | -0.00649 |    0.06031 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13935 | -0.00571 |    0.05122 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11052 | -0.00547 |    0.04603 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08707 |  0.00050 |    0.03910 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06259 | -0.00087 |    0.02021 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04806 | -0.00121 |    0.01179 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53280 | -0.03621 |    0.31277 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:38:29,061 - Total sparsity: 74.94

2018-10-28 00:38:29,061 - --- validate (epoch=85)-----------
2018-10-28 00:38:29,061 - 10000 samples (128 per mini-batch)
2018-10-28 00:38:29,779 - Epoch: [85][   50/   78]    Loss 0.828590    Top1 76.281250    Top5 99.015625    
2018-10-28 00:38:30,167 - ==> Top1: 76.100    Top5: 98.950    Loss: 0.840

2018-10-28 00:38:30,168 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:38:30,168 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:38:30,186 - 

2018-10-28 00:38:30,186 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:38:31,333 - Epoch: [86][   50/  391]    Overall Loss 0.408791    Objective Loss 0.408791    Top1 85.859375    Top5 99.453125    LR 0.300000    Time 0.022904    
2018-10-28 00:38:32,444 - Epoch: [86][  100/  391]    Overall Loss 0.409687    Objective Loss 0.409687    Top1 85.781250    Top5 99.492188    LR 0.300000    Time 0.022552    
2018-10-28 00:38:33,555 - Epoch: [86][  150/  391]    Overall Loss 0.411369    Objective Loss 0.411369    Top1 85.692708    Top5 99.484375    LR 0.300000    Time 0.022429    
2018-10-28 00:38:34,666 - Epoch: [86][  200/  391]    Overall Loss 0.406724    Objective Loss 0.406724    Top1 85.808594    Top5 99.460938    LR 0.300000    Time 0.022373    
2018-10-28 00:38:35,777 - Epoch: [86][  250/  391]    Overall Loss 0.406907    Objective Loss 0.406907    Top1 85.800000    Top5 99.450000    LR 0.300000    Time 0.022337    
2018-10-28 00:38:36,888 - Epoch: [86][  300/  391]    Overall Loss 0.406939    Objective Loss 0.406939    Top1 85.820312    Top5 99.455729    LR 0.300000    Time 0.022303    
2018-10-28 00:38:38,000 - Epoch: [86][  350/  391]    Overall Loss 0.404374    Objective Loss 0.404374    Top1 85.939732    Top5 99.462054    LR 0.300000    Time 0.022290    
2018-10-28 00:38:38,992 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58080 | -0.00267 |    0.29843 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18852 | -0.00652 |    0.06139 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18681 | -0.00574 |    0.06753 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18069 | -0.01370 |    0.07829 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13798 | -0.00721 |    0.04037 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19939 | -0.00839 |    0.08031 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13572 |  0.00203 |    0.03710 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18335 | -0.00785 |    0.08800 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16644 | -0.00863 |    0.09592 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25567 | -0.00842 |    0.12675 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13154 | -0.00702 |    0.05308 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10819 |  0.00201 |    0.03941 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13588 | -0.01067 |    0.05983 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11034 | -0.00091 |    0.04985 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11784 | -0.00432 |    0.04576 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.12026 | -0.00658 |    0.06058 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13893 | -0.00481 |    0.05089 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11058 | -0.00563 |    0.04600 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08715 |  0.00048 |    0.03904 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06259 | -0.00106 |    0.02021 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04801 | -0.00127 |    0.01178 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53252 | -0.03681 |    0.31384 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:38:38,992 - Total sparsity: 74.94

2018-10-28 00:38:38,992 - --- validate (epoch=86)-----------
2018-10-28 00:38:38,992 - 10000 samples (128 per mini-batch)
2018-10-28 00:38:39,711 - Epoch: [86][   50/   78]    Loss 0.523115    Top1 83.125000    Top5 98.890625    
2018-10-28 00:38:40,102 - ==> Top1: 83.000    Top5: 98.910    Loss: 0.522

2018-10-28 00:38:40,102 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:38:40,102 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:38:40,120 - 

2018-10-28 00:38:40,120 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:38:41,266 - Epoch: [87][   50/  391]    Overall Loss 0.380659    Objective Loss 0.380659    Top1 87.406250    Top5 99.406250    LR 0.300000    Time 0.022881    
2018-10-28 00:38:42,379 - Epoch: [87][  100/  391]    Overall Loss 0.395164    Objective Loss 0.395164    Top1 86.406250    Top5 99.437500    LR 0.300000    Time 0.022559    
2018-10-28 00:38:43,492 - Epoch: [87][  150/  391]    Overall Loss 0.391306    Objective Loss 0.391306    Top1 86.588542    Top5 99.468750    LR 0.300000    Time 0.022454    
2018-10-28 00:38:44,604 - Epoch: [87][  200/  391]    Overall Loss 0.398273    Objective Loss 0.398273    Top1 86.257812    Top5 99.468750    LR 0.300000    Time 0.022395    
2018-10-28 00:38:45,718 - Epoch: [87][  250/  391]    Overall Loss 0.398845    Objective Loss 0.398845    Top1 86.246875    Top5 99.478125    LR 0.300000    Time 0.022364    
2018-10-28 00:38:46,831 - Epoch: [87][  300/  391]    Overall Loss 0.399296    Objective Loss 0.399296    Top1 86.221354    Top5 99.502604    LR 0.300000    Time 0.022344    
2018-10-28 00:38:47,944 - Epoch: [87][  350/  391]    Overall Loss 0.399313    Objective Loss 0.399313    Top1 86.241071    Top5 99.506696    LR 0.300000    Time 0.022328    
2018-10-28 00:38:48,938 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57712 |  0.00152 |    0.29883 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18762 | -0.00581 |    0.06100 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18593 | -0.00319 |    0.06701 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18061 | -0.01270 |    0.07763 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13836 | -0.00673 |    0.04052 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19832 | -0.00879 |    0.08063 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13618 |  0.00256 |    0.03783 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18333 | -0.00768 |    0.08759 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16641 | -0.00903 |    0.09590 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25664 | -0.00923 |    0.12728 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13132 | -0.00722 |    0.05314 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10790 |  0.00131 |    0.03920 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13528 | -0.01049 |    0.05948 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10986 | -0.00192 |    0.04953 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11788 | -0.00464 |    0.04584 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.12044 | -0.00649 |    0.06056 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13940 | -0.00482 |    0.05042 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11068 | -0.00575 |    0.04618 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08738 |  0.00036 |    0.03924 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06273 | -0.00091 |    0.02030 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04819 | -0.00133 |    0.01182 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53572 | -0.03609 |    0.31751 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:38:48,938 - Total sparsity: 74.94

2018-10-28 00:38:48,938 - --- validate (epoch=87)-----------
2018-10-28 00:38:48,938 - 10000 samples (128 per mini-batch)
2018-10-28 00:38:49,668 - Epoch: [87][   50/   78]    Loss 0.591996    Top1 81.750000    Top5 99.093750    
2018-10-28 00:38:50,065 - ==> Top1: 81.140    Top5: 99.100    Loss: 0.605

2018-10-28 00:38:50,066 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:38:50,066 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:38:50,087 - 

2018-10-28 00:38:50,087 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:38:51,231 - Epoch: [88][   50/  391]    Overall Loss 0.398206    Objective Loss 0.398206    Top1 86.203125    Top5 99.578125    LR 0.300000    Time 0.022845    
2018-10-28 00:38:52,346 - Epoch: [88][  100/  391]    Overall Loss 0.396989    Objective Loss 0.396989    Top1 86.070312    Top5 99.585938    LR 0.300000    Time 0.022551    
2018-10-28 00:38:53,459 - Epoch: [88][  150/  391]    Overall Loss 0.393745    Objective Loss 0.393745    Top1 86.255208    Top5 99.531250    LR 0.300000    Time 0.022447    
2018-10-28 00:38:54,572 - Epoch: [88][  200/  391]    Overall Loss 0.394461    Objective Loss 0.394461    Top1 86.253906    Top5 99.531250    LR 0.300000    Time 0.022395    
2018-10-28 00:38:55,684 - Epoch: [88][  250/  391]    Overall Loss 0.406475    Objective Loss 0.406475    Top1 85.903125    Top5 99.465625    LR 0.300000    Time 0.022357    
2018-10-28 00:38:56,795 - Epoch: [88][  300/  391]    Overall Loss 0.408533    Objective Loss 0.408533    Top1 85.807292    Top5 99.440104    LR 0.300000    Time 0.022331    
2018-10-28 00:38:57,906 - Epoch: [88][  350/  391]    Overall Loss 0.410012    Objective Loss 0.410012    Top1 85.781250    Top5 99.419643    LR 0.300000    Time 0.022312    
2018-10-28 00:38:58,900 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57612 |  0.00073 |    0.29952 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18760 | -0.00720 |    0.06110 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18578 | -0.00511 |    0.06741 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18087 | -0.01312 |    0.07781 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13842 | -0.00638 |    0.04033 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19878 | -0.00644 |    0.08128 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13618 |  0.00230 |    0.03744 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18366 | -0.00658 |    0.08823 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16665 | -0.00929 |    0.09588 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25679 | -0.00897 |    0.12815 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13110 | -0.00704 |    0.05301 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10790 |  0.00136 |    0.03891 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13537 | -0.01013 |    0.05979 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10985 | -0.00187 |    0.04928 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11812 | -0.00455 |    0.04613 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.12058 | -0.00724 |    0.06081 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13925 | -0.00329 |    0.05045 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11074 | -0.00579 |    0.04618 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08755 |  0.00038 |    0.03924 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06291 | -0.00099 |    0.02030 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04831 | -0.00128 |    0.01188 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53809 | -0.03701 |    0.31852 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:38:58,900 - Total sparsity: 74.94

2018-10-28 00:38:58,900 - --- validate (epoch=88)-----------
2018-10-28 00:38:58,900 - 10000 samples (128 per mini-batch)
2018-10-28 00:38:59,618 - Epoch: [88][   50/   78]    Loss 0.687683    Top1 79.421875    Top5 98.609375    
2018-10-28 00:39:00,007 - ==> Top1: 79.340    Top5: 98.800    Loss: 0.685

2018-10-28 00:39:00,007 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:39:00,008 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:39:00,019 - 

2018-10-28 00:39:00,019 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:39:01,191 - Epoch: [89][   50/  391]    Overall Loss 0.392088    Objective Loss 0.392088    Top1 86.109375    Top5 99.531250    LR 0.300000    Time 0.023405    
2018-10-28 00:39:02,302 - Epoch: [89][  100/  391]    Overall Loss 0.390588    Objective Loss 0.390588    Top1 86.351562    Top5 99.539062    LR 0.300000    Time 0.022795    
2018-10-28 00:39:03,413 - Epoch: [89][  150/  391]    Overall Loss 0.395484    Objective Loss 0.395484    Top1 86.322917    Top5 99.520833    LR 0.300000    Time 0.022597    
2018-10-28 00:39:04,524 - Epoch: [89][  200/  391]    Overall Loss 0.396783    Objective Loss 0.396783    Top1 86.246094    Top5 99.476562    LR 0.300000    Time 0.022495    
2018-10-28 00:39:05,635 - Epoch: [89][  250/  391]    Overall Loss 0.402122    Objective Loss 0.402122    Top1 86.053125    Top5 99.468750    LR 0.300000    Time 0.022436    
2018-10-28 00:39:06,746 - Epoch: [89][  300/  391]    Overall Loss 0.403958    Objective Loss 0.403958    Top1 85.960938    Top5 99.486979    LR 0.300000    Time 0.022394    
2018-10-28 00:39:07,854 - Epoch: [89][  350/  391]    Overall Loss 0.406170    Objective Loss 0.406170    Top1 85.892857    Top5 99.488839    LR 0.300000    Time 0.022359    
2018-10-28 00:39:08,844 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58016 | -0.00167 |    0.29974 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18824 | -0.00613 |    0.06107 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18642 | -0.00196 |    0.06692 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18062 | -0.01401 |    0.07806 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13781 | -0.00674 |    0.04047 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19990 | -0.00782 |    0.08040 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13644 |  0.00231 |    0.03739 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18352 | -0.00724 |    0.08810 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16651 | -0.00907 |    0.09552 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25601 | -0.01217 |    0.12847 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13130 | -0.00716 |    0.05296 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10799 |  0.00170 |    0.03910 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13532 | -0.00988 |    0.05967 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10990 | -0.00182 |    0.04937 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11773 | -0.00461 |    0.04600 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.12033 | -0.00711 |    0.06071 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13870 | -0.00423 |    0.05005 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11052 | -0.00587 |    0.04606 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08732 |  0.00029 |    0.03920 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06271 | -0.00101 |    0.02026 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04824 | -0.00131 |    0.01191 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53680 | -0.03640 |    0.31749 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:39:08,844 - Total sparsity: 74.94

2018-10-28 00:39:08,844 - --- validate (epoch=89)-----------
2018-10-28 00:39:08,844 - 10000 samples (128 per mini-batch)
2018-10-28 00:39:09,563 - Epoch: [89][   50/   78]    Loss 0.671483    Top1 79.875000    Top5 98.703125    
2018-10-28 00:39:09,955 - ==> Top1: 79.570    Top5: 98.740    Loss: 0.675

2018-10-28 00:39:09,955 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:39:09,956 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:39:09,967 - 

2018-10-28 00:39:09,968 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:39:11,139 - Epoch: [90][   50/  391]    Overall Loss 0.381112    Objective Loss 0.381112    Top1 87.093750    Top5 99.453125    LR 0.300000    Time 0.023385    
2018-10-28 00:39:12,248 - Epoch: [90][  100/  391]    Overall Loss 0.390282    Objective Loss 0.390282    Top1 86.492188    Top5 99.500000    LR 0.300000    Time 0.022777    
2018-10-28 00:39:13,360 - Epoch: [90][  150/  391]    Overall Loss 0.395175    Objective Loss 0.395175    Top1 86.343750    Top5 99.505208    LR 0.300000    Time 0.022583    
2018-10-28 00:39:14,471 - Epoch: [90][  200/  391]    Overall Loss 0.395247    Objective Loss 0.395247    Top1 86.324219    Top5 99.535156    LR 0.300000    Time 0.022490    
2018-10-28 00:39:15,582 - Epoch: [90][  250/  391]    Overall Loss 0.399133    Objective Loss 0.399133    Top1 86.165625    Top5 99.506250    LR 0.300000    Time 0.022430    
2018-10-28 00:39:16,694 - Epoch: [90][  300/  391]    Overall Loss 0.401201    Objective Loss 0.401201    Top1 86.127604    Top5 99.502604    LR 0.300000    Time 0.022394    
2018-10-28 00:39:17,805 - Epoch: [90][  350/  391]    Overall Loss 0.403730    Objective Loss 0.403730    Top1 86.035714    Top5 99.502232    LR 0.300000    Time 0.022366    
2018-10-28 00:39:18,798 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57844 | -0.00318 |    0.29712 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18723 | -0.00596 |    0.06063 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18601 | -0.00334 |    0.06625 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18028 | -0.01417 |    0.07839 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13815 | -0.00681 |    0.03997 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19875 | -0.00715 |    0.08048 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13624 |  0.00187 |    0.03763 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18347 | -0.00788 |    0.08821 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16679 | -0.00884 |    0.09599 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25502 | -0.00883 |    0.12554 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13093 | -0.00756 |    0.05296 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10772 |  0.00187 |    0.03875 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13550 | -0.00975 |    0.05976 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11045 | -0.00213 |    0.05000 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11769 | -0.00453 |    0.04583 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.12030 | -0.00697 |    0.06049 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13948 | -0.00472 |    0.05040 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11072 | -0.00570 |    0.04604 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08719 |  0.00043 |    0.03916 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06273 | -0.00111 |    0.02032 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04824 | -0.00135 |    0.01187 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53347 | -0.03636 |    0.31517 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:39:18,798 - Total sparsity: 74.94

2018-10-28 00:39:18,798 - --- validate (epoch=90)-----------
2018-10-28 00:39:18,798 - 10000 samples (128 per mini-batch)
2018-10-28 00:39:19,519 - Epoch: [90][   50/   78]    Loss 0.592267    Top1 81.781250    Top5 98.890625    
2018-10-28 00:39:19,909 - ==> Top1: 81.720    Top5: 98.950    Loss: 0.589

2018-10-28 00:39:19,910 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:39:19,910 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:39:19,919 - 

2018-10-28 00:39:19,920 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:39:21,090 - Epoch: [91][   50/  391]    Overall Loss 0.389881    Objective Loss 0.389881    Top1 86.437500    Top5 99.546875    LR 0.300000    Time 0.023376    
2018-10-28 00:39:22,202 - Epoch: [91][  100/  391]    Overall Loss 0.398532    Objective Loss 0.398532    Top1 86.164062    Top5 99.515625    LR 0.300000    Time 0.022787    
2018-10-28 00:39:23,313 - Epoch: [91][  150/  391]    Overall Loss 0.404222    Objective Loss 0.404222    Top1 85.947917    Top5 99.515625    LR 0.300000    Time 0.022589    
2018-10-28 00:39:24,424 - Epoch: [91][  200/  391]    Overall Loss 0.404179    Objective Loss 0.404179    Top1 86.000000    Top5 99.496094    LR 0.300000    Time 0.022494    
2018-10-28 00:39:25,534 - Epoch: [91][  250/  391]    Overall Loss 0.406287    Objective Loss 0.406287    Top1 85.906250    Top5 99.471875    LR 0.300000    Time 0.022429    
2018-10-28 00:39:26,644 - Epoch: [91][  300/  391]    Overall Loss 0.405752    Objective Loss 0.405752    Top1 85.906250    Top5 99.463542    LR 0.300000    Time 0.022387    
2018-10-28 00:39:27,756 - Epoch: [91][  350/  391]    Overall Loss 0.403951    Objective Loss 0.403951    Top1 85.975446    Top5 99.457589    LR 0.300000    Time 0.022361    
2018-10-28 00:39:28,752 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58252 |  0.00605 |    0.30242 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18781 | -0.00553 |    0.06065 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18586 | -0.00351 |    0.06632 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18151 | -0.01419 |    0.07854 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13815 | -0.00692 |    0.04053 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19877 | -0.00784 |    0.08056 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13613 |  0.00225 |    0.03724 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18319 | -0.00992 |    0.08876 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16686 | -0.00870 |    0.09587 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25634 | -0.01161 |    0.12810 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13113 | -0.00787 |    0.05267 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10763 |  0.00217 |    0.03865 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13552 | -0.00993 |    0.05978 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11076 | -0.00211 |    0.04994 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11749 | -0.00442 |    0.04580 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11996 | -0.00693 |    0.06031 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13946 | -0.00393 |    0.05031 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11055 | -0.00570 |    0.04595 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08705 |  0.00046 |    0.03897 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06268 | -0.00091 |    0.02032 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04807 | -0.00129 |    0.01186 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53064 | -0.03667 |    0.31413 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:39:28,752 - Total sparsity: 74.94

2018-10-28 00:39:28,752 - --- validate (epoch=91)-----------
2018-10-28 00:39:28,752 - 10000 samples (128 per mini-batch)
2018-10-28 00:39:29,481 - Epoch: [91][   50/   78]    Loss 0.577728    Top1 81.125000    Top5 99.000000    
2018-10-28 00:39:29,878 - ==> Top1: 81.200    Top5: 99.080    Loss: 0.583

2018-10-28 00:39:29,879 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:39:29,879 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:39:29,897 - 

2018-10-28 00:39:29,897 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:39:31,040 - Epoch: [92][   50/  391]    Overall Loss 0.383735    Objective Loss 0.383735    Top1 86.578125    Top5 99.546875    LR 0.300000    Time 0.022822    
2018-10-28 00:39:32,153 - Epoch: [92][  100/  391]    Overall Loss 0.379017    Objective Loss 0.379017    Top1 86.750000    Top5 99.546875    LR 0.300000    Time 0.022523    
2018-10-28 00:39:33,265 - Epoch: [92][  150/  391]    Overall Loss 0.390212    Objective Loss 0.390212    Top1 86.234375    Top5 99.520833    LR 0.300000    Time 0.022423    
2018-10-28 00:39:34,375 - Epoch: [92][  200/  391]    Overall Loss 0.395093    Objective Loss 0.395093    Top1 86.113281    Top5 99.484375    LR 0.300000    Time 0.022357    
2018-10-28 00:39:35,483 - Epoch: [92][  250/  391]    Overall Loss 0.399301    Objective Loss 0.399301    Top1 85.993750    Top5 99.471875    LR 0.300000    Time 0.022314    
2018-10-28 00:39:36,593 - Epoch: [92][  300/  391]    Overall Loss 0.400453    Objective Loss 0.400453    Top1 86.000000    Top5 99.473958    LR 0.300000    Time 0.022291    
2018-10-28 00:39:37,703 - Epoch: [92][  350/  391]    Overall Loss 0.403149    Objective Loss 0.403149    Top1 85.935268    Top5 99.466518    LR 0.300000    Time 0.022263    
2018-10-28 00:39:38,693 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58461 |  0.00477 |    0.30264 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18622 | -0.00595 |    0.06029 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18573 | -0.00380 |    0.06678 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18178 | -0.01481 |    0.07841 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13857 | -0.00612 |    0.04070 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19790 | -0.00721 |    0.08017 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13582 |  0.00177 |    0.03706 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18256 | -0.00920 |    0.08815 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16609 | -0.00924 |    0.09517 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25760 | -0.01325 |    0.12838 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13134 | -0.00799 |    0.05298 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10801 |  0.00213 |    0.03904 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13537 | -0.01105 |    0.05960 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11080 | -0.00138 |    0.05009 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11724 | -0.00460 |    0.04577 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11982 | -0.00648 |    0.06033 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13907 | -0.00464 |    0.05047 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11061 | -0.00568 |    0.04601 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08713 |  0.00059 |    0.03904 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06263 | -0.00117 |    0.02029 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04805 | -0.00131 |    0.01187 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53080 | -0.03648 |    0.31454 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:39:38,693 - Total sparsity: 74.94

2018-10-28 00:39:38,694 - --- validate (epoch=92)-----------
2018-10-28 00:39:38,694 - 10000 samples (128 per mini-batch)
2018-10-28 00:39:39,419 - Epoch: [92][   50/   78]    Loss 0.563403    Top1 81.734375    Top5 99.156250    
2018-10-28 00:39:39,813 - ==> Top1: 82.000    Top5: 99.190    Loss: 0.556

2018-10-28 00:39:39,814 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:39:39,814 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:39:39,831 - 

2018-10-28 00:39:39,832 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:39:40,974 - Epoch: [93][   50/  391]    Overall Loss 0.397736    Objective Loss 0.397736    Top1 86.703125    Top5 99.375000    LR 0.300000    Time 0.022806    
2018-10-28 00:39:42,083 - Epoch: [93][  100/  391]    Overall Loss 0.387260    Objective Loss 0.387260    Top1 86.765625    Top5 99.539062    LR 0.300000    Time 0.022481    
2018-10-28 00:39:43,194 - Epoch: [93][  150/  391]    Overall Loss 0.396049    Objective Loss 0.396049    Top1 86.489583    Top5 99.536458    LR 0.300000    Time 0.022386    
2018-10-28 00:39:44,308 - Epoch: [93][  200/  391]    Overall Loss 0.393431    Objective Loss 0.393431    Top1 86.523438    Top5 99.554688    LR 0.300000    Time 0.022354    
2018-10-28 00:39:45,419 - Epoch: [93][  250/  391]    Overall Loss 0.398931    Objective Loss 0.398931    Top1 86.303125    Top5 99.528125    LR 0.300000    Time 0.022323    
2018-10-28 00:39:46,532 - Epoch: [93][  300/  391]    Overall Loss 0.399815    Objective Loss 0.399815    Top1 86.276042    Top5 99.533854    LR 0.300000    Time 0.022307    
2018-10-28 00:39:47,645 - Epoch: [93][  350/  391]    Overall Loss 0.404456    Objective Loss 0.404456    Top1 86.073661    Top5 99.526786    LR 0.300000    Time 0.022297    
2018-10-28 00:39:48,638 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.59033 |  0.00325 |    0.30447 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18602 | -0.00568 |    0.05976 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18624 | -0.00425 |    0.06691 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18159 | -0.01335 |    0.07822 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13746 | -0.00636 |    0.04027 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19732 | -0.00797 |    0.07954 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13510 |  0.00164 |    0.03675 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18201 | -0.00947 |    0.08778 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16600 | -0.00852 |    0.09511 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25767 | -0.01200 |    0.12999 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13170 | -0.00705 |    0.05319 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10843 |  0.00237 |    0.03936 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13518 | -0.01084 |    0.05970 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11075 | -0.00261 |    0.04998 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11743 | -0.00485 |    0.04594 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11988 | -0.00679 |    0.06039 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13878 | -0.00403 |    0.05055 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11054 | -0.00576 |    0.04610 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08725 |  0.00022 |    0.03914 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06277 | -0.00089 |    0.02037 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04807 | -0.00133 |    0.01187 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53533 | -0.03838 |    0.31801 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:39:48,638 - Total sparsity: 74.94

2018-10-28 00:39:48,638 - --- validate (epoch=93)-----------
2018-10-28 00:39:48,638 - 10000 samples (128 per mini-batch)
2018-10-28 00:39:49,356 - Epoch: [93][   50/   78]    Loss 0.666918    Top1 78.859375    Top5 98.812500    
2018-10-28 00:39:49,752 - ==> Top1: 78.740    Top5: 98.850    Loss: 0.672

2018-10-28 00:39:49,752 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:39:49,753 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:39:49,764 - 

2018-10-28 00:39:49,765 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:39:50,937 - Epoch: [94][   50/  391]    Overall Loss 0.377633    Objective Loss 0.377633    Top1 87.031250    Top5 99.593750    LR 0.300000    Time 0.023410    
2018-10-28 00:39:52,048 - Epoch: [94][  100/  391]    Overall Loss 0.390820    Objective Loss 0.390820    Top1 86.757812    Top5 99.484375    LR 0.300000    Time 0.022803    
2018-10-28 00:39:53,159 - Epoch: [94][  150/  391]    Overall Loss 0.394235    Objective Loss 0.394235    Top1 86.583333    Top5 99.468750    LR 0.300000    Time 0.022601    
2018-10-28 00:39:54,270 - Epoch: [94][  200/  391]    Overall Loss 0.403586    Objective Loss 0.403586    Top1 86.300781    Top5 99.453125    LR 0.300000    Time 0.022501    
2018-10-28 00:39:55,382 - Epoch: [94][  250/  391]    Overall Loss 0.408163    Objective Loss 0.408163    Top1 86.112500    Top5 99.415625    LR 0.300000    Time 0.022444    
2018-10-28 00:39:56,495 - Epoch: [94][  300/  391]    Overall Loss 0.406880    Objective Loss 0.406880    Top1 86.148438    Top5 99.447917    LR 0.300000    Time 0.022406    
2018-10-28 00:39:57,607 - Epoch: [94][  350/  391]    Overall Loss 0.404962    Objective Loss 0.404962    Top1 86.120536    Top5 99.444196    LR 0.300000    Time 0.022380    
2018-10-28 00:39:58,599 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.59156 |  0.00217 |    0.30761 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18653 | -0.00445 |    0.06018 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18569 | -0.00489 |    0.06638 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18068 | -0.01199 |    0.07765 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13617 | -0.00549 |    0.03991 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19573 | -0.00909 |    0.07917 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13393 |  0.00257 |    0.03635 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18184 | -0.00963 |    0.08788 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16520 | -0.00808 |    0.09419 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25822 | -0.01217 |    0.13029 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13179 | -0.00727 |    0.05301 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10839 |  0.00176 |    0.03942 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13491 | -0.01096 |    0.05966 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11053 | -0.00275 |    0.05011 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11714 | -0.00412 |    0.04571 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11986 | -0.00691 |    0.06058 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13843 | -0.00351 |    0.05015 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11037 | -0.00561 |    0.04600 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08719 |  0.00043 |    0.03901 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06291 | -0.00091 |    0.02036 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04819 | -0.00125 |    0.01180 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53509 | -0.03621 |    0.31735 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:39:58,599 - Total sparsity: 74.94

2018-10-28 00:39:58,599 - --- validate (epoch=94)-----------
2018-10-28 00:39:58,599 - 10000 samples (128 per mini-batch)
2018-10-28 00:39:59,319 - Epoch: [94][   50/   78]    Loss 0.627106    Top1 80.250000    Top5 98.968750    
2018-10-28 00:39:59,709 - ==> Top1: 79.950    Top5: 99.050    Loss: 0.629

2018-10-28 00:39:59,710 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:39:59,710 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:39:59,721 - 

2018-10-28 00:39:59,721 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:40:00,898 - Epoch: [95][   50/  391]    Overall Loss 0.392475    Objective Loss 0.392475    Top1 86.125000    Top5 99.562500    LR 0.300000    Time 0.023489    
2018-10-28 00:40:02,010 - Epoch: [95][  100/  391]    Overall Loss 0.398429    Objective Loss 0.398429    Top1 85.968750    Top5 99.515625    LR 0.300000    Time 0.022852    
2018-10-28 00:40:03,122 - Epoch: [95][  150/  391]    Overall Loss 0.405328    Objective Loss 0.405328    Top1 85.822917    Top5 99.458333    LR 0.300000    Time 0.022641    
2018-10-28 00:40:04,235 - Epoch: [95][  200/  391]    Overall Loss 0.403835    Objective Loss 0.403835    Top1 85.890625    Top5 99.457031    LR 0.300000    Time 0.022539    
2018-10-28 00:40:05,348 - Epoch: [95][  250/  391]    Overall Loss 0.402264    Objective Loss 0.402264    Top1 86.109375    Top5 99.450000    LR 0.300000    Time 0.022480    
2018-10-28 00:40:06,463 - Epoch: [95][  300/  391]    Overall Loss 0.402853    Objective Loss 0.402853    Top1 86.010417    Top5 99.447917    LR 0.300000    Time 0.022443    
2018-10-28 00:40:07,572 - Epoch: [95][  350/  391]    Overall Loss 0.402007    Objective Loss 0.402007    Top1 85.962054    Top5 99.477679    LR 0.300000    Time 0.022402    
2018-10-28 00:40:08,560 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58815 |  0.00282 |    0.30681 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18638 | -0.00441 |    0.06034 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18521 | -0.00448 |    0.06611 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18037 | -0.01246 |    0.07725 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13617 | -0.00623 |    0.03972 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19807 | -0.00907 |    0.07964 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13363 |  0.00219 |    0.03610 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18223 | -0.00838 |    0.08831 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16551 | -0.00825 |    0.09466 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25894 | -0.00923 |    0.12972 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13188 | -0.00681 |    0.05297 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10869 |  0.00179 |    0.03950 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13533 | -0.01087 |    0.05997 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11057 | -0.00219 |    0.05025 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11723 | -0.00447 |    0.04583 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11997 | -0.00690 |    0.06054 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13862 | -0.00430 |    0.05061 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11061 | -0.00562 |    0.04600 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08762 |  0.00035 |    0.03924 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06308 | -0.00099 |    0.02045 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04841 | -0.00117 |    0.01188 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53961 | -0.03766 |    0.32006 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:40:08,560 - Total sparsity: 74.94

2018-10-28 00:40:08,561 - --- validate (epoch=95)-----------
2018-10-28 00:40:08,561 - 10000 samples (128 per mini-batch)
2018-10-28 00:40:09,281 - Epoch: [95][   50/   78]    Loss 0.695999    Top1 79.046875    Top5 98.703125    
2018-10-28 00:40:09,674 - ==> Top1: 79.230    Top5: 98.740    Loss: 0.684

2018-10-28 00:40:09,674 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:40:09,675 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:40:09,684 - 

2018-10-28 00:40:09,685 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:40:10,853 - Epoch: [96][   50/  391]    Overall Loss 0.403877    Objective Loss 0.403877    Top1 86.453125    Top5 99.484375    LR 0.300000    Time 0.023333    
2018-10-28 00:40:11,963 - Epoch: [96][  100/  391]    Overall Loss 0.408771    Objective Loss 0.408771    Top1 85.976562    Top5 99.375000    LR 0.300000    Time 0.022750    
2018-10-28 00:40:13,074 - Epoch: [96][  150/  391]    Overall Loss 0.407120    Objective Loss 0.407120    Top1 85.838542    Top5 99.447917    LR 0.300000    Time 0.022569    
2018-10-28 00:40:14,186 - Epoch: [96][  200/  391]    Overall Loss 0.405259    Objective Loss 0.405259    Top1 86.027344    Top5 99.472656    LR 0.300000    Time 0.022478    
2018-10-28 00:40:15,298 - Epoch: [96][  250/  391]    Overall Loss 0.411431    Objective Loss 0.411431    Top1 85.753125    Top5 99.450000    LR 0.300000    Time 0.022427    
2018-10-28 00:40:16,408 - Epoch: [96][  300/  391]    Overall Loss 0.412058    Objective Loss 0.412058    Top1 85.768229    Top5 99.450521    LR 0.300000    Time 0.022385    
2018-10-28 00:40:17,519 - Epoch: [96][  350/  391]    Overall Loss 0.410221    Objective Loss 0.410221    Top1 85.783482    Top5 99.446429    LR 0.300000    Time 0.022357    
2018-10-28 00:40:18,509 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58716 | -0.00759 |    0.30314 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18736 | -0.00482 |    0.06061 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18621 | -0.00420 |    0.06624 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18092 | -0.01375 |    0.07810 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13591 | -0.00718 |    0.03943 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19888 | -0.00834 |    0.08087 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13525 |  0.00200 |    0.03663 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18324 | -0.00881 |    0.08845 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16639 | -0.00860 |    0.09540 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25819 | -0.01128 |    0.12952 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13231 | -0.00695 |    0.05318 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10897 |  0.00157 |    0.03958 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13549 | -0.01099 |    0.05995 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11082 | -0.00219 |    0.05049 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11762 | -0.00476 |    0.04577 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.12023 | -0.00691 |    0.06066 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13901 | -0.00453 |    0.05102 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11082 | -0.00548 |    0.04615 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08779 |  0.00026 |    0.03937 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06308 | -0.00082 |    0.02037 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04849 | -0.00115 |    0.01186 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53541 | -0.03765 |    0.31694 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:40:18,509 - Total sparsity: 74.94

2018-10-28 00:40:18,509 - --- validate (epoch=96)-----------
2018-10-28 00:40:18,509 - 10000 samples (128 per mini-batch)
2018-10-28 00:40:19,237 - Epoch: [96][   50/   78]    Loss 0.556304    Top1 81.828125    Top5 98.906250    
2018-10-28 00:40:19,631 - ==> Top1: 81.890    Top5: 98.960    Loss: 0.557

2018-10-28 00:40:19,631 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:40:19,631 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:40:19,643 - 

2018-10-28 00:40:19,644 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:40:20,813 - Epoch: [97][   50/  391]    Overall Loss 0.408823    Objective Loss 0.408823    Top1 86.125000    Top5 99.406250    LR 0.300000    Time 0.023351    
2018-10-28 00:40:21,924 - Epoch: [97][  100/  391]    Overall Loss 0.416639    Objective Loss 0.416639    Top1 85.859375    Top5 99.414062    LR 0.300000    Time 0.022775    
2018-10-28 00:40:23,035 - Epoch: [97][  150/  391]    Overall Loss 0.412944    Objective Loss 0.412944    Top1 85.885417    Top5 99.385417    LR 0.300000    Time 0.022580    
2018-10-28 00:40:24,146 - Epoch: [97][  200/  391]    Overall Loss 0.415695    Objective Loss 0.415695    Top1 85.769531    Top5 99.410156    LR 0.300000    Time 0.022484    
2018-10-28 00:40:25,257 - Epoch: [97][  250/  391]    Overall Loss 0.413355    Objective Loss 0.413355    Top1 85.790625    Top5 99.421875    LR 0.300000    Time 0.022424    
2018-10-28 00:40:26,368 - Epoch: [97][  300/  391]    Overall Loss 0.410935    Objective Loss 0.410935    Top1 85.888021    Top5 99.421875    LR 0.300000    Time 0.022388    
2018-10-28 00:40:27,481 - Epoch: [97][  350/  391]    Overall Loss 0.409421    Objective Loss 0.409421    Top1 85.881696    Top5 99.448661    LR 0.300000    Time 0.022364    
2018-10-28 00:40:28,468 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58741 | -0.00543 |    0.30462 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18554 | -0.00651 |    0.06074 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18506 | -0.00314 |    0.06672 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18125 | -0.01307 |    0.07833 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13675 | -0.00764 |    0.04001 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19861 | -0.00880 |    0.07980 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13527 |  0.00229 |    0.03733 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18245 | -0.00976 |    0.08792 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16605 | -0.00799 |    0.09537 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25755 | -0.01038 |    0.12890 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13196 | -0.00662 |    0.05311 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10857 |  0.00222 |    0.03912 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13531 | -0.01134 |    0.06011 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11046 | -0.00149 |    0.05005 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11754 | -0.00432 |    0.04575 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.12015 | -0.00687 |    0.06048 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13881 | -0.00513 |    0.05088 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11067 | -0.00560 |    0.04609 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08793 | -0.00004 |    0.03943 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06306 | -0.00087 |    0.02038 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04838 | -0.00107 |    0.01183 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53814 | -0.04048 |    0.31843 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:40:28,468 - Total sparsity: 74.94

2018-10-28 00:40:28,468 - --- validate (epoch=97)-----------
2018-10-28 00:40:28,468 - 10000 samples (128 per mini-batch)
2018-10-28 00:40:29,193 - Epoch: [97][   50/   78]    Loss 0.652611    Top1 79.437500    Top5 98.546875    
2018-10-28 00:40:29,585 - ==> Top1: 79.840    Top5: 98.710    Loss: 0.644

2018-10-28 00:40:29,586 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:40:29,586 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:40:29,603 - 

2018-10-28 00:40:29,603 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:40:30,747 - Epoch: [98][   50/  391]    Overall Loss 0.396139    Objective Loss 0.396139    Top1 86.328125    Top5 99.375000    LR 0.300000    Time 0.022838    
2018-10-28 00:40:31,859 - Epoch: [98][  100/  391]    Overall Loss 0.401806    Objective Loss 0.401806    Top1 85.992188    Top5 99.390625    LR 0.300000    Time 0.022521    
2018-10-28 00:40:32,970 - Epoch: [98][  150/  391]    Overall Loss 0.408578    Objective Loss 0.408578    Top1 85.770833    Top5 99.447917    LR 0.300000    Time 0.022413    
2018-10-28 00:40:34,082 - Epoch: [98][  200/  391]    Overall Loss 0.405087    Objective Loss 0.405087    Top1 85.917969    Top5 99.468750    LR 0.300000    Time 0.022363    
2018-10-28 00:40:35,191 - Epoch: [98][  250/  391]    Overall Loss 0.402594    Objective Loss 0.402594    Top1 86.081250    Top5 99.468750    LR 0.300000    Time 0.022324    
2018-10-28 00:40:36,302 - Epoch: [98][  300/  391]    Overall Loss 0.399748    Objective Loss 0.399748    Top1 86.114583    Top5 99.489583    LR 0.300000    Time 0.022300    
2018-10-28 00:40:37,414 - Epoch: [98][  350/  391]    Overall Loss 0.402706    Objective Loss 0.402706    Top1 85.977679    Top5 99.482143    LR 0.300000    Time 0.022287    
2018-10-28 00:40:38,405 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58272 | -0.00192 |    0.30212 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18482 | -0.00500 |    0.06049 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18388 | -0.00406 |    0.06614 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18127 | -0.01234 |    0.07826 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13723 | -0.00501 |    0.04018 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19844 | -0.00833 |    0.07985 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13498 |  0.00253 |    0.03636 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18257 | -0.00831 |    0.08791 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16587 | -0.00804 |    0.09514 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25630 | -0.01106 |    0.12795 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13154 | -0.00657 |    0.05271 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10840 |  0.00187 |    0.03933 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13570 | -0.01188 |    0.06015 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11084 | -0.00179 |    0.05031 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11764 | -0.00395 |    0.04565 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.12010 | -0.00630 |    0.06032 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13847 | -0.00487 |    0.05047 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11069 | -0.00544 |    0.04590 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08814 |  0.00022 |    0.03951 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06317 | -0.00092 |    0.02034 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04851 | -0.00101 |    0.01187 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53708 | -0.04145 |    0.31801 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:40:38,406 - Total sparsity: 74.94

2018-10-28 00:40:38,406 - --- validate (epoch=98)-----------
2018-10-28 00:40:38,406 - 10000 samples (128 per mini-batch)
2018-10-28 00:40:39,131 - Epoch: [98][   50/   78]    Loss 0.678388    Top1 79.359375    Top5 98.953125    
2018-10-28 00:40:39,525 - ==> Top1: 79.290    Top5: 99.000    Loss: 0.668

2018-10-28 00:40:39,525 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:40:39,525 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:40:39,535 - 

2018-10-28 00:40:39,535 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:40:40,708 - Epoch: [99][   50/  391]    Overall Loss 0.391391    Objective Loss 0.391391    Top1 86.812500    Top5 99.515625    LR 0.300000    Time 0.023418    
2018-10-28 00:40:41,817 - Epoch: [99][  100/  391]    Overall Loss 0.390874    Objective Loss 0.390874    Top1 86.656250    Top5 99.562500    LR 0.300000    Time 0.022779    
2018-10-28 00:40:42,927 - Epoch: [99][  150/  391]    Overall Loss 0.395814    Objective Loss 0.395814    Top1 86.359375    Top5 99.546875    LR 0.300000    Time 0.022583    
2018-10-28 00:40:44,038 - Epoch: [99][  200/  391]    Overall Loss 0.402987    Objective Loss 0.402987    Top1 86.023438    Top5 99.511719    LR 0.300000    Time 0.022483    
2018-10-28 00:40:45,150 - Epoch: [99][  250/  391]    Overall Loss 0.406599    Objective Loss 0.406599    Top1 85.975000    Top5 99.487500    LR 0.300000    Time 0.022428    
2018-10-28 00:40:46,262 - Epoch: [99][  300/  391]    Overall Loss 0.406359    Objective Loss 0.406359    Top1 85.979167    Top5 99.497396    LR 0.300000    Time 0.022392    
2018-10-28 00:40:47,375 - Epoch: [99][  350/  391]    Overall Loss 0.405414    Objective Loss 0.405414    Top1 86.037946    Top5 99.495536    LR 0.300000    Time 0.022371    
2018-10-28 00:40:48,369 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.58281 | -0.00176 |    0.30058 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18566 | -0.00624 |    0.06050 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18490 | -0.00378 |    0.06639 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18189 | -0.01212 |    0.07871 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13815 | -0.00585 |    0.04030 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19932 | -0.00906 |    0.08070 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13552 |  0.00220 |    0.03677 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18335 | -0.00897 |    0.08830 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16664 | -0.00717 |    0.09574 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25539 | -0.01072 |    0.12749 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13206 | -0.00697 |    0.05288 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10868 |  0.00186 |    0.03931 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13563 | -0.00994 |    0.05994 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.11104 | -0.00155 |    0.05040 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11806 | -0.00424 |    0.04582 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.12051 | -0.00675 |    0.06070 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13849 | -0.00472 |    0.04982 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.11092 | -0.00565 |    0.04613 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08824 |  0.00008 |    0.03951 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06328 | -0.00106 |    0.02040 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04856 | -0.00109 |    0.01185 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.53291 | -0.03997 |    0.31512 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:40:48,369 - Total sparsity: 74.94

2018-10-28 00:40:48,369 - --- validate (epoch=99)-----------
2018-10-28 00:40:48,369 - 10000 samples (128 per mini-batch)
2018-10-28 00:40:49,091 - Epoch: [99][   50/   78]    Loss 0.627834    Top1 80.000000    Top5 98.562500    
2018-10-28 00:40:49,484 - ==> Top1: 79.370    Top5: 98.600    Loss: 0.638

2018-10-28 00:40:49,484 - ==> Best Top1: 84.520   On Epoch: 61

2018-10-28 00:40:49,485 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:40:49,494 - 

2018-10-28 00:40:49,495 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:40:50,669 - Epoch: [100][   50/  391]    Overall Loss 0.353792    Objective Loss 0.353792    Top1 87.562500    Top5 99.578125    LR 0.030000    Time 0.023443    
2018-10-28 00:40:51,780 - Epoch: [100][  100/  391]    Overall Loss 0.332373    Objective Loss 0.332373    Top1 88.515625    Top5 99.679688    LR 0.030000    Time 0.022820    
2018-10-28 00:40:52,891 - Epoch: [100][  150/  391]    Overall Loss 0.316936    Objective Loss 0.316936    Top1 89.119792    Top5 99.682292    LR 0.030000    Time 0.022611    
2018-10-28 00:40:54,001 - Epoch: [100][  200/  391]    Overall Loss 0.305231    Objective Loss 0.305231    Top1 89.484375    Top5 99.726562    LR 0.030000    Time 0.022504    
2018-10-28 00:40:55,110 - Epoch: [100][  250/  391]    Overall Loss 0.295851    Objective Loss 0.295851    Top1 89.871875    Top5 99.734375    LR 0.030000    Time 0.022435    
2018-10-28 00:40:56,221 - Epoch: [100][  300/  391]    Overall Loss 0.293055    Objective Loss 0.293055    Top1 89.986979    Top5 99.734375    LR 0.030000    Time 0.022395    
2018-10-28 00:40:57,332 - Epoch: [100][  350/  391]    Overall Loss 0.289669    Objective Loss 0.289669    Top1 90.107143    Top5 99.738839    LR 0.030000    Time 0.022364    
2018-10-28 00:40:58,320 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57677 | -0.00014 |    0.29783 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18372 | -0.00608 |    0.06001 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18296 | -0.00363 |    0.06585 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.18002 | -0.01167 |    0.07785 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13671 | -0.00551 |    0.03990 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19723 | -0.00851 |    0.07980 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13407 |  0.00248 |    0.03635 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.18143 | -0.00872 |    0.08738 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16490 | -0.00722 |    0.09479 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.25263 | -0.01189 |    0.12638 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.13070 | -0.00673 |    0.05239 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10754 |  0.00168 |    0.03897 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13420 | -0.00984 |    0.05933 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10988 | -0.00149 |    0.04985 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11682 | -0.00417 |    0.04536 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11925 | -0.00656 |    0.06004 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13703 | -0.00471 |    0.04937 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.10976 | -0.00571 |    0.04568 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08732 |  0.00001 |    0.03913 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06262 | -0.00117 |    0.02023 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04805 | -0.00104 |    0.01173 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.54335 | -0.03921 |    0.32116 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:40:58,320 - Total sparsity: 74.94

2018-10-28 00:40:58,321 - --- validate (epoch=100)-----------
2018-10-28 00:40:58,321 - 10000 samples (128 per mini-batch)
2018-10-28 00:40:59,041 - Epoch: [100][   50/   78]    Loss 0.359981    Top1 87.937500    Top5 99.593750    
2018-10-28 00:40:59,434 - ==> Top1: 88.250    Top5: 99.590    Loss: 0.353

2018-10-28 00:40:59,435 - ==> Best Top1: 88.250   On Epoch: 100

2018-10-28 00:40:59,435 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:40:59,453 - 

2018-10-28 00:40:59,453 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:41:00,600 - Epoch: [101][   50/  391]    Overall Loss 0.270749    Objective Loss 0.270749    Top1 90.671875    Top5 99.765625    LR 0.030000    Time 0.022892    
2018-10-28 00:41:01,710 - Epoch: [101][  100/  391]    Overall Loss 0.260296    Objective Loss 0.260296    Top1 91.015625    Top5 99.828125    LR 0.030000    Time 0.022540    
2018-10-28 00:41:02,821 - Epoch: [101][  150/  391]    Overall Loss 0.264464    Objective Loss 0.264464    Top1 90.864583    Top5 99.776042    LR 0.030000    Time 0.022426    
2018-10-28 00:41:03,934 - Epoch: [101][  200/  391]    Overall Loss 0.266424    Objective Loss 0.266424    Top1 90.769531    Top5 99.761719    LR 0.030000    Time 0.022374    
2018-10-28 00:41:05,045 - Epoch: [101][  250/  391]    Overall Loss 0.262839    Objective Loss 0.262839    Top1 90.871875    Top5 99.771875    LR 0.030000    Time 0.022339    
2018-10-28 00:41:06,158 - Epoch: [101][  300/  391]    Overall Loss 0.260482    Objective Loss 0.260482    Top1 90.903646    Top5 99.776042    LR 0.030000    Time 0.022320    
2018-10-28 00:41:07,271 - Epoch: [101][  350/  391]    Overall Loss 0.258307    Objective Loss 0.258307    Top1 90.977679    Top5 99.785714    LR 0.030000    Time 0.022307    
2018-10-28 00:41:08,258 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.57062 | -0.00225 |    0.29452 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.18173 | -0.00603 |    0.05936 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.18097 | -0.00368 |    0.06516 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.17808 | -0.01155 |    0.07698 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13524 | -0.00528 |    0.03944 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19508 | -0.00866 |    0.07896 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13262 |  0.00262 |    0.03602 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.17949 | -0.00855 |    0.08644 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16313 | -0.00712 |    0.09375 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.24989 | -0.01194 |    0.12505 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.12930 | -0.00660 |    0.05184 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10638 |  0.00176 |    0.03857 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13277 | -0.00975 |    0.05869 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10870 | -0.00151 |    0.04932 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11557 | -0.00413 |    0.04487 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11798 | -0.00648 |    0.05940 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13556 | -0.00469 |    0.04893 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.10859 | -0.00569 |    0.04518 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08639 | -0.00002 |    0.03872 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06195 | -0.00126 |    0.02003 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04753 | -0.00099 |    0.01161 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.54872 | -0.03895 |    0.32423 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:41:08,259 - Total sparsity: 74.94

2018-10-28 00:41:08,259 - --- validate (epoch=101)-----------
2018-10-28 00:41:08,259 - 10000 samples (128 per mini-batch)
2018-10-28 00:41:08,983 - Epoch: [101][   50/   78]    Loss 0.348899    Top1 88.328125    Top5 99.562500    
2018-10-28 00:41:09,374 - ==> Top1: 88.590    Top5: 99.590    Loss: 0.340

2018-10-28 00:41:09,374 - ==> Best Top1: 88.590   On Epoch: 101

2018-10-28 00:41:09,375 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:41:09,396 - 

2018-10-28 00:41:09,396 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:41:10,538 - Epoch: [102][   50/  391]    Overall Loss 0.247802    Objective Loss 0.247802    Top1 91.437500    Top5 99.859375    LR 0.030000    Time 0.022813    
2018-10-28 00:41:11,647 - Epoch: [102][  100/  391]    Overall Loss 0.242117    Objective Loss 0.242117    Top1 91.656250    Top5 99.843750    LR 0.030000    Time 0.022480    
2018-10-28 00:41:12,758 - Epoch: [102][  150/  391]    Overall Loss 0.245529    Objective Loss 0.245529    Top1 91.619792    Top5 99.817708    LR 0.030000    Time 0.022383    
2018-10-28 00:41:13,868 - Epoch: [102][  200/  391]    Overall Loss 0.241615    Objective Loss 0.241615    Top1 91.730469    Top5 99.824219    LR 0.030000    Time 0.022335    
2018-10-28 00:41:14,981 - Epoch: [102][  250/  391]    Overall Loss 0.240713    Objective Loss 0.240713    Top1 91.718750    Top5 99.818750    LR 0.030000    Time 0.022313    
2018-10-28 00:41:16,093 - Epoch: [102][  300/  391]    Overall Loss 0.242642    Objective Loss 0.242642    Top1 91.575521    Top5 99.817708    LR 0.030000    Time 0.022295    
2018-10-28 00:41:17,206 - Epoch: [102][  350/  391]    Overall Loss 0.242808    Objective Loss 0.242808    Top1 91.562500    Top5 99.805804    LR 0.030000    Time 0.022286    
2018-10-28 00:41:18,196 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.56461 |  0.00157 |    0.29184 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.17977 | -0.00599 |    0.05878 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.17902 | -0.00377 |    0.06455 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.17619 | -0.01137 |    0.07620 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13379 | -0.00523 |    0.03908 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19300 | -0.00823 |    0.07808 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.13119 |  0.00249 |    0.03573 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.17758 | -0.00856 |    0.08551 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.16139 | -0.00719 |    0.09278 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.24720 | -0.01170 |    0.12360 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.12794 | -0.00636 |    0.05128 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10525 |  0.00169 |    0.03820 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.13136 | -0.00965 |    0.05806 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10755 | -0.00151 |    0.04885 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11433 | -0.00416 |    0.04443 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11673 | -0.00638 |    0.05879 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13411 | -0.00463 |    0.04849 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.10744 | -0.00562 |    0.04470 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08547 | -0.00005 |    0.03833 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06129 | -0.00130 |    0.01984 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04703 | -0.00096 |    0.01149 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55190 | -0.03868 |    0.32606 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:41:18,196 - Total sparsity: 74.94

2018-10-28 00:41:18,196 - --- validate (epoch=102)-----------
2018-10-28 00:41:18,196 - 10000 samples (128 per mini-batch)
2018-10-28 00:41:18,915 - Epoch: [102][   50/   78]    Loss 0.347899    Top1 88.593750    Top5 99.609375    
2018-10-28 00:41:19,305 - ==> Top1: 88.740    Top5: 99.640    Loss: 0.342

2018-10-28 00:41:19,305 - ==> Best Top1: 88.740   On Epoch: 102

2018-10-28 00:41:19,306 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:41:19,324 - 

2018-10-28 00:41:19,325 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:41:20,467 - Epoch: [103][   50/  391]    Overall Loss 0.226683    Objective Loss 0.226683    Top1 92.078125    Top5 99.828125    LR 0.030000    Time 0.022822    
2018-10-28 00:41:21,576 - Epoch: [103][  100/  391]    Overall Loss 0.229462    Objective Loss 0.229462    Top1 91.929688    Top5 99.804688    LR 0.030000    Time 0.022489    
2018-10-28 00:41:22,686 - Epoch: [103][  150/  391]    Overall Loss 0.235704    Objective Loss 0.235704    Top1 91.583333    Top5 99.786458    LR 0.030000    Time 0.022382    
2018-10-28 00:41:23,796 - Epoch: [103][  200/  391]    Overall Loss 0.235487    Objective Loss 0.235487    Top1 91.734375    Top5 99.800781    LR 0.030000    Time 0.022329    
2018-10-28 00:41:24,905 - Epoch: [103][  250/  391]    Overall Loss 0.235960    Objective Loss 0.235960    Top1 91.778125    Top5 99.800000    LR 0.030000    Time 0.022294    
2018-10-28 00:41:26,016 - Epoch: [103][  300/  391]    Overall Loss 0.235642    Objective Loss 0.235642    Top1 91.710938    Top5 99.809896    LR 0.030000    Time 0.022277    
2018-10-28 00:41:27,127 - Epoch: [103][  350/  391]    Overall Loss 0.235168    Objective Loss 0.235168    Top1 91.808036    Top5 99.805804    LR 0.030000    Time 0.022265    
2018-10-28 00:41:28,113 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.55870 |  0.00001 |    0.28825 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.17787 | -0.00590 |    0.05819 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.17712 | -0.00378 |    0.06391 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.17434 | -0.01132 |    0.07526 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13237 | -0.00529 |    0.03868 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.19093 | -0.00840 |    0.07726 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.12979 |  0.00256 |    0.03534 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.17572 | -0.00830 |    0.08460 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.15970 | -0.00701 |    0.09184 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.24455 | -0.01154 |    0.12237 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.12660 | -0.00641 |    0.05070 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10415 |  0.00164 |    0.03780 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.12998 | -0.00966 |    0.05741 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10643 | -0.00153 |    0.04831 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11313 | -0.00417 |    0.04394 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11550 | -0.00634 |    0.05818 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13268 | -0.00445 |    0.04804 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.10632 | -0.00558 |    0.04422 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08458 | -0.00015 |    0.03796 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06065 | -0.00134 |    0.01966 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04653 | -0.00094 |    0.01137 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55386 | -0.03823 |    0.32714 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:41:28,114 - Total sparsity: 74.94

2018-10-28 00:41:28,114 - --- validate (epoch=103)-----------
2018-10-28 00:41:28,114 - 10000 samples (128 per mini-batch)
2018-10-28 00:41:28,831 - Epoch: [103][   50/   78]    Loss 0.351467    Top1 88.515625    Top5 99.671875    
2018-10-28 00:41:29,224 - ==> Top1: 88.600    Top5: 99.670    Loss: 0.346

2018-10-28 00:41:29,224 - ==> Best Top1: 88.740   On Epoch: 102

2018-10-28 00:41:29,225 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:41:29,235 - 

2018-10-28 00:41:29,236 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:41:30,406 - Epoch: [104][   50/  391]    Overall Loss 0.227515    Objective Loss 0.227515    Top1 91.765625    Top5 99.875000    LR 0.030000    Time 0.023374    
2018-10-28 00:41:31,517 - Epoch: [104][  100/  391]    Overall Loss 0.220540    Objective Loss 0.220540    Top1 92.109375    Top5 99.875000    LR 0.030000    Time 0.022786    
2018-10-28 00:41:32,628 - Epoch: [104][  150/  391]    Overall Loss 0.224267    Objective Loss 0.224267    Top1 92.088542    Top5 99.854167    LR 0.030000    Time 0.022587    
2018-10-28 00:41:33,739 - Epoch: [104][  200/  391]    Overall Loss 0.225826    Objective Loss 0.225826    Top1 92.027344    Top5 99.843750    LR 0.030000    Time 0.022488    
2018-10-28 00:41:34,850 - Epoch: [104][  250/  391]    Overall Loss 0.226290    Objective Loss 0.226290    Top1 91.990625    Top5 99.818750    LR 0.030000    Time 0.022430    
2018-10-28 00:41:35,961 - Epoch: [104][  300/  391]    Overall Loss 0.229846    Objective Loss 0.229846    Top1 91.828125    Top5 99.817708    LR 0.030000    Time 0.022392    
2018-10-28 00:41:37,073 - Epoch: [104][  350/  391]    Overall Loss 0.229688    Objective Loss 0.229688    Top1 91.854911    Top5 99.812500    LR 0.030000    Time 0.022365    
2018-10-28 00:41:38,060 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.55290 | -0.00097 |    0.28553 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.17601 | -0.00577 |    0.05750 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.17524 | -0.00404 |    0.06319 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.17253 | -0.01102 |    0.07456 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.13098 | -0.00531 |    0.03830 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.18894 | -0.00805 |    0.07643 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.12843 |  0.00256 |    0.03496 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.17387 | -0.00852 |    0.08372 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.15804 | -0.00691 |    0.09088 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.24193 | -0.01161 |    0.12133 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.12529 | -0.00621 |    0.05016 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10307 |  0.00157 |    0.03742 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.12865 | -0.00947 |    0.05681 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10533 | -0.00161 |    0.04779 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11195 | -0.00406 |    0.04349 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11431 | -0.00624 |    0.05757 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.13129 | -0.00434 |    0.04757 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.10523 | -0.00549 |    0.04373 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08371 | -0.00025 |    0.03759 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.06003 | -0.00137 |    0.01946 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04605 | -0.00091 |    0.01124 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55508 | -0.03794 |    0.32783 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:41:38,060 - Total sparsity: 74.94

2018-10-28 00:41:38,060 - --- validate (epoch=104)-----------
2018-10-28 00:41:38,061 - 10000 samples (128 per mini-batch)
2018-10-28 00:41:38,781 - Epoch: [104][   50/   78]    Loss 0.348429    Top1 88.921875    Top5 99.531250    
2018-10-28 00:41:39,169 - ==> Top1: 88.990    Top5: 99.580    Loss: 0.344

2018-10-28 00:41:39,170 - ==> Best Top1: 88.990   On Epoch: 104

2018-10-28 00:41:39,170 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:41:39,186 - 

2018-10-28 00:41:39,187 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:41:40,331 - Epoch: [105][   50/  391]    Overall Loss 0.218545    Objective Loss 0.218545    Top1 92.187500    Top5 99.875000    LR 0.030000    Time 0.022857    
2018-10-28 00:41:41,444 - Epoch: [105][  100/  391]    Overall Loss 0.222288    Objective Loss 0.222288    Top1 92.281250    Top5 99.835938    LR 0.030000    Time 0.022544    
2018-10-28 00:41:42,556 - Epoch: [105][  150/  391]    Overall Loss 0.219851    Objective Loss 0.219851    Top1 92.447917    Top5 99.828125    LR 0.030000    Time 0.022435    
2018-10-28 00:41:43,669 - Epoch: [105][  200/  391]    Overall Loss 0.221276    Objective Loss 0.221276    Top1 92.371094    Top5 99.824219    LR 0.030000    Time 0.022386    
2018-10-28 00:41:44,783 - Epoch: [105][  250/  391]    Overall Loss 0.222304    Objective Loss 0.222304    Top1 92.259375    Top5 99.809375    LR 0.030000    Time 0.022344    
2018-10-28 00:41:45,897 - Epoch: [105][  300/  391]    Overall Loss 0.224158    Objective Loss 0.224158    Top1 92.184896    Top5 99.807292    LR 0.030000    Time 0.022328    
2018-10-28 00:41:47,010 - Epoch: [105][  350/  391]    Overall Loss 0.224187    Objective Loss 0.224187    Top1 92.158482    Top5 99.810268    LR 0.030000    Time 0.022315    
2018-10-28 00:41:48,003 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.54719 | -0.00058 |    0.28198 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.17416 | -0.00578 |    0.05684 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.17340 | -0.00400 |    0.06253 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.17074 | -0.01097 |    0.07383 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.12962 | -0.00516 |    0.03787 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.18698 | -0.00790 |    0.07558 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.12709 |  0.00249 |    0.03469 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.17209 | -0.00822 |    0.08280 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.15640 | -0.00700 |    0.08991 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.23937 | -0.01124 |    0.12011 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.12401 | -0.00611 |    0.04965 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10201 |  0.00155 |    0.03703 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.12733 | -0.00945 |    0.05616 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10426 | -0.00157 |    0.04730 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.11078 | -0.00404 |    0.04304 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11313 | -0.00618 |    0.05696 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.12990 | -0.00433 |    0.04707 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.10416 | -0.00541 |    0.04326 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08285 | -0.00026 |    0.03724 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05943 | -0.00139 |    0.01926 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04557 | -0.00086 |    0.01112 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55709 | -0.03758 |    0.32894 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:41:48,003 - Total sparsity: 74.94

2018-10-28 00:41:48,003 - --- validate (epoch=105)-----------
2018-10-28 00:41:48,003 - 10000 samples (128 per mini-batch)
2018-10-28 00:41:48,722 - Epoch: [105][   50/   78]    Loss 0.354887    Top1 88.906250    Top5 99.562500    
2018-10-28 00:41:49,111 - ==> Top1: 88.940    Top5: 99.630    Loss: 0.349

2018-10-28 00:41:49,112 - ==> Best Top1: 88.990   On Epoch: 104

2018-10-28 00:41:49,112 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:41:49,122 - 

2018-10-28 00:41:49,122 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:41:50,293 - Epoch: [106][   50/  391]    Overall Loss 0.217801    Objective Loss 0.217801    Top1 92.250000    Top5 99.859375    LR 0.030000    Time 0.023374    
2018-10-28 00:41:51,402 - Epoch: [106][  100/  391]    Overall Loss 0.216773    Objective Loss 0.216773    Top1 92.398438    Top5 99.867188    LR 0.030000    Time 0.022763    
2018-10-28 00:41:52,512 - Epoch: [106][  150/  391]    Overall Loss 0.220069    Objective Loss 0.220069    Top1 92.286458    Top5 99.880208    LR 0.030000    Time 0.022569    
2018-10-28 00:41:53,622 - Epoch: [106][  200/  391]    Overall Loss 0.217896    Objective Loss 0.217896    Top1 92.378906    Top5 99.882812    LR 0.030000    Time 0.022471    
2018-10-28 00:41:54,732 - Epoch: [106][  250/  391]    Overall Loss 0.217354    Objective Loss 0.217354    Top1 92.381250    Top5 99.875000    LR 0.030000    Time 0.022410    
2018-10-28 00:41:55,842 - Epoch: [106][  300/  391]    Overall Loss 0.217068    Objective Loss 0.217068    Top1 92.369792    Top5 99.880208    LR 0.030000    Time 0.022372    
2018-10-28 00:41:56,955 - Epoch: [106][  350/  391]    Overall Loss 0.220066    Objective Loss 0.220066    Top1 92.287946    Top5 99.866071    LR 0.030000    Time 0.022351    
2018-10-28 00:41:57,950 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.54165 |  0.00195 |    0.27885 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.17237 | -0.00577 |    0.05613 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.17162 | -0.00388 |    0.06192 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.16900 | -0.01084 |    0.07312 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.12828 | -0.00516 |    0.03750 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.18508 | -0.00772 |    0.07477 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.12578 |  0.00238 |    0.03435 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.17033 | -0.00813 |    0.08194 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.15481 | -0.00699 |    0.08899 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.23688 | -0.01066 |    0.11883 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.12275 | -0.00604 |    0.04913 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.10097 |  0.00160 |    0.03665 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.12606 | -0.00922 |    0.05556 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10321 | -0.00158 |    0.04679 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.10965 | -0.00396 |    0.04259 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11198 | -0.00609 |    0.05635 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.12853 | -0.00427 |    0.04665 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.10311 | -0.00538 |    0.04283 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08202 | -0.00031 |    0.03687 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05884 | -0.00138 |    0.01907 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04510 | -0.00084 |    0.01100 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55845 | -0.03733 |    0.32972 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:41:57,950 - Total sparsity: 74.94

2018-10-28 00:41:57,950 - --- validate (epoch=106)-----------
2018-10-28 00:41:57,951 - 10000 samples (128 per mini-batch)
2018-10-28 00:41:58,675 - Epoch: [106][   50/   78]    Loss 0.353702    Top1 89.093750    Top5 99.500000    
2018-10-28 00:41:59,064 - ==> Top1: 89.130    Top5: 99.580    Loss: 0.350

2018-10-28 00:41:59,065 - ==> Best Top1: 89.130   On Epoch: 106

2018-10-28 00:41:59,065 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:41:59,087 - 

2018-10-28 00:41:59,087 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:42:00,237 - Epoch: [107][   50/  391]    Overall Loss 0.223912    Objective Loss 0.223912    Top1 92.203125    Top5 99.781250    LR 0.030000    Time 0.022966    
2018-10-28 00:42:01,350 - Epoch: [107][  100/  391]    Overall Loss 0.219365    Objective Loss 0.219365    Top1 92.437500    Top5 99.796875    LR 0.030000    Time 0.022592    
2018-10-28 00:42:02,462 - Epoch: [107][  150/  391]    Overall Loss 0.217004    Objective Loss 0.217004    Top1 92.479167    Top5 99.817708    LR 0.030000    Time 0.022468    
2018-10-28 00:42:03,575 - Epoch: [107][  200/  391]    Overall Loss 0.214021    Objective Loss 0.214021    Top1 92.636719    Top5 99.839844    LR 0.030000    Time 0.022408    
2018-10-28 00:42:04,686 - Epoch: [107][  250/  391]    Overall Loss 0.217433    Objective Loss 0.217433    Top1 92.468750    Top5 99.834375    LR 0.030000    Time 0.022367    
2018-10-28 00:42:05,796 - Epoch: [107][  300/  391]    Overall Loss 0.217429    Objective Loss 0.217429    Top1 92.434896    Top5 99.851562    LR 0.030000    Time 0.022334    
2018-10-28 00:42:06,907 - Epoch: [107][  350/  391]    Overall Loss 0.217061    Objective Loss 0.217061    Top1 92.421875    Top5 99.854911    LR 0.030000    Time 0.022306    
2018-10-28 00:42:07,902 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.53625 | -0.00085 |    0.27668 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.17060 | -0.00578 |    0.05544 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.16985 | -0.00373 |    0.06120 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.16729 | -0.01064 |    0.07236 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.12696 | -0.00512 |    0.03718 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.18320 | -0.00746 |    0.07403 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.12450 |  0.00233 |    0.03401 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.16861 | -0.00800 |    0.08115 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.15324 | -0.00705 |    0.08805 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.23440 | -0.01046 |    0.11723 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.12153 | -0.00605 |    0.04865 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.09995 |  0.00153 |    0.03625 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.12480 | -0.00934 |    0.05503 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10219 | -0.00147 |    0.04628 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.10853 | -0.00397 |    0.04217 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.11086 | -0.00598 |    0.05577 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.12719 | -0.00416 |    0.04619 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.10209 | -0.00530 |    0.04238 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08121 | -0.00034 |    0.03652 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05827 | -0.00138 |    0.01887 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04465 | -0.00083 |    0.01089 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55972 | -0.03719 |    0.33050 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:42:07,902 - Total sparsity: 74.94

2018-10-28 00:42:07,902 - --- validate (epoch=107)-----------
2018-10-28 00:42:07,902 - 10000 samples (128 per mini-batch)
2018-10-28 00:42:08,627 - Epoch: [107][   50/   78]    Loss 0.350195    Top1 89.093750    Top5 99.578125    
2018-10-28 00:42:09,023 - ==> Top1: 89.030    Top5: 99.640    Loss: 0.345

2018-10-28 00:42:09,023 - ==> Best Top1: 89.130   On Epoch: 106

2018-10-28 00:42:09,023 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:42:09,034 - 

2018-10-28 00:42:09,034 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:42:10,205 - Epoch: [108][   50/  391]    Overall Loss 0.204721    Objective Loss 0.204721    Top1 92.656250    Top5 99.937500    LR 0.030000    Time 0.023383    
2018-10-28 00:42:11,317 - Epoch: [108][  100/  391]    Overall Loss 0.205368    Objective Loss 0.205368    Top1 92.835938    Top5 99.921875    LR 0.030000    Time 0.022801    
2018-10-28 00:42:12,429 - Epoch: [108][  150/  391]    Overall Loss 0.204287    Objective Loss 0.204287    Top1 92.875000    Top5 99.885417    LR 0.030000    Time 0.022606    
2018-10-28 00:42:13,542 - Epoch: [108][  200/  391]    Overall Loss 0.206447    Objective Loss 0.206447    Top1 92.820312    Top5 99.875000    LR 0.030000    Time 0.022511    
2018-10-28 00:42:14,655 - Epoch: [108][  250/  391]    Overall Loss 0.207629    Objective Loss 0.207629    Top1 92.765625    Top5 99.850000    LR 0.030000    Time 0.022453    
2018-10-28 00:42:15,766 - Epoch: [108][  300/  391]    Overall Loss 0.210277    Objective Loss 0.210277    Top1 92.664062    Top5 99.859375    LR 0.030000    Time 0.022413    
2018-10-28 00:42:16,880 - Epoch: [108][  350/  391]    Overall Loss 0.209453    Objective Loss 0.209453    Top1 92.687500    Top5 99.861607    LR 0.030000    Time 0.022388    
2018-10-28 00:42:17,873 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.53113 | -0.00060 |    0.27361 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.16887 | -0.00566 |    0.05486 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.16811 | -0.00388 |    0.06058 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.16560 | -0.01085 |    0.07146 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.12568 | -0.00500 |    0.03680 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.18138 | -0.00693 |    0.07325 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.12324 |  0.00227 |    0.03367 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.16693 | -0.00799 |    0.08029 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.15171 | -0.00689 |    0.08719 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.23199 | -0.01028 |    0.11606 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.12033 | -0.00597 |    0.04818 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.09896 |  0.00157 |    0.03594 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.12360 | -0.00904 |    0.05446 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10119 | -0.00142 |    0.04581 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.10744 | -0.00390 |    0.04171 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.10976 | -0.00579 |    0.05521 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.12588 | -0.00414 |    0.04578 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.10110 | -0.00516 |    0.04195 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.08041 | -0.00035 |    0.03617 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05771 | -0.00136 |    0.01871 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04420 | -0.00079 |    0.01079 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56085 | -0.03711 |    0.33113 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:42:17,873 - Total sparsity: 74.94

2018-10-28 00:42:17,873 - --- validate (epoch=108)-----------
2018-10-28 00:42:17,873 - 10000 samples (128 per mini-batch)
2018-10-28 00:42:18,599 - Epoch: [108][   50/   78]    Loss 0.353538    Top1 88.875000    Top5 99.562500    
2018-10-28 00:42:18,993 - ==> Top1: 88.910    Top5: 99.620    Loss: 0.347

2018-10-28 00:42:18,994 - ==> Best Top1: 89.130   On Epoch: 106

2018-10-28 00:42:18,994 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:42:19,005 - 

2018-10-28 00:42:19,005 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:42:20,176 - Epoch: [109][   50/  391]    Overall Loss 0.213465    Objective Loss 0.213465    Top1 92.359375    Top5 99.812500    LR 0.030000    Time 0.023372    
2018-10-28 00:42:21,288 - Epoch: [109][  100/  391]    Overall Loss 0.203671    Objective Loss 0.203671    Top1 92.664062    Top5 99.828125    LR 0.030000    Time 0.022793    
2018-10-28 00:42:22,400 - Epoch: [109][  150/  391]    Overall Loss 0.206309    Objective Loss 0.206309    Top1 92.562500    Top5 99.828125    LR 0.030000    Time 0.022600    
2018-10-28 00:42:23,513 - Epoch: [109][  200/  391]    Overall Loss 0.205780    Objective Loss 0.205780    Top1 92.578125    Top5 99.847656    LR 0.030000    Time 0.022491    
2018-10-28 00:42:24,624 - Epoch: [109][  250/  391]    Overall Loss 0.208777    Objective Loss 0.208777    Top1 92.515625    Top5 99.853125    LR 0.030000    Time 0.022433    
2018-10-28 00:42:25,735 - Epoch: [109][  300/  391]    Overall Loss 0.207694    Objective Loss 0.207694    Top1 92.585938    Top5 99.872396    LR 0.030000    Time 0.022391    
2018-10-28 00:42:26,945 - Epoch: [109][  350/  391]    Overall Loss 0.208045    Objective Loss 0.208045    Top1 92.609375    Top5 99.872768    LR 0.030000    Time 0.022646    
2018-10-28 00:42:28,024 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.52597 |  0.00061 |    0.27064 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.16720 | -0.00538 |    0.05424 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.16642 | -0.00411 |    0.05997 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.16399 | -0.01045 |    0.07072 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.12442 | -0.00507 |    0.03640 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.17958 | -0.00671 |    0.07243 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.12202 |  0.00209 |    0.03337 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.16529 | -0.00779 |    0.07947 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.15023 | -0.00671 |    0.08633 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.22962 | -0.00983 |    0.11471 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.11916 | -0.00582 |    0.04771 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.09799 |  0.00152 |    0.03556 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.12243 | -0.00881 |    0.05393 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.10022 | -0.00148 |    0.04535 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.10637 | -0.00386 |    0.04129 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.10868 | -0.00580 |    0.05466 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.12459 | -0.00404 |    0.04535 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.10012 | -0.00517 |    0.04151 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.07964 | -0.00036 |    0.03582 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05717 | -0.00139 |    0.01853 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04377 | -0.00075 |    0.01067 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56176 | -0.03694 |    0.33173 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:42:28,024 - Total sparsity: 74.94

2018-10-28 00:42:28,025 - --- validate (epoch=109)-----------
2018-10-28 00:42:28,025 - 10000 samples (128 per mini-batch)
2018-10-28 00:42:28,752 - Epoch: [109][   50/   78]    Loss 0.344469    Top1 89.000000    Top5 99.562500    
2018-10-28 00:42:29,151 - ==> Top1: 89.020    Top5: 99.610    Loss: 0.342

2018-10-28 00:42:29,152 - ==> Best Top1: 89.130   On Epoch: 106

2018-10-28 00:42:29,152 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:42:29,163 - 

2018-10-28 00:42:29,163 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:42:30,433 - Epoch: [110][   50/  391]    Overall Loss 0.202481    Objective Loss 0.202481    Top1 93.078125    Top5 99.875000    LR 0.030000    Time 0.025359    
2018-10-28 00:42:31,638 - Epoch: [110][  100/  391]    Overall Loss 0.202112    Objective Loss 0.202112    Top1 93.078125    Top5 99.890625    LR 0.030000    Time 0.024715    
2018-10-28 00:42:32,859 - Epoch: [110][  150/  391]    Overall Loss 0.199260    Objective Loss 0.199260    Top1 93.062500    Top5 99.885417    LR 0.030000    Time 0.024608    
2018-10-28 00:42:34,068 - Epoch: [110][  200/  391]    Overall Loss 0.200349    Objective Loss 0.200349    Top1 92.949219    Top5 99.878906    LR 0.030000    Time 0.024496    
2018-10-28 00:42:35,286 - Epoch: [110][  250/  391]    Overall Loss 0.202151    Objective Loss 0.202151    Top1 92.868750    Top5 99.881250    LR 0.030000    Time 0.024463    
2018-10-28 00:42:36,473 - Epoch: [110][  300/  391]    Overall Loss 0.204377    Objective Loss 0.204377    Top1 92.776042    Top5 99.875000    LR 0.030000    Time 0.024338    
2018-10-28 00:42:37,582 - Epoch: [110][  350/  391]    Overall Loss 0.206220    Objective Loss 0.206220    Top1 92.736607    Top5 99.872768    LR 0.030000    Time 0.024024    
2018-10-28 00:42:38,567 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.52092 |  0.00014 |    0.26793 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.16552 | -0.00548 |    0.05360 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.16474 | -0.00423 |    0.05946 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.16243 | -0.01004 |    0.07001 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.12319 | -0.00491 |    0.03601 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.17777 | -0.00711 |    0.07160 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.12080 |  0.00230 |    0.03298 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.16368 | -0.00758 |    0.07879 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.14876 | -0.00670 |    0.08546 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.22725 | -0.00944 |    0.11337 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.11801 | -0.00570 |    0.04728 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.09704 |  0.00150 |    0.03521 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.12128 | -0.00871 |    0.05346 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.09927 | -0.00150 |    0.04491 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.10533 | -0.00380 |    0.04090 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.10763 | -0.00578 |    0.05412 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.12331 | -0.00410 |    0.04488 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.09918 | -0.00502 |    0.04109 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.07889 | -0.00038 |    0.03550 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05666 | -0.00136 |    0.01834 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04334 | -0.00073 |    0.01055 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56252 | -0.03660 |    0.33199 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:42:38,567 - Total sparsity: 74.94

2018-10-28 00:42:38,568 - --- validate (epoch=110)-----------
2018-10-28 00:42:38,568 - 10000 samples (128 per mini-batch)
2018-10-28 00:42:39,288 - Epoch: [110][   50/   78]    Loss 0.358080    Top1 89.140625    Top5 99.484375    
2018-10-28 00:42:39,680 - ==> Top1: 89.190    Top5: 99.550    Loss: 0.352

2018-10-28 00:42:39,680 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:42:39,680 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:42:39,694 - 

2018-10-28 00:42:39,694 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:42:40,863 - Epoch: [111][   50/  391]    Overall Loss 0.203846    Objective Loss 0.203846    Top1 92.875000    Top5 99.843750    LR 0.030000    Time 0.023333    
2018-10-28 00:42:41,972 - Epoch: [111][  100/  391]    Overall Loss 0.205936    Objective Loss 0.205936    Top1 92.796875    Top5 99.859375    LR 0.030000    Time 0.022749    
2018-10-28 00:42:43,082 - Epoch: [111][  150/  391]    Overall Loss 0.204518    Objective Loss 0.204518    Top1 92.833333    Top5 99.843750    LR 0.030000    Time 0.022559    
2018-10-28 00:42:44,193 - Epoch: [111][  200/  391]    Overall Loss 0.202542    Objective Loss 0.202542    Top1 92.851562    Top5 99.855469    LR 0.030000    Time 0.022464    
2018-10-28 00:42:45,303 - Epoch: [111][  250/  391]    Overall Loss 0.201855    Objective Loss 0.201855    Top1 92.862500    Top5 99.865625    LR 0.030000    Time 0.022409    
2018-10-28 00:42:46,413 - Epoch: [111][  300/  391]    Overall Loss 0.204218    Objective Loss 0.204218    Top1 92.770833    Top5 99.856771    LR 0.030000    Time 0.022370    
2018-10-28 00:42:47,524 - Epoch: [111][  350/  391]    Overall Loss 0.202460    Objective Loss 0.202460    Top1 92.850446    Top5 99.870536    LR 0.030000    Time 0.022343    
2018-10-28 00:42:48,514 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.51606 |  0.00080 |    0.26498 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.16390 | -0.00536 |    0.05301 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.16312 | -0.00422 |    0.05879 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.16090 | -0.00988 |    0.06945 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.12198 | -0.00463 |    0.03562 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.17603 | -0.00703 |    0.07089 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.11960 |  0.00244 |    0.03268 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.16211 | -0.00742 |    0.07806 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.14733 | -0.00666 |    0.08464 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.22493 | -0.00901 |    0.11228 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.11690 | -0.00550 |    0.04686 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.09611 |  0.00148 |    0.03489 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.12015 | -0.00857 |    0.05296 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.09834 | -0.00152 |    0.04449 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.10430 | -0.00376 |    0.04049 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.10660 | -0.00568 |    0.05358 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.12206 | -0.00409 |    0.04435 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.09825 | -0.00498 |    0.04068 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.07816 | -0.00035 |    0.03518 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05616 | -0.00137 |    0.01818 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04293 | -0.00069 |    0.01045 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56296 | -0.03626 |    0.33211 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:42:48,514 - Total sparsity: 74.94

2018-10-28 00:42:48,514 - --- validate (epoch=111)-----------
2018-10-28 00:42:48,514 - 10000 samples (128 per mini-batch)
2018-10-28 00:42:49,230 - Epoch: [111][   50/   78]    Loss 0.355877    Top1 88.937500    Top5 99.546875    
2018-10-28 00:42:49,620 - ==> Top1: 89.060    Top5: 99.600    Loss: 0.347

2018-10-28 00:42:49,621 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:42:49,621 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:42:49,632 - 

2018-10-28 00:42:49,632 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:42:50,801 - Epoch: [112][   50/  391]    Overall Loss 0.203806    Objective Loss 0.203806    Top1 92.812500    Top5 99.890625    LR 0.030000    Time 0.023338    
2018-10-28 00:42:51,911 - Epoch: [112][  100/  391]    Overall Loss 0.200131    Objective Loss 0.200131    Top1 92.906250    Top5 99.875000    LR 0.030000    Time 0.022759    
2018-10-28 00:42:53,022 - Epoch: [112][  150/  391]    Overall Loss 0.200108    Objective Loss 0.200108    Top1 92.885417    Top5 99.869792    LR 0.030000    Time 0.022569    
2018-10-28 00:42:54,132 - Epoch: [112][  200/  391]    Overall Loss 0.200094    Objective Loss 0.200094    Top1 92.917969    Top5 99.875000    LR 0.030000    Time 0.022468    
2018-10-28 00:42:55,242 - Epoch: [112][  250/  391]    Overall Loss 0.201597    Objective Loss 0.201597    Top1 92.921875    Top5 99.862500    LR 0.030000    Time 0.022409    
2018-10-28 00:42:56,351 - Epoch: [112][  300/  391]    Overall Loss 0.202298    Objective Loss 0.202298    Top1 92.929688    Top5 99.859375    LR 0.030000    Time 0.022367    
2018-10-28 00:42:57,476 - Epoch: [112][  350/  391]    Overall Loss 0.201096    Objective Loss 0.201096    Top1 92.955357    Top5 99.875000    LR 0.030000    Time 0.022373    
2018-10-28 00:42:58,480 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.51108 | -0.00070 |    0.26248 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.16230 | -0.00524 |    0.05263 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.16151 | -0.00385 |    0.05830 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.15936 | -0.00976 |    0.06879 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.12079 | -0.00453 |    0.03529 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.17432 | -0.00670 |    0.07018 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.11844 |  0.00219 |    0.03244 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.16056 | -0.00723 |    0.07726 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.14593 | -0.00656 |    0.08386 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.22262 | -0.00934 |    0.11120 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.11580 | -0.00547 |    0.04639 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.09521 |  0.00152 |    0.03455 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.11904 | -0.00860 |    0.05245 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.09743 | -0.00133 |    0.04406 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.10329 | -0.00368 |    0.04010 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.10559 | -0.00568 |    0.05308 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.12083 | -0.00393 |    0.04390 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.09734 | -0.00497 |    0.04028 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.07743 | -0.00038 |    0.03485 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05567 | -0.00136 |    0.01801 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04252 | -0.00064 |    0.01034 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56336 | -0.03602 |    0.33237 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:42:58,480 - Total sparsity: 74.94

2018-10-28 00:42:58,480 - --- validate (epoch=112)-----------
2018-10-28 00:42:58,480 - 10000 samples (128 per mini-batch)
2018-10-28 00:42:59,217 - Epoch: [112][   50/   78]    Loss 0.357889    Top1 89.187500    Top5 99.468750    
2018-10-28 00:42:59,623 - ==> Top1: 89.070    Top5: 99.580    Loss: 0.349

2018-10-28 00:42:59,624 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:42:59,624 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:42:59,636 - 

2018-10-28 00:42:59,637 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:43:00,840 - Epoch: [113][   50/  391]    Overall Loss 0.194123    Objective Loss 0.194123    Top1 93.015625    Top5 99.984375    LR 0.030000    Time 0.024028    
2018-10-28 00:43:01,949 - Epoch: [113][  100/  391]    Overall Loss 0.195563    Objective Loss 0.195563    Top1 92.921875    Top5 99.976562    LR 0.030000    Time 0.023096    
2018-10-28 00:43:03,059 - Epoch: [113][  150/  391]    Overall Loss 0.195252    Objective Loss 0.195252    Top1 92.963542    Top5 99.947917    LR 0.030000    Time 0.022786    
2018-10-28 00:43:04,168 - Epoch: [113][  200/  391]    Overall Loss 0.195583    Objective Loss 0.195583    Top1 93.062500    Top5 99.937500    LR 0.030000    Time 0.022629    
2018-10-28 00:43:05,276 - Epoch: [113][  250/  391]    Overall Loss 0.197413    Objective Loss 0.197413    Top1 93.059375    Top5 99.915625    LR 0.030000    Time 0.022531    
2018-10-28 00:43:06,384 - Epoch: [113][  300/  391]    Overall Loss 0.199819    Objective Loss 0.199819    Top1 93.013021    Top5 99.903646    LR 0.030000    Time 0.022464    
2018-10-28 00:43:07,495 - Epoch: [113][  350/  391]    Overall Loss 0.199032    Objective Loss 0.199032    Top1 93.062500    Top5 99.895089    LR 0.030000    Time 0.022424    
2018-10-28 00:43:08,485 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.50634 |  0.00067 |    0.26042 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.16076 | -0.00538 |    0.05196 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.15995 | -0.00344 |    0.05773 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.15788 | -0.00970 |    0.06818 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.11965 | -0.00440 |    0.03499 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.17270 | -0.00647 |    0.06958 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.11732 |  0.00237 |    0.03213 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.15905 | -0.00733 |    0.07644 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.14457 | -0.00674 |    0.08305 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.22036 | -0.00958 |    0.11021 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.11476 | -0.00519 |    0.04595 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.09433 |  0.00143 |    0.03417 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.11799 | -0.00839 |    0.05205 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.09655 | -0.00143 |    0.04364 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.10232 | -0.00366 |    0.03969 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.10462 | -0.00552 |    0.05258 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.11963 | -0.00387 |    0.04336 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.09647 | -0.00481 |    0.03991 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.07675 | -0.00039 |    0.03456 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05521 | -0.00128 |    0.01785 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04212 | -0.00062 |    0.01025 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56335 | -0.03566 |    0.33228 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:43:08,485 - Total sparsity: 74.94

2018-10-28 00:43:08,485 - --- validate (epoch=113)-----------
2018-10-28 00:43:08,485 - 10000 samples (128 per mini-batch)
2018-10-28 00:43:09,209 - Epoch: [113][   50/   78]    Loss 0.361005    Top1 88.515625    Top5 99.406250    
2018-10-28 00:43:09,601 - ==> Top1: 88.680    Top5: 99.520    Loss: 0.350

2018-10-28 00:43:09,602 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:43:09,602 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:43:09,613 - 

2018-10-28 00:43:09,613 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:43:10,779 - Epoch: [114][   50/  391]    Overall Loss 0.194636    Objective Loss 0.194636    Top1 93.062500    Top5 99.843750    LR 0.030000    Time 0.023275    
2018-10-28 00:43:11,887 - Epoch: [114][  100/  391]    Overall Loss 0.191459    Objective Loss 0.191459    Top1 93.304688    Top5 99.851562    LR 0.030000    Time 0.022712    
2018-10-28 00:43:12,998 - Epoch: [114][  150/  391]    Overall Loss 0.189852    Objective Loss 0.189852    Top1 93.312500    Top5 99.864583    LR 0.030000    Time 0.022535    
2018-10-28 00:43:14,106 - Epoch: [114][  200/  391]    Overall Loss 0.192798    Objective Loss 0.192798    Top1 93.230469    Top5 99.863281    LR 0.030000    Time 0.022436    
2018-10-28 00:43:15,215 - Epoch: [114][  250/  391]    Overall Loss 0.194397    Objective Loss 0.194397    Top1 93.175000    Top5 99.853125    LR 0.030000    Time 0.022381    
2018-10-28 00:43:16,324 - Epoch: [114][  300/  391]    Overall Loss 0.195226    Objective Loss 0.195226    Top1 93.138021    Top5 99.867188    LR 0.030000    Time 0.022344    
2018-10-28 00:43:17,435 - Epoch: [114][  350/  391]    Overall Loss 0.194241    Objective Loss 0.194241    Top1 93.147321    Top5 99.866071    LR 0.030000    Time 0.022320    
2018-10-28 00:43:18,423 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.50185 |  0.00071 |    0.25797 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.15925 | -0.00523 |    0.05152 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.15840 | -0.00348 |    0.05712 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.15644 | -0.00968 |    0.06767 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.11852 | -0.00424 |    0.03468 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.17108 | -0.00669 |    0.06884 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.11621 |  0.00227 |    0.03178 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.15759 | -0.00709 |    0.07580 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.14324 | -0.00653 |    0.08228 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.21817 | -0.00898 |    0.10886 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.11371 | -0.00518 |    0.04555 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.09347 |  0.00149 |    0.03385 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.11694 | -0.00827 |    0.05154 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.09569 | -0.00139 |    0.04329 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.10135 | -0.00364 |    0.03929 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.10365 | -0.00548 |    0.05210 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.11845 | -0.00377 |    0.04287 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.09560 | -0.00482 |    0.03955 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.07607 | -0.00036 |    0.03425 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05475 | -0.00125 |    0.01770 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04173 | -0.00060 |    0.01015 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56487 | -0.03548 |    0.33332 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:43:18,423 - Total sparsity: 74.94

2018-10-28 00:43:18,423 - --- validate (epoch=114)-----------
2018-10-28 00:43:18,423 - 10000 samples (128 per mini-batch)
2018-10-28 00:43:19,139 - Epoch: [114][   50/   78]    Loss 0.366106    Top1 88.906250    Top5 99.484375    
2018-10-28 00:43:19,526 - ==> Top1: 88.710    Top5: 99.570    Loss: 0.360

2018-10-28 00:43:19,527 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:43:19,527 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:43:19,539 - 

2018-10-28 00:43:19,539 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:43:20,707 - Epoch: [115][   50/  391]    Overall Loss 0.183754    Objective Loss 0.183754    Top1 93.328125    Top5 99.953125    LR 0.030000    Time 0.023334    
2018-10-28 00:43:21,818 - Epoch: [115][  100/  391]    Overall Loss 0.182353    Objective Loss 0.182353    Top1 93.617188    Top5 99.914062    LR 0.030000    Time 0.022759    
2018-10-28 00:43:22,928 - Epoch: [115][  150/  391]    Overall Loss 0.187136    Objective Loss 0.187136    Top1 93.364583    Top5 99.901042    LR 0.030000    Time 0.022561    
2018-10-28 00:43:24,038 - Epoch: [115][  200/  391]    Overall Loss 0.189340    Objective Loss 0.189340    Top1 93.277344    Top5 99.906250    LR 0.030000    Time 0.022467    
2018-10-28 00:43:25,149 - Epoch: [115][  250/  391]    Overall Loss 0.192921    Objective Loss 0.192921    Top1 93.162500    Top5 99.906250    LR 0.030000    Time 0.022412    
2018-10-28 00:43:26,260 - Epoch: [115][  300/  391]    Overall Loss 0.195522    Objective Loss 0.195522    Top1 93.098958    Top5 99.911458    LR 0.030000    Time 0.022376    
2018-10-28 00:43:27,371 - Epoch: [115][  350/  391]    Overall Loss 0.195845    Objective Loss 0.195845    Top1 93.084821    Top5 99.910714    LR 0.030000    Time 0.022350    
2018-10-28 00:43:28,367 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.49744 |  0.00007 |    0.25524 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.15778 | -0.00521 |    0.05085 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.15689 | -0.00353 |    0.05653 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.15504 | -0.00960 |    0.06712 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.11741 | -0.00443 |    0.03436 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.16951 | -0.00683 |    0.06819 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.11513 |  0.00234 |    0.03151 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.15615 | -0.00704 |    0.07517 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.14194 | -0.00661 |    0.08151 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.21602 | -0.00842 |    0.10774 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.11271 | -0.00509 |    0.04513 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.09264 |  0.00153 |    0.03357 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.11594 | -0.00815 |    0.05109 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.09487 | -0.00150 |    0.04293 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.10042 | -0.00369 |    0.03894 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.10273 | -0.00542 |    0.05164 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.11730 | -0.00370 |    0.04248 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.09478 | -0.00481 |    0.03920 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.07543 | -0.00035 |    0.03396 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05433 | -0.00128 |    0.01756 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04136 | -0.00057 |    0.01005 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56546 | -0.03531 |    0.33349 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:43:28,367 - Total sparsity: 74.94

2018-10-28 00:43:28,367 - --- validate (epoch=115)-----------
2018-10-28 00:43:28,367 - 10000 samples (128 per mini-batch)
2018-10-28 00:43:29,087 - Epoch: [115][   50/   78]    Loss 0.371643    Top1 88.765625    Top5 99.468750    
2018-10-28 00:43:29,479 - ==> Top1: 88.820    Top5: 99.550    Loss: 0.363

2018-10-28 00:43:29,480 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:43:29,480 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:43:29,489 - 

2018-10-28 00:43:29,490 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:43:30,661 - Epoch: [116][   50/  391]    Overall Loss 0.189966    Objective Loss 0.189966    Top1 93.343750    Top5 99.968750    LR 0.030000    Time 0.023393    
2018-10-28 00:43:31,772 - Epoch: [116][  100/  391]    Overall Loss 0.187924    Objective Loss 0.187924    Top1 93.585938    Top5 99.937500    LR 0.030000    Time 0.022791    
2018-10-28 00:43:32,883 - Epoch: [116][  150/  391]    Overall Loss 0.189328    Objective Loss 0.189328    Top1 93.557292    Top5 99.921875    LR 0.030000    Time 0.022592    
2018-10-28 00:43:33,994 - Epoch: [116][  200/  391]    Overall Loss 0.191298    Objective Loss 0.191298    Top1 93.421875    Top5 99.925781    LR 0.030000    Time 0.022492    
2018-10-28 00:43:35,103 - Epoch: [116][  250/  391]    Overall Loss 0.190848    Objective Loss 0.190848    Top1 93.400000    Top5 99.921875    LR 0.030000    Time 0.022424    
2018-10-28 00:43:36,212 - Epoch: [116][  300/  391]    Overall Loss 0.193419    Objective Loss 0.193419    Top1 93.252604    Top5 99.908854    LR 0.030000    Time 0.022380    
2018-10-28 00:43:37,322 - Epoch: [116][  350/  391]    Overall Loss 0.194732    Objective Loss 0.194732    Top1 93.241071    Top5 99.910714    LR 0.030000    Time 0.022353    
2018-10-28 00:43:38,309 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.49295 |  0.00135 |    0.25276 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.15634 | -0.00514 |    0.05048 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.15541 | -0.00349 |    0.05608 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.15369 | -0.00951 |    0.06646 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.11634 | -0.00429 |    0.03410 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.16793 | -0.00673 |    0.06734 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.11406 |  0.00239 |    0.03123 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.15474 | -0.00682 |    0.07442 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.14069 | -0.00624 |    0.08079 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.21385 | -0.00870 |    0.10635 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.11173 | -0.00493 |    0.04467 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.09183 |  0.00149 |    0.03329 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.11497 | -0.00797 |    0.05054 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.09407 | -0.00132 |    0.04259 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.09950 | -0.00370 |    0.03862 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.10183 | -0.00530 |    0.05118 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.11615 | -0.00364 |    0.04207 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.09398 | -0.00470 |    0.03887 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.07479 | -0.00034 |    0.03366 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05391 | -0.00125 |    0.01742 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04099 | -0.00053 |    0.00996 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56567 | -0.03534 |    0.33358 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:43:38,309 - Total sparsity: 74.94

2018-10-28 00:43:38,309 - --- validate (epoch=116)-----------
2018-10-28 00:43:38,309 - 10000 samples (128 per mini-batch)
2018-10-28 00:43:39,037 - Epoch: [116][   50/   78]    Loss 0.359406    Top1 88.750000    Top5 99.593750    
2018-10-28 00:43:39,433 - ==> Top1: 88.830    Top5: 99.650    Loss: 0.350

2018-10-28 00:43:39,433 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:43:39,433 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:43:39,445 - 

2018-10-28 00:43:39,445 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:43:40,612 - Epoch: [117][   50/  391]    Overall Loss 0.190845    Objective Loss 0.190845    Top1 93.109375    Top5 99.890625    LR 0.030000    Time 0.023308    
2018-10-28 00:43:41,723 - Epoch: [117][  100/  391]    Overall Loss 0.188959    Objective Loss 0.188959    Top1 93.304688    Top5 99.875000    LR 0.030000    Time 0.022747    
2018-10-28 00:43:42,832 - Epoch: [117][  150/  391]    Overall Loss 0.193934    Objective Loss 0.193934    Top1 93.187500    Top5 99.885417    LR 0.030000    Time 0.022554    
2018-10-28 00:43:43,943 - Epoch: [117][  200/  391]    Overall Loss 0.191890    Objective Loss 0.191890    Top1 93.242188    Top5 99.906250    LR 0.030000    Time 0.022460    
2018-10-28 00:43:45,053 - Epoch: [117][  250/  391]    Overall Loss 0.193349    Objective Loss 0.193349    Top1 93.184375    Top5 99.912500    LR 0.030000    Time 0.022406    
2018-10-28 00:43:46,164 - Epoch: [117][  300/  391]    Overall Loss 0.192262    Objective Loss 0.192262    Top1 93.247396    Top5 99.914062    LR 0.030000    Time 0.022368    
2018-10-28 00:43:47,275 - Epoch: [117][  350/  391]    Overall Loss 0.194250    Objective Loss 0.194250    Top1 93.142857    Top5 99.915179    LR 0.030000    Time 0.022343    
2018-10-28 00:43:48,264 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.48870 | -0.00007 |    0.25012 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.15495 | -0.00474 |    0.05011 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.15399 | -0.00334 |    0.05558 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.15237 | -0.00942 |    0.06600 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.11530 | -0.00435 |    0.03369 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.16644 | -0.00651 |    0.06670 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.11301 |  0.00238 |    0.03102 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.15337 | -0.00699 |    0.07368 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.13947 | -0.00627 |    0.08003 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.21176 | -0.00839 |    0.10538 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.11079 | -0.00480 |    0.04428 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.09104 |  0.00145 |    0.03299 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.11403 | -0.00789 |    0.05009 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.09330 | -0.00126 |    0.04218 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.09861 | -0.00363 |    0.03830 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.10096 | -0.00530 |    0.05071 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.11504 | -0.00366 |    0.04165 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.09319 | -0.00467 |    0.03852 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.07418 | -0.00033 |    0.03338 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05351 | -0.00129 |    0.01730 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04064 | -0.00050 |    0.00986 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56543 | -0.03516 |    0.33343 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:43:48,264 - Total sparsity: 74.94

2018-10-28 00:43:48,264 - --- validate (epoch=117)-----------
2018-10-28 00:43:48,264 - 10000 samples (128 per mini-batch)
2018-10-28 00:43:48,978 - Epoch: [117][   50/   78]    Loss 0.366497    Top1 88.937500    Top5 99.500000    
2018-10-28 00:43:49,369 - ==> Top1: 89.010    Top5: 99.570    Loss: 0.357

2018-10-28 00:43:49,370 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:43:49,370 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:43:49,380 - 

2018-10-28 00:43:49,381 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:43:50,549 - Epoch: [118][   50/  391]    Overall Loss 0.180532    Objective Loss 0.180532    Top1 93.812500    Top5 99.890625    LR 0.030000    Time 0.023329    
2018-10-28 00:43:51,658 - Epoch: [118][  100/  391]    Overall Loss 0.185316    Objective Loss 0.185316    Top1 93.562500    Top5 99.898438    LR 0.030000    Time 0.022744    
2018-10-28 00:43:52,767 - Epoch: [118][  150/  391]    Overall Loss 0.188379    Objective Loss 0.188379    Top1 93.505208    Top5 99.885417    LR 0.030000    Time 0.022547    
2018-10-28 00:43:53,878 - Epoch: [118][  200/  391]    Overall Loss 0.188039    Objective Loss 0.188039    Top1 93.457031    Top5 99.886719    LR 0.030000    Time 0.022456    
2018-10-28 00:43:54,986 - Epoch: [118][  250/  391]    Overall Loss 0.189209    Objective Loss 0.189209    Top1 93.337500    Top5 99.890625    LR 0.030000    Time 0.022393    
2018-10-28 00:43:56,095 - Epoch: [118][  300/  391]    Overall Loss 0.190432    Objective Loss 0.190432    Top1 93.265625    Top5 99.885417    LR 0.030000    Time 0.022353    
2018-10-28 00:43:57,203 - Epoch: [118][  350/  391]    Overall Loss 0.190016    Objective Loss 0.190016    Top1 93.290179    Top5 99.881696    LR 0.030000    Time 0.022322    
2018-10-28 00:43:58,190 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.48441 |  0.00014 |    0.24814 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.15357 | -0.00474 |    0.04955 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.15258 | -0.00345 |    0.05500 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.15105 | -0.00971 |    0.06533 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.11428 | -0.00406 |    0.03333 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.16494 | -0.00664 |    0.06612 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.11199 |  0.00237 |    0.03068 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.15203 | -0.00688 |    0.07309 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.13827 | -0.00633 |    0.07929 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.20973 | -0.00764 |    0.10450 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.10987 | -0.00478 |    0.04385 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.09028 |  0.00154 |    0.03272 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.11312 | -0.00781 |    0.04975 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.09254 | -0.00125 |    0.04180 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.09774 | -0.00353 |    0.03797 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.10010 | -0.00523 |    0.05029 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.11395 | -0.00372 |    0.04135 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.09243 | -0.00463 |    0.03821 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.07359 | -0.00033 |    0.03316 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05313 | -0.00125 |    0.01717 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.04029 | -0.00047 |    0.00978 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56623 | -0.03510 |    0.33378 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:43:58,190 - Total sparsity: 74.94

2018-10-28 00:43:58,190 - --- validate (epoch=118)-----------
2018-10-28 00:43:58,191 - 10000 samples (128 per mini-batch)
2018-10-28 00:43:58,913 - Epoch: [118][   50/   78]    Loss 0.368325    Top1 89.250000    Top5 99.500000    
2018-10-28 00:43:59,307 - ==> Top1: 89.100    Top5: 99.590    Loss: 0.362

2018-10-28 00:43:59,308 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:43:59,308 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:43:59,319 - 

2018-10-28 00:43:59,319 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:44:00,490 - Epoch: [119][   50/  391]    Overall Loss 0.182854    Objective Loss 0.182854    Top1 93.734375    Top5 99.937500    LR 0.030000    Time 0.023380    
2018-10-28 00:44:01,598 - Epoch: [119][  100/  391]    Overall Loss 0.185844    Objective Loss 0.185844    Top1 93.609375    Top5 99.945312    LR 0.030000    Time 0.022757    
2018-10-28 00:44:02,705 - Epoch: [119][  150/  391]    Overall Loss 0.186079    Objective Loss 0.186079    Top1 93.458333    Top5 99.942708    LR 0.030000    Time 0.022546    
2018-10-28 00:44:03,816 - Epoch: [119][  200/  391]    Overall Loss 0.189817    Objective Loss 0.189817    Top1 93.320312    Top5 99.917969    LR 0.030000    Time 0.022440    
2018-10-28 00:44:04,928 - Epoch: [119][  250/  391]    Overall Loss 0.189842    Objective Loss 0.189842    Top1 93.337500    Top5 99.925000    LR 0.030000    Time 0.022394    
2018-10-28 00:44:06,039 - Epoch: [119][  300/  391]    Overall Loss 0.189963    Objective Loss 0.189963    Top1 93.312500    Top5 99.924479    LR 0.030000    Time 0.022361    
2018-10-28 00:44:07,149 - Epoch: [119][  350/  391]    Overall Loss 0.191019    Objective Loss 0.191019    Top1 93.274554    Top5 99.917411    LR 0.030000    Time 0.022335    
2018-10-28 00:44:08,137 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.48028 |  0.00071 |    0.24594 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.15223 | -0.00473 |    0.04903 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.15121 | -0.00331 |    0.05440 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.14983 | -0.00906 |    0.06493 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.11327 | -0.00433 |    0.03309 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.16346 | -0.00691 |    0.06541 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.11099 |  0.00221 |    0.03033 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.15074 | -0.00653 |    0.07247 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.13710 | -0.00598 |    0.07852 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.20774 | -0.00691 |    0.10341 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.10896 | -0.00481 |    0.04353 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.08953 |  0.00153 |    0.03246 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.11222 | -0.00767 |    0.04943 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.09180 | -0.00125 |    0.04148 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.09689 | -0.00348 |    0.03762 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.09927 | -0.00510 |    0.04982 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.11287 | -0.00375 |    0.04097 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.09170 | -0.00447 |    0.03788 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.07302 | -0.00037 |    0.03289 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05277 | -0.00122 |    0.01704 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03996 | -0.00045 |    0.00970 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56627 | -0.03487 |    0.33368 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:44:08,138 - Total sparsity: 74.94

2018-10-28 00:44:08,138 - --- validate (epoch=119)-----------
2018-10-28 00:44:08,138 - 10000 samples (128 per mini-batch)
2018-10-28 00:44:08,857 - Epoch: [119][   50/   78]    Loss 0.377740    Top1 88.796875    Top5 99.500000    
2018-10-28 00:44:09,241 - ==> Top1: 88.650    Top5: 99.590    Loss: 0.371

2018-10-28 00:44:09,242 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:44:09,242 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:44:09,252 - 

2018-10-28 00:44:09,252 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:44:10,420 - Epoch: [120][   50/  391]    Overall Loss 0.177712    Objective Loss 0.177712    Top1 93.671875    Top5 99.937500    LR 0.030000    Time 0.023312    
2018-10-28 00:44:11,531 - Epoch: [120][  100/  391]    Overall Loss 0.185966    Objective Loss 0.185966    Top1 93.351562    Top5 99.914062    LR 0.030000    Time 0.022759    
2018-10-28 00:44:12,646 - Epoch: [120][  150/  391]    Overall Loss 0.184527    Objective Loss 0.184527    Top1 93.468750    Top5 99.895833    LR 0.030000    Time 0.022592    
2018-10-28 00:44:13,758 - Epoch: [120][  200/  391]    Overall Loss 0.184984    Objective Loss 0.184984    Top1 93.425781    Top5 99.886719    LR 0.030000    Time 0.022500    
2018-10-28 00:44:14,871 - Epoch: [120][  250/  391]    Overall Loss 0.185283    Objective Loss 0.185283    Top1 93.368750    Top5 99.896875    LR 0.030000    Time 0.022432    
2018-10-28 00:44:15,984 - Epoch: [120][  300/  391]    Overall Loss 0.187744    Objective Loss 0.187744    Top1 93.268229    Top5 99.901042    LR 0.030000    Time 0.022398    
2018-10-28 00:44:17,097 - Epoch: [120][  350/  391]    Overall Loss 0.187854    Objective Loss 0.187854    Top1 93.279018    Top5 99.906250    LR 0.030000    Time 0.022375    
2018-10-28 00:44:18,091 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.47641 |  0.00208 |    0.24350 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.15093 | -0.00440 |    0.04868 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.14989 | -0.00322 |    0.05384 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.14859 | -0.00911 |    0.06430 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.11229 | -0.00423 |    0.03288 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.16205 | -0.00691 |    0.06481 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.11003 |  0.00214 |    0.02998 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.14947 | -0.00645 |    0.07168 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.13598 | -0.00587 |    0.07794 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.20577 | -0.00716 |    0.10230 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.10809 | -0.00470 |    0.04320 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.08882 |  0.00153 |    0.03216 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.11138 | -0.00743 |    0.04895 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.09109 | -0.00121 |    0.04120 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.09607 | -0.00343 |    0.03726 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.09847 | -0.00498 |    0.04940 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.11184 | -0.00354 |    0.04049 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.09099 | -0.00434 |    0.03759 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.07248 | -0.00043 |    0.03265 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05243 | -0.00120 |    0.01693 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03964 | -0.00043 |    0.00962 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56698 | -0.03470 |    0.33396 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:44:18,091 - Total sparsity: 74.94

2018-10-28 00:44:18,091 - --- validate (epoch=120)-----------
2018-10-28 00:44:18,091 - 10000 samples (128 per mini-batch)
2018-10-28 00:44:18,818 - Epoch: [120][   50/   78]    Loss 0.365313    Top1 89.000000    Top5 99.578125    
2018-10-28 00:44:19,208 - ==> Top1: 88.750    Top5: 99.660    Loss: 0.360

2018-10-28 00:44:19,208 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:44:19,208 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:44:19,218 - 

2018-10-28 00:44:19,218 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:44:20,387 - Epoch: [121][   50/  391]    Overall Loss 0.193734    Objective Loss 0.193734    Top1 92.921875    Top5 99.890625    LR 0.030000    Time 0.023325    
2018-10-28 00:44:21,497 - Epoch: [121][  100/  391]    Overall Loss 0.193700    Objective Loss 0.193700    Top1 93.085938    Top5 99.843750    LR 0.030000    Time 0.022751    
2018-10-28 00:44:22,607 - Epoch: [121][  150/  391]    Overall Loss 0.192765    Objective Loss 0.192765    Top1 93.192708    Top5 99.864583    LR 0.030000    Time 0.022559    
2018-10-28 00:44:23,715 - Epoch: [121][  200/  391]    Overall Loss 0.190200    Objective Loss 0.190200    Top1 93.265625    Top5 99.878906    LR 0.030000    Time 0.022455    
2018-10-28 00:44:24,823 - Epoch: [121][  250/  391]    Overall Loss 0.188597    Objective Loss 0.188597    Top1 93.343750    Top5 99.881250    LR 0.030000    Time 0.022392    
2018-10-28 00:44:25,934 - Epoch: [121][  300/  391]    Overall Loss 0.188966    Objective Loss 0.188966    Top1 93.343750    Top5 99.875000    LR 0.030000    Time 0.022359    
2018-10-28 00:44:27,044 - Epoch: [121][  350/  391]    Overall Loss 0.189248    Objective Loss 0.189248    Top1 93.386161    Top5 99.875000    LR 0.030000    Time 0.022333    
2018-10-28 00:44:28,034 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.47267 |  0.00223 |    0.24217 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.14967 | -0.00477 |    0.04823 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.14859 | -0.00343 |    0.05345 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.14742 | -0.00912 |    0.06374 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.11133 | -0.00417 |    0.03267 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.16072 | -0.00645 |    0.06426 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.10911 |  0.00237 |    0.02976 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.14824 | -0.00656 |    0.07114 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.13488 | -0.00589 |    0.07729 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.20381 | -0.00721 |    0.10164 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.10725 | -0.00468 |    0.04284 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.08813 |  0.00154 |    0.03187 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.11056 | -0.00725 |    0.04863 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.09041 | -0.00130 |    0.04087 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.09527 | -0.00350 |    0.03693 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.09770 | -0.00493 |    0.04901 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.11082 | -0.00351 |    0.04005 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.09031 | -0.00429 |    0.03730 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.07196 | -0.00041 |    0.03242 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05212 | -0.00120 |    0.01683 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03932 | -0.00043 |    0.00953 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56705 | -0.03455 |    0.33409 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:44:28,034 - Total sparsity: 74.94

2018-10-28 00:44:28,034 - --- validate (epoch=121)-----------
2018-10-28 00:44:28,034 - 10000 samples (128 per mini-batch)
2018-10-28 00:44:28,753 - Epoch: [121][   50/   78]    Loss 0.362946    Top1 88.609375    Top5 99.531250    
2018-10-28 00:44:29,142 - ==> Top1: 88.640    Top5: 99.640    Loss: 0.359

2018-10-28 00:44:29,143 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:44:29,143 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:44:29,159 - 

2018-10-28 00:44:29,160 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:44:30,302 - Epoch: [122][   50/  391]    Overall Loss 0.173589    Objective Loss 0.173589    Top1 93.968750    Top5 99.937500    LR 0.030000    Time 0.022819    
2018-10-28 00:44:31,413 - Epoch: [122][  100/  391]    Overall Loss 0.176653    Objective Loss 0.176653    Top1 93.757812    Top5 99.953125    LR 0.030000    Time 0.022502    
2018-10-28 00:44:32,523 - Epoch: [122][  150/  391]    Overall Loss 0.179702    Objective Loss 0.179702    Top1 93.552083    Top5 99.953125    LR 0.030000    Time 0.022394    
2018-10-28 00:44:33,634 - Epoch: [122][  200/  391]    Overall Loss 0.183984    Objective Loss 0.183984    Top1 93.402344    Top5 99.925781    LR 0.030000    Time 0.022344    
2018-10-28 00:44:34,746 - Epoch: [122][  250/  391]    Overall Loss 0.185288    Objective Loss 0.185288    Top1 93.356250    Top5 99.915625    LR 0.030000    Time 0.022317    
2018-10-28 00:44:35,858 - Epoch: [122][  300/  391]    Overall Loss 0.186417    Objective Loss 0.186417    Top1 93.356771    Top5 99.911458    LR 0.030000    Time 0.022300    
2018-10-28 00:44:36,971 - Epoch: [122][  350/  391]    Overall Loss 0.187325    Objective Loss 0.187325    Top1 93.332589    Top5 99.904018    LR 0.030000    Time 0.022291    
2018-10-28 00:44:37,960 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.46884 | -0.00002 |    0.24014 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.14842 | -0.00470 |    0.04789 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.14733 | -0.00323 |    0.05290 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.14627 | -0.00869 |    0.06314 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.11040 | -0.00394 |    0.03234 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.15941 | -0.00625 |    0.06371 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.10819 |  0.00209 |    0.02954 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.14704 | -0.00651 |    0.07048 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.13381 | -0.00584 |    0.07665 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.20190 | -0.00709 |    0.10033 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.10643 | -0.00464 |    0.04252 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.08746 |  0.00136 |    0.03162 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.10974 | -0.00722 |    0.04830 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08975 | -0.00127 |    0.04048 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.09449 | -0.00346 |    0.03658 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.09695 | -0.00480 |    0.04862 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.10983 | -0.00331 |    0.03976 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08965 | -0.00426 |    0.03701 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.07145 | -0.00042 |    0.03219 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05182 | -0.00111 |    0.01672 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03903 | -0.00039 |    0.00946 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56714 | -0.03481 |    0.33426 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:44:37,961 - Total sparsity: 74.94

2018-10-28 00:44:37,961 - --- validate (epoch=122)-----------
2018-10-28 00:44:37,961 - 10000 samples (128 per mini-batch)
2018-10-28 00:44:38,677 - Epoch: [122][   50/   78]    Loss 0.371645    Top1 88.750000    Top5 99.500000    
2018-10-28 00:44:39,064 - ==> Top1: 88.620    Top5: 99.580    Loss: 0.372

2018-10-28 00:44:39,065 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:44:39,065 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:44:39,082 - 

2018-10-28 00:44:39,082 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:44:40,226 - Epoch: [123][   50/  391]    Overall Loss 0.175141    Objective Loss 0.175141    Top1 93.921875    Top5 99.906250    LR 0.030000    Time 0.022847    
2018-10-28 00:44:41,339 - Epoch: [123][  100/  391]    Overall Loss 0.179112    Objective Loss 0.179112    Top1 93.718750    Top5 99.898438    LR 0.030000    Time 0.022535    
2018-10-28 00:44:42,451 - Epoch: [123][  150/  391]    Overall Loss 0.181887    Objective Loss 0.181887    Top1 93.625000    Top5 99.890625    LR 0.030000    Time 0.022429    
2018-10-28 00:44:43,564 - Epoch: [123][  200/  391]    Overall Loss 0.181744    Objective Loss 0.181744    Top1 93.636719    Top5 99.902344    LR 0.030000    Time 0.022379    
2018-10-28 00:44:44,675 - Epoch: [123][  250/  391]    Overall Loss 0.185473    Objective Loss 0.185473    Top1 93.493750    Top5 99.893750    LR 0.030000    Time 0.022344    
2018-10-28 00:44:45,787 - Epoch: [123][  300/  391]    Overall Loss 0.185827    Objective Loss 0.185827    Top1 93.442708    Top5 99.882812    LR 0.030000    Time 0.022323    
2018-10-28 00:44:46,898 - Epoch: [123][  350/  391]    Overall Loss 0.188548    Objective Loss 0.188548    Top1 93.352679    Top5 99.886161    LR 0.030000    Time 0.022304    
2018-10-28 00:44:47,887 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.46500 |  0.00026 |    0.23737 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.14718 | -0.00434 |    0.04751 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.14605 | -0.00386 |    0.05245 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.14510 | -0.00915 |    0.06266 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.10948 | -0.00392 |    0.03204 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.15809 | -0.00640 |    0.06318 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.10728 |  0.00233 |    0.02932 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.14584 | -0.00667 |    0.06987 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.13276 | -0.00578 |    0.07600 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.20001 | -0.00719 |    0.09909 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.10563 | -0.00459 |    0.04219 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.08680 |  0.00137 |    0.03135 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.10896 | -0.00720 |    0.04795 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08911 | -0.00118 |    0.04022 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.09373 | -0.00339 |    0.03630 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.09622 | -0.00474 |    0.04826 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.10883 | -0.00330 |    0.03945 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08901 | -0.00412 |    0.03672 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.07096 | -0.00037 |    0.03196 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05152 | -0.00116 |    0.01663 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03874 | -0.00036 |    0.00939 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56723 | -0.03475 |    0.33427 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:44:47,887 - Total sparsity: 74.94

2018-10-28 00:44:47,887 - --- validate (epoch=123)-----------
2018-10-28 00:44:47,887 - 10000 samples (128 per mini-batch)
2018-10-28 00:44:48,609 - Epoch: [123][   50/   78]    Loss 0.376847    Top1 88.937500    Top5 99.515625    
2018-10-28 00:44:48,999 - ==> Top1: 88.920    Top5: 99.580    Loss: 0.368

2018-10-28 00:44:49,000 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:44:49,000 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:44:49,011 - 

2018-10-28 00:44:49,011 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:44:50,181 - Epoch: [124][   50/  391]    Overall Loss 0.184781    Objective Loss 0.184781    Top1 93.703125    Top5 99.906250    LR 0.030000    Time 0.023366    
2018-10-28 00:44:51,291 - Epoch: [124][  100/  391]    Overall Loss 0.179304    Objective Loss 0.179304    Top1 93.875000    Top5 99.875000    LR 0.030000    Time 0.022771    
2018-10-28 00:44:52,402 - Epoch: [124][  150/  391]    Overall Loss 0.180501    Objective Loss 0.180501    Top1 93.833333    Top5 99.885417    LR 0.030000    Time 0.022576    
2018-10-28 00:44:53,513 - Epoch: [124][  200/  391]    Overall Loss 0.181050    Objective Loss 0.181050    Top1 93.707031    Top5 99.898438    LR 0.030000    Time 0.022480    
2018-10-28 00:44:54,623 - Epoch: [124][  250/  391]    Overall Loss 0.182506    Objective Loss 0.182506    Top1 93.675000    Top5 99.896875    LR 0.030000    Time 0.022421    
2018-10-28 00:44:55,735 - Epoch: [124][  300/  391]    Overall Loss 0.184026    Objective Loss 0.184026    Top1 93.572917    Top5 99.901042    LR 0.030000    Time 0.022387    
2018-10-28 00:44:56,847 - Epoch: [124][  350/  391]    Overall Loss 0.186416    Objective Loss 0.186416    Top1 93.540179    Top5 99.906250    LR 0.030000    Time 0.022362    
2018-10-28 00:44:57,841 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.46153 |  0.00007 |    0.23553 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.14602 | -0.00435 |    0.04721 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.14485 | -0.00339 |    0.05221 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.14403 | -0.00902 |    0.06220 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.10860 | -0.00396 |    0.03189 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.15686 | -0.00659 |    0.06269 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.10641 |  0.00249 |    0.02906 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.14473 | -0.00657 |    0.06936 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.13178 | -0.00564 |    0.07539 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.19818 | -0.00653 |    0.09832 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.10487 | -0.00455 |    0.04186 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.08618 |  0.00138 |    0.03108 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.10821 | -0.00705 |    0.04755 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08850 | -0.00120 |    0.03995 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.09299 | -0.00334 |    0.03596 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.09552 | -0.00457 |    0.04792 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.10787 | -0.00295 |    0.03910 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08839 | -0.00410 |    0.03646 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.07049 | -0.00035 |    0.03175 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05125 | -0.00113 |    0.01652 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03846 | -0.00038 |    0.00933 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56769 | -0.03454 |    0.33449 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:44:57,841 - Total sparsity: 74.94

2018-10-28 00:44:57,841 - --- validate (epoch=124)-----------
2018-10-28 00:44:57,841 - 10000 samples (128 per mini-batch)
2018-10-28 00:44:58,577 - Epoch: [124][   50/   78]    Loss 0.365011    Top1 89.062500    Top5 99.453125    
2018-10-28 00:44:58,963 - ==> Top1: 88.890    Top5: 99.570    Loss: 0.362

2018-10-28 00:44:58,964 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:44:58,964 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:44:58,979 - 

2018-10-28 00:44:58,979 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:45:00,155 - Epoch: [125][   50/  391]    Overall Loss 0.186896    Objective Loss 0.186896    Top1 93.171875    Top5 99.953125    LR 0.030000    Time 0.023464    
2018-10-28 00:45:01,266 - Epoch: [125][  100/  391]    Overall Loss 0.185680    Objective Loss 0.185680    Top1 93.390625    Top5 99.945312    LR 0.030000    Time 0.022835    
2018-10-28 00:45:02,378 - Epoch: [125][  150/  391]    Overall Loss 0.185686    Objective Loss 0.185686    Top1 93.442708    Top5 99.947917    LR 0.030000    Time 0.022624    
2018-10-28 00:45:03,490 - Epoch: [125][  200/  391]    Overall Loss 0.183473    Objective Loss 0.183473    Top1 93.492188    Top5 99.941406    LR 0.030000    Time 0.022524    
2018-10-28 00:45:04,602 - Epoch: [125][  250/  391]    Overall Loss 0.184744    Objective Loss 0.184744    Top1 93.459375    Top5 99.934375    LR 0.030000    Time 0.022462    
2018-10-28 00:45:05,715 - Epoch: [125][  300/  391]    Overall Loss 0.186284    Objective Loss 0.186284    Top1 93.434896    Top5 99.929688    LR 0.030000    Time 0.022421    
2018-10-28 00:45:06,826 - Epoch: [125][  350/  391]    Overall Loss 0.188015    Objective Loss 0.188015    Top1 93.390625    Top5 99.915179    LR 0.030000    Time 0.022390    
2018-10-28 00:45:07,821 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.45842 | -0.00065 |    0.23373 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.14490 | -0.00420 |    0.04684 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.14370 | -0.00381 |    0.05182 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.14298 | -0.00874 |    0.06195 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.10776 | -0.00403 |    0.03163 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.15567 | -0.00634 |    0.06220 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.10557 |  0.00240 |    0.02879 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.14366 | -0.00644 |    0.06884 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.13082 | -0.00570 |    0.07492 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.19640 | -0.00624 |    0.09708 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.10412 | -0.00468 |    0.04154 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.08557 |  0.00146 |    0.03091 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.10748 | -0.00705 |    0.04725 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08790 | -0.00124 |    0.03963 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.09228 | -0.00333 |    0.03570 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.09485 | -0.00449 |    0.04755 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.10692 | -0.00281 |    0.03876 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08779 | -0.00409 |    0.03621 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.07004 | -0.00040 |    0.03156 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05099 | -0.00113 |    0.01643 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03819 | -0.00035 |    0.00925 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56757 | -0.03424 |    0.33444 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:45:07,821 - Total sparsity: 74.94

2018-10-28 00:45:07,821 - --- validate (epoch=125)-----------
2018-10-28 00:45:07,821 - 10000 samples (128 per mini-batch)
2018-10-28 00:45:08,539 - Epoch: [125][   50/   78]    Loss 0.370581    Top1 88.875000    Top5 99.578125    
2018-10-28 00:45:08,925 - ==> Top1: 88.940    Top5: 99.640    Loss: 0.365

2018-10-28 00:45:08,926 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:45:08,926 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:45:08,936 - 

2018-10-28 00:45:08,936 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:45:10,103 - Epoch: [126][   50/  391]    Overall Loss 0.177947    Objective Loss 0.177947    Top1 93.531250    Top5 99.906250    LR 0.030000    Time 0.023297    
2018-10-28 00:45:11,212 - Epoch: [126][  100/  391]    Overall Loss 0.175968    Objective Loss 0.175968    Top1 93.679688    Top5 99.929688    LR 0.030000    Time 0.022723    
2018-10-28 00:45:12,321 - Epoch: [126][  150/  391]    Overall Loss 0.180328    Objective Loss 0.180328    Top1 93.473958    Top5 99.916667    LR 0.030000    Time 0.022535    
2018-10-28 00:45:13,431 - Epoch: [126][  200/  391]    Overall Loss 0.181395    Objective Loss 0.181395    Top1 93.542969    Top5 99.917969    LR 0.030000    Time 0.022444    
2018-10-28 00:45:14,541 - Epoch: [126][  250/  391]    Overall Loss 0.182497    Objective Loss 0.182497    Top1 93.531250    Top5 99.928125    LR 0.030000    Time 0.022389    
2018-10-28 00:45:15,650 - Epoch: [126][  300/  391]    Overall Loss 0.183913    Objective Loss 0.183913    Top1 93.502604    Top5 99.929688    LR 0.030000    Time 0.022339    
2018-10-28 00:45:16,759 - Epoch: [126][  350/  391]    Overall Loss 0.186399    Objective Loss 0.186399    Top1 93.415179    Top5 99.930804    LR 0.030000    Time 0.022311    
2018-10-28 00:45:17,746 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.45509 |  0.00298 |    0.23201 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.14377 | -0.00444 |    0.04647 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.14259 | -0.00320 |    0.05144 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.14194 | -0.00912 |    0.06137 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.10693 | -0.00387 |    0.03137 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.15451 | -0.00617 |    0.06165 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.10475 |  0.00246 |    0.02861 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.14262 | -0.00628 |    0.06826 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.12990 | -0.00565 |    0.07426 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.19466 | -0.00628 |    0.09622 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.10342 | -0.00448 |    0.04125 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.08499 |  0.00146 |    0.03072 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.10681 | -0.00701 |    0.04693 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08735 | -0.00124 |    0.03943 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.09160 | -0.00328 |    0.03543 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.09420 | -0.00446 |    0.04720 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.10601 | -0.00248 |    0.03832 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08723 | -0.00397 |    0.03596 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06963 | -0.00032 |    0.03135 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05075 | -0.00116 |    0.01634 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03794 | -0.00031 |    0.00919 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56701 | -0.03404 |    0.33415 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:45:17,746 - Total sparsity: 74.94

2018-10-28 00:45:17,747 - --- validate (epoch=126)-----------
2018-10-28 00:45:17,747 - 10000 samples (128 per mini-batch)
2018-10-28 00:45:18,471 - Epoch: [126][   50/   78]    Loss 0.369511    Top1 88.843750    Top5 99.484375    
2018-10-28 00:45:18,863 - ==> Top1: 88.830    Top5: 99.600    Loss: 0.362

2018-10-28 00:45:18,864 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:45:18,864 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:45:18,881 - 

2018-10-28 00:45:18,882 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:45:20,024 - Epoch: [127][   50/  391]    Overall Loss 0.177815    Objective Loss 0.177815    Top1 93.718750    Top5 99.875000    LR 0.030000    Time 0.022807    
2018-10-28 00:45:21,135 - Epoch: [127][  100/  391]    Overall Loss 0.175871    Objective Loss 0.175871    Top1 93.859375    Top5 99.914062    LR 0.030000    Time 0.022505    
2018-10-28 00:45:22,248 - Epoch: [127][  150/  391]    Overall Loss 0.175256    Objective Loss 0.175256    Top1 93.828125    Top5 99.895833    LR 0.030000    Time 0.022413    
2018-10-28 00:45:23,359 - Epoch: [127][  200/  391]    Overall Loss 0.176176    Objective Loss 0.176176    Top1 93.832031    Top5 99.902344    LR 0.030000    Time 0.022361    
2018-10-28 00:45:24,472 - Epoch: [127][  250/  391]    Overall Loss 0.177817    Objective Loss 0.177817    Top1 93.731250    Top5 99.906250    LR 0.030000    Time 0.022336    
2018-10-28 00:45:25,585 - Epoch: [127][  300/  391]    Overall Loss 0.181881    Objective Loss 0.181881    Top1 93.510417    Top5 99.914062    LR 0.030000    Time 0.022317    
2018-10-28 00:45:26,698 - Epoch: [127][  350/  391]    Overall Loss 0.183312    Objective Loss 0.183312    Top1 93.513393    Top5 99.917411    LR 0.030000    Time 0.022305    
2018-10-28 00:45:27,687 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.45167 |  0.00145 |    0.23000 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.14271 | -0.00429 |    0.04617 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.14150 | -0.00338 |    0.05104 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.14096 | -0.00933 |    0.06099 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.10612 | -0.00385 |    0.03115 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.15338 | -0.00599 |    0.06109 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.10397 |  0.00213 |    0.02829 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.14160 | -0.00626 |    0.06762 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.12902 | -0.00545 |    0.07379 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.19297 | -0.00600 |    0.09532 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.10275 | -0.00446 |    0.04100 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.08444 |  0.00149 |    0.03045 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.10616 | -0.00698 |    0.04668 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08682 | -0.00115 |    0.03928 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.09093 | -0.00318 |    0.03516 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.09357 | -0.00432 |    0.04689 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.10511 | -0.00265 |    0.03800 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08669 | -0.00388 |    0.03570 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06922 | -0.00043 |    0.03117 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05054 | -0.00109 |    0.01626 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03769 | -0.00029 |    0.00913 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56688 | -0.03395 |    0.33381 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:45:27,687 - Total sparsity: 74.94

2018-10-28 00:45:27,688 - --- validate (epoch=127)-----------
2018-10-28 00:45:27,688 - 10000 samples (128 per mini-batch)
2018-10-28 00:45:28,411 - Epoch: [127][   50/   78]    Loss 0.360874    Top1 89.218750    Top5 99.546875    
2018-10-28 00:45:28,803 - ==> Top1: 89.050    Top5: 99.620    Loss: 0.358

2018-10-28 00:45:28,804 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:45:28,804 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:45:28,820 - 

2018-10-28 00:45:28,821 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:45:29,963 - Epoch: [128][   50/  391]    Overall Loss 0.174508    Objective Loss 0.174508    Top1 93.968750    Top5 99.921875    LR 0.030000    Time 0.022812    
2018-10-28 00:45:31,075 - Epoch: [128][  100/  391]    Overall Loss 0.179187    Objective Loss 0.179187    Top1 93.757812    Top5 99.929688    LR 0.030000    Time 0.022509    
2018-10-28 00:45:32,187 - Epoch: [128][  150/  391]    Overall Loss 0.178483    Objective Loss 0.178483    Top1 93.697917    Top5 99.932292    LR 0.030000    Time 0.022413    
2018-10-28 00:45:33,298 - Epoch: [128][  200/  391]    Overall Loss 0.181064    Objective Loss 0.181064    Top1 93.582031    Top5 99.894531    LR 0.030000    Time 0.022360    
2018-10-28 00:45:34,409 - Epoch: [128][  250/  391]    Overall Loss 0.178179    Objective Loss 0.178179    Top1 93.684375    Top5 99.900000    LR 0.030000    Time 0.022327    
2018-10-28 00:45:35,520 - Epoch: [128][  300/  391]    Overall Loss 0.180139    Objective Loss 0.180139    Top1 93.575521    Top5 99.906250    LR 0.030000    Time 0.022305    
2018-10-28 00:45:36,629 - Epoch: [128][  350/  391]    Overall Loss 0.181388    Objective Loss 0.181388    Top1 93.562500    Top5 99.901786    LR 0.030000    Time 0.022283    
2018-10-28 00:45:37,618 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.44861 | -0.00031 |    0.22835 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.14170 | -0.00393 |    0.04570 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.14042 | -0.00321 |    0.05072 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.14004 | -0.00886 |    0.06033 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.10533 | -0.00364 |    0.03078 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.15228 | -0.00539 |    0.06060 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.10320 |  0.00208 |    0.02811 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.14058 | -0.00651 |    0.06716 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.12814 | -0.00533 |    0.07328 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.19130 | -0.00590 |    0.09470 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.10207 | -0.00426 |    0.04074 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.08390 |  0.00138 |    0.03028 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.10552 | -0.00683 |    0.04642 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08630 | -0.00095 |    0.03902 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.09027 | -0.00316 |    0.03494 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.09295 | -0.00425 |    0.04658 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.10422 | -0.00239 |    0.03770 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08614 | -0.00389 |    0.03548 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06883 | -0.00037 |    0.03099 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05032 | -0.00108 |    0.01619 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03744 | -0.00028 |    0.00907 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56759 | -0.03403 |    0.33435 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:45:37,618 - Total sparsity: 74.94

2018-10-28 00:45:37,618 - --- validate (epoch=128)-----------
2018-10-28 00:45:37,618 - 10000 samples (128 per mini-batch)
2018-10-28 00:45:38,335 - Epoch: [128][   50/   78]    Loss 0.377046    Top1 89.093750    Top5 99.562500    
2018-10-28 00:45:38,719 - ==> Top1: 89.020    Top5: 99.630    Loss: 0.368

2018-10-28 00:45:38,720 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:45:38,720 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:45:38,732 - 

2018-10-28 00:45:38,732 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:45:39,902 - Epoch: [129][   50/  391]    Overall Loss 0.183157    Objective Loss 0.183157    Top1 93.500000    Top5 99.906250    LR 0.030000    Time 0.023365    
2018-10-28 00:45:41,013 - Epoch: [129][  100/  391]    Overall Loss 0.186806    Objective Loss 0.186806    Top1 93.445312    Top5 99.906250    LR 0.030000    Time 0.022785    
2018-10-28 00:45:42,127 - Epoch: [129][  150/  391]    Overall Loss 0.188979    Objective Loss 0.188979    Top1 93.364583    Top5 99.880208    LR 0.030000    Time 0.022601    
2018-10-28 00:45:43,238 - Epoch: [129][  200/  391]    Overall Loss 0.187775    Objective Loss 0.187775    Top1 93.390625    Top5 99.890625    LR 0.030000    Time 0.022501    
2018-10-28 00:45:44,350 - Epoch: [129][  250/  391]    Overall Loss 0.188088    Objective Loss 0.188088    Top1 93.418750    Top5 99.896875    LR 0.030000    Time 0.022445    
2018-10-28 00:45:45,464 - Epoch: [129][  300/  391]    Overall Loss 0.186602    Objective Loss 0.186602    Top1 93.445312    Top5 99.885417    LR 0.030000    Time 0.022411    
2018-10-28 00:45:46,575 - Epoch: [129][  350/  391]    Overall Loss 0.187668    Objective Loss 0.187668    Top1 93.399554    Top5 99.890625    LR 0.030000    Time 0.022381    
2018-10-28 00:45:47,566 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.44563 |  0.00028 |    0.22733 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.14073 | -0.00392 |    0.04518 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.13939 | -0.00357 |    0.05018 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.13913 | -0.00915 |    0.06000 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.10458 | -0.00364 |    0.03058 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.15122 | -0.00587 |    0.06016 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.10248 |  0.00219 |    0.02797 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.13963 | -0.00651 |    0.06662 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.12731 | -0.00521 |    0.07279 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.18963 | -0.00682 |    0.09392 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.10144 | -0.00427 |    0.04042 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.08338 |  0.00136 |    0.03010 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.10491 | -0.00697 |    0.04615 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08579 | -0.00106 |    0.03880 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08964 | -0.00312 |    0.03466 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.09237 | -0.00417 |    0.04627 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.10338 | -0.00232 |    0.03736 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08564 | -0.00380 |    0.03526 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06847 | -0.00044 |    0.03083 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05013 | -0.00107 |    0.01612 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03722 | -0.00029 |    0.00901 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56724 | -0.03389 |    0.33416 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:45:47,566 - Total sparsity: 74.94

2018-10-28 00:45:47,566 - --- validate (epoch=129)-----------
2018-10-28 00:45:47,567 - 10000 samples (128 per mini-batch)
2018-10-28 00:45:48,293 - Epoch: [129][   50/   78]    Loss 0.402235    Top1 87.796875    Top5 99.515625    
2018-10-28 00:45:48,682 - ==> Top1: 87.580    Top5: 99.570    Loss: 0.405

2018-10-28 00:45:48,683 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:45:48,683 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:45:48,694 - 

2018-10-28 00:45:48,694 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:45:49,862 - Epoch: [130][   50/  391]    Overall Loss 0.194777    Objective Loss 0.194777    Top1 93.203125    Top5 99.875000    LR 0.030000    Time 0.023325    
2018-10-28 00:45:50,971 - Epoch: [130][  100/  391]    Overall Loss 0.189721    Objective Loss 0.189721    Top1 93.320312    Top5 99.859375    LR 0.030000    Time 0.022738    
2018-10-28 00:45:52,080 - Epoch: [130][  150/  391]    Overall Loss 0.185099    Objective Loss 0.185099    Top1 93.520833    Top5 99.875000    LR 0.030000    Time 0.022549    
2018-10-28 00:45:53,188 - Epoch: [130][  200/  391]    Overall Loss 0.184830    Objective Loss 0.184830    Top1 93.460938    Top5 99.890625    LR 0.030000    Time 0.022445    
2018-10-28 00:45:54,298 - Epoch: [130][  250/  391]    Overall Loss 0.183741    Objective Loss 0.183741    Top1 93.421875    Top5 99.893750    LR 0.030000    Time 0.022389    
2018-10-28 00:45:55,407 - Epoch: [130][  300/  391]    Overall Loss 0.185366    Objective Loss 0.185366    Top1 93.382812    Top5 99.888021    LR 0.030000    Time 0.022349    
2018-10-28 00:45:56,515 - Epoch: [130][  350/  391]    Overall Loss 0.186510    Objective Loss 0.186510    Top1 93.330357    Top5 99.895089    LR 0.030000    Time 0.022321    
2018-10-28 00:45:57,504 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.44259 | -0.00037 |    0.22552 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.13979 | -0.00411 |    0.04491 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.13838 | -0.00308 |    0.04993 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.13829 | -0.00858 |    0.05953 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.10385 | -0.00380 |    0.03043 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.15019 | -0.00555 |    0.05980 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.10178 |  0.00215 |    0.02778 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.13872 | -0.00638 |    0.06616 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.12651 | -0.00509 |    0.07226 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.18801 | -0.00729 |    0.09299 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.10083 | -0.00430 |    0.04021 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.08288 |  0.00132 |    0.02991 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.10431 | -0.00680 |    0.04579 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08531 | -0.00088 |    0.03855 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08903 | -0.00306 |    0.03444 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.09181 | -0.00426 |    0.04601 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.10253 | -0.00234 |    0.03708 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08516 | -0.00375 |    0.03507 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06813 | -0.00042 |    0.03067 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04996 | -0.00098 |    0.01605 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03700 | -0.00024 |    0.00894 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56768 | -0.03372 |    0.33431 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:45:57,504 - Total sparsity: 74.94

2018-10-28 00:45:57,505 - --- validate (epoch=130)-----------
2018-10-28 00:45:57,505 - 10000 samples (128 per mini-batch)
2018-10-28 00:45:58,227 - Epoch: [130][   50/   78]    Loss 0.417855    Top1 87.687500    Top5 99.515625    
2018-10-28 00:45:58,621 - ==> Top1: 87.680    Top5: 99.590    Loss: 0.408

2018-10-28 00:45:58,621 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:45:58,622 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:45:58,631 - 

2018-10-28 00:45:58,631 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:45:59,803 - Epoch: [131][   50/  391]    Overall Loss 0.184942    Objective Loss 0.184942    Top1 93.359375    Top5 99.906250    LR 0.030000    Time 0.023394    
2018-10-28 00:46:00,913 - Epoch: [131][  100/  391]    Overall Loss 0.180338    Objective Loss 0.180338    Top1 93.539062    Top5 99.906250    LR 0.030000    Time 0.022781    
2018-10-28 00:46:02,025 - Epoch: [131][  150/  391]    Overall Loss 0.181699    Objective Loss 0.181699    Top1 93.536458    Top5 99.906250    LR 0.030000    Time 0.022594    
2018-10-28 00:46:03,137 - Epoch: [131][  200/  391]    Overall Loss 0.184035    Objective Loss 0.184035    Top1 93.500000    Top5 99.910156    LR 0.030000    Time 0.022498    
2018-10-28 00:46:04,250 - Epoch: [131][  250/  391]    Overall Loss 0.182800    Objective Loss 0.182800    Top1 93.609375    Top5 99.909375    LR 0.030000    Time 0.022445    
2018-10-28 00:46:05,364 - Epoch: [131][  300/  391]    Overall Loss 0.184040    Objective Loss 0.184040    Top1 93.554688    Top5 99.895833    LR 0.030000    Time 0.022410    
2018-10-28 00:46:06,476 - Epoch: [131][  350/  391]    Overall Loss 0.183542    Objective Loss 0.183542    Top1 93.535714    Top5 99.904018    LR 0.030000    Time 0.022383    
2018-10-28 00:46:07,466 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.43959 |  0.00151 |    0.22425 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.13889 | -0.00412 |    0.04460 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.13742 | -0.00312 |    0.04946 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.13743 | -0.00833 |    0.05904 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.10312 | -0.00370 |    0.03015 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.14921 | -0.00522 |    0.05940 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.10106 |  0.00220 |    0.02757 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.13785 | -0.00613 |    0.06578 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.12574 | -0.00519 |    0.07183 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.18650 | -0.00606 |    0.09227 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.10025 | -0.00417 |    0.03989 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.08240 |  0.00127 |    0.02972 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.10378 | -0.00674 |    0.04560 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08483 | -0.00099 |    0.03830 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08844 | -0.00310 |    0.03424 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.09126 | -0.00427 |    0.04572 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.10172 | -0.00205 |    0.03675 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08469 | -0.00365 |    0.03488 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06780 | -0.00039 |    0.03053 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04978 | -0.00100 |    0.01598 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03678 | -0.00021 |    0.00888 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56756 | -0.03374 |    0.33420 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:46:07,467 - Total sparsity: 74.94

2018-10-28 00:46:07,467 - --- validate (epoch=131)-----------
2018-10-28 00:46:07,467 - 10000 samples (128 per mini-batch)
2018-10-28 00:46:08,187 - Epoch: [131][   50/   78]    Loss 0.380224    Top1 88.703125    Top5 99.500000    
2018-10-28 00:46:08,577 - ==> Top1: 88.500    Top5: 99.580    Loss: 0.381

2018-10-28 00:46:08,577 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:46:08,577 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:46:08,588 - 

2018-10-28 00:46:08,589 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:46:09,758 - Epoch: [132][   50/  391]    Overall Loss 0.183036    Objective Loss 0.183036    Top1 93.484375    Top5 99.843750    LR 0.030000    Time 0.023357    
2018-10-28 00:46:10,868 - Epoch: [132][  100/  391]    Overall Loss 0.178569    Objective Loss 0.178569    Top1 93.664062    Top5 99.906250    LR 0.030000    Time 0.022761    
2018-10-28 00:46:11,978 - Epoch: [132][  150/  391]    Overall Loss 0.175394    Objective Loss 0.175394    Top1 93.729167    Top5 99.927083    LR 0.030000    Time 0.022566    
2018-10-28 00:46:13,088 - Epoch: [132][  200/  391]    Overall Loss 0.177748    Objective Loss 0.177748    Top1 93.695312    Top5 99.906250    LR 0.030000    Time 0.022467    
2018-10-28 00:46:14,197 - Epoch: [132][  250/  391]    Overall Loss 0.182381    Objective Loss 0.182381    Top1 93.503125    Top5 99.903125    LR 0.030000    Time 0.022406    
2018-10-28 00:46:15,306 - Epoch: [132][  300/  391]    Overall Loss 0.183448    Objective Loss 0.183448    Top1 93.505208    Top5 99.906250    LR 0.030000    Time 0.022364    
2018-10-28 00:46:16,416 - Epoch: [132][  350/  391]    Overall Loss 0.184453    Objective Loss 0.184453    Top1 93.462054    Top5 99.906250    LR 0.030000    Time 0.022338    
2018-10-28 00:46:17,404 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.43681 |  0.00089 |    0.22234 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.13802 | -0.00444 |    0.04428 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.13649 | -0.00333 |    0.04895 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.13659 | -0.00837 |    0.05883 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.10243 | -0.00377 |    0.02994 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.14824 | -0.00575 |    0.05889 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.10039 |  0.00202 |    0.02739 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.13700 | -0.00589 |    0.06531 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.12501 | -0.00541 |    0.07141 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.18500 | -0.00614 |    0.09131 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09970 | -0.00406 |    0.03965 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.08196 |  0.00123 |    0.02960 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.10328 | -0.00687 |    0.04539 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08441 | -0.00089 |    0.03813 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08788 | -0.00313 |    0.03399 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.09077 | -0.00409 |    0.04546 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.10091 | -0.00218 |    0.03651 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08425 | -0.00365 |    0.03470 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06749 | -0.00036 |    0.03042 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04964 | -0.00099 |    0.01592 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03659 | -0.00020 |    0.00882 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56700 | -0.03372 |    0.33395 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:46:17,404 - Total sparsity: 74.94

2018-10-28 00:46:17,404 - --- validate (epoch=132)-----------
2018-10-28 00:46:17,404 - 10000 samples (128 per mini-batch)
2018-10-28 00:46:18,130 - Epoch: [132][   50/   78]    Loss 0.372337    Top1 88.875000    Top5 99.437500    
2018-10-28 00:46:18,525 - ==> Top1: 88.920    Top5: 99.580    Loss: 0.363

2018-10-28 00:46:18,526 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:46:18,526 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:46:18,543 - 

2018-10-28 00:46:18,543 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:46:19,687 - Epoch: [133][   50/  391]    Overall Loss 0.169215    Objective Loss 0.169215    Top1 93.953125    Top5 99.968750    LR 0.030000    Time 0.022843    
2018-10-28 00:46:20,798 - Epoch: [133][  100/  391]    Overall Loss 0.169078    Objective Loss 0.169078    Top1 93.984375    Top5 99.937500    LR 0.030000    Time 0.022525    
2018-10-28 00:46:21,911 - Epoch: [133][  150/  391]    Overall Loss 0.173138    Objective Loss 0.173138    Top1 93.776042    Top5 99.916667    LR 0.030000    Time 0.022425    
2018-10-28 00:46:23,023 - Epoch: [133][  200/  391]    Overall Loss 0.176221    Objective Loss 0.176221    Top1 93.695312    Top5 99.894531    LR 0.030000    Time 0.022375    
2018-10-28 00:46:24,136 - Epoch: [133][  250/  391]    Overall Loss 0.179565    Objective Loss 0.179565    Top1 93.565625    Top5 99.893750    LR 0.030000    Time 0.022346    
2018-10-28 00:46:25,249 - Epoch: [133][  300/  391]    Overall Loss 0.180245    Objective Loss 0.180245    Top1 93.572917    Top5 99.898438    LR 0.030000    Time 0.022328    
2018-10-28 00:46:26,362 - Epoch: [133][  350/  391]    Overall Loss 0.181676    Objective Loss 0.181676    Top1 93.546875    Top5 99.906250    LR 0.030000    Time 0.022313    
2018-10-28 00:46:27,355 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.43395 |  0.00012 |    0.22143 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.13714 | -0.00428 |    0.04396 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.13555 | -0.00318 |    0.04874 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.13575 | -0.00850 |    0.05848 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.10175 | -0.00365 |    0.02975 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.14729 | -0.00540 |    0.05846 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09973 |  0.00195 |    0.02733 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.13615 | -0.00585 |    0.06500 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.12427 | -0.00547 |    0.07106 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.18353 | -0.00548 |    0.09058 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09917 | -0.00390 |    0.03943 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.08152 |  0.00116 |    0.02939 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.10279 | -0.00654 |    0.04519 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08397 | -0.00089 |    0.03791 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08733 | -0.00304 |    0.03378 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.09027 | -0.00418 |    0.04520 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.10011 | -0.00222 |    0.03618 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08382 | -0.00362 |    0.03453 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06720 | -0.00031 |    0.03027 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04950 | -0.00098 |    0.01588 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03640 | -0.00018 |    0.00879 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56775 | -0.03364 |    0.33431 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:46:27,355 - Total sparsity: 74.94

2018-10-28 00:46:27,355 - --- validate (epoch=133)-----------
2018-10-28 00:46:27,355 - 10000 samples (128 per mini-batch)
2018-10-28 00:46:28,081 - Epoch: [133][   50/   78]    Loss 0.393301    Top1 88.640625    Top5 99.375000    
2018-10-28 00:46:28,473 - ==> Top1: 88.510    Top5: 99.520    Loss: 0.391

2018-10-28 00:46:28,473 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:46:28,474 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:46:28,484 - 

2018-10-28 00:46:28,485 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:46:29,655 - Epoch: [134][   50/  391]    Overall Loss 0.173029    Objective Loss 0.173029    Top1 93.593750    Top5 99.906250    LR 0.030000    Time 0.023361    
2018-10-28 00:46:30,767 - Epoch: [134][  100/  391]    Overall Loss 0.176923    Objective Loss 0.176923    Top1 93.679688    Top5 99.953125    LR 0.030000    Time 0.022789    
2018-10-28 00:46:31,877 - Epoch: [134][  150/  391]    Overall Loss 0.182515    Objective Loss 0.182515    Top1 93.427083    Top5 99.921875    LR 0.030000    Time 0.022588    
2018-10-28 00:46:32,989 - Epoch: [134][  200/  391]    Overall Loss 0.184196    Objective Loss 0.184196    Top1 93.378906    Top5 99.925781    LR 0.030000    Time 0.022472    
2018-10-28 00:46:34,099 - Epoch: [134][  250/  391]    Overall Loss 0.183487    Objective Loss 0.183487    Top1 93.387500    Top5 99.928125    LR 0.030000    Time 0.022416    
2018-10-28 00:46:35,211 - Epoch: [134][  300/  391]    Overall Loss 0.185298    Objective Loss 0.185298    Top1 93.328125    Top5 99.908854    LR 0.030000    Time 0.022380    
2018-10-28 00:46:36,323 - Epoch: [134][  350/  391]    Overall Loss 0.185395    Objective Loss 0.185395    Top1 93.310268    Top5 99.906250    LR 0.030000    Time 0.022356    
2018-10-28 00:46:37,315 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.43144 |  0.00162 |    0.21968 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.13632 | -0.00402 |    0.04354 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.13466 | -0.00335 |    0.04836 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.13495 | -0.00872 |    0.05824 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.10107 | -0.00406 |    0.02956 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.14640 | -0.00522 |    0.05819 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09913 |  0.00188 |    0.02731 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.13534 | -0.00598 |    0.06474 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.12359 | -0.00548 |    0.07064 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.18210 | -0.00584 |    0.08955 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09869 | -0.00385 |    0.03929 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.08113 |  0.00109 |    0.02926 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.10235 | -0.00623 |    0.04492 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08357 | -0.00088 |    0.03775 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08682 | -0.00305 |    0.03359 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08981 | -0.00411 |    0.04496 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.09938 | -0.00223 |    0.03585 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08343 | -0.00355 |    0.03436 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06694 | -0.00035 |    0.03016 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04938 | -0.00105 |    0.01583 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03622 | -0.00018 |    0.00874 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56842 | -0.03339 |    0.33465 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:46:37,315 - Total sparsity: 74.94

2018-10-28 00:46:37,315 - --- validate (epoch=134)-----------
2018-10-28 00:46:37,315 - 10000 samples (128 per mini-batch)
2018-10-28 00:46:38,041 - Epoch: [134][   50/   78]    Loss 0.361629    Top1 88.796875    Top5 99.515625    
2018-10-28 00:46:38,433 - ==> Top1: 88.910    Top5: 99.580    Loss: 0.357

2018-10-28 00:46:38,434 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:46:38,434 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:46:38,444 - 

2018-10-28 00:46:38,444 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:46:39,614 - Epoch: [135][   50/  391]    Overall Loss 0.177153    Objective Loss 0.177153    Top1 93.875000    Top5 99.953125    LR 0.030000    Time 0.023359    
2018-10-28 00:46:40,723 - Epoch: [135][  100/  391]    Overall Loss 0.186962    Objective Loss 0.186962    Top1 93.335938    Top5 99.921875    LR 0.030000    Time 0.022753    
2018-10-28 00:46:41,832 - Epoch: [135][  150/  391]    Overall Loss 0.186745    Objective Loss 0.186745    Top1 93.244792    Top5 99.921875    LR 0.030000    Time 0.022557    
2018-10-28 00:46:42,942 - Epoch: [135][  200/  391]    Overall Loss 0.185678    Objective Loss 0.185678    Top1 93.324219    Top5 99.902344    LR 0.030000    Time 0.022458    
2018-10-28 00:46:44,051 - Epoch: [135][  250/  391]    Overall Loss 0.185679    Objective Loss 0.185679    Top1 93.315625    Top5 99.900000    LR 0.030000    Time 0.022398    
2018-10-28 00:46:45,162 - Epoch: [135][  300/  391]    Overall Loss 0.186194    Objective Loss 0.186194    Top1 93.315104    Top5 99.906250    LR 0.030000    Time 0.022365    
2018-10-28 00:46:46,272 - Epoch: [135][  350/  391]    Overall Loss 0.187247    Objective Loss 0.187247    Top1 93.332589    Top5 99.906250    LR 0.030000    Time 0.022339    
2018-10-28 00:46:47,260 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.42921 |  0.00068 |    0.21851 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.13558 | -0.00388 |    0.04328 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.13385 | -0.00308 |    0.04809 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.13425 | -0.00888 |    0.05800 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.10045 | -0.00426 |    0.02938 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.14556 | -0.00506 |    0.05777 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09851 |  0.00205 |    0.02709 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.13458 | -0.00604 |    0.06442 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.12295 | -0.00548 |    0.07026 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.18072 | -0.00531 |    0.08851 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09822 | -0.00388 |    0.03913 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.08075 |  0.00119 |    0.02916 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.10188 | -0.00639 |    0.04481 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08320 | -0.00095 |    0.03757 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08632 | -0.00299 |    0.03342 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08938 | -0.00392 |    0.04474 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.09865 | -0.00230 |    0.03555 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08306 | -0.00354 |    0.03422 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06670 | -0.00037 |    0.03004 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04929 | -0.00101 |    0.01579 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03607 | -0.00019 |    0.00871 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56716 | -0.03305 |    0.33391 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:46:47,260 - Total sparsity: 74.94

2018-10-28 00:46:47,260 - --- validate (epoch=135)-----------
2018-10-28 00:46:47,260 - 10000 samples (128 per mini-batch)
2018-10-28 00:46:47,991 - Epoch: [135][   50/   78]    Loss 0.385743    Top1 88.531250    Top5 99.328125    
2018-10-28 00:46:48,385 - ==> Top1: 88.430    Top5: 99.450    Loss: 0.382

2018-10-28 00:46:48,385 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:46:48,386 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:46:48,395 - 

2018-10-28 00:46:48,396 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:46:49,564 - Epoch: [136][   50/  391]    Overall Loss 0.177470    Objective Loss 0.177470    Top1 93.796875    Top5 99.921875    LR 0.030000    Time 0.023320    
2018-10-28 00:46:50,673 - Epoch: [136][  100/  391]    Overall Loss 0.187195    Objective Loss 0.187195    Top1 93.343750    Top5 99.875000    LR 0.030000    Time 0.022736    
2018-10-28 00:46:51,784 - Epoch: [136][  150/  391]    Overall Loss 0.183384    Objective Loss 0.183384    Top1 93.598958    Top5 99.890625    LR 0.030000    Time 0.022560    
2018-10-28 00:46:52,893 - Epoch: [136][  200/  391]    Overall Loss 0.180840    Objective Loss 0.180840    Top1 93.687500    Top5 99.886719    LR 0.030000    Time 0.022456    
2018-10-28 00:46:54,001 - Epoch: [136][  250/  391]    Overall Loss 0.180901    Objective Loss 0.180901    Top1 93.687500    Top5 99.875000    LR 0.030000    Time 0.022393    
2018-10-28 00:46:55,110 - Epoch: [136][  300/  391]    Overall Loss 0.182505    Objective Loss 0.182505    Top1 93.609375    Top5 99.888021    LR 0.030000    Time 0.022352    
2018-10-28 00:46:56,220 - Epoch: [136][  350/  391]    Overall Loss 0.183923    Objective Loss 0.183923    Top1 93.517857    Top5 99.886161    LR 0.030000    Time 0.022326    
2018-10-28 00:46:57,207 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.42659 |  0.00160 |    0.21709 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.13479 | -0.00380 |    0.04312 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.13302 | -0.00297 |    0.04785 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.13353 | -0.00869 |    0.05751 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09982 | -0.00403 |    0.02911 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.14467 | -0.00554 |    0.05743 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09790 |  0.00208 |    0.02689 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.13380 | -0.00614 |    0.06389 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.12231 | -0.00531 |    0.06991 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.17935 | -0.00516 |    0.08776 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09773 | -0.00387 |    0.03891 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.08035 |  0.00128 |    0.02902 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.10142 | -0.00642 |    0.04456 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08282 | -0.00082 |    0.03735 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08583 | -0.00291 |    0.03318 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08894 | -0.00392 |    0.04449 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.09793 | -0.00220 |    0.03533 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08268 | -0.00350 |    0.03405 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06645 | -0.00032 |    0.02996 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04918 | -0.00111 |    0.01577 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03589 | -0.00014 |    0.00866 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56667 | -0.03314 |    0.33359 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:46:57,207 - Total sparsity: 74.94

2018-10-28 00:46:57,208 - --- validate (epoch=136)-----------
2018-10-28 00:46:57,208 - 10000 samples (128 per mini-batch)
2018-10-28 00:46:57,932 - Epoch: [136][   50/   78]    Loss 0.393742    Top1 88.281250    Top5 99.484375    
2018-10-28 00:46:58,327 - ==> Top1: 88.410    Top5: 99.580    Loss: 0.388

2018-10-28 00:46:58,328 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:46:58,328 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:46:58,337 - 

2018-10-28 00:46:58,338 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:46:59,511 - Epoch: [137][   50/  391]    Overall Loss 0.188401    Objective Loss 0.188401    Top1 93.328125    Top5 99.859375    LR 0.030000    Time 0.023430    
2018-10-28 00:47:00,623 - Epoch: [137][  100/  391]    Overall Loss 0.185802    Objective Loss 0.185802    Top1 93.460938    Top5 99.882812    LR 0.030000    Time 0.022818    
2018-10-28 00:47:01,735 - Epoch: [137][  150/  391]    Overall Loss 0.182024    Objective Loss 0.182024    Top1 93.614583    Top5 99.895833    LR 0.030000    Time 0.022616    
2018-10-28 00:47:02,843 - Epoch: [137][  200/  391]    Overall Loss 0.183191    Objective Loss 0.183191    Top1 93.507812    Top5 99.910156    LR 0.030000    Time 0.022495    
2018-10-28 00:47:03,953 - Epoch: [137][  250/  391]    Overall Loss 0.181078    Objective Loss 0.181078    Top1 93.565625    Top5 99.918750    LR 0.030000    Time 0.022430    
2018-10-28 00:47:05,062 - Epoch: [137][  300/  391]    Overall Loss 0.179494    Objective Loss 0.179494    Top1 93.635417    Top5 99.924479    LR 0.030000    Time 0.022384    
2018-10-28 00:47:06,171 - Epoch: [137][  350/  391]    Overall Loss 0.180648    Objective Loss 0.180648    Top1 93.580357    Top5 99.921875    LR 0.030000    Time 0.022351    
2018-10-28 00:47:07,158 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.42425 |  0.00127 |    0.21592 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.13403 | -0.00387 |    0.04293 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.13219 | -0.00305 |    0.04746 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.13279 | -0.00830 |    0.05699 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09920 | -0.00376 |    0.02900 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.14379 | -0.00528 |    0.05707 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09730 |  0.00190 |    0.02686 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.13303 | -0.00602 |    0.06343 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.12166 | -0.00539 |    0.06954 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.17797 | -0.00529 |    0.08738 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09728 | -0.00386 |    0.03876 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07995 |  0.00129 |    0.02881 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.10096 | -0.00639 |    0.04441 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08243 | -0.00092 |    0.03712 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08534 | -0.00291 |    0.03294 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08851 | -0.00388 |    0.04427 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.09721 | -0.00217 |    0.03519 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08232 | -0.00341 |    0.03385 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06622 | -0.00040 |    0.02981 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04907 | -0.00109 |    0.01572 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03572 | -0.00014 |    0.00861 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56685 | -0.03307 |    0.33369 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:47:07,159 - Total sparsity: 74.94

2018-10-28 00:47:07,159 - --- validate (epoch=137)-----------
2018-10-28 00:47:07,159 - 10000 samples (128 per mini-batch)
2018-10-28 00:47:07,864 - Epoch: [137][   50/   78]    Loss 0.372149    Top1 88.625000    Top5 99.515625    
2018-10-28 00:47:08,245 - ==> Top1: 88.790    Top5: 99.610    Loss: 0.365

2018-10-28 00:47:08,245 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:47:08,246 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:47:08,262 - 

2018-10-28 00:47:08,262 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:47:09,403 - Epoch: [138][   50/  391]    Overall Loss 0.179087    Objective Loss 0.179087    Top1 93.796875    Top5 99.984375    LR 0.030000    Time 0.022784    
2018-10-28 00:47:10,514 - Epoch: [138][  100/  391]    Overall Loss 0.179123    Objective Loss 0.179123    Top1 93.773438    Top5 99.953125    LR 0.030000    Time 0.022484    
2018-10-28 00:47:11,625 - Epoch: [138][  150/  391]    Overall Loss 0.178895    Objective Loss 0.178895    Top1 93.755208    Top5 99.937500    LR 0.030000    Time 0.022392    
2018-10-28 00:47:12,738 - Epoch: [138][  200/  391]    Overall Loss 0.181194    Objective Loss 0.181194    Top1 93.683594    Top5 99.925781    LR 0.030000    Time 0.022351    
2018-10-28 00:47:13,849 - Epoch: [138][  250/  391]    Overall Loss 0.181736    Objective Loss 0.181736    Top1 93.587500    Top5 99.903125    LR 0.030000    Time 0.022321    
2018-10-28 00:47:14,960 - Epoch: [138][  300/  391]    Overall Loss 0.180524    Objective Loss 0.180524    Top1 93.651042    Top5 99.906250    LR 0.030000    Time 0.022300    
2018-10-28 00:47:16,071 - Epoch: [138][  350/  391]    Overall Loss 0.181541    Objective Loss 0.181541    Top1 93.616071    Top5 99.917411    LR 0.030000    Time 0.022284    
2018-10-28 00:47:17,063 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.42194 |  0.00027 |    0.21477 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.13326 | -0.00379 |    0.04252 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.13137 | -0.00341 |    0.04714 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.13212 | -0.00834 |    0.05670 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09862 | -0.00363 |    0.02883 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.14293 | -0.00525 |    0.05676 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09672 |  0.00190 |    0.02667 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.13232 | -0.00605 |    0.06315 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.12107 | -0.00515 |    0.06925 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.17659 | -0.00548 |    0.08630 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09685 | -0.00382 |    0.03854 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07958 |  0.00113 |    0.02866 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.10055 | -0.00613 |    0.04417 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08208 | -0.00097 |    0.03697 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08487 | -0.00290 |    0.03276 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08810 | -0.00381 |    0.04404 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.09651 | -0.00193 |    0.03485 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08196 | -0.00340 |    0.03368 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06599 | -0.00036 |    0.02969 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04897 | -0.00108 |    0.01567 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03555 | -0.00011 |    0.00858 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56684 | -0.03279 |    0.33355 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:47:17,063 - Total sparsity: 74.94

2018-10-28 00:47:17,064 - --- validate (epoch=138)-----------
2018-10-28 00:47:17,064 - 10000 samples (128 per mini-batch)
2018-10-28 00:47:17,887 - Epoch: [138][   50/   78]    Loss 0.411543    Top1 88.156250    Top5 99.375000    
2018-10-28 00:47:18,337 - ==> Top1: 88.090    Top5: 99.520    Loss: 0.409

2018-10-28 00:47:18,338 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:47:18,338 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:47:18,349 - 

2018-10-28 00:47:18,349 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:47:19,520 - Epoch: [139][   50/  391]    Overall Loss 0.179725    Objective Loss 0.179725    Top1 93.734375    Top5 99.937500    LR 0.030000    Time 0.023372    
2018-10-28 00:47:20,631 - Epoch: [139][  100/  391]    Overall Loss 0.175980    Objective Loss 0.175980    Top1 93.765625    Top5 99.921875    LR 0.030000    Time 0.022789    
2018-10-28 00:47:21,743 - Epoch: [139][  150/  391]    Overall Loss 0.177411    Objective Loss 0.177411    Top1 93.854167    Top5 99.916667    LR 0.030000    Time 0.022596    
2018-10-28 00:47:22,855 - Epoch: [139][  200/  391]    Overall Loss 0.177679    Objective Loss 0.177679    Top1 93.843750    Top5 99.917969    LR 0.030000    Time 0.022498    
2018-10-28 00:47:23,968 - Epoch: [139][  250/  391]    Overall Loss 0.177783    Objective Loss 0.177783    Top1 93.843750    Top5 99.903125    LR 0.030000    Time 0.022444    
2018-10-28 00:47:25,079 - Epoch: [139][  300/  391]    Overall Loss 0.178244    Objective Loss 0.178244    Top1 93.812500    Top5 99.916667    LR 0.030000    Time 0.022404    
2018-10-28 00:47:26,191 - Epoch: [139][  350/  391]    Overall Loss 0.180712    Objective Loss 0.180712    Top1 93.732143    Top5 99.910714    LR 0.030000    Time 0.022378    
2018-10-28 00:47:27,183 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.41949 |  0.00221 |    0.21375 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.13257 | -0.00361 |    0.04229 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.13065 | -0.00300 |    0.04691 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.13141 | -0.00857 |    0.05650 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09805 | -0.00377 |    0.02853 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.14218 | -0.00520 |    0.05645 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09622 |  0.00198 |    0.02654 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.13169 | -0.00611 |    0.06284 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.12051 | -0.00517 |    0.06888 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.17529 | -0.00520 |    0.08580 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09648 | -0.00376 |    0.03839 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07924 |  0.00106 |    0.02855 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.10016 | -0.00619 |    0.04402 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08174 | -0.00098 |    0.03677 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08442 | -0.00284 |    0.03261 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08770 | -0.00376 |    0.04383 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.09580 | -0.00194 |    0.03446 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08162 | -0.00338 |    0.03354 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06578 | -0.00031 |    0.02959 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04890 | -0.00105 |    0.01564 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03541 | -0.00012 |    0.00852 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56679 | -0.03292 |    0.33341 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:47:27,183 - Total sparsity: 74.94

2018-10-28 00:47:27,183 - --- validate (epoch=139)-----------
2018-10-28 00:47:27,183 - 10000 samples (128 per mini-batch)
2018-10-28 00:47:27,899 - Epoch: [139][   50/   78]    Loss 0.366944    Top1 88.656250    Top5 99.625000    
2018-10-28 00:47:28,285 - ==> Top1: 88.880    Top5: 99.660    Loss: 0.361

2018-10-28 00:47:28,286 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:47:28,286 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:47:28,301 - 

2018-10-28 00:47:28,301 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:47:29,476 - Epoch: [140][   50/  391]    Overall Loss 0.166182    Objective Loss 0.166182    Top1 94.187500    Top5 100.000000    LR 0.030000    Time 0.023474    
2018-10-28 00:47:30,589 - Epoch: [140][  100/  391]    Overall Loss 0.170895    Objective Loss 0.170895    Top1 93.984375    Top5 99.968750    LR 0.030000    Time 0.022846    
2018-10-28 00:47:31,700 - Epoch: [140][  150/  391]    Overall Loss 0.176816    Objective Loss 0.176816    Top1 93.760417    Top5 99.942708    LR 0.030000    Time 0.022633    
2018-10-28 00:47:32,813 - Epoch: [140][  200/  391]    Overall Loss 0.178402    Objective Loss 0.178402    Top1 93.671875    Top5 99.945312    LR 0.030000    Time 0.022532    
2018-10-28 00:47:33,926 - Epoch: [140][  250/  391]    Overall Loss 0.183355    Objective Loss 0.183355    Top1 93.521875    Top5 99.931250    LR 0.030000    Time 0.022470    
2018-10-28 00:47:35,037 - Epoch: [140][  300/  391]    Overall Loss 0.186049    Objective Loss 0.186049    Top1 93.406250    Top5 99.919271    LR 0.030000    Time 0.022425    
2018-10-28 00:47:36,148 - Epoch: [140][  350/  391]    Overall Loss 0.185544    Objective Loss 0.185544    Top1 93.412946    Top5 99.917411    LR 0.030000    Time 0.022392    
2018-10-28 00:47:37,136 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.41739 |  0.00063 |    0.21193 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.13197 | -0.00390 |    0.04194 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.12995 | -0.00316 |    0.04671 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.13073 | -0.00866 |    0.05598 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09750 | -0.00392 |    0.02835 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.14145 | -0.00546 |    0.05613 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09570 |  0.00187 |    0.02648 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.13105 | -0.00592 |    0.06247 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11999 | -0.00504 |    0.06855 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.17408 | -0.00488 |    0.08512 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09612 | -0.00356 |    0.03823 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07892 |  0.00103 |    0.02843 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09981 | -0.00618 |    0.04387 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08144 | -0.00114 |    0.03660 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08401 | -0.00296 |    0.03247 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08734 | -0.00387 |    0.04366 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.09514 | -0.00195 |    0.03422 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08132 | -0.00342 |    0.03342 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06561 | -0.00031 |    0.02951 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04885 | -0.00104 |    0.01562 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03528 | -0.00011 |    0.00850 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56639 | -0.03282 |    0.33320 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:47:37,137 - Total sparsity: 74.94

2018-10-28 00:47:37,137 - --- validate (epoch=140)-----------
2018-10-28 00:47:37,137 - 10000 samples (128 per mini-batch)
2018-10-28 00:47:37,868 - Epoch: [140][   50/   78]    Loss 0.372406    Top1 88.734375    Top5 99.609375    
2018-10-28 00:47:38,269 - ==> Top1: 88.740    Top5: 99.620    Loss: 0.369

2018-10-28 00:47:38,270 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:47:38,270 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:47:38,280 - 

2018-10-28 00:47:38,280 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:47:39,451 - Epoch: [141][   50/  391]    Overall Loss 0.167248    Objective Loss 0.167248    Top1 94.281250    Top5 99.937500    LR 0.030000    Time 0.023369    
2018-10-28 00:47:40,563 - Epoch: [141][  100/  391]    Overall Loss 0.166184    Objective Loss 0.166184    Top1 94.218750    Top5 99.960938    LR 0.030000    Time 0.022793    
2018-10-28 00:47:41,675 - Epoch: [141][  150/  391]    Overall Loss 0.170048    Objective Loss 0.170048    Top1 93.979167    Top5 99.947917    LR 0.030000    Time 0.022603    
2018-10-28 00:47:42,786 - Epoch: [141][  200/  391]    Overall Loss 0.176248    Objective Loss 0.176248    Top1 93.714844    Top5 99.945312    LR 0.030000    Time 0.022502    
2018-10-28 00:47:43,896 - Epoch: [141][  250/  391]    Overall Loss 0.181663    Objective Loss 0.181663    Top1 93.575000    Top5 99.937500    LR 0.030000    Time 0.022435    
2018-10-28 00:47:45,007 - Epoch: [141][  300/  391]    Overall Loss 0.183366    Objective Loss 0.183366    Top1 93.507812    Top5 99.937500    LR 0.030000    Time 0.022381    
2018-10-28 00:47:46,117 - Epoch: [141][  350/  391]    Overall Loss 0.185496    Objective Loss 0.185496    Top1 93.406250    Top5 99.928571    LR 0.030000    Time 0.022353    
2018-10-28 00:47:47,109 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.41530 | -0.00187 |    0.21159 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.13140 | -0.00364 |    0.04187 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.12928 | -0.00324 |    0.04656 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.13013 | -0.00852 |    0.05576 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09695 | -0.00401 |    0.02816 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.14075 | -0.00535 |    0.05574 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09520 |  0.00212 |    0.02619 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.13046 | -0.00564 |    0.06226 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11950 | -0.00509 |    0.06828 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.17289 | -0.00461 |    0.08466 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09576 | -0.00366 |    0.03814 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07860 |  0.00110 |    0.02840 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09948 | -0.00610 |    0.04375 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08116 | -0.00099 |    0.03645 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08362 | -0.00280 |    0.03228 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08700 | -0.00390 |    0.04352 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.09449 | -0.00197 |    0.03397 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08105 | -0.00333 |    0.03330 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06545 | -0.00030 |    0.02946 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04881 | -0.00100 |    0.01561 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03517 | -0.00011 |    0.00847 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56535 | -0.03256 |    0.33260 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:47:47,109 - Total sparsity: 74.94

2018-10-28 00:47:47,109 - --- validate (epoch=141)-----------
2018-10-28 00:47:47,109 - 10000 samples (128 per mini-batch)
2018-10-28 00:47:47,830 - Epoch: [141][   50/   78]    Loss 0.377332    Top1 88.453125    Top5 99.546875    
2018-10-28 00:47:48,223 - ==> Top1: 88.520    Top5: 99.600    Loss: 0.369

2018-10-28 00:47:48,224 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:47:48,224 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:47:48,240 - 

2018-10-28 00:47:48,241 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:47:49,384 - Epoch: [142][   50/  391]    Overall Loss 0.175746    Objective Loss 0.175746    Top1 93.984375    Top5 99.921875    LR 0.030000    Time 0.022837    
2018-10-28 00:47:50,494 - Epoch: [142][  100/  391]    Overall Loss 0.175851    Objective Loss 0.175851    Top1 93.781250    Top5 99.906250    LR 0.030000    Time 0.022509    
2018-10-28 00:47:51,605 - Epoch: [142][  150/  391]    Overall Loss 0.178133    Objective Loss 0.178133    Top1 93.541667    Top5 99.927083    LR 0.030000    Time 0.022400    
2018-10-28 00:47:52,716 - Epoch: [142][  200/  391]    Overall Loss 0.178128    Objective Loss 0.178128    Top1 93.574219    Top5 99.925781    LR 0.030000    Time 0.022348    
2018-10-28 00:47:53,828 - Epoch: [142][  250/  391]    Overall Loss 0.179778    Objective Loss 0.179778    Top1 93.587500    Top5 99.921875    LR 0.030000    Time 0.022323    
2018-10-28 00:47:54,940 - Epoch: [142][  300/  391]    Overall Loss 0.181568    Objective Loss 0.181568    Top1 93.520833    Top5 99.924479    LR 0.030000    Time 0.022305    
2018-10-28 00:47:56,050 - Epoch: [142][  350/  391]    Overall Loss 0.182594    Objective Loss 0.182594    Top1 93.457589    Top5 99.933036    LR 0.030000    Time 0.022277    
2018-10-28 00:47:57,039 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.41321 |  0.00048 |    0.20975 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.13079 | -0.00346 |    0.04154 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.12861 | -0.00320 |    0.04623 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12954 | -0.00827 |    0.05526 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09639 | -0.00369 |    0.02798 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.14005 | -0.00545 |    0.05537 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09472 |  0.00190 |    0.02594 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12987 | -0.00555 |    0.06200 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11901 | -0.00498 |    0.06807 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.17169 | -0.00455 |    0.08425 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09542 | -0.00363 |    0.03800 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07830 |  0.00118 |    0.02830 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09916 | -0.00581 |    0.04361 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08088 | -0.00104 |    0.03632 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08323 | -0.00283 |    0.03211 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08667 | -0.00383 |    0.04336 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.09388 | -0.00181 |    0.03365 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08076 | -0.00333 |    0.03315 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06529 | -0.00030 |    0.02939 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04877 | -0.00103 |    0.01560 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03505 | -0.00008 |    0.00843 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56507 | -0.03235 |    0.33254 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:47:57,039 - Total sparsity: 74.94

2018-10-28 00:47:57,039 - --- validate (epoch=142)-----------
2018-10-28 00:47:57,039 - 10000 samples (128 per mini-batch)
2018-10-28 00:47:57,751 - Epoch: [142][   50/   78]    Loss 0.384218    Top1 88.218750    Top5 99.640625    
2018-10-28 00:47:58,137 - ==> Top1: 88.250    Top5: 99.650    Loss: 0.385

2018-10-28 00:47:58,137 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:47:58,138 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:47:58,154 - 

2018-10-28 00:47:58,154 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:47:59,304 - Epoch: [143][   50/  391]    Overall Loss 0.191004    Objective Loss 0.191004    Top1 93.328125    Top5 99.890625    LR 0.030000    Time 0.022959    
2018-10-28 00:48:00,504 - Epoch: [143][  100/  391]    Overall Loss 0.184408    Objective Loss 0.184408    Top1 93.656250    Top5 99.921875    LR 0.030000    Time 0.023474    
2018-10-28 00:48:01,704 - Epoch: [143][  150/  391]    Overall Loss 0.183898    Objective Loss 0.183898    Top1 93.687500    Top5 99.921875    LR 0.030000    Time 0.023633    
2018-10-28 00:48:02,904 - Epoch: [143][  200/  391]    Overall Loss 0.184109    Objective Loss 0.184109    Top1 93.726562    Top5 99.929688    LR 0.030000    Time 0.023719    
2018-10-28 00:48:04,101 - Epoch: [143][  250/  391]    Overall Loss 0.186246    Objective Loss 0.186246    Top1 93.678125    Top5 99.925000    LR 0.030000    Time 0.023760    
2018-10-28 00:48:05,298 - Epoch: [143][  300/  391]    Overall Loss 0.184872    Objective Loss 0.184872    Top1 93.671875    Top5 99.927083    LR 0.030000    Time 0.023782    
2018-10-28 00:48:06,492 - Epoch: [143][  350/  391]    Overall Loss 0.185055    Objective Loss 0.185055    Top1 93.642857    Top5 99.921875    LR 0.030000    Time 0.023795    
2018-10-28 00:48:07,559 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.41147 |  0.00017 |    0.20957 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.13025 | -0.00379 |    0.04147 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.12799 | -0.00309 |    0.04606 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12903 | -0.00818 |    0.05511 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09591 | -0.00378 |    0.02789 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13940 | -0.00568 |    0.05520 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09428 |  0.00191 |    0.02584 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12932 | -0.00561 |    0.06178 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11856 | -0.00491 |    0.06787 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.17059 | -0.00379 |    0.08390 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09512 | -0.00371 |    0.03787 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07803 |  0.00113 |    0.02815 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09886 | -0.00572 |    0.04335 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08063 | -0.00078 |    0.03622 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08285 | -0.00283 |    0.03197 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08636 | -0.00375 |    0.04317 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.09329 | -0.00198 |    0.03347 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08049 | -0.00334 |    0.03306 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06514 | -0.00031 |    0.02931 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04874 | -0.00095 |    0.01558 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03494 | -0.00006 |    0.00841 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56435 | -0.03231 |    0.33187 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:48:07,560 - Total sparsity: 74.94

2018-10-28 00:48:07,560 - --- validate (epoch=143)-----------
2018-10-28 00:48:07,560 - 10000 samples (128 per mini-batch)
2018-10-28 00:48:08,305 - Epoch: [143][   50/   78]    Loss 0.387833    Top1 88.437500    Top5 99.515625    
2018-10-28 00:48:08,706 - ==> Top1: 88.410    Top5: 99.550    Loss: 0.382

2018-10-28 00:48:08,706 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:48:08,707 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:48:08,717 - 

2018-10-28 00:48:08,718 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:48:09,957 - Epoch: [144][   50/  391]    Overall Loss 0.173338    Objective Loss 0.173338    Top1 93.531250    Top5 99.921875    LR 0.030000    Time 0.024751    
2018-10-28 00:48:11,146 - Epoch: [144][  100/  391]    Overall Loss 0.176483    Objective Loss 0.176483    Top1 93.562500    Top5 99.914062    LR 0.030000    Time 0.024255    
2018-10-28 00:48:12,258 - Epoch: [144][  150/  391]    Overall Loss 0.183836    Objective Loss 0.183836    Top1 93.343750    Top5 99.901042    LR 0.030000    Time 0.023569    
2018-10-28 00:48:13,368 - Epoch: [144][  200/  391]    Overall Loss 0.186397    Objective Loss 0.186397    Top1 93.222656    Top5 99.906250    LR 0.030000    Time 0.023224    
2018-10-28 00:48:14,480 - Epoch: [144][  250/  391]    Overall Loss 0.185114    Objective Loss 0.185114    Top1 93.318750    Top5 99.921875    LR 0.030000    Time 0.023022    
2018-10-28 00:48:15,592 - Epoch: [144][  300/  391]    Overall Loss 0.184749    Objective Loss 0.184749    Top1 93.286458    Top5 99.929688    LR 0.030000    Time 0.022887    
2018-10-28 00:48:16,704 - Epoch: [144][  350/  391]    Overall Loss 0.184759    Objective Loss 0.184759    Top1 93.270089    Top5 99.933036    LR 0.030000    Time 0.022790    
2018-10-28 00:48:17,695 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.40961 |  0.00082 |    0.20834 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12973 | -0.00346 |    0.04132 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.12741 | -0.00303 |    0.04579 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12852 | -0.00780 |    0.05487 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09542 | -0.00394 |    0.02774 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13882 | -0.00545 |    0.05503 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09389 |  0.00181 |    0.02575 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12878 | -0.00600 |    0.06148 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11812 | -0.00495 |    0.06750 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.16953 | -0.00403 |    0.08335 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09483 | -0.00366 |    0.03773 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07780 |  0.00121 |    0.02797 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09856 | -0.00576 |    0.04328 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08039 | -0.00077 |    0.03609 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08250 | -0.00286 |    0.03183 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08607 | -0.00371 |    0.04302 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.09272 | -0.00211 |    0.03333 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08025 | -0.00339 |    0.03298 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06500 | -0.00033 |    0.02926 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04871 | -0.00101 |    0.01558 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03484 | -0.00006 |    0.00839 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56433 | -0.03243 |    0.33205 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:48:17,695 - Total sparsity: 74.94

2018-10-28 00:48:17,695 - --- validate (epoch=144)-----------
2018-10-28 00:48:17,695 - 10000 samples (128 per mini-batch)
2018-10-28 00:48:18,416 - Epoch: [144][   50/   78]    Loss 0.408965    Top1 88.031250    Top5 99.421875    
2018-10-28 00:48:18,804 - ==> Top1: 88.040    Top5: 99.550    Loss: 0.400

2018-10-28 00:48:18,805 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:48:18,805 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:48:18,816 - 

2018-10-28 00:48:18,816 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:48:19,986 - Epoch: [145][   50/  391]    Overall Loss 0.171337    Objective Loss 0.171337    Top1 94.265625    Top5 99.890625    LR 0.030000    Time 0.023367    
2018-10-28 00:48:21,094 - Epoch: [145][  100/  391]    Overall Loss 0.170444    Objective Loss 0.170444    Top1 94.140625    Top5 99.898438    LR 0.030000    Time 0.022742    
2018-10-28 00:48:22,204 - Epoch: [145][  150/  391]    Overall Loss 0.173343    Objective Loss 0.173343    Top1 93.958333    Top5 99.901042    LR 0.030000    Time 0.022554    
2018-10-28 00:48:23,317 - Epoch: [145][  200/  391]    Overall Loss 0.175881    Objective Loss 0.175881    Top1 93.789062    Top5 99.902344    LR 0.030000    Time 0.022474    
2018-10-28 00:48:24,429 - Epoch: [145][  250/  391]    Overall Loss 0.179137    Objective Loss 0.179137    Top1 93.631250    Top5 99.909375    LR 0.030000    Time 0.022424    
2018-10-28 00:48:25,542 - Epoch: [145][  300/  391]    Overall Loss 0.180248    Objective Loss 0.180248    Top1 93.606771    Top5 99.906250    LR 0.030000    Time 0.022391    
2018-10-28 00:48:26,653 - Epoch: [145][  350/  391]    Overall Loss 0.180295    Objective Loss 0.180295    Top1 93.600446    Top5 99.904018    LR 0.030000    Time 0.022361    
2018-10-28 00:48:27,647 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.40766 |  0.00138 |    0.20765 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12920 | -0.00360 |    0.04113 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.12679 | -0.00330 |    0.04551 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12799 | -0.00815 |    0.05453 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09493 | -0.00402 |    0.02767 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13819 | -0.00558 |    0.05453 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09345 |  0.00177 |    0.02555 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12825 | -0.00581 |    0.06102 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11765 | -0.00483 |    0.06731 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.16846 | -0.00335 |    0.08254 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09453 | -0.00341 |    0.03761 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07754 |  0.00126 |    0.02788 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09826 | -0.00583 |    0.04307 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.08014 | -0.00078 |    0.03595 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08215 | -0.00286 |    0.03169 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08577 | -0.00380 |    0.04286 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.09217 | -0.00210 |    0.03319 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.08000 | -0.00348 |    0.03285 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06487 | -0.00035 |    0.02918 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04866 | -0.00106 |    0.01557 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03473 | -0.00003 |    0.00836 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56379 | -0.03217 |    0.33177 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:48:27,647 - Total sparsity: 74.94

2018-10-28 00:48:27,647 - --- validate (epoch=145)-----------
2018-10-28 00:48:27,647 - 10000 samples (128 per mini-batch)
2018-10-28 00:48:28,371 - Epoch: [145][   50/   78]    Loss 0.401326    Top1 88.375000    Top5 99.515625    
2018-10-28 00:48:28,759 - ==> Top1: 88.040    Top5: 99.630    Loss: 0.403

2018-10-28 00:48:28,759 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:48:28,760 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:48:28,769 - 

2018-10-28 00:48:28,770 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:48:29,942 - Epoch: [146][   50/  391]    Overall Loss 0.170620    Objective Loss 0.170620    Top1 94.046875    Top5 99.937500    LR 0.030000    Time 0.023421    
2018-10-28 00:48:31,053 - Epoch: [146][  100/  391]    Overall Loss 0.175435    Objective Loss 0.175435    Top1 93.773438    Top5 99.945312    LR 0.030000    Time 0.022801    
2018-10-28 00:48:32,166 - Epoch: [146][  150/  391]    Overall Loss 0.176154    Objective Loss 0.176154    Top1 93.661458    Top5 99.947917    LR 0.030000    Time 0.022615    
2018-10-28 00:48:33,278 - Epoch: [146][  200/  391]    Overall Loss 0.176079    Objective Loss 0.176079    Top1 93.636719    Top5 99.941406    LR 0.030000    Time 0.022515    
2018-10-28 00:48:34,391 - Epoch: [146][  250/  391]    Overall Loss 0.174777    Objective Loss 0.174777    Top1 93.762500    Top5 99.946875    LR 0.030000    Time 0.022457    
2018-10-28 00:48:35,502 - Epoch: [146][  300/  391]    Overall Loss 0.175231    Objective Loss 0.175231    Top1 93.734375    Top5 99.940104    LR 0.030000    Time 0.022415    
2018-10-28 00:48:36,615 - Epoch: [146][  350/  391]    Overall Loss 0.176494    Objective Loss 0.176494    Top1 93.743304    Top5 99.937500    LR 0.030000    Time 0.022386    
2018-10-28 00:48:37,605 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.40582 | -0.00003 |    0.20661 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12865 | -0.00336 |    0.04105 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.12621 | -0.00310 |    0.04524 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12751 | -0.00782 |    0.05437 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09447 | -0.00384 |    0.02754 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13754 | -0.00570 |    0.05429 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09303 |  0.00186 |    0.02548 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12776 | -0.00521 |    0.06084 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11723 | -0.00489 |    0.06708 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.16747 | -0.00300 |    0.08245 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09423 | -0.00337 |    0.03749 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07729 |  0.00124 |    0.02778 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09798 | -0.00586 |    0.04295 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07993 | -0.00086 |    0.03583 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08182 | -0.00279 |    0.03152 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08551 | -0.00370 |    0.04271 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.09164 | -0.00226 |    0.03304 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07978 | -0.00345 |    0.03275 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06473 | -0.00030 |    0.02912 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04863 | -0.00095 |    0.01554 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03463 | -0.00001 |    0.00832 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56404 | -0.03212 |    0.33164 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:48:37,605 - Total sparsity: 74.94

2018-10-28 00:48:37,605 - --- validate (epoch=146)-----------
2018-10-28 00:48:37,605 - 10000 samples (128 per mini-batch)
2018-10-28 00:48:38,323 - Epoch: [146][   50/   78]    Loss 0.398368    Top1 88.484375    Top5 99.531250    
2018-10-28 00:48:38,712 - ==> Top1: 88.240    Top5: 99.600    Loss: 0.403

2018-10-28 00:48:38,712 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:48:38,712 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:48:38,728 - 

2018-10-28 00:48:38,729 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:48:39,871 - Epoch: [147][   50/  391]    Overall Loss 0.177625    Objective Loss 0.177625    Top1 93.515625    Top5 99.921875    LR 0.030000    Time 0.022805    
2018-10-28 00:48:40,982 - Epoch: [147][  100/  391]    Overall Loss 0.180507    Objective Loss 0.180507    Top1 93.398438    Top5 99.921875    LR 0.030000    Time 0.022499    
2018-10-28 00:48:42,092 - Epoch: [147][  150/  391]    Overall Loss 0.185930    Objective Loss 0.185930    Top1 93.265625    Top5 99.916667    LR 0.030000    Time 0.022393    
2018-10-28 00:48:43,204 - Epoch: [147][  200/  391]    Overall Loss 0.184905    Objective Loss 0.184905    Top1 93.292969    Top5 99.906250    LR 0.030000    Time 0.022348    
2018-10-28 00:48:44,315 - Epoch: [147][  250/  391]    Overall Loss 0.184529    Objective Loss 0.184529    Top1 93.293750    Top5 99.915625    LR 0.030000    Time 0.022315    
2018-10-28 00:48:45,427 - Epoch: [147][  300/  391]    Overall Loss 0.185368    Objective Loss 0.185368    Top1 93.255208    Top5 99.914062    LR 0.030000    Time 0.022300    
2018-10-28 00:48:46,540 - Epoch: [147][  350/  391]    Overall Loss 0.185549    Objective Loss 0.185549    Top1 93.285714    Top5 99.912946    LR 0.030000    Time 0.022291    
2018-10-28 00:48:47,533 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.40419 |  0.00074 |    0.20533 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12824 | -0.00323 |    0.04081 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.12568 | -0.00352 |    0.04518 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12714 | -0.00771 |    0.05428 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09410 | -0.00340 |    0.02744 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13705 | -0.00530 |    0.05415 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09268 |  0.00177 |    0.02531 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12731 | -0.00531 |    0.06064 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11689 | -0.00518 |    0.06686 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.16649 | -0.00367 |    0.08203 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09398 | -0.00358 |    0.03732 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07710 |  0.00111 |    0.02775 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09775 | -0.00600 |    0.04284 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07974 | -0.00086 |    0.03577 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08154 | -0.00274 |    0.03143 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08531 | -0.00362 |    0.04255 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.09115 | -0.00214 |    0.03292 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07961 | -0.00331 |    0.03267 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06466 | -0.00030 |    0.02910 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04864 | -0.00093 |    0.01553 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03455 | -0.00002 |    0.00831 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56309 | -0.03203 |    0.33139 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:48:47,533 - Total sparsity: 74.94

2018-10-28 00:48:47,533 - --- validate (epoch=147)-----------
2018-10-28 00:48:47,534 - 10000 samples (128 per mini-batch)
2018-10-28 00:48:48,256 - Epoch: [147][   50/   78]    Loss 0.445732    Top1 87.093750    Top5 99.406250    
2018-10-28 00:48:48,646 - ==> Top1: 87.190    Top5: 99.530    Loss: 0.437

2018-10-28 00:48:48,647 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:48:48,647 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:48:48,664 - 

2018-10-28 00:48:48,664 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:48:49,809 - Epoch: [148][   50/  391]    Overall Loss 0.186921    Objective Loss 0.186921    Top1 93.453125    Top5 99.921875    LR 0.030000    Time 0.022877    
2018-10-28 00:48:50,922 - Epoch: [148][  100/  391]    Overall Loss 0.181908    Objective Loss 0.181908    Top1 93.632812    Top5 99.921875    LR 0.030000    Time 0.022549    
2018-10-28 00:48:52,033 - Epoch: [148][  150/  391]    Overall Loss 0.179977    Objective Loss 0.179977    Top1 93.640625    Top5 99.927083    LR 0.030000    Time 0.022429    
2018-10-28 00:48:53,145 - Epoch: [148][  200/  391]    Overall Loss 0.182277    Objective Loss 0.182277    Top1 93.585938    Top5 99.906250    LR 0.030000    Time 0.022377    
2018-10-28 00:48:54,257 - Epoch: [148][  250/  391]    Overall Loss 0.182180    Objective Loss 0.182180    Top1 93.581250    Top5 99.912500    LR 0.030000    Time 0.022346    
2018-10-28 00:48:55,370 - Epoch: [148][  300/  391]    Overall Loss 0.184180    Objective Loss 0.184180    Top1 93.479167    Top5 99.908854    LR 0.030000    Time 0.022325    
2018-10-28 00:48:56,481 - Epoch: [148][  350/  391]    Overall Loss 0.183595    Objective Loss 0.183595    Top1 93.475446    Top5 99.908482    LR 0.030000    Time 0.022308    
2018-10-28 00:48:57,470 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.40298 |  0.00081 |    0.20442 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12787 | -0.00335 |    0.04075 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.12526 | -0.00301 |    0.04503 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12680 | -0.00761 |    0.05410 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09372 | -0.00363 |    0.02727 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13658 | -0.00545 |    0.05400 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09233 |  0.00162 |    0.02525 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12682 | -0.00561 |    0.06040 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11656 | -0.00474 |    0.06662 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.16553 | -0.00388 |    0.08170 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09372 | -0.00361 |    0.03725 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07688 |  0.00124 |    0.02759 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09753 | -0.00587 |    0.04273 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07953 | -0.00087 |    0.03570 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08124 | -0.00274 |    0.03127 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08508 | -0.00352 |    0.04244 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.09063 | -0.00225 |    0.03278 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07943 | -0.00330 |    0.03261 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06456 | -0.00034 |    0.02906 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04864 | -0.00089 |    0.01553 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03447 |  0.00002 |    0.00828 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56347 | -0.03198 |    0.33139 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:48:57,470 - Total sparsity: 74.94

2018-10-28 00:48:57,470 - --- validate (epoch=148)-----------
2018-10-28 00:48:57,470 - 10000 samples (128 per mini-batch)
2018-10-28 00:48:58,180 - Epoch: [148][   50/   78]    Loss 0.396507    Top1 88.734375    Top5 99.531250    
2018-10-28 00:48:58,563 - ==> Top1: 88.560    Top5: 99.570    Loss: 0.402

2018-10-28 00:48:58,563 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:48:58,564 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:48:58,574 - 

2018-10-28 00:48:58,574 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:48:59,750 - Epoch: [149][   50/  391]    Overall Loss 0.162307    Objective Loss 0.162307    Top1 94.109375    Top5 99.921875    LR 0.030000    Time 0.023478    
2018-10-28 00:49:00,866 - Epoch: [149][  100/  391]    Overall Loss 0.168566    Objective Loss 0.168566    Top1 94.093750    Top5 99.929688    LR 0.030000    Time 0.022883    
2018-10-28 00:49:01,978 - Epoch: [149][  150/  391]    Overall Loss 0.169472    Objective Loss 0.169472    Top1 93.895833    Top5 99.932292    LR 0.030000    Time 0.022660    
2018-10-28 00:49:03,090 - Epoch: [149][  200/  391]    Overall Loss 0.173963    Objective Loss 0.173963    Top1 93.859375    Top5 99.941406    LR 0.030000    Time 0.022550    
2018-10-28 00:49:04,201 - Epoch: [149][  250/  391]    Overall Loss 0.178085    Objective Loss 0.178085    Top1 93.675000    Top5 99.934375    LR 0.030000    Time 0.022478    
2018-10-28 00:49:05,313 - Epoch: [149][  300/  391]    Overall Loss 0.178572    Objective Loss 0.178572    Top1 93.653646    Top5 99.924479    LR 0.030000    Time 0.022436    
2018-10-28 00:49:06,426 - Epoch: [149][  350/  391]    Overall Loss 0.178640    Objective Loss 0.178640    Top1 93.647321    Top5 99.924107    LR 0.030000    Time 0.022405    
2018-10-28 00:49:07,416 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.40154 |  0.00142 |    0.20410 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12746 | -0.00357 |    0.04064 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.12483 | -0.00318 |    0.04486 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12649 | -0.00721 |    0.05406 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09333 | -0.00385 |    0.02700 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13606 | -0.00535 |    0.05379 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09195 |  0.00168 |    0.02522 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12636 | -0.00553 |    0.06018 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11620 | -0.00500 |    0.06641 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.16465 | -0.00334 |    0.08151 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09347 | -0.00367 |    0.03722 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07666 |  0.00125 |    0.02757 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09732 | -0.00569 |    0.04268 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07935 | -0.00107 |    0.03565 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08096 | -0.00273 |    0.03119 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08486 | -0.00364 |    0.04232 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.09012 | -0.00227 |    0.03263 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07923 | -0.00329 |    0.03254 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06448 | -0.00044 |    0.02898 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04864 | -0.00085 |    0.01553 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03440 |  0.00002 |    0.00826 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56336 | -0.03202 |    0.33155 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:49:07,417 - Total sparsity: 74.94

2018-10-28 00:49:07,417 - --- validate (epoch=149)-----------
2018-10-28 00:49:07,417 - 10000 samples (128 per mini-batch)
2018-10-28 00:49:08,144 - Epoch: [149][   50/   78]    Loss 0.453529    Top1 86.953125    Top5 99.468750    
2018-10-28 00:49:08,539 - ==> Top1: 86.970    Top5: 99.510    Loss: 0.449

2018-10-28 00:49:08,540 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:49:08,540 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:49:08,551 - 

2018-10-28 00:49:08,551 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:49:09,724 - Epoch: [150][   50/  391]    Overall Loss 0.170010    Objective Loss 0.170010    Top1 93.859375    Top5 99.921875    LR 0.030000    Time 0.023417    
2018-10-28 00:49:10,836 - Epoch: [150][  100/  391]    Overall Loss 0.179080    Objective Loss 0.179080    Top1 93.617188    Top5 99.890625    LR 0.030000    Time 0.022821    
2018-10-28 00:49:11,949 - Epoch: [150][  150/  391]    Overall Loss 0.180451    Objective Loss 0.180451    Top1 93.536458    Top5 99.890625    LR 0.030000    Time 0.022620    
2018-10-28 00:49:13,061 - Epoch: [150][  200/  391]    Overall Loss 0.182364    Objective Loss 0.182364    Top1 93.453125    Top5 99.914062    LR 0.030000    Time 0.022521    
2018-10-28 00:49:14,173 - Epoch: [150][  250/  391]    Overall Loss 0.181823    Objective Loss 0.181823    Top1 93.490625    Top5 99.912500    LR 0.030000    Time 0.022458    
2018-10-28 00:49:15,285 - Epoch: [150][  300/  391]    Overall Loss 0.184094    Objective Loss 0.184094    Top1 93.460938    Top5 99.901042    LR 0.030000    Time 0.022419    
2018-10-28 00:49:16,398 - Epoch: [150][  350/  391]    Overall Loss 0.186148    Objective Loss 0.186148    Top1 93.412946    Top5 99.897321    LR 0.030000    Time 0.022391    
2018-10-28 00:49:17,386 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.39986 |  0.00078 |    0.20330 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12705 | -0.00334 |    0.04065 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.12438 | -0.00295 |    0.04458 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12613 | -0.00747 |    0.05375 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09298 | -0.00356 |    0.02683 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13560 | -0.00540 |    0.05384 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09164 |  0.00180 |    0.02509 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12593 | -0.00574 |    0.06001 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11589 | -0.00494 |    0.06623 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.16380 | -0.00421 |    0.08101 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09325 | -0.00377 |    0.03714 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07648 |  0.00120 |    0.02741 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09712 | -0.00567 |    0.04260 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07918 | -0.00096 |    0.03549 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08069 | -0.00272 |    0.03108 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08466 | -0.00367 |    0.04225 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08967 | -0.00192 |    0.03237 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07909 | -0.00316 |    0.03247 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06442 | -0.00033 |    0.02899 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04866 | -0.00084 |    0.01554 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03434 |  0.00003 |    0.00824 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56247 | -0.03173 |    0.33102 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:49:17,386 - Total sparsity: 74.94

2018-10-28 00:49:17,386 - --- validate (epoch=150)-----------
2018-10-28 00:49:17,386 - 10000 samples (128 per mini-batch)
2018-10-28 00:49:18,110 - Epoch: [150][   50/   78]    Loss 0.386752    Top1 88.578125    Top5 99.500000    
2018-10-28 00:49:18,502 - ==> Top1: 88.300    Top5: 99.590    Loss: 0.392

2018-10-28 00:49:18,502 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:49:18,503 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:49:18,517 - 

2018-10-28 00:49:18,517 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:49:19,689 - Epoch: [151][   50/  391]    Overall Loss 0.187701    Objective Loss 0.187701    Top1 93.515625    Top5 99.828125    LR 0.030000    Time 0.023405    
2018-10-28 00:49:20,803 - Epoch: [151][  100/  391]    Overall Loss 0.181665    Objective Loss 0.181665    Top1 93.671875    Top5 99.906250    LR 0.030000    Time 0.022822    
2018-10-28 00:49:21,915 - Epoch: [151][  150/  391]    Overall Loss 0.181834    Objective Loss 0.181834    Top1 93.557292    Top5 99.927083    LR 0.030000    Time 0.022620    
2018-10-28 00:49:23,028 - Epoch: [151][  200/  391]    Overall Loss 0.183406    Objective Loss 0.183406    Top1 93.515625    Top5 99.917969    LR 0.030000    Time 0.022525    
2018-10-28 00:49:24,141 - Epoch: [151][  250/  391]    Overall Loss 0.184684    Objective Loss 0.184684    Top1 93.493750    Top5 99.906250    LR 0.030000    Time 0.022464    
2018-10-28 00:49:25,250 - Epoch: [151][  300/  391]    Overall Loss 0.187255    Objective Loss 0.187255    Top1 93.364583    Top5 99.914062    LR 0.030000    Time 0.022416    
2018-10-28 00:49:26,360 - Epoch: [151][  350/  391]    Overall Loss 0.188183    Objective Loss 0.188183    Top1 93.368304    Top5 99.906250    LR 0.030000    Time 0.022380    
2018-10-28 00:49:27,350 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.39854 |  0.00323 |    0.20232 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12660 | -0.00373 |    0.04052 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.12391 | -0.00288 |    0.04435 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12573 | -0.00776 |    0.05364 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09263 | -0.00370 |    0.02675 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13515 | -0.00574 |    0.05355 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09132 |  0.00185 |    0.02500 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12552 | -0.00538 |    0.05975 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11556 | -0.00483 |    0.06604 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.16296 | -0.00393 |    0.08072 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09305 | -0.00361 |    0.03706 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07629 |  0.00120 |    0.02738 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09691 | -0.00572 |    0.04246 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07899 | -0.00081 |    0.03532 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08043 | -0.00256 |    0.03093 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08446 | -0.00360 |    0.04216 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08924 | -0.00191 |    0.03202 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07892 | -0.00314 |    0.03239 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06437 | -0.00039 |    0.02896 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04868 | -0.00089 |    0.01557 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03429 |  0.00004 |    0.00821 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56189 | -0.03186 |    0.33074 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:49:27,350 - Total sparsity: 74.94

2018-10-28 00:49:27,350 - --- validate (epoch=151)-----------
2018-10-28 00:49:27,350 - 10000 samples (128 per mini-batch)
2018-10-28 00:49:28,075 - Epoch: [151][   50/   78]    Loss 0.434999    Top1 87.265625    Top5 99.468750    
2018-10-28 00:49:28,469 - ==> Top1: 87.400    Top5: 99.540    Loss: 0.421

2018-10-28 00:49:28,469 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:49:28,470 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:49:28,479 - 

2018-10-28 00:49:28,480 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:49:29,652 - Epoch: [152][   50/  391]    Overall Loss 0.170880    Objective Loss 0.170880    Top1 93.812500    Top5 99.937500    LR 0.030000    Time 0.023410    
2018-10-28 00:49:30,762 - Epoch: [152][  100/  391]    Overall Loss 0.170859    Objective Loss 0.170859    Top1 93.804688    Top5 99.953125    LR 0.030000    Time 0.022797    
2018-10-28 00:49:31,873 - Epoch: [152][  150/  391]    Overall Loss 0.176458    Objective Loss 0.176458    Top1 93.635417    Top5 99.916667    LR 0.030000    Time 0.022593    
2018-10-28 00:49:32,984 - Epoch: [152][  200/  391]    Overall Loss 0.177342    Objective Loss 0.177342    Top1 93.660156    Top5 99.925781    LR 0.030000    Time 0.022491    
2018-10-28 00:49:34,096 - Epoch: [152][  250/  391]    Overall Loss 0.181773    Objective Loss 0.181773    Top1 93.543750    Top5 99.912500    LR 0.030000    Time 0.022438    
2018-10-28 00:49:35,208 - Epoch: [152][  300/  391]    Overall Loss 0.184032    Objective Loss 0.184032    Top1 93.476562    Top5 99.916667    LR 0.030000    Time 0.022400    
2018-10-28 00:49:36,320 - Epoch: [152][  350/  391]    Overall Loss 0.183023    Objective Loss 0.183023    Top1 93.508929    Top5 99.926339    LR 0.030000    Time 0.022375    
2018-10-28 00:49:37,311 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.39750 | -0.00140 |    0.20130 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12625 | -0.00313 |    0.04035 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.12350 | -0.00301 |    0.04423 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12546 | -0.00768 |    0.05358 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09229 | -0.00408 |    0.02674 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13476 | -0.00556 |    0.05336 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09105 |  0.00177 |    0.02501 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12514 | -0.00510 |    0.05950 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11527 | -0.00475 |    0.06583 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.16217 | -0.00283 |    0.07969 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09283 | -0.00361 |    0.03695 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07611 |  0.00114 |    0.02730 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09675 | -0.00565 |    0.04233 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07882 | -0.00092 |    0.03532 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.08017 | -0.00253 |    0.03091 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08427 | -0.00356 |    0.04207 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08878 | -0.00205 |    0.03195 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07876 | -0.00324 |    0.03231 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06432 | -0.00029 |    0.02894 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04868 | -0.00088 |    0.01553 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03422 |  0.00005 |    0.00820 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56183 | -0.03194 |    0.33053 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:49:37,311 - Total sparsity: 74.94

2018-10-28 00:49:37,311 - --- validate (epoch=152)-----------
2018-10-28 00:49:37,311 - 10000 samples (128 per mini-batch)
2018-10-28 00:49:38,037 - Epoch: [152][   50/   78]    Loss 0.385470    Top1 88.421875    Top5 99.562500    
2018-10-28 00:49:38,429 - ==> Top1: 88.530    Top5: 99.620    Loss: 0.383

2018-10-28 00:49:38,430 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:49:38,430 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:49:38,446 - 

2018-10-28 00:49:38,447 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:49:39,591 - Epoch: [153][   50/  391]    Overall Loss 0.189633    Objective Loss 0.189633    Top1 93.500000    Top5 99.953125    LR 0.030000    Time 0.022856    
2018-10-28 00:49:40,703 - Epoch: [153][  100/  391]    Overall Loss 0.190477    Objective Loss 0.190477    Top1 93.445312    Top5 99.914062    LR 0.030000    Time 0.022533    
2018-10-28 00:49:41,815 - Epoch: [153][  150/  391]    Overall Loss 0.186490    Objective Loss 0.186490    Top1 93.541667    Top5 99.911458    LR 0.030000    Time 0.022428    
2018-10-28 00:49:42,929 - Epoch: [153][  200/  391]    Overall Loss 0.183356    Objective Loss 0.183356    Top1 93.566406    Top5 99.906250    LR 0.030000    Time 0.022383    
2018-10-28 00:49:44,044 - Epoch: [153][  250/  391]    Overall Loss 0.183944    Objective Loss 0.183944    Top1 93.503125    Top5 99.909375    LR 0.030000    Time 0.022361    
2018-10-28 00:49:45,154 - Epoch: [153][  300/  391]    Overall Loss 0.185752    Objective Loss 0.185752    Top1 93.388021    Top5 99.914062    LR 0.030000    Time 0.022330    
2018-10-28 00:49:46,267 - Epoch: [153][  350/  391]    Overall Loss 0.184931    Objective Loss 0.184931    Top1 93.455357    Top5 99.910714    LR 0.030000    Time 0.022317    
2018-10-28 00:49:47,259 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.39660 |  0.00096 |    0.20146 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12583 | -0.00335 |    0.04012 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.12307 | -0.00278 |    0.04409 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12511 | -0.00788 |    0.05367 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09196 | -0.00399 |    0.02662 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13433 | -0.00563 |    0.05334 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09072 |  0.00206 |    0.02497 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12474 | -0.00490 |    0.05936 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11494 | -0.00483 |    0.06567 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.16136 | -0.00312 |    0.07959 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09263 | -0.00347 |    0.03688 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07594 |  0.00118 |    0.02727 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09658 | -0.00566 |    0.04228 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07864 | -0.00083 |    0.03529 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07992 | -0.00253 |    0.03082 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08409 | -0.00355 |    0.04193 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08833 | -0.00229 |    0.03186 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07860 | -0.00318 |    0.03223 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06427 | -0.00043 |    0.02890 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04870 | -0.00091 |    0.01555 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03417 |  0.00003 |    0.00819 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56200 | -0.03186 |    0.33061 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:49:47,259 - Total sparsity: 74.94

2018-10-28 00:49:47,259 - --- validate (epoch=153)-----------
2018-10-28 00:49:47,259 - 10000 samples (128 per mini-batch)
2018-10-28 00:49:47,979 - Epoch: [153][   50/   78]    Loss 0.381625    Top1 88.656250    Top5 99.453125    
2018-10-28 00:49:48,367 - ==> Top1: 88.730    Top5: 99.580    Loss: 0.377

2018-10-28 00:49:48,368 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:49:48,368 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:49:48,378 - 

2018-10-28 00:49:48,379 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:49:49,551 - Epoch: [154][   50/  391]    Overall Loss 0.170929    Objective Loss 0.170929    Top1 94.062500    Top5 99.921875    LR 0.030000    Time 0.023412    
2018-10-28 00:49:50,663 - Epoch: [154][  100/  391]    Overall Loss 0.173963    Objective Loss 0.173963    Top1 93.945312    Top5 99.937500    LR 0.030000    Time 0.022807    
2018-10-28 00:49:51,773 - Epoch: [154][  150/  391]    Overall Loss 0.172677    Objective Loss 0.172677    Top1 93.932292    Top5 99.927083    LR 0.030000    Time 0.022601    
2018-10-28 00:49:52,883 - Epoch: [154][  200/  391]    Overall Loss 0.175554    Objective Loss 0.175554    Top1 93.730469    Top5 99.925781    LR 0.030000    Time 0.022494    
2018-10-28 00:49:53,993 - Epoch: [154][  250/  391]    Overall Loss 0.179482    Objective Loss 0.179482    Top1 93.571875    Top5 99.925000    LR 0.030000    Time 0.022429    
2018-10-28 00:49:55,103 - Epoch: [154][  300/  391]    Overall Loss 0.182098    Objective Loss 0.182098    Top1 93.520833    Top5 99.906250    LR 0.030000    Time 0.022387    
2018-10-28 00:49:56,215 - Epoch: [154][  350/  391]    Overall Loss 0.183301    Objective Loss 0.183301    Top1 93.464286    Top5 99.897321    LR 0.030000    Time 0.022362    
2018-10-28 00:49:57,204 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.39523 |  0.00358 |    0.19982 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12551 | -0.00313 |    0.03996 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.12264 | -0.00264 |    0.04402 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12482 | -0.00738 |    0.05366 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09165 | -0.00422 |    0.02661 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13391 | -0.00521 |    0.05300 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09041 |  0.00213 |    0.02468 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12437 | -0.00489 |    0.05917 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11467 | -0.00492 |    0.06553 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.16058 | -0.00358 |    0.07884 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09247 | -0.00334 |    0.03689 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07578 |  0.00120 |    0.02714 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09645 | -0.00559 |    0.04223 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07851 | -0.00106 |    0.03528 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07971 | -0.00250 |    0.03073 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08393 | -0.00358 |    0.04187 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08790 | -0.00223 |    0.03162 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07848 | -0.00309 |    0.03217 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06425 | -0.00037 |    0.02889 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04873 | -0.00099 |    0.01556 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03413 |  0.00005 |    0.00818 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56150 | -0.03156 |    0.33026 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:49:57,205 - Total sparsity: 74.94

2018-10-28 00:49:57,205 - --- validate (epoch=154)-----------
2018-10-28 00:49:57,205 - 10000 samples (128 per mini-batch)
2018-10-28 00:49:57,930 - Epoch: [154][   50/   78]    Loss 0.392889    Top1 88.375000    Top5 99.343750    
2018-10-28 00:49:58,323 - ==> Top1: 88.350    Top5: 99.460    Loss: 0.392

2018-10-28 00:49:58,324 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:49:58,324 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:49:58,334 - 

2018-10-28 00:49:58,335 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:49:59,508 - Epoch: [155][   50/  391]    Overall Loss 0.176793    Objective Loss 0.176793    Top1 93.640625    Top5 99.968750    LR 0.030000    Time 0.023436    
2018-10-28 00:50:00,623 - Epoch: [155][  100/  391]    Overall Loss 0.179240    Objective Loss 0.179240    Top1 93.468750    Top5 99.945312    LR 0.030000    Time 0.022856    
2018-10-28 00:50:01,734 - Epoch: [155][  150/  391]    Overall Loss 0.181570    Objective Loss 0.181570    Top1 93.447917    Top5 99.947917    LR 0.030000    Time 0.022633    
2018-10-28 00:50:02,846 - Epoch: [155][  200/  391]    Overall Loss 0.184260    Objective Loss 0.184260    Top1 93.332031    Top5 99.949219    LR 0.030000    Time 0.022512    
2018-10-28 00:50:03,958 - Epoch: [155][  250/  391]    Overall Loss 0.182040    Objective Loss 0.182040    Top1 93.378125    Top5 99.953125    LR 0.030000    Time 0.022453    
2018-10-28 00:50:05,070 - Epoch: [155][  300/  391]    Overall Loss 0.184628    Objective Loss 0.184628    Top1 93.307292    Top5 99.942708    LR 0.030000    Time 0.022413    
2018-10-28 00:50:06,183 - Epoch: [155][  350/  391]    Overall Loss 0.187231    Objective Loss 0.187231    Top1 93.252232    Top5 99.930804    LR 0.030000    Time 0.022386    
2018-10-28 00:50:07,174 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.39454 |  0.00059 |    0.20006 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12520 | -0.00368 |    0.03994 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.12226 | -0.00274 |    0.04384 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12460 | -0.00773 |    0.05335 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09139 | -0.00410 |    0.02652 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13352 | -0.00517 |    0.05289 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.09011 |  0.00203 |    0.02463 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12401 | -0.00490 |    0.05899 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11442 | -0.00475 |    0.06536 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.15991 | -0.00375 |    0.07897 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09229 | -0.00336 |    0.03673 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07561 |  0.00107 |    0.02718 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09630 | -0.00547 |    0.04213 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07839 | -0.00106 |    0.03521 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07951 | -0.00250 |    0.03064 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08379 | -0.00358 |    0.04179 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08749 | -0.00228 |    0.03148 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07836 | -0.00317 |    0.03209 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06421 | -0.00028 |    0.02887 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04875 | -0.00091 |    0.01554 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03408 |  0.00004 |    0.00816 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56091 | -0.03145 |    0.32991 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:50:07,174 - Total sparsity: 74.94

2018-10-28 00:50:07,174 - --- validate (epoch=155)-----------
2018-10-28 00:50:07,174 - 10000 samples (128 per mini-batch)
2018-10-28 00:50:07,898 - Epoch: [155][   50/   78]    Loss 0.377999    Top1 88.687500    Top5 99.500000    
2018-10-28 00:50:08,289 - ==> Top1: 88.820    Top5: 99.580    Loss: 0.368

2018-10-28 00:50:08,289 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:50:08,290 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:50:08,300 - 

2018-10-28 00:50:08,301 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:50:09,471 - Epoch: [156][   50/  391]    Overall Loss 0.181465    Objective Loss 0.181465    Top1 93.468750    Top5 99.906250    LR 0.030000    Time 0.023374    
2018-10-28 00:50:10,582 - Epoch: [156][  100/  391]    Overall Loss 0.181914    Objective Loss 0.181914    Top1 93.375000    Top5 99.906250    LR 0.030000    Time 0.022786    
2018-10-28 00:50:11,694 - Epoch: [156][  150/  391]    Overall Loss 0.180114    Objective Loss 0.180114    Top1 93.489583    Top5 99.937500    LR 0.030000    Time 0.022594    
2018-10-28 00:50:12,807 - Epoch: [156][  200/  391]    Overall Loss 0.181283    Objective Loss 0.181283    Top1 93.402344    Top5 99.917969    LR 0.030000    Time 0.022501    
2018-10-28 00:50:13,920 - Epoch: [156][  250/  391]    Overall Loss 0.182265    Objective Loss 0.182265    Top1 93.403125    Top5 99.912500    LR 0.030000    Time 0.022448    
2018-10-28 00:50:15,034 - Epoch: [156][  300/  391]    Overall Loss 0.183414    Objective Loss 0.183414    Top1 93.395833    Top5 99.919271    LR 0.030000    Time 0.022417    
2018-10-28 00:50:16,148 - Epoch: [156][  350/  391]    Overall Loss 0.183925    Objective Loss 0.183925    Top1 93.390625    Top5 99.924107    LR 0.030000    Time 0.022393    
2018-10-28 00:50:17,139 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.39318 |  0.00075 |    0.19916 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12489 | -0.00302 |    0.03981 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.12188 | -0.00231 |    0.04370 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12425 | -0.00859 |    0.05336 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09112 | -0.00380 |    0.02635 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13312 | -0.00555 |    0.05279 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08982 |  0.00196 |    0.02460 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12369 | -0.00493 |    0.05896 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11418 | -0.00463 |    0.06519 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.15927 | -0.00371 |    0.07836 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09212 | -0.00353 |    0.03665 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07546 |  0.00111 |    0.02712 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09616 | -0.00557 |    0.04199 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07826 | -0.00114 |    0.03511 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07930 | -0.00260 |    0.03050 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08365 | -0.00339 |    0.04174 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08710 | -0.00213 |    0.03122 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07825 | -0.00324 |    0.03207 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06419 | -0.00043 |    0.02884 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04878 | -0.00091 |    0.01555 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03403 |  0.00006 |    0.00815 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56092 | -0.03158 |    0.33001 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:50:17,140 - Total sparsity: 74.94

2018-10-28 00:50:17,140 - --- validate (epoch=156)-----------
2018-10-28 00:50:17,140 - 10000 samples (128 per mini-batch)
2018-10-28 00:50:17,857 - Epoch: [156][   50/   78]    Loss 0.421331    Top1 88.031250    Top5 99.453125    
2018-10-28 00:50:18,249 - ==> Top1: 88.160    Top5: 99.500    Loss: 0.408

2018-10-28 00:50:18,249 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:50:18,249 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:50:18,258 - 

2018-10-28 00:50:18,258 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:50:19,431 - Epoch: [157][   50/  391]    Overall Loss 0.168767    Objective Loss 0.168767    Top1 94.203125    Top5 99.859375    LR 0.030000    Time 0.023436    
2018-10-28 00:50:20,545 - Epoch: [157][  100/  391]    Overall Loss 0.174895    Objective Loss 0.174895    Top1 93.843750    Top5 99.867188    LR 0.030000    Time 0.022836    
2018-10-28 00:50:21,654 - Epoch: [157][  150/  391]    Overall Loss 0.178409    Objective Loss 0.178409    Top1 93.604167    Top5 99.885417    LR 0.030000    Time 0.022611    
2018-10-28 00:50:22,765 - Epoch: [157][  200/  391]    Overall Loss 0.181289    Objective Loss 0.181289    Top1 93.503906    Top5 99.898438    LR 0.030000    Time 0.022508    
2018-10-28 00:50:23,876 - Epoch: [157][  250/  391]    Overall Loss 0.181642    Objective Loss 0.181642    Top1 93.518750    Top5 99.903125    LR 0.030000    Time 0.022446    
2018-10-28 00:50:24,991 - Epoch: [157][  300/  391]    Overall Loss 0.183683    Objective Loss 0.183683    Top1 93.486979    Top5 99.901042    LR 0.030000    Time 0.022404    
2018-10-28 00:50:26,104 - Epoch: [157][  350/  391]    Overall Loss 0.184180    Objective Loss 0.184180    Top1 93.477679    Top5 99.908482    LR 0.030000    Time 0.022381    
2018-10-28 00:50:27,093 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.39222 | -0.00098 |    0.19827 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12457 | -0.00296 |    0.03954 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.12153 | -0.00258 |    0.04363 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12407 | -0.00788 |    0.05341 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09088 | -0.00391 |    0.02634 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13277 | -0.00542 |    0.05285 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08955 |  0.00215 |    0.02441 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12338 | -0.00489 |    0.05891 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11393 | -0.00481 |    0.06505 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.15855 | -0.00493 |    0.07734 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09195 | -0.00351 |    0.03658 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07532 |  0.00134 |    0.02704 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09602 | -0.00564 |    0.04208 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07814 | -0.00103 |    0.03506 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07911 | -0.00257 |    0.03043 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08351 | -0.00345 |    0.04164 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08670 | -0.00200 |    0.03115 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07815 | -0.00319 |    0.03202 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06417 | -0.00049 |    0.02886 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04881 | -0.00089 |    0.01558 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03399 |  0.00005 |    0.00815 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56104 | -0.03149 |    0.32995 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:50:27,093 - Total sparsity: 74.94

2018-10-28 00:50:27,093 - --- validate (epoch=157)-----------
2018-10-28 00:50:27,093 - 10000 samples (128 per mini-batch)
2018-10-28 00:50:27,811 - Epoch: [157][   50/   78]    Loss 0.419871    Top1 87.828125    Top5 99.406250    
2018-10-28 00:50:28,200 - ==> Top1: 87.870    Top5: 99.500    Loss: 0.421

2018-10-28 00:50:28,201 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:50:28,201 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:50:28,211 - 

2018-10-28 00:50:28,211 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:50:29,381 - Epoch: [158][   50/  391]    Overall Loss 0.186326    Objective Loss 0.186326    Top1 93.546875    Top5 99.906250    LR 0.030000    Time 0.023362    
2018-10-28 00:50:30,492 - Epoch: [158][  100/  391]    Overall Loss 0.184752    Objective Loss 0.184752    Top1 93.492188    Top5 99.914062    LR 0.030000    Time 0.022776    
2018-10-28 00:50:31,604 - Epoch: [158][  150/  391]    Overall Loss 0.183334    Objective Loss 0.183334    Top1 93.541667    Top5 99.911458    LR 0.030000    Time 0.022591    
2018-10-28 00:50:32,717 - Epoch: [158][  200/  391]    Overall Loss 0.184258    Objective Loss 0.184258    Top1 93.476562    Top5 99.914062    LR 0.030000    Time 0.022499    
2018-10-28 00:50:33,828 - Epoch: [158][  250/  391]    Overall Loss 0.184374    Objective Loss 0.184374    Top1 93.462500    Top5 99.906250    LR 0.030000    Time 0.022438    
2018-10-28 00:50:34,938 - Epoch: [158][  300/  391]    Overall Loss 0.186254    Objective Loss 0.186254    Top1 93.440104    Top5 99.901042    LR 0.030000    Time 0.022395    
2018-10-28 00:50:36,048 - Epoch: [158][  350/  391]    Overall Loss 0.187820    Objective Loss 0.187820    Top1 93.339286    Top5 99.899554    LR 0.030000    Time 0.022364    
2018-10-28 00:50:37,041 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.39119 | -0.00057 |    0.19746 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12425 | -0.00305 |    0.03958 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.12124 | -0.00208 |    0.04341 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12375 | -0.00825 |    0.05314 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09060 | -0.00392 |    0.02624 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13239 | -0.00576 |    0.05268 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08927 |  0.00244 |    0.02454 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12306 | -0.00506 |    0.05876 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11370 | -0.00478 |    0.06494 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.15789 | -0.00483 |    0.07723 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09179 | -0.00342 |    0.03648 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07520 |  0.00106 |    0.02704 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09594 | -0.00540 |    0.04198 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07804 | -0.00097 |    0.03495 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07893 | -0.00246 |    0.03035 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08340 | -0.00349 |    0.04155 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08631 | -0.00202 |    0.03100 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07806 | -0.00309 |    0.03200 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06416 | -0.00053 |    0.02885 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04884 | -0.00091 |    0.01555 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03395 |  0.00008 |    0.00812 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56047 | -0.03165 |    0.32950 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:50:37,041 - Total sparsity: 74.94

2018-10-28 00:50:37,041 - --- validate (epoch=158)-----------
2018-10-28 00:50:37,041 - 10000 samples (128 per mini-batch)
2018-10-28 00:50:37,761 - Epoch: [158][   50/   78]    Loss 0.400862    Top1 88.218750    Top5 99.562500    
2018-10-28 00:50:38,151 - ==> Top1: 88.320    Top5: 99.600    Loss: 0.394

2018-10-28 00:50:38,152 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:50:38,152 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:50:38,165 - 

2018-10-28 00:50:38,166 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:50:39,340 - Epoch: [159][   50/  391]    Overall Loss 0.174649    Objective Loss 0.174649    Top1 93.625000    Top5 99.906250    LR 0.030000    Time 0.023456    
2018-10-28 00:50:40,452 - Epoch: [159][  100/  391]    Overall Loss 0.181717    Objective Loss 0.181717    Top1 93.523438    Top5 99.906250    LR 0.030000    Time 0.022828    
2018-10-28 00:50:41,563 - Epoch: [159][  150/  391]    Overall Loss 0.180435    Objective Loss 0.180435    Top1 93.619792    Top5 99.906250    LR 0.030000    Time 0.022620    
2018-10-28 00:50:42,675 - Epoch: [159][  200/  391]    Overall Loss 0.183833    Objective Loss 0.183833    Top1 93.578125    Top5 99.890625    LR 0.030000    Time 0.022517    
2018-10-28 00:50:43,788 - Epoch: [159][  250/  391]    Overall Loss 0.183304    Objective Loss 0.183304    Top1 93.518750    Top5 99.903125    LR 0.030000    Time 0.022457    
2018-10-28 00:50:44,899 - Epoch: [159][  300/  391]    Overall Loss 0.184523    Objective Loss 0.184523    Top1 93.479167    Top5 99.901042    LR 0.030000    Time 0.022414    
2018-10-28 00:50:46,011 - Epoch: [159][  350/  391]    Overall Loss 0.184814    Objective Loss 0.184814    Top1 93.482143    Top5 99.904018    LR 0.030000    Time 0.022385    
2018-10-28 00:50:47,003 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.39018 |  0.00019 |    0.19753 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12402 | -0.00328 |    0.03954 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.12095 | -0.00243 |    0.04315 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12351 | -0.00811 |    0.05296 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09044 | -0.00335 |    0.02614 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13207 | -0.00529 |    0.05254 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08903 |  0.00236 |    0.02435 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12275 | -0.00505 |    0.05850 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11349 | -0.00456 |    0.06477 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.15722 | -0.00519 |    0.07709 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09166 | -0.00345 |    0.03646 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07507 |  0.00116 |    0.02697 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09582 | -0.00546 |    0.04189 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07793 | -0.00079 |    0.03489 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07875 | -0.00254 |    0.03024 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08327 | -0.00334 |    0.04150 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08595 | -0.00207 |    0.03080 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07796 | -0.00314 |    0.03196 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06415 | -0.00046 |    0.02886 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04888 | -0.00092 |    0.01558 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03391 |  0.00009 |    0.00811 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55992 | -0.03155 |    0.32931 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:50:47,004 - Total sparsity: 74.94

2018-10-28 00:50:47,004 - --- validate (epoch=159)-----------
2018-10-28 00:50:47,004 - 10000 samples (128 per mini-batch)
2018-10-28 00:50:47,731 - Epoch: [159][   50/   78]    Loss 0.391964    Top1 88.484375    Top5 99.578125    
2018-10-28 00:50:48,123 - ==> Top1: 88.380    Top5: 99.620    Loss: 0.396

2018-10-28 00:50:48,124 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:50:48,124 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:50:48,135 - 

2018-10-28 00:50:48,136 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:50:49,306 - Epoch: [160][   50/  391]    Overall Loss 0.182291    Objective Loss 0.182291    Top1 93.515625    Top5 99.890625    LR 0.030000    Time 0.023378    
2018-10-28 00:50:50,418 - Epoch: [160][  100/  391]    Overall Loss 0.183418    Objective Loss 0.183418    Top1 93.484375    Top5 99.890625    LR 0.030000    Time 0.022788    
2018-10-28 00:50:51,529 - Epoch: [160][  150/  391]    Overall Loss 0.177392    Objective Loss 0.177392    Top1 93.661458    Top5 99.911458    LR 0.030000    Time 0.022597    
2018-10-28 00:50:52,641 - Epoch: [160][  200/  391]    Overall Loss 0.177824    Objective Loss 0.177824    Top1 93.691406    Top5 99.921875    LR 0.030000    Time 0.022483    
2018-10-28 00:50:53,754 - Epoch: [160][  250/  391]    Overall Loss 0.182143    Objective Loss 0.182143    Top1 93.518750    Top5 99.912500    LR 0.030000    Time 0.022429    
2018-10-28 00:50:54,865 - Epoch: [160][  300/  391]    Overall Loss 0.182786    Objective Loss 0.182786    Top1 93.505208    Top5 99.921875    LR 0.030000    Time 0.022393    
2018-10-28 00:50:55,977 - Epoch: [160][  350/  391]    Overall Loss 0.182703    Objective Loss 0.182703    Top1 93.526786    Top5 99.926339    LR 0.030000    Time 0.022367    
2018-10-28 00:50:56,968 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.38936 |  0.00048 |    0.19696 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12370 | -0.00371 |    0.03949 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.12059 | -0.00258 |    0.04311 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12322 | -0.00854 |    0.05285 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09020 | -0.00350 |    0.02616 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13176 | -0.00525 |    0.05241 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08876 |  0.00239 |    0.02433 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12245 | -0.00513 |    0.05844 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11328 | -0.00459 |    0.06473 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.15662 | -0.00477 |    0.07687 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09152 | -0.00346 |    0.03641 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07495 |  0.00132 |    0.02694 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09572 | -0.00546 |    0.04187 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07787 | -0.00079 |    0.03490 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07859 | -0.00253 |    0.03020 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08317 | -0.00335 |    0.04144 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08560 | -0.00230 |    0.03073 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07789 | -0.00314 |    0.03191 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06415 | -0.00054 |    0.02887 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04893 | -0.00094 |    0.01560 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03390 |  0.00012 |    0.00811 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55928 | -0.03160 |    0.32871 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:50:56,969 - Total sparsity: 74.94

2018-10-28 00:50:56,969 - --- validate (epoch=160)-----------
2018-10-28 00:50:56,969 - 10000 samples (128 per mini-batch)
2018-10-28 00:50:57,693 - Epoch: [160][   50/   78]    Loss 0.412756    Top1 87.609375    Top5 99.531250    
2018-10-28 00:50:58,085 - ==> Top1: 87.560    Top5: 99.560    Loss: 0.406

2018-10-28 00:50:58,085 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:50:58,086 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:50:58,096 - 

2018-10-28 00:50:58,096 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:50:59,267 - Epoch: [161][   50/  391]    Overall Loss 0.167658    Objective Loss 0.167658    Top1 93.843750    Top5 99.921875    LR 0.030000    Time 0.023385    
2018-10-28 00:51:00,383 - Epoch: [161][  100/  391]    Overall Loss 0.177654    Objective Loss 0.177654    Top1 93.398438    Top5 99.937500    LR 0.030000    Time 0.022838    
2018-10-28 00:51:01,497 - Epoch: [161][  150/  391]    Overall Loss 0.180830    Objective Loss 0.180830    Top1 93.307292    Top5 99.937500    LR 0.030000    Time 0.022643    
2018-10-28 00:51:02,611 - Epoch: [161][  200/  391]    Overall Loss 0.187662    Objective Loss 0.187662    Top1 93.058594    Top5 99.933594    LR 0.030000    Time 0.022547    
2018-10-28 00:51:03,724 - Epoch: [161][  250/  391]    Overall Loss 0.189157    Objective Loss 0.189157    Top1 93.053125    Top5 99.921875    LR 0.030000    Time 0.022470    
2018-10-28 00:51:04,839 - Epoch: [161][  300/  391]    Overall Loss 0.186663    Objective Loss 0.186663    Top1 93.153646    Top5 99.924479    LR 0.030000    Time 0.022436    
2018-10-28 00:51:05,954 - Epoch: [161][  350/  391]    Overall Loss 0.187857    Objective Loss 0.187857    Top1 93.127232    Top5 99.919643    LR 0.030000    Time 0.022413    
2018-10-28 00:51:06,946 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.38869 |  0.00044 |    0.19699 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12350 | -0.00358 |    0.03932 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.12030 | -0.00243 |    0.04295 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12306 | -0.00788 |    0.05281 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.09001 | -0.00353 |    0.02607 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13152 | -0.00536 |    0.05230 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08857 |  0.00238 |    0.02416 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12221 | -0.00512 |    0.05832 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11314 | -0.00468 |    0.06464 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.15609 | -0.00431 |    0.07654 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09146 | -0.00351 |    0.03638 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07488 |  0.00129 |    0.02685 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09569 | -0.00543 |    0.04182 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07783 | -0.00078 |    0.03483 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07846 | -0.00245 |    0.03014 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08311 | -0.00346 |    0.04140 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08531 | -0.00218 |    0.03056 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07785 | -0.00306 |    0.03190 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06418 | -0.00046 |    0.02886 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04899 | -0.00092 |    0.01560 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03387 |  0.00012 |    0.00811 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55852 | -0.03119 |    0.32834 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:51:06,946 - Total sparsity: 74.94

2018-10-28 00:51:06,946 - --- validate (epoch=161)-----------
2018-10-28 00:51:06,946 - 10000 samples (128 per mini-batch)
2018-10-28 00:51:07,663 - Epoch: [161][   50/   78]    Loss 0.420870    Top1 88.046875    Top5 99.453125    
2018-10-28 00:51:08,053 - ==> Top1: 88.180    Top5: 99.540    Loss: 0.408

2018-10-28 00:51:08,054 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:51:08,054 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:51:08,064 - 

2018-10-28 00:51:08,064 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:51:09,235 - Epoch: [162][   50/  391]    Overall Loss 0.171648    Objective Loss 0.171648    Top1 93.625000    Top5 100.000000    LR 0.030000    Time 0.023375    
2018-10-28 00:51:10,346 - Epoch: [162][  100/  391]    Overall Loss 0.183022    Objective Loss 0.183022    Top1 93.234375    Top5 99.945312    LR 0.030000    Time 0.022790    
2018-10-28 00:51:11,459 - Epoch: [162][  150/  391]    Overall Loss 0.179992    Objective Loss 0.179992    Top1 93.411458    Top5 99.947917    LR 0.030000    Time 0.022601    
2018-10-28 00:51:12,571 - Epoch: [162][  200/  391]    Overall Loss 0.182574    Objective Loss 0.182574    Top1 93.343750    Top5 99.937500    LR 0.030000    Time 0.022506    
2018-10-28 00:51:13,683 - Epoch: [162][  250/  391]    Overall Loss 0.184137    Objective Loss 0.184137    Top1 93.325000    Top5 99.928125    LR 0.030000    Time 0.022449    
2018-10-28 00:51:14,796 - Epoch: [162][  300/  391]    Overall Loss 0.184536    Objective Loss 0.184536    Top1 93.317708    Top5 99.932292    LR 0.030000    Time 0.022401    
2018-10-28 00:51:15,908 - Epoch: [162][  350/  391]    Overall Loss 0.186462    Objective Loss 0.186462    Top1 93.294643    Top5 99.928571    LR 0.030000    Time 0.022374    
2018-10-28 00:51:16,900 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.38791 |  0.00062 |    0.19660 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12323 | -0.00295 |    0.03922 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11997 | -0.00284 |    0.04293 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12278 | -0.00854 |    0.05236 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08988 | -0.00340 |    0.02601 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13126 | -0.00524 |    0.05217 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08835 |  0.00215 |    0.02420 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12195 | -0.00508 |    0.05819 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11294 | -0.00485 |    0.06445 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.15557 | -0.00487 |    0.07581 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09132 | -0.00362 |    0.03634 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07478 |  0.00124 |    0.02682 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09559 | -0.00556 |    0.04180 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07776 | -0.00071 |    0.03484 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07832 | -0.00232 |    0.03006 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08302 | -0.00356 |    0.04134 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08502 | -0.00209 |    0.03047 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07781 | -0.00300 |    0.03188 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06420 | -0.00042 |    0.02883 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04905 | -0.00102 |    0.01562 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03385 |  0.00013 |    0.00809 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55843 | -0.03162 |    0.32813 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:51:16,900 - Total sparsity: 74.94

2018-10-28 00:51:16,900 - --- validate (epoch=162)-----------
2018-10-28 00:51:16,900 - 10000 samples (128 per mini-batch)
2018-10-28 00:51:17,614 - Epoch: [162][   50/   78]    Loss 0.422545    Top1 87.828125    Top5 99.453125    
2018-10-28 00:51:17,998 - ==> Top1: 87.720    Top5: 99.530    Loss: 0.412

2018-10-28 00:51:17,999 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:51:17,999 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:51:18,016 - 

2018-10-28 00:51:18,017 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:51:19,162 - Epoch: [163][   50/  391]    Overall Loss 0.177758    Objective Loss 0.177758    Top1 93.968750    Top5 99.859375    LR 0.030000    Time 0.022871    
2018-10-28 00:51:20,275 - Epoch: [163][  100/  391]    Overall Loss 0.179086    Objective Loss 0.179086    Top1 93.750000    Top5 99.898438    LR 0.030000    Time 0.022545    
2018-10-28 00:51:21,385 - Epoch: [163][  150/  391]    Overall Loss 0.179891    Objective Loss 0.179891    Top1 93.697917    Top5 99.906250    LR 0.030000    Time 0.022421    
2018-10-28 00:51:22,497 - Epoch: [163][  200/  391]    Overall Loss 0.181619    Objective Loss 0.181619    Top1 93.628906    Top5 99.910156    LR 0.030000    Time 0.022369    
2018-10-28 00:51:23,608 - Epoch: [163][  250/  391]    Overall Loss 0.183981    Objective Loss 0.183981    Top1 93.534375    Top5 99.909375    LR 0.030000    Time 0.022337    
2018-10-28 00:51:24,721 - Epoch: [163][  300/  391]    Overall Loss 0.184558    Objective Loss 0.184558    Top1 93.458333    Top5 99.908854    LR 0.030000    Time 0.022321    
2018-10-28 00:51:25,833 - Epoch: [163][  350/  391]    Overall Loss 0.186167    Objective Loss 0.186167    Top1 93.435268    Top5 99.910714    LR 0.030000    Time 0.022303    
2018-10-28 00:51:26,821 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.38720 |  0.00116 |    0.19614 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12292 | -0.00323 |    0.03914 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11970 | -0.00252 |    0.04294 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12261 | -0.00860 |    0.05226 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08973 | -0.00339 |    0.02607 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13097 | -0.00533 |    0.05207 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08811 |  0.00215 |    0.02402 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12174 | -0.00529 |    0.05797 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11279 | -0.00477 |    0.06435 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.15504 | -0.00489 |    0.07593 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09120 | -0.00347 |    0.03622 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07468 |  0.00129 |    0.02677 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09550 | -0.00555 |    0.04177 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07766 | -0.00066 |    0.03475 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07818 | -0.00225 |    0.03003 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08294 | -0.00341 |    0.04132 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08472 | -0.00214 |    0.03039 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07775 | -0.00298 |    0.03187 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06421 | -0.00043 |    0.02886 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04909 | -0.00102 |    0.01564 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03383 |  0.00014 |    0.00808 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55833 | -0.03158 |    0.32817 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:51:26,821 - Total sparsity: 74.94

2018-10-28 00:51:26,821 - --- validate (epoch=163)-----------
2018-10-28 00:51:26,821 - 10000 samples (128 per mini-batch)
2018-10-28 00:51:27,545 - Epoch: [163][   50/   78]    Loss 0.410417    Top1 88.390625    Top5 99.468750    
2018-10-28 00:51:27,937 - ==> Top1: 88.550    Top5: 99.540    Loss: 0.398

2018-10-28 00:51:27,937 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:51:27,937 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:51:27,955 - 

2018-10-28 00:51:27,955 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:51:29,100 - Epoch: [164][   50/  391]    Overall Loss 0.164331    Objective Loss 0.164331    Top1 94.171875    Top5 99.953125    LR 0.030000    Time 0.022874    
2018-10-28 00:51:30,213 - Epoch: [164][  100/  391]    Overall Loss 0.170336    Objective Loss 0.170336    Top1 93.984375    Top5 99.953125    LR 0.030000    Time 0.022552    
2018-10-28 00:51:31,326 - Epoch: [164][  150/  391]    Overall Loss 0.176475    Objective Loss 0.176475    Top1 93.776042    Top5 99.947917    LR 0.030000    Time 0.022444    
2018-10-28 00:51:32,438 - Epoch: [164][  200/  391]    Overall Loss 0.180479    Objective Loss 0.180479    Top1 93.593750    Top5 99.917969    LR 0.030000    Time 0.022386    
2018-10-28 00:51:33,550 - Epoch: [164][  250/  391]    Overall Loss 0.182733    Objective Loss 0.182733    Top1 93.459375    Top5 99.921875    LR 0.030000    Time 0.022353    
2018-10-28 00:51:34,660 - Epoch: [164][  300/  391]    Overall Loss 0.185292    Objective Loss 0.185292    Top1 93.395833    Top5 99.919271    LR 0.030000    Time 0.022325    
2018-10-28 00:51:35,771 - Epoch: [164][  350/  391]    Overall Loss 0.184988    Objective Loss 0.184988    Top1 93.372768    Top5 99.921875    LR 0.030000    Time 0.022304    
2018-10-28 00:51:36,769 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.38680 | -0.00034 |    0.19626 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12275 | -0.00335 |    0.03915 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11948 | -0.00247 |    0.04295 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12249 | -0.00754 |    0.05225 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08951 | -0.00388 |    0.02594 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13067 | -0.00537 |    0.05190 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08786 |  0.00221 |    0.02404 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12152 | -0.00501 |    0.05789 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11263 | -0.00500 |    0.06431 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.15457 | -0.00499 |    0.07554 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09113 | -0.00366 |    0.03617 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07463 |  0.00125 |    0.02680 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09543 | -0.00539 |    0.04166 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07759 | -0.00056 |    0.03476 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07806 | -0.00231 |    0.03001 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08289 | -0.00348 |    0.04126 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08444 | -0.00206 |    0.03035 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07771 | -0.00297 |    0.03187 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06424 | -0.00043 |    0.02888 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04914 | -0.00108 |    0.01565 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03381 |  0.00015 |    0.00808 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55759 | -0.03164 |    0.32775 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:51:36,769 - Total sparsity: 74.94

2018-10-28 00:51:36,769 - --- validate (epoch=164)-----------
2018-10-28 00:51:36,769 - 10000 samples (128 per mini-batch)
2018-10-28 00:51:37,495 - Epoch: [164][   50/   78]    Loss 0.388911    Top1 88.296875    Top5 99.406250    
2018-10-28 00:51:37,889 - ==> Top1: 88.300    Top5: 99.500    Loss: 0.383

2018-10-28 00:51:37,890 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:51:37,890 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:51:37,900 - 

2018-10-28 00:51:37,900 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:51:39,074 - Epoch: [165][   50/  391]    Overall Loss 0.178990    Objective Loss 0.178990    Top1 93.750000    Top5 99.968750    LR 0.030000    Time 0.023433    
2018-10-28 00:51:40,187 - Epoch: [165][  100/  391]    Overall Loss 0.175146    Objective Loss 0.175146    Top1 93.765625    Top5 99.960938    LR 0.030000    Time 0.022832    
2018-10-28 00:51:41,301 - Epoch: [165][  150/  391]    Overall Loss 0.176255    Objective Loss 0.176255    Top1 93.755208    Top5 99.942708    LR 0.030000    Time 0.022642    
2018-10-28 00:51:42,414 - Epoch: [165][  200/  391]    Overall Loss 0.180692    Objective Loss 0.180692    Top1 93.546875    Top5 99.933594    LR 0.030000    Time 0.022523    
2018-10-28 00:51:43,527 - Epoch: [165][  250/  391]    Overall Loss 0.185471    Objective Loss 0.185471    Top1 93.393750    Top5 99.940625    LR 0.030000    Time 0.022463    
2018-10-28 00:51:44,639 - Epoch: [165][  300/  391]    Overall Loss 0.189497    Objective Loss 0.189497    Top1 93.247396    Top5 99.932292    LR 0.030000    Time 0.022423    
2018-10-28 00:51:45,753 - Epoch: [165][  350/  391]    Overall Loss 0.189430    Objective Loss 0.189430    Top1 93.243304    Top5 99.933036    LR 0.030000    Time 0.022398    
2018-10-28 00:51:46,745 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.38617 | -0.00043 |    0.19507 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12255 | -0.00325 |    0.03894 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11921 | -0.00255 |    0.04289 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12235 | -0.00751 |    0.05193 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08935 | -0.00403 |    0.02589 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13047 | -0.00522 |    0.05158 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08767 |  0.00194 |    0.02414 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12131 | -0.00471 |    0.05759 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11249 | -0.00485 |    0.06414 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.15416 | -0.00380 |    0.07520 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09105 | -0.00340 |    0.03614 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07455 |  0.00124 |    0.02678 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09533 | -0.00513 |    0.04161 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07750 | -0.00054 |    0.03470 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07794 | -0.00233 |    0.02996 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08283 | -0.00354 |    0.04123 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08419 | -0.00190 |    0.03022 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07766 | -0.00299 |    0.03185 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06429 | -0.00040 |    0.02890 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04921 | -0.00101 |    0.01566 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03380 |  0.00019 |    0.00807 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55695 | -0.03156 |    0.32743 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:51:46,745 - Total sparsity: 74.94

2018-10-28 00:51:46,745 - --- validate (epoch=165)-----------
2018-10-28 00:51:46,745 - 10000 samples (128 per mini-batch)
2018-10-28 00:51:47,466 - Epoch: [165][   50/   78]    Loss 0.393321    Top1 88.312500    Top5 99.484375    
2018-10-28 00:51:47,858 - ==> Top1: 88.380    Top5: 99.540    Loss: 0.386

2018-10-28 00:51:47,859 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:51:47,859 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:51:47,870 - 

2018-10-28 00:51:47,870 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:51:49,039 - Epoch: [166][   50/  391]    Overall Loss 0.177988    Objective Loss 0.177988    Top1 93.781250    Top5 99.890625    LR 0.030000    Time 0.023343    
2018-10-28 00:51:50,151 - Epoch: [166][  100/  391]    Overall Loss 0.181112    Objective Loss 0.181112    Top1 93.656250    Top5 99.914062    LR 0.030000    Time 0.022777    
2018-10-28 00:51:51,265 - Epoch: [166][  150/  391]    Overall Loss 0.185490    Objective Loss 0.185490    Top1 93.468750    Top5 99.916667    LR 0.030000    Time 0.022602    
2018-10-28 00:51:52,379 - Epoch: [166][  200/  391]    Overall Loss 0.186518    Objective Loss 0.186518    Top1 93.406250    Top5 99.917969    LR 0.030000    Time 0.022518    
2018-10-28 00:51:53,493 - Epoch: [166][  250/  391]    Overall Loss 0.186194    Objective Loss 0.186194    Top1 93.431250    Top5 99.912500    LR 0.030000    Time 0.022464    
2018-10-28 00:51:54,607 - Epoch: [166][  300/  391]    Overall Loss 0.186769    Objective Loss 0.186769    Top1 93.377604    Top5 99.924479    LR 0.030000    Time 0.022430    
2018-10-28 00:51:55,721 - Epoch: [166][  350/  391]    Overall Loss 0.188887    Objective Loss 0.188887    Top1 93.274554    Top5 99.919643    LR 0.030000    Time 0.022404    
2018-10-28 00:51:56,712 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.38549 | -0.00018 |    0.19519 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12238 | -0.00325 |    0.03889 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11897 | -0.00284 |    0.04275 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12222 | -0.00791 |    0.05201 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08920 | -0.00381 |    0.02587 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13031 | -0.00502 |    0.05147 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08750 |  0.00174 |    0.02400 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12110 | -0.00500 |    0.05759 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11237 | -0.00461 |    0.06405 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.15373 | -0.00353 |    0.07527 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09096 | -0.00336 |    0.03608 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07445 |  0.00129 |    0.02673 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09525 | -0.00555 |    0.04158 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07743 | -0.00061 |    0.03471 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07782 | -0.00253 |    0.02996 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08278 | -0.00350 |    0.04123 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08394 | -0.00193 |    0.03021 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07762 | -0.00309 |    0.03182 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06431 | -0.00039 |    0.02894 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04928 | -0.00103 |    0.01570 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03379 |  0.00020 |    0.00807 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55655 | -0.03180 |    0.32730 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:51:56,712 - Total sparsity: 74.94

2018-10-28 00:51:56,712 - --- validate (epoch=166)-----------
2018-10-28 00:51:56,712 - 10000 samples (128 per mini-batch)
2018-10-28 00:51:57,429 - Epoch: [166][   50/   78]    Loss 0.421437    Top1 87.937500    Top5 99.437500    
2018-10-28 00:51:57,817 - ==> Top1: 87.920    Top5: 99.510    Loss: 0.426

2018-10-28 00:51:57,818 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:51:57,818 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:51:57,828 - 

2018-10-28 00:51:57,828 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:51:58,998 - Epoch: [167][   50/  391]    Overall Loss 0.163127    Objective Loss 0.163127    Top1 94.234375    Top5 99.921875    LR 0.030000    Time 0.023370    
2018-10-28 00:52:00,113 - Epoch: [167][  100/  391]    Overall Loss 0.171153    Objective Loss 0.171153    Top1 94.015625    Top5 99.929688    LR 0.030000    Time 0.022813    
2018-10-28 00:52:01,226 - Epoch: [167][  150/  391]    Overall Loss 0.174220    Objective Loss 0.174220    Top1 93.843750    Top5 99.942708    LR 0.030000    Time 0.022625    
2018-10-28 00:52:02,339 - Epoch: [167][  200/  391]    Overall Loss 0.180418    Objective Loss 0.180418    Top1 93.593750    Top5 99.937500    LR 0.030000    Time 0.022527    
2018-10-28 00:52:03,451 - Epoch: [167][  250/  391]    Overall Loss 0.182148    Objective Loss 0.182148    Top1 93.490625    Top5 99.931250    LR 0.030000    Time 0.022465    
2018-10-28 00:52:04,564 - Epoch: [167][  300/  391]    Overall Loss 0.184032    Objective Loss 0.184032    Top1 93.437500    Top5 99.932292    LR 0.030000    Time 0.022414    
2018-10-28 00:52:05,678 - Epoch: [167][  350/  391]    Overall Loss 0.185758    Objective Loss 0.185758    Top1 93.383929    Top5 99.921875    LR 0.030000    Time 0.022390    
2018-10-28 00:52:06,670 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.38486 |  0.00038 |    0.19504 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12218 | -0.00318 |    0.03884 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11882 | -0.00235 |    0.04273 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12210 | -0.00771 |    0.05218 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08906 | -0.00377 |    0.02580 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.13012 | -0.00530 |    0.05129 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08733 |  0.00184 |    0.02391 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12089 | -0.00501 |    0.05754 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11225 | -0.00475 |    0.06401 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.15331 | -0.00392 |    0.07464 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09087 | -0.00349 |    0.03607 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07437 |  0.00125 |    0.02665 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09515 | -0.00551 |    0.04155 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07735 | -0.00058 |    0.03464 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07771 | -0.00251 |    0.02992 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08271 | -0.00345 |    0.04119 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08372 | -0.00190 |    0.03009 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07758 | -0.00315 |    0.03180 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06434 | -0.00038 |    0.02896 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04936 | -0.00098 |    0.01574 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03378 |  0.00021 |    0.00807 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55573 | -0.03173 |    0.32674 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:52:06,671 - Total sparsity: 74.94

2018-10-28 00:52:06,671 - --- validate (epoch=167)-----------
2018-10-28 00:52:06,671 - 10000 samples (128 per mini-batch)
2018-10-28 00:52:07,395 - Epoch: [167][   50/   78]    Loss 0.396226    Top1 88.343750    Top5 99.484375    
2018-10-28 00:52:07,785 - ==> Top1: 88.240    Top5: 99.550    Loss: 0.382

2018-10-28 00:52:07,786 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:52:07,786 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:52:07,797 - 

2018-10-28 00:52:07,798 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:52:08,966 - Epoch: [168][   50/  391]    Overall Loss 0.165032    Objective Loss 0.165032    Top1 93.953125    Top5 99.953125    LR 0.030000    Time 0.023329    
2018-10-28 00:52:10,078 - Epoch: [168][  100/  391]    Overall Loss 0.177869    Objective Loss 0.177869    Top1 93.554688    Top5 99.945312    LR 0.030000    Time 0.022772    
2018-10-28 00:52:11,190 - Epoch: [168][  150/  391]    Overall Loss 0.181488    Objective Loss 0.181488    Top1 93.500000    Top5 99.921875    LR 0.030000    Time 0.022588    
2018-10-28 00:52:12,300 - Epoch: [168][  200/  391]    Overall Loss 0.180973    Objective Loss 0.180973    Top1 93.476562    Top5 99.929688    LR 0.030000    Time 0.022484    
2018-10-28 00:52:13,411 - Epoch: [168][  250/  391]    Overall Loss 0.183170    Objective Loss 0.183170    Top1 93.425000    Top5 99.934375    LR 0.030000    Time 0.022425    
2018-10-28 00:52:14,522 - Epoch: [168][  300/  391]    Overall Loss 0.183257    Objective Loss 0.183257    Top1 93.401042    Top5 99.937500    LR 0.030000    Time 0.022388    
2018-10-28 00:52:15,633 - Epoch: [168][  350/  391]    Overall Loss 0.184155    Objective Loss 0.184155    Top1 93.359375    Top5 99.937500    LR 0.030000    Time 0.022360    
2018-10-28 00:52:16,620 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.38453 | -0.00061 |    0.19502 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12204 | -0.00331 |    0.03868 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11865 | -0.00221 |    0.04273 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12191 | -0.00775 |    0.05225 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08889 | -0.00350 |    0.02583 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12990 | -0.00540 |    0.05141 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08712 |  0.00204 |    0.02369 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12070 | -0.00482 |    0.05738 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11212 | -0.00456 |    0.06390 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.15293 | -0.00382 |    0.07446 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09078 | -0.00348 |    0.03600 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07428 |  0.00115 |    0.02661 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09507 | -0.00543 |    0.04147 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07727 | -0.00084 |    0.03460 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07758 | -0.00256 |    0.02986 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08264 | -0.00350 |    0.04111 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08346 | -0.00198 |    0.03008 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07753 | -0.00310 |    0.03177 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06437 | -0.00042 |    0.02897 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04942 | -0.00096 |    0.01576 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03376 |  0.00019 |    0.00806 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55544 | -0.03174 |    0.32677 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:52:16,621 - Total sparsity: 74.94

2018-10-28 00:52:16,621 - --- validate (epoch=168)-----------
2018-10-28 00:52:16,621 - 10000 samples (128 per mini-batch)
2018-10-28 00:52:17,350 - Epoch: [168][   50/   78]    Loss 0.410799    Top1 87.515625    Top5 99.468750    
2018-10-28 00:52:17,732 - ==> Top1: 87.610    Top5: 99.550    Loss: 0.400

2018-10-28 00:52:17,733 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:52:17,733 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:52:17,749 - 

2018-10-28 00:52:17,750 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:52:18,895 - Epoch: [169][   50/  391]    Overall Loss 0.198239    Objective Loss 0.198239    Top1 92.968750    Top5 99.890625    LR 0.030000    Time 0.022867    
2018-10-28 00:52:20,010 - Epoch: [169][  100/  391]    Overall Loss 0.188119    Objective Loss 0.188119    Top1 93.421875    Top5 99.898438    LR 0.030000    Time 0.022573    
2018-10-28 00:52:21,123 - Epoch: [169][  150/  391]    Overall Loss 0.184720    Objective Loss 0.184720    Top1 93.578125    Top5 99.906250    LR 0.030000    Time 0.022463    
2018-10-28 00:52:22,235 - Epoch: [169][  200/  391]    Overall Loss 0.183951    Objective Loss 0.183951    Top1 93.523438    Top5 99.906250    LR 0.030000    Time 0.022399    
2018-10-28 00:52:23,346 - Epoch: [169][  250/  391]    Overall Loss 0.185834    Objective Loss 0.185834    Top1 93.425000    Top5 99.906250    LR 0.030000    Time 0.022358    
2018-10-28 00:52:24,457 - Epoch: [169][  300/  391]    Overall Loss 0.185997    Objective Loss 0.185997    Top1 93.411458    Top5 99.908854    LR 0.030000    Time 0.022331    
2018-10-28 00:52:25,568 - Epoch: [169][  350/  391]    Overall Loss 0.186688    Objective Loss 0.186688    Top1 93.370536    Top5 99.910714    LR 0.030000    Time 0.022311    
2018-10-28 00:52:26,557 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.38379 |  0.00076 |    0.19402 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12187 | -0.00282 |    0.03881 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11846 | -0.00178 |    0.04257 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12170 | -0.00784 |    0.05220 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08870 | -0.00342 |    0.02574 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12966 | -0.00535 |    0.05133 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08692 |  0.00195 |    0.02375 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12048 | -0.00496 |    0.05714 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11196 | -0.00472 |    0.06382 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.15253 | -0.00335 |    0.07442 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09067 | -0.00346 |    0.03605 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07419 |  0.00123 |    0.02659 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09497 | -0.00544 |    0.04150 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07719 | -0.00091 |    0.03459 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07747 | -0.00253 |    0.02984 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08258 | -0.00341 |    0.04109 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08320 | -0.00188 |    0.02992 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07746 | -0.00315 |    0.03175 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06438 | -0.00041 |    0.02899 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04946 | -0.00105 |    0.01576 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03373 |  0.00016 |    0.00806 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55516 | -0.03164 |    0.32654 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:52:26,557 - Total sparsity: 74.94

2018-10-28 00:52:26,557 - --- validate (epoch=169)-----------
2018-10-28 00:52:26,558 - 10000 samples (128 per mini-batch)
2018-10-28 00:52:27,287 - Epoch: [169][   50/   78]    Loss 0.406775    Top1 87.906250    Top5 99.359375    
2018-10-28 00:52:27,676 - ==> Top1: 88.180    Top5: 99.500    Loss: 0.394

2018-10-28 00:52:27,677 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:52:27,677 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:52:27,689 - 

2018-10-28 00:52:27,689 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:52:28,862 - Epoch: [170][   50/  391]    Overall Loss 0.170528    Objective Loss 0.170528    Top1 94.171875    Top5 99.937500    LR 0.030000    Time 0.023413    
2018-10-28 00:52:29,975 - Epoch: [170][  100/  391]    Overall Loss 0.171651    Objective Loss 0.171651    Top1 93.906250    Top5 99.945312    LR 0.030000    Time 0.022827    
2018-10-28 00:52:31,087 - Epoch: [170][  150/  391]    Overall Loss 0.174834    Objective Loss 0.174834    Top1 93.708333    Top5 99.942708    LR 0.030000    Time 0.022620    
2018-10-28 00:52:32,198 - Epoch: [170][  200/  391]    Overall Loss 0.176383    Objective Loss 0.176383    Top1 93.714844    Top5 99.945312    LR 0.030000    Time 0.022517    
2018-10-28 00:52:33,311 - Epoch: [170][  250/  391]    Overall Loss 0.175574    Objective Loss 0.175574    Top1 93.731250    Top5 99.953125    LR 0.030000    Time 0.022459    
2018-10-28 00:52:34,423 - Epoch: [170][  300/  391]    Overall Loss 0.180128    Objective Loss 0.180128    Top1 93.559896    Top5 99.942708    LR 0.030000    Time 0.022418    
2018-10-28 00:52:35,536 - Epoch: [170][  350/  391]    Overall Loss 0.182854    Objective Loss 0.182854    Top1 93.486607    Top5 99.928571    LR 0.030000    Time 0.022392    
2018-10-28 00:52:36,524 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.38349 |  0.00171 |    0.19450 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12164 | -0.00286 |    0.03873 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11827 | -0.00189 |    0.04252 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12157 | -0.00782 |    0.05205 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08853 | -0.00389 |    0.02573 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12942 | -0.00519 |    0.05125 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08672 |  0.00200 |    0.02361 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12028 | -0.00483 |    0.05711 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11179 | -0.00466 |    0.06379 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.15207 | -0.00438 |    0.07476 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09056 | -0.00340 |    0.03607 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07413 |  0.00120 |    0.02654 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09490 | -0.00544 |    0.04147 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07713 | -0.00106 |    0.03460 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07736 | -0.00238 |    0.02973 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08252 | -0.00336 |    0.04110 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08298 | -0.00191 |    0.02990 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07742 | -0.00304 |    0.03167 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06440 | -0.00040 |    0.02900 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04950 | -0.00097 |    0.01577 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03371 |  0.00021 |    0.00806 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55513 | -0.03158 |    0.32644 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:52:36,524 - Total sparsity: 74.94

2018-10-28 00:52:36,525 - --- validate (epoch=170)-----------
2018-10-28 00:52:36,525 - 10000 samples (128 per mini-batch)
2018-10-28 00:52:37,247 - Epoch: [170][   50/   78]    Loss 0.398829    Top1 87.812500    Top5 99.578125    
2018-10-28 00:52:37,636 - ==> Top1: 87.820    Top5: 99.630    Loss: 0.394

2018-10-28 00:52:37,637 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:52:37,637 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:52:37,648 - 

2018-10-28 00:52:37,648 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:52:38,820 - Epoch: [171][   50/  391]    Overall Loss 0.183808    Objective Loss 0.183808    Top1 93.343750    Top5 99.890625    LR 0.030000    Time 0.023413    
2018-10-28 00:52:39,931 - Epoch: [171][  100/  391]    Overall Loss 0.172563    Objective Loss 0.172563    Top1 93.828125    Top5 99.937500    LR 0.030000    Time 0.022798    
2018-10-28 00:52:41,043 - Epoch: [171][  150/  391]    Overall Loss 0.176847    Objective Loss 0.176847    Top1 93.562500    Top5 99.937500    LR 0.030000    Time 0.022606    
2018-10-28 00:52:42,156 - Epoch: [171][  200/  391]    Overall Loss 0.181341    Objective Loss 0.181341    Top1 93.417969    Top5 99.921875    LR 0.030000    Time 0.022513    
2018-10-28 00:52:43,268 - Epoch: [171][  250/  391]    Overall Loss 0.180936    Objective Loss 0.180936    Top1 93.465625    Top5 99.928125    LR 0.030000    Time 0.022455    
2018-10-28 00:52:44,381 - Epoch: [171][  300/  391]    Overall Loss 0.185550    Objective Loss 0.185550    Top1 93.328125    Top5 99.919271    LR 0.030000    Time 0.022418    
2018-10-28 00:52:45,491 - Epoch: [171][  350/  391]    Overall Loss 0.185272    Objective Loss 0.185272    Top1 93.314732    Top5 99.919643    LR 0.030000    Time 0.022383    
2018-10-28 00:52:46,483 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.38284 | -0.00065 |    0.19318 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12145 | -0.00285 |    0.03846 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11807 | -0.00192 |    0.04225 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12137 | -0.00833 |    0.05228 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08834 | -0.00356 |    0.02552 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12922 | -0.00522 |    0.05091 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08660 |  0.00198 |    0.02360 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.12010 | -0.00501 |    0.05702 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11168 | -0.00469 |    0.06373 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.15168 | -0.00504 |    0.07462 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09051 | -0.00339 |    0.03610 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07407 |  0.00118 |    0.02654 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09482 | -0.00550 |    0.04145 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07706 | -0.00084 |    0.03460 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07727 | -0.00239 |    0.02971 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08247 | -0.00348 |    0.04107 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08277 | -0.00207 |    0.02976 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07737 | -0.00295 |    0.03165 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06443 | -0.00039 |    0.02901 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04957 | -0.00095 |    0.01581 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03371 |  0.00019 |    0.00806 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55530 | -0.03173 |    0.32656 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:52:46,484 - Total sparsity: 74.94

2018-10-28 00:52:46,484 - --- validate (epoch=171)-----------
2018-10-28 00:52:46,484 - 10000 samples (128 per mini-batch)
2018-10-28 00:52:47,207 - Epoch: [171][   50/   78]    Loss 0.417370    Top1 87.609375    Top5 99.421875    
2018-10-28 00:52:47,597 - ==> Top1: 87.860    Top5: 99.440    Loss: 0.406

2018-10-28 00:52:47,598 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:52:47,598 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:52:47,614 - 

2018-10-28 00:52:47,614 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:52:48,786 - Epoch: [172][   50/  391]    Overall Loss 0.179301    Objective Loss 0.179301    Top1 93.796875    Top5 99.921875    LR 0.030000    Time 0.023405    
2018-10-28 00:52:49,899 - Epoch: [172][  100/  391]    Overall Loss 0.179014    Objective Loss 0.179014    Top1 93.695312    Top5 99.945312    LR 0.030000    Time 0.022816    
2018-10-28 00:52:51,010 - Epoch: [172][  150/  391]    Overall Loss 0.179044    Objective Loss 0.179044    Top1 93.744792    Top5 99.947917    LR 0.030000    Time 0.022612    
2018-10-28 00:52:52,123 - Epoch: [172][  200/  391]    Overall Loss 0.180748    Objective Loss 0.180748    Top1 93.695312    Top5 99.937500    LR 0.030000    Time 0.022516    
2018-10-28 00:52:53,235 - Epoch: [172][  250/  391]    Overall Loss 0.182017    Objective Loss 0.182017    Top1 93.675000    Top5 99.915625    LR 0.030000    Time 0.022458    
2018-10-28 00:52:54,346 - Epoch: [172][  300/  391]    Overall Loss 0.184698    Objective Loss 0.184698    Top1 93.645833    Top5 99.916667    LR 0.030000    Time 0.022413    
2018-10-28 00:52:55,459 - Epoch: [172][  350/  391]    Overall Loss 0.184404    Objective Loss 0.184404    Top1 93.607143    Top5 99.915179    LR 0.030000    Time 0.022387    
2018-10-28 00:52:56,452 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.38226 | -0.00009 |    0.19266 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12131 | -0.00303 |    0.03849 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11785 | -0.00263 |    0.04218 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12123 | -0.00860 |    0.05225 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08823 | -0.00349 |    0.02552 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12906 | -0.00483 |    0.05096 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08647 |  0.00207 |    0.02348 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11993 | -0.00519 |    0.05708 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11156 | -0.00495 |    0.06366 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.15134 | -0.00504 |    0.07407 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09044 | -0.00327 |    0.03606 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07400 |  0.00121 |    0.02656 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09476 | -0.00553 |    0.04143 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07698 | -0.00092 |    0.03447 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07716 | -0.00244 |    0.02973 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08241 | -0.00336 |    0.04101 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08252 | -0.00242 |    0.02961 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07732 | -0.00298 |    0.03163 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06444 | -0.00039 |    0.02900 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04961 | -0.00102 |    0.01583 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03370 |  0.00017 |    0.00806 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55538 | -0.03174 |    0.32629 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:52:56,452 - Total sparsity: 74.94

2018-10-28 00:52:56,452 - --- validate (epoch=172)-----------
2018-10-28 00:52:56,452 - 10000 samples (128 per mini-batch)
2018-10-28 00:52:57,173 - Epoch: [172][   50/   78]    Loss 0.415233    Top1 87.609375    Top5 99.406250    
2018-10-28 00:52:57,564 - ==> Top1: 88.070    Top5: 99.460    Loss: 0.401

2018-10-28 00:52:57,565 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:52:57,565 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:52:57,575 - 

2018-10-28 00:52:57,576 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:52:58,747 - Epoch: [173][   50/  391]    Overall Loss 0.185563    Objective Loss 0.185563    Top1 93.500000    Top5 99.937500    LR 0.030000    Time 0.023388    
2018-10-28 00:52:59,860 - Epoch: [173][  100/  391]    Overall Loss 0.181356    Objective Loss 0.181356    Top1 93.601562    Top5 99.906250    LR 0.030000    Time 0.022817    
2018-10-28 00:53:00,974 - Epoch: [173][  150/  391]    Overall Loss 0.181272    Objective Loss 0.181272    Top1 93.708333    Top5 99.906250    LR 0.030000    Time 0.022630    
2018-10-28 00:53:02,087 - Epoch: [173][  200/  391]    Overall Loss 0.178878    Objective Loss 0.178878    Top1 93.796875    Top5 99.906250    LR 0.030000    Time 0.022527    
2018-10-28 00:53:03,199 - Epoch: [173][  250/  391]    Overall Loss 0.181227    Objective Loss 0.181227    Top1 93.675000    Top5 99.921875    LR 0.030000    Time 0.022466    
2018-10-28 00:53:04,312 - Epoch: [173][  300/  391]    Overall Loss 0.182227    Objective Loss 0.182227    Top1 93.664062    Top5 99.921875    LR 0.030000    Time 0.022427    
2018-10-28 00:53:05,424 - Epoch: [173][  350/  391]    Overall Loss 0.183373    Objective Loss 0.183373    Top1 93.551339    Top5 99.919643    LR 0.030000    Time 0.022397    
2018-10-28 00:53:06,419 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.38158 |  0.00015 |    0.19271 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12105 | -0.00302 |    0.03822 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11754 | -0.00234 |    0.04216 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12111 | -0.00857 |    0.05200 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08808 | -0.00330 |    0.02553 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12882 | -0.00500 |    0.05069 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08636 |  0.00167 |    0.02357 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11975 | -0.00490 |    0.05708 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11145 | -0.00458 |    0.06363 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.15092 | -0.00481 |    0.07387 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09036 | -0.00337 |    0.03598 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07393 |  0.00130 |    0.02650 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09467 | -0.00563 |    0.04131 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07691 | -0.00090 |    0.03449 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07707 | -0.00240 |    0.02965 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08235 | -0.00341 |    0.04097 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08227 | -0.00235 |    0.02949 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07726 | -0.00297 |    0.03159 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06445 | -0.00039 |    0.02900 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04965 | -0.00092 |    0.01583 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03367 |  0.00015 |    0.00805 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55568 | -0.03158 |    0.32644 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:53:06,419 - Total sparsity: 74.94

2018-10-28 00:53:06,419 - --- validate (epoch=173)-----------
2018-10-28 00:53:06,419 - 10000 samples (128 per mini-batch)
2018-10-28 00:53:07,141 - Epoch: [173][   50/   78]    Loss 0.400857    Top1 88.437500    Top5 99.515625    
2018-10-28 00:53:07,535 - ==> Top1: 88.260    Top5: 99.620    Loss: 0.400

2018-10-28 00:53:07,536 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:53:07,536 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:53:07,547 - 

2018-10-28 00:53:07,548 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:53:08,718 - Epoch: [174][   50/  391]    Overall Loss 0.170659    Objective Loss 0.170659    Top1 94.421875    Top5 99.953125    LR 0.030000    Time 0.023381    
2018-10-28 00:53:09,831 - Epoch: [174][  100/  391]    Overall Loss 0.175767    Objective Loss 0.175767    Top1 94.046875    Top5 99.937500    LR 0.030000    Time 0.022803    
2018-10-28 00:53:10,943 - Epoch: [174][  150/  391]    Overall Loss 0.181460    Objective Loss 0.181460    Top1 93.765625    Top5 99.911458    LR 0.030000    Time 0.022609    
2018-10-28 00:53:12,055 - Epoch: [174][  200/  391]    Overall Loss 0.183027    Objective Loss 0.183027    Top1 93.644531    Top5 99.929688    LR 0.030000    Time 0.022511    
2018-10-28 00:53:13,167 - Epoch: [174][  250/  391]    Overall Loss 0.184991    Objective Loss 0.184991    Top1 93.540625    Top5 99.931250    LR 0.030000    Time 0.022449    
2018-10-28 00:53:14,280 - Epoch: [174][  300/  391]    Overall Loss 0.185295    Objective Loss 0.185295    Top1 93.481771    Top5 99.940104    LR 0.030000    Time 0.022415    
2018-10-28 00:53:15,395 - Epoch: [174][  350/  391]    Overall Loss 0.185405    Objective Loss 0.185405    Top1 93.444196    Top5 99.941964    LR 0.030000    Time 0.022394    
2018-10-28 00:53:16,389 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.38110 |  0.00084 |    0.19263 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12087 | -0.00276 |    0.03832 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11736 | -0.00258 |    0.04215 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12093 | -0.00831 |    0.05179 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08794 | -0.00355 |    0.02537 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12861 | -0.00519 |    0.05103 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08623 |  0.00164 |    0.02355 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11963 | -0.00473 |    0.05683 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11137 | -0.00478 |    0.06348 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.15054 | -0.00573 |    0.07369 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09034 | -0.00322 |    0.03597 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07390 |  0.00125 |    0.02650 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09463 | -0.00557 |    0.04124 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07688 | -0.00115 |    0.03444 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07701 | -0.00240 |    0.02967 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08234 | -0.00332 |    0.04089 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08212 | -0.00230 |    0.02949 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07727 | -0.00301 |    0.03161 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06451 | -0.00037 |    0.02905 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04973 | -0.00092 |    0.01586 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03367 |  0.00020 |    0.00804 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55522 | -0.03176 |    0.32618 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:53:16,389 - Total sparsity: 74.94

2018-10-28 00:53:16,389 - --- validate (epoch=174)-----------
2018-10-28 00:53:16,389 - 10000 samples (128 per mini-batch)
2018-10-28 00:53:17,100 - Epoch: [174][   50/   78]    Loss 0.406328    Top1 88.468750    Top5 99.468750    
2018-10-28 00:53:17,483 - ==> Top1: 88.310    Top5: 99.520    Loss: 0.408

2018-10-28 00:53:17,484 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:53:17,484 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:53:17,498 - 

2018-10-28 00:53:17,498 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:53:18,670 - Epoch: [175][   50/  391]    Overall Loss 0.179502    Objective Loss 0.179502    Top1 93.750000    Top5 99.875000    LR 0.030000    Time 0.023399    
2018-10-28 00:53:19,782 - Epoch: [175][  100/  391]    Overall Loss 0.175708    Objective Loss 0.175708    Top1 93.820312    Top5 99.890625    LR 0.030000    Time 0.022809    
2018-10-28 00:53:20,895 - Epoch: [175][  150/  391]    Overall Loss 0.180573    Objective Loss 0.180573    Top1 93.651042    Top5 99.906250    LR 0.030000    Time 0.022614    
2018-10-28 00:53:22,005 - Epoch: [175][  200/  391]    Overall Loss 0.184572    Objective Loss 0.184572    Top1 93.437500    Top5 99.914062    LR 0.030000    Time 0.022508    
2018-10-28 00:53:23,117 - Epoch: [175][  250/  391]    Overall Loss 0.187370    Objective Loss 0.187370    Top1 93.350000    Top5 99.921875    LR 0.030000    Time 0.022449    
2018-10-28 00:53:24,227 - Epoch: [175][  300/  391]    Overall Loss 0.185056    Objective Loss 0.185056    Top1 93.411458    Top5 99.921875    LR 0.030000    Time 0.022404    
2018-10-28 00:53:25,338 - Epoch: [175][  350/  391]    Overall Loss 0.183552    Objective Loss 0.183552    Top1 93.388393    Top5 99.933036    LR 0.030000    Time 0.022374    
2018-10-28 00:53:26,326 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.38063 |  0.00019 |    0.19170 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12078 | -0.00309 |    0.03826 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11727 | -0.00265 |    0.04209 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12077 | -0.00783 |    0.05169 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08778 | -0.00339 |    0.02533 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12851 | -0.00514 |    0.05084 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08613 |  0.00175 |    0.02349 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11951 | -0.00486 |    0.05673 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11131 | -0.00453 |    0.06353 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.15009 | -0.00527 |    0.07341 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09029 | -0.00351 |    0.03596 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07387 |  0.00139 |    0.02649 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09463 | -0.00556 |    0.04125 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07687 | -0.00110 |    0.03443 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07695 | -0.00250 |    0.02968 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08233 | -0.00330 |    0.04091 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08198 | -0.00209 |    0.02957 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07726 | -0.00318 |    0.03160 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06454 | -0.00034 |    0.02905 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04978 | -0.00098 |    0.01589 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03367 |  0.00019 |    0.00803 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55515 | -0.03156 |    0.32629 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:53:26,326 - Total sparsity: 74.94

2018-10-28 00:53:26,326 - --- validate (epoch=175)-----------
2018-10-28 00:53:26,327 - 10000 samples (128 per mini-batch)
2018-10-28 00:53:27,051 - Epoch: [175][   50/   78]    Loss 0.413537    Top1 88.390625    Top5 99.406250    
2018-10-28 00:53:27,442 - ==> Top1: 88.160    Top5: 99.480    Loss: 0.406

2018-10-28 00:53:27,443 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:53:27,443 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:53:27,454 - 

2018-10-28 00:53:27,455 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:53:28,625 - Epoch: [176][   50/  391]    Overall Loss 0.176217    Objective Loss 0.176217    Top1 93.265625    Top5 99.937500    LR 0.030000    Time 0.023367    
2018-10-28 00:53:29,736 - Epoch: [176][  100/  391]    Overall Loss 0.173921    Objective Loss 0.173921    Top1 93.562500    Top5 99.929688    LR 0.030000    Time 0.022780    
2018-10-28 00:53:30,848 - Epoch: [176][  150/  391]    Overall Loss 0.176289    Objective Loss 0.176289    Top1 93.614583    Top5 99.927083    LR 0.030000    Time 0.022594    
2018-10-28 00:53:31,959 - Epoch: [176][  200/  391]    Overall Loss 0.178436    Objective Loss 0.178436    Top1 93.515625    Top5 99.917969    LR 0.030000    Time 0.022491    
2018-10-28 00:53:33,069 - Epoch: [176][  250/  391]    Overall Loss 0.180926    Objective Loss 0.180926    Top1 93.431250    Top5 99.921875    LR 0.030000    Time 0.022428    
2018-10-28 00:53:34,180 - Epoch: [176][  300/  391]    Overall Loss 0.180840    Objective Loss 0.180840    Top1 93.416667    Top5 99.919271    LR 0.030000    Time 0.022389    
2018-10-28 00:53:35,290 - Epoch: [176][  350/  391]    Overall Loss 0.184274    Objective Loss 0.184274    Top1 93.292411    Top5 99.924107    LR 0.030000    Time 0.022360    
2018-10-28 00:53:36,280 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.38071 |  0.00325 |    0.19086 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12077 | -0.00288 |    0.03829 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11720 | -0.00236 |    0.04206 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12066 | -0.00805 |    0.05186 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08766 | -0.00346 |    0.02544 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12842 | -0.00498 |    0.05082 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08602 |  0.00195 |    0.02340 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11943 | -0.00468 |    0.05675 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11129 | -0.00463 |    0.06356 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14981 | -0.00463 |    0.07372 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09032 | -0.00363 |    0.03592 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07389 |  0.00154 |    0.02643 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09467 | -0.00545 |    0.04118 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07688 | -0.00117 |    0.03439 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07692 | -0.00237 |    0.02961 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08234 | -0.00313 |    0.04093 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08185 | -0.00193 |    0.02942 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07728 | -0.00316 |    0.03163 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06460 | -0.00038 |    0.02908 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04985 | -0.00091 |    0.01589 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03367 |  0.00018 |    0.00805 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55472 | -0.03160 |    0.32610 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:53:36,280 - Total sparsity: 74.94

2018-10-28 00:53:36,280 - --- validate (epoch=176)-----------
2018-10-28 00:53:36,280 - 10000 samples (128 per mini-batch)
2018-10-28 00:53:36,997 - Epoch: [176][   50/   78]    Loss 0.461117    Top1 86.640625    Top5 99.359375    
2018-10-28 00:53:37,384 - ==> Top1: 86.700    Top5: 99.460    Loss: 0.459

2018-10-28 00:53:37,384 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:53:37,385 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:53:37,394 - 

2018-10-28 00:53:37,394 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:53:38,567 - Epoch: [177][   50/  391]    Overall Loss 0.183720    Objective Loss 0.183720    Top1 93.437500    Top5 99.921875    LR 0.030000    Time 0.023415    
2018-10-28 00:53:39,681 - Epoch: [177][  100/  391]    Overall Loss 0.175223    Objective Loss 0.175223    Top1 93.859375    Top5 99.921875    LR 0.030000    Time 0.022838    
2018-10-28 00:53:40,794 - Epoch: [177][  150/  391]    Overall Loss 0.174848    Objective Loss 0.174848    Top1 93.984375    Top5 99.927083    LR 0.030000    Time 0.022631    
2018-10-28 00:53:41,906 - Epoch: [177][  200/  391]    Overall Loss 0.177629    Objective Loss 0.177629    Top1 93.742188    Top5 99.933594    LR 0.030000    Time 0.022529    
2018-10-28 00:53:43,017 - Epoch: [177][  250/  391]    Overall Loss 0.179913    Objective Loss 0.179913    Top1 93.637500    Top5 99.937500    LR 0.030000    Time 0.022461    
2018-10-28 00:53:44,128 - Epoch: [177][  300/  391]    Overall Loss 0.182574    Objective Loss 0.182574    Top1 93.546875    Top5 99.932292    LR 0.030000    Time 0.022417    
2018-10-28 00:53:45,241 - Epoch: [177][  350/  391]    Overall Loss 0.183602    Objective Loss 0.183602    Top1 93.491071    Top5 99.930804    LR 0.030000    Time 0.022390    
2018-10-28 00:53:46,233 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.38073 |  0.00127 |    0.19149 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12074 | -0.00282 |    0.03828 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11717 | -0.00220 |    0.04194 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12061 | -0.00799 |    0.05184 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08762 | -0.00315 |    0.02538 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12837 | -0.00515 |    0.05068 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08598 |  0.00191 |    0.02320 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11935 | -0.00461 |    0.05661 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11125 | -0.00429 |    0.06345 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14953 | -0.00450 |    0.07375 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09033 | -0.00339 |    0.03602 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07386 |  0.00142 |    0.02641 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09468 | -0.00550 |    0.04124 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07688 | -0.00100 |    0.03440 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07687 | -0.00240 |    0.02963 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08233 | -0.00309 |    0.04091 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08169 | -0.00202 |    0.02952 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07728 | -0.00309 |    0.03164 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06464 | -0.00039 |    0.02910 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04991 | -0.00089 |    0.01591 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03367 |  0.00016 |    0.00806 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55464 | -0.03163 |    0.32602 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:53:46,233 - Total sparsity: 74.94

2018-10-28 00:53:46,233 - --- validate (epoch=177)-----------
2018-10-28 00:53:46,233 - 10000 samples (128 per mini-batch)
2018-10-28 00:53:46,956 - Epoch: [177][   50/   78]    Loss 0.406011    Top1 88.375000    Top5 99.500000    
2018-10-28 00:53:47,354 - ==> Top1: 88.530    Top5: 99.550    Loss: 0.386

2018-10-28 00:53:47,355 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:53:47,355 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:53:47,368 - 

2018-10-28 00:53:47,369 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:53:48,541 - Epoch: [178][   50/  391]    Overall Loss 0.178875    Objective Loss 0.178875    Top1 93.531250    Top5 99.921875    LR 0.030000    Time 0.023408    
2018-10-28 00:53:49,653 - Epoch: [178][  100/  391]    Overall Loss 0.177418    Objective Loss 0.177418    Top1 93.671875    Top5 99.937500    LR 0.030000    Time 0.022809    
2018-10-28 00:53:50,767 - Epoch: [178][  150/  391]    Overall Loss 0.175549    Objective Loss 0.175549    Top1 93.760417    Top5 99.906250    LR 0.030000    Time 0.022622    
2018-10-28 00:53:51,880 - Epoch: [178][  200/  391]    Overall Loss 0.180594    Objective Loss 0.180594    Top1 93.613281    Top5 99.894531    LR 0.030000    Time 0.022527    
2018-10-28 00:53:52,993 - Epoch: [178][  250/  391]    Overall Loss 0.182320    Objective Loss 0.182320    Top1 93.562500    Top5 99.881250    LR 0.030000    Time 0.022470    
2018-10-28 00:53:54,105 - Epoch: [178][  300/  391]    Overall Loss 0.183761    Objective Loss 0.183761    Top1 93.489583    Top5 99.890625    LR 0.030000    Time 0.022428    
2018-10-28 00:53:55,216 - Epoch: [178][  350/  391]    Overall Loss 0.184920    Objective Loss 0.184920    Top1 93.441964    Top5 99.895089    LR 0.030000    Time 0.022395    
2018-10-28 00:53:56,215 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.38053 |  0.00130 |    0.19121 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12066 | -0.00306 |    0.03823 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11706 | -0.00243 |    0.04198 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12059 | -0.00812 |    0.05184 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08759 | -0.00289 |    0.02529 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12827 | -0.00517 |    0.05065 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08591 |  0.00197 |    0.02336 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11924 | -0.00482 |    0.05669 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11115 | -0.00456 |    0.06344 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14927 | -0.00509 |    0.07339 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09033 | -0.00326 |    0.03601 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07384 |  0.00144 |    0.02641 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09466 | -0.00540 |    0.04122 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07687 | -0.00096 |    0.03436 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07681 | -0.00248 |    0.02960 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08231 | -0.00319 |    0.04087 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08152 | -0.00195 |    0.02940 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07728 | -0.00300 |    0.03164 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06469 | -0.00037 |    0.02912 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04998 | -0.00089 |    0.01594 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03367 |  0.00018 |    0.00805 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55398 | -0.03151 |    0.32585 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:53:56,215 - Total sparsity: 74.94

2018-10-28 00:53:56,215 - --- validate (epoch=178)-----------
2018-10-28 00:53:56,215 - 10000 samples (128 per mini-batch)
2018-10-28 00:53:56,929 - Epoch: [178][   50/   78]    Loss 0.422569    Top1 87.843750    Top5 99.406250    
2018-10-28 00:53:57,317 - ==> Top1: 87.810    Top5: 99.490    Loss: 0.420

2018-10-28 00:53:57,318 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:53:57,318 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:53:57,335 - 

2018-10-28 00:53:57,336 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:53:58,481 - Epoch: [179][   50/  391]    Overall Loss 0.174285    Objective Loss 0.174285    Top1 93.984375    Top5 99.906250    LR 0.030000    Time 0.022861    
2018-10-28 00:53:59,595 - Epoch: [179][  100/  391]    Overall Loss 0.179509    Objective Loss 0.179509    Top1 93.750000    Top5 99.921875    LR 0.030000    Time 0.022554    
2018-10-28 00:54:00,709 - Epoch: [179][  150/  391]    Overall Loss 0.182522    Objective Loss 0.182522    Top1 93.583333    Top5 99.906250    LR 0.030000    Time 0.022457    
2018-10-28 00:54:01,821 - Epoch: [179][  200/  391]    Overall Loss 0.184145    Objective Loss 0.184145    Top1 93.468750    Top5 99.914062    LR 0.030000    Time 0.022398    
2018-10-28 00:54:02,933 - Epoch: [179][  250/  391]    Overall Loss 0.183687    Objective Loss 0.183687    Top1 93.434375    Top5 99.925000    LR 0.030000    Time 0.022359    
2018-10-28 00:54:04,043 - Epoch: [179][  300/  391]    Overall Loss 0.185699    Objective Loss 0.185699    Top1 93.372396    Top5 99.908854    LR 0.030000    Time 0.022330    
2018-10-28 00:54:05,155 - Epoch: [179][  350/  391]    Overall Loss 0.184680    Objective Loss 0.184680    Top1 93.377232    Top5 99.915179    LR 0.030000    Time 0.022312    
2018-10-28 00:54:06,147 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37986 |  0.00028 |    0.19095 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12055 | -0.00305 |    0.03836 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11693 | -0.00257 |    0.04197 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12065 | -0.00794 |    0.05161 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08749 | -0.00323 |    0.02539 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12811 | -0.00551 |    0.05039 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08581 |  0.00178 |    0.02334 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11914 | -0.00476 |    0.05667 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11109 | -0.00450 |    0.06347 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14899 | -0.00457 |    0.07300 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09034 | -0.00336 |    0.03597 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07384 |  0.00145 |    0.02644 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09465 | -0.00522 |    0.04113 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07688 | -0.00106 |    0.03433 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07676 | -0.00243 |    0.02958 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08231 | -0.00305 |    0.04083 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08134 | -0.00191 |    0.02932 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07728 | -0.00301 |    0.03162 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06474 | -0.00032 |    0.02915 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05005 | -0.00095 |    0.01596 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03368 |  0.00019 |    0.00804 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55422 | -0.03124 |    0.32591 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:54:06,148 - Total sparsity: 74.94

2018-10-28 00:54:06,148 - --- validate (epoch=179)-----------
2018-10-28 00:54:06,148 - 10000 samples (128 per mini-batch)
2018-10-28 00:54:06,875 - Epoch: [179][   50/   78]    Loss 0.411295    Top1 87.578125    Top5 99.562500    
2018-10-28 00:54:07,269 - ==> Top1: 87.730    Top5: 99.580    Loss: 0.410

2018-10-28 00:54:07,269 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:54:07,270 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:54:07,287 - 

2018-10-28 00:54:07,287 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:54:08,429 - Epoch: [180][   50/  391]    Overall Loss 0.165526    Objective Loss 0.165526    Top1 94.109375    Top5 99.953125    LR 0.030000    Time 0.022798    
2018-10-28 00:54:09,539 - Epoch: [180][  100/  391]    Overall Loss 0.170436    Objective Loss 0.170436    Top1 93.835938    Top5 99.921875    LR 0.030000    Time 0.022485    
2018-10-28 00:54:10,648 - Epoch: [180][  150/  391]    Overall Loss 0.174624    Objective Loss 0.174624    Top1 93.734375    Top5 99.916667    LR 0.030000    Time 0.022380    
2018-10-28 00:54:11,759 - Epoch: [180][  200/  391]    Overall Loss 0.182249    Objective Loss 0.182249    Top1 93.406250    Top5 99.906250    LR 0.030000    Time 0.022332    
2018-10-28 00:54:12,870 - Epoch: [180][  250/  391]    Overall Loss 0.186400    Objective Loss 0.186400    Top1 93.265625    Top5 99.909375    LR 0.030000    Time 0.022306    
2018-10-28 00:54:13,984 - Epoch: [180][  300/  391]    Overall Loss 0.190366    Objective Loss 0.190366    Top1 93.169271    Top5 99.914062    LR 0.030000    Time 0.022296    
2018-10-28 00:54:15,098 - Epoch: [180][  350/  391]    Overall Loss 0.190388    Objective Loss 0.190388    Top1 93.149554    Top5 99.917411    LR 0.030000    Time 0.022290    
2018-10-28 00:54:16,087 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37967 |  0.00251 |    0.19104 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12053 | -0.00284 |    0.03838 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11688 | -0.00185 |    0.04184 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12059 | -0.00771 |    0.05157 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08744 | -0.00342 |    0.02539 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12800 | -0.00558 |    0.05054 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08568 |  0.00188 |    0.02338 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11911 | -0.00471 |    0.05654 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11106 | -0.00454 |    0.06365 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14863 | -0.00449 |    0.07305 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09036 | -0.00338 |    0.03597 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07385 |  0.00143 |    0.02646 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09468 | -0.00526 |    0.04114 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07693 | -0.00102 |    0.03442 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07674 | -0.00231 |    0.02954 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08234 | -0.00301 |    0.04083 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08124 | -0.00206 |    0.02929 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07731 | -0.00296 |    0.03159 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06483 | -0.00032 |    0.02915 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05015 | -0.00083 |    0.01599 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03371 |  0.00019 |    0.00803 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55250 | -0.03118 |    0.32501 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:54:16,087 - Total sparsity: 74.94

2018-10-28 00:54:16,087 - --- validate (epoch=180)-----------
2018-10-28 00:54:16,088 - 10000 samples (128 per mini-batch)
2018-10-28 00:54:16,810 - Epoch: [180][   50/   78]    Loss 0.398805    Top1 87.890625    Top5 99.578125    
2018-10-28 00:54:17,199 - ==> Top1: 88.060    Top5: 99.600    Loss: 0.392

2018-10-28 00:54:17,200 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:54:17,200 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:54:17,210 - 

2018-10-28 00:54:17,211 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:54:18,381 - Epoch: [181][   50/  391]    Overall Loss 0.188274    Objective Loss 0.188274    Top1 93.281250    Top5 99.921875    LR 0.030000    Time 0.023370    
2018-10-28 00:54:19,491 - Epoch: [181][  100/  391]    Overall Loss 0.183317    Objective Loss 0.183317    Top1 93.328125    Top5 99.937500    LR 0.030000    Time 0.022769    
2018-10-28 00:54:20,601 - Epoch: [181][  150/  391]    Overall Loss 0.185391    Objective Loss 0.185391    Top1 93.359375    Top5 99.937500    LR 0.030000    Time 0.022573    
2018-10-28 00:54:21,711 - Epoch: [181][  200/  391]    Overall Loss 0.183186    Objective Loss 0.183186    Top1 93.464844    Top5 99.929688    LR 0.030000    Time 0.022457    
2018-10-28 00:54:22,823 - Epoch: [181][  250/  391]    Overall Loss 0.184118    Objective Loss 0.184118    Top1 93.503125    Top5 99.928125    LR 0.030000    Time 0.022407    
2018-10-28 00:54:23,934 - Epoch: [181][  300/  391]    Overall Loss 0.183536    Objective Loss 0.183536    Top1 93.494792    Top5 99.914062    LR 0.030000    Time 0.022371    
2018-10-28 00:54:25,045 - Epoch: [181][  350/  391]    Overall Loss 0.184766    Objective Loss 0.184766    Top1 93.437500    Top5 99.921875    LR 0.030000    Time 0.022347    
2018-10-28 00:54:26,039 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37944 |  0.00141 |    0.19121 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12043 | -0.00300 |    0.03832 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11679 | -0.00237 |    0.04167 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12050 | -0.00728 |    0.05154 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08735 | -0.00360 |    0.02541 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12793 | -0.00544 |    0.05036 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08558 |  0.00193 |    0.02338 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11902 | -0.00471 |    0.05649 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11097 | -0.00460 |    0.06351 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14823 | -0.00436 |    0.07232 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09033 | -0.00356 |    0.03598 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07380 |  0.00163 |    0.02640 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09467 | -0.00504 |    0.04117 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07691 | -0.00103 |    0.03437 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07667 | -0.00226 |    0.02951 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08231 | -0.00295 |    0.04080 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08103 | -0.00218 |    0.02920 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07728 | -0.00298 |    0.03159 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06486 | -0.00035 |    0.02916 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05020 | -0.00090 |    0.01602 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03371 |  0.00022 |    0.00804 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55257 | -0.03131 |    0.32488 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:54:26,039 - Total sparsity: 74.94

2018-10-28 00:54:26,039 - --- validate (epoch=181)-----------
2018-10-28 00:54:26,039 - 10000 samples (128 per mini-batch)
2018-10-28 00:54:26,763 - Epoch: [181][   50/   78]    Loss 0.431462    Top1 87.140625    Top5 99.421875    
2018-10-28 00:54:27,155 - ==> Top1: 87.400    Top5: 99.510    Loss: 0.415

2018-10-28 00:54:27,156 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:54:27,156 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:54:27,167 - 

2018-10-28 00:54:27,168 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:54:28,338 - Epoch: [182][   50/  391]    Overall Loss 0.183112    Objective Loss 0.183112    Top1 93.750000    Top5 99.906250    LR 0.030000    Time 0.023377    
2018-10-28 00:54:29,451 - Epoch: [182][  100/  391]    Overall Loss 0.178436    Objective Loss 0.178436    Top1 93.914062    Top5 99.914062    LR 0.030000    Time 0.022808    
2018-10-28 00:54:30,564 - Epoch: [182][  150/  391]    Overall Loss 0.177905    Objective Loss 0.177905    Top1 93.848958    Top5 99.901042    LR 0.030000    Time 0.022616    
2018-10-28 00:54:31,677 - Epoch: [182][  200/  391]    Overall Loss 0.177647    Objective Loss 0.177647    Top1 93.808594    Top5 99.914062    LR 0.030000    Time 0.022518    
2018-10-28 00:54:32,790 - Epoch: [182][  250/  391]    Overall Loss 0.178538    Objective Loss 0.178538    Top1 93.712500    Top5 99.921875    LR 0.030000    Time 0.022447    
2018-10-28 00:54:33,902 - Epoch: [182][  300/  391]    Overall Loss 0.181364    Objective Loss 0.181364    Top1 93.591146    Top5 99.934896    LR 0.030000    Time 0.022410    
2018-10-28 00:54:35,015 - Epoch: [182][  350/  391]    Overall Loss 0.182247    Objective Loss 0.182247    Top1 93.544643    Top5 99.926339    LR 0.030000    Time 0.022383    
2018-10-28 00:54:36,012 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37930 |  0.00119 |    0.19096 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12034 | -0.00340 |    0.03831 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11670 | -0.00223 |    0.04169 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12041 | -0.00772 |    0.05154 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08727 | -0.00360 |    0.02526 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12783 | -0.00546 |    0.05039 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08555 |  0.00186 |    0.02336 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11895 | -0.00454 |    0.05648 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11089 | -0.00457 |    0.06341 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14785 | -0.00485 |    0.07215 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09026 | -0.00351 |    0.03595 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07373 |  0.00163 |    0.02631 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09463 | -0.00525 |    0.04113 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07689 | -0.00095 |    0.03431 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07662 | -0.00228 |    0.02950 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08229 | -0.00303 |    0.04083 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08088 | -0.00199 |    0.02920 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07728 | -0.00300 |    0.03159 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06492 | -0.00027 |    0.02919 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05029 | -0.00090 |    0.01603 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03372 |  0.00026 |    0.00802 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55264 | -0.03089 |    0.32489 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:54:36,012 - Total sparsity: 74.94

2018-10-28 00:54:36,012 - --- validate (epoch=182)-----------
2018-10-28 00:54:36,012 - 10000 samples (128 per mini-batch)
2018-10-28 00:54:36,731 - Epoch: [182][   50/   78]    Loss 0.385255    Top1 88.359375    Top5 99.562500    
2018-10-28 00:54:37,115 - ==> Top1: 88.310    Top5: 99.610    Loss: 0.378

2018-10-28 00:54:37,116 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:54:37,116 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:54:37,130 - 

2018-10-28 00:54:37,130 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:54:38,304 - Epoch: [183][   50/  391]    Overall Loss 0.182324    Objective Loss 0.182324    Top1 93.406250    Top5 99.937500    LR 0.030000    Time 0.023442    
2018-10-28 00:54:39,417 - Epoch: [183][  100/  391]    Overall Loss 0.186410    Objective Loss 0.186410    Top1 93.195312    Top5 99.937500    LR 0.030000    Time 0.022838    
2018-10-28 00:54:40,531 - Epoch: [183][  150/  391]    Overall Loss 0.183401    Objective Loss 0.183401    Top1 93.281250    Top5 99.942708    LR 0.030000    Time 0.022645    
2018-10-28 00:54:41,645 - Epoch: [183][  200/  391]    Overall Loss 0.189449    Objective Loss 0.189449    Top1 93.070312    Top5 99.945312    LR 0.030000    Time 0.022545    
2018-10-28 00:54:42,758 - Epoch: [183][  250/  391]    Overall Loss 0.189068    Objective Loss 0.189068    Top1 93.131250    Top5 99.937500    LR 0.030000    Time 0.022483    
2018-10-28 00:54:43,871 - Epoch: [183][  300/  391]    Overall Loss 0.188804    Objective Loss 0.188804    Top1 93.138021    Top5 99.932292    LR 0.030000    Time 0.022429    
2018-10-28 00:54:44,981 - Epoch: [183][  350/  391]    Overall Loss 0.188334    Objective Loss 0.188334    Top1 93.116071    Top5 99.928571    LR 0.030000    Time 0.022393    
2018-10-28 00:54:45,969 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37961 | -0.00121 |    0.19120 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12030 | -0.00275 |    0.03812 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11662 | -0.00241 |    0.04158 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12040 | -0.00798 |    0.05153 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08728 | -0.00340 |    0.02517 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12783 | -0.00533 |    0.05024 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08548 |  0.00185 |    0.02343 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11893 | -0.00464 |    0.05646 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11090 | -0.00465 |    0.06329 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14764 | -0.00487 |    0.07218 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09029 | -0.00359 |    0.03593 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07377 |  0.00139 |    0.02638 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09468 | -0.00505 |    0.04113 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07693 | -0.00071 |    0.03436 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07660 | -0.00235 |    0.02948 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08232 | -0.00311 |    0.04089 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08077 | -0.00186 |    0.02903 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07732 | -0.00295 |    0.03163 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06499 | -0.00031 |    0.02924 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05037 | -0.00091 |    0.01604 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03373 |  0.00026 |    0.00803 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55258 | -0.03083 |    0.32468 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:54:45,969 - Total sparsity: 74.94

2018-10-28 00:54:45,969 - --- validate (epoch=183)-----------
2018-10-28 00:54:45,969 - 10000 samples (128 per mini-batch)
2018-10-28 00:54:46,691 - Epoch: [183][   50/   78]    Loss 0.421705    Top1 87.890625    Top5 99.546875    
2018-10-28 00:54:47,083 - ==> Top1: 87.810    Top5: 99.540    Loss: 0.414

2018-10-28 00:54:47,084 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:54:47,084 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:54:47,101 - 

2018-10-28 00:54:47,102 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:54:48,264 - Epoch: [184][   50/  391]    Overall Loss 0.168080    Objective Loss 0.168080    Top1 94.062500    Top5 99.875000    LR 0.030000    Time 0.023208    
2018-10-28 00:54:49,376 - Epoch: [184][  100/  391]    Overall Loss 0.169396    Objective Loss 0.169396    Top1 93.953125    Top5 99.890625    LR 0.030000    Time 0.022707    
2018-10-28 00:54:50,491 - Epoch: [184][  150/  391]    Overall Loss 0.173456    Objective Loss 0.173456    Top1 93.848958    Top5 99.901042    LR 0.030000    Time 0.022561    
2018-10-28 00:54:51,604 - Epoch: [184][  200/  391]    Overall Loss 0.178352    Objective Loss 0.178352    Top1 93.656250    Top5 99.914062    LR 0.030000    Time 0.022479    
2018-10-28 00:54:52,717 - Epoch: [184][  250/  391]    Overall Loss 0.180503    Objective Loss 0.180503    Top1 93.562500    Top5 99.925000    LR 0.030000    Time 0.022429    
2018-10-28 00:54:53,830 - Epoch: [184][  300/  391]    Overall Loss 0.180746    Objective Loss 0.180746    Top1 93.606771    Top5 99.927083    LR 0.030000    Time 0.022399    
2018-10-28 00:54:54,945 - Epoch: [184][  350/  391]    Overall Loss 0.182575    Objective Loss 0.182575    Top1 93.535714    Top5 99.928571    LR 0.030000    Time 0.022368    
2018-10-28 00:54:55,940 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37988 |  0.00160 |    0.19122 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12021 | -0.00319 |    0.03822 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11652 | -0.00197 |    0.04176 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12033 | -0.00846 |    0.05153 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08726 | -0.00326 |    0.02519 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12773 | -0.00480 |    0.05003 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08536 |  0.00204 |    0.02329 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11886 | -0.00500 |    0.05644 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11086 | -0.00439 |    0.06321 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14733 | -0.00433 |    0.07186 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09026 | -0.00339 |    0.03587 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07375 |  0.00143 |    0.02638 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09469 | -0.00518 |    0.04117 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07694 | -0.00100 |    0.03449 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07654 | -0.00239 |    0.02941 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08229 | -0.00302 |    0.04092 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08063 | -0.00197 |    0.02917 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07730 | -0.00291 |    0.03160 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06503 | -0.00030 |    0.02926 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05042 | -0.00091 |    0.01605 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03374 |  0.00025 |    0.00802 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55186 | -0.03095 |    0.32431 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:54:55,940 - Total sparsity: 74.94

2018-10-28 00:54:55,940 - --- validate (epoch=184)-----------
2018-10-28 00:54:55,940 - 10000 samples (128 per mini-batch)
2018-10-28 00:54:56,665 - Epoch: [184][   50/   78]    Loss 0.447102    Top1 87.203125    Top5 99.453125    
2018-10-28 00:54:57,056 - ==> Top1: 87.150    Top5: 99.530    Loss: 0.437

2018-10-28 00:54:57,056 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:54:57,057 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:54:57,067 - 

2018-10-28 00:54:57,068 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:54:58,240 - Epoch: [185][   50/  391]    Overall Loss 0.171716    Objective Loss 0.171716    Top1 93.796875    Top5 99.906250    LR 0.030000    Time 0.023419    
2018-10-28 00:54:59,351 - Epoch: [185][  100/  391]    Overall Loss 0.173196    Objective Loss 0.173196    Top1 93.890625    Top5 99.906250    LR 0.030000    Time 0.022803    
2018-10-28 00:55:00,465 - Epoch: [185][  150/  391]    Overall Loss 0.172630    Objective Loss 0.172630    Top1 93.812500    Top5 99.911458    LR 0.030000    Time 0.022620    
2018-10-28 00:55:01,578 - Epoch: [185][  200/  391]    Overall Loss 0.177356    Objective Loss 0.177356    Top1 93.656250    Top5 99.917969    LR 0.030000    Time 0.022525    
2018-10-28 00:55:02,690 - Epoch: [185][  250/  391]    Overall Loss 0.182633    Objective Loss 0.182633    Top1 93.500000    Top5 99.921875    LR 0.030000    Time 0.022461    
2018-10-28 00:55:03,806 - Epoch: [185][  300/  391]    Overall Loss 0.183213    Objective Loss 0.183213    Top1 93.460938    Top5 99.908854    LR 0.030000    Time 0.022434    
2018-10-28 00:55:04,920 - Epoch: [185][  350/  391]    Overall Loss 0.184997    Objective Loss 0.184997    Top1 93.392857    Top5 99.906250    LR 0.030000    Time 0.022407    
2018-10-28 00:55:05,913 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37919 |  0.00146 |    0.19072 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12011 | -0.00307 |    0.03830 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11643 | -0.00216 |    0.04173 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12024 | -0.00899 |    0.05142 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08717 | -0.00330 |    0.02512 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12764 | -0.00488 |    0.05018 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08528 |  0.00181 |    0.02332 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11884 | -0.00453 |    0.05652 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11082 | -0.00452 |    0.06326 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14703 | -0.00414 |    0.07175 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09028 | -0.00333 |    0.03590 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07376 |  0.00147 |    0.02638 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09473 | -0.00533 |    0.04130 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07696 | -0.00082 |    0.03445 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07650 | -0.00240 |    0.02941 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08228 | -0.00302 |    0.04087 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08049 | -0.00195 |    0.02897 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07728 | -0.00294 |    0.03160 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06507 | -0.00022 |    0.02925 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05048 | -0.00097 |    0.01610 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03374 |  0.00025 |    0.00802 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55138 | -0.03095 |    0.32387 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:55:05,913 - Total sparsity: 74.94

2018-10-28 00:55:05,914 - --- validate (epoch=185)-----------
2018-10-28 00:55:05,914 - 10000 samples (128 per mini-batch)
2018-10-28 00:55:06,637 - Epoch: [185][   50/   78]    Loss 0.420342    Top1 87.593750    Top5 99.406250    
2018-10-28 00:55:07,026 - ==> Top1: 87.750    Top5: 99.500    Loss: 0.405

2018-10-28 00:55:07,027 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:55:07,027 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:55:07,038 - 

2018-10-28 00:55:07,039 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:55:08,208 - Epoch: [186][   50/  391]    Overall Loss 0.171318    Objective Loss 0.171318    Top1 94.046875    Top5 99.906250    LR 0.030000    Time 0.023354    
2018-10-28 00:55:09,318 - Epoch: [186][  100/  391]    Overall Loss 0.174019    Objective Loss 0.174019    Top1 93.937500    Top5 99.914062    LR 0.030000    Time 0.022768    
2018-10-28 00:55:10,428 - Epoch: [186][  150/  391]    Overall Loss 0.175719    Objective Loss 0.175719    Top1 93.718750    Top5 99.942708    LR 0.030000    Time 0.022570    
2018-10-28 00:55:11,542 - Epoch: [186][  200/  391]    Overall Loss 0.176368    Objective Loss 0.176368    Top1 93.664062    Top5 99.925781    LR 0.030000    Time 0.022470    
2018-10-28 00:55:12,655 - Epoch: [186][  250/  391]    Overall Loss 0.178706    Objective Loss 0.178706    Top1 93.553125    Top5 99.934375    LR 0.030000    Time 0.022423    
2018-10-28 00:55:13,765 - Epoch: [186][  300/  391]    Overall Loss 0.180629    Objective Loss 0.180629    Top1 93.486979    Top5 99.927083    LR 0.030000    Time 0.022383    
2018-10-28 00:55:14,877 - Epoch: [186][  350/  391]    Overall Loss 0.182172    Objective Loss 0.182172    Top1 93.468750    Top5 99.928571    LR 0.030000    Time 0.022357    
2018-10-28 00:55:15,865 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37911 |  0.00077 |    0.19140 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12006 | -0.00332 |    0.03828 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11634 | -0.00201 |    0.04152 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12019 | -0.00824 |    0.05144 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08707 | -0.00311 |    0.02514 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12748 | -0.00509 |    0.05026 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08516 |  0.00154 |    0.02336 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11879 | -0.00437 |    0.05661 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11077 | -0.00428 |    0.06317 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14669 | -0.00411 |    0.07145 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09022 | -0.00342 |    0.03592 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07369 |  0.00152 |    0.02642 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09473 | -0.00517 |    0.04119 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07696 | -0.00061 |    0.03435 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07644 | -0.00236 |    0.02938 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08226 | -0.00307 |    0.04084 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08036 | -0.00202 |    0.02900 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07726 | -0.00294 |    0.03160 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06511 | -0.00032 |    0.02926 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05053 | -0.00102 |    0.01613 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03374 |  0.00023 |    0.00802 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55102 | -0.03097 |    0.32352 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:55:15,865 - Total sparsity: 74.94

2018-10-28 00:55:15,865 - --- validate (epoch=186)-----------
2018-10-28 00:55:15,866 - 10000 samples (128 per mini-batch)
2018-10-28 00:55:16,589 - Epoch: [186][   50/   78]    Loss 0.415241    Top1 87.500000    Top5 99.625000    
2018-10-28 00:55:16,976 - ==> Top1: 87.390    Top5: 99.600    Loss: 0.415

2018-10-28 00:55:16,977 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:55:16,977 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:55:16,988 - 

2018-10-28 00:55:16,988 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:55:18,159 - Epoch: [187][   50/  391]    Overall Loss 0.173743    Objective Loss 0.173743    Top1 93.750000    Top5 99.937500    LR 0.030000    Time 0.023378    
2018-10-28 00:55:19,270 - Epoch: [187][  100/  391]    Overall Loss 0.173470    Objective Loss 0.173470    Top1 93.843750    Top5 99.960938    LR 0.030000    Time 0.022786    
2018-10-28 00:55:20,381 - Epoch: [187][  150/  391]    Overall Loss 0.176577    Objective Loss 0.176577    Top1 93.598958    Top5 99.958333    LR 0.030000    Time 0.022593    
2018-10-28 00:55:21,493 - Epoch: [187][  200/  391]    Overall Loss 0.178379    Objective Loss 0.178379    Top1 93.531250    Top5 99.941406    LR 0.030000    Time 0.022497    
2018-10-28 00:55:22,605 - Epoch: [187][  250/  391]    Overall Loss 0.180034    Objective Loss 0.180034    Top1 93.490625    Top5 99.937500    LR 0.030000    Time 0.022442    
2018-10-28 00:55:23,718 - Epoch: [187][  300/  391]    Overall Loss 0.180856    Objective Loss 0.180856    Top1 93.453125    Top5 99.916667    LR 0.030000    Time 0.022406    
2018-10-28 00:55:24,832 - Epoch: [187][  350/  391]    Overall Loss 0.182755    Objective Loss 0.182755    Top1 93.377232    Top5 99.924107    LR 0.030000    Time 0.022384    
2018-10-28 00:55:25,822 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37883 |  0.00112 |    0.19037 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12004 | -0.00320 |    0.03802 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11630 | -0.00228 |    0.04166 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12008 | -0.00845 |    0.05117 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08700 | -0.00322 |    0.02523 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12737 | -0.00523 |    0.05021 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08512 |  0.00159 |    0.02316 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11875 | -0.00455 |    0.05650 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11073 | -0.00420 |    0.06317 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14641 | -0.00444 |    0.07157 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09017 | -0.00340 |    0.03590 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07365 |  0.00155 |    0.02641 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09476 | -0.00522 |    0.04123 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07697 | -0.00068 |    0.03442 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07639 | -0.00232 |    0.02935 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08226 | -0.00315 |    0.04083 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08028 | -0.00206 |    0.02883 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07726 | -0.00291 |    0.03159 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06516 | -0.00033 |    0.02931 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05060 | -0.00103 |    0.01614 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03375 |  0.00023 |    0.00803 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55105 | -0.03099 |    0.32365 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:55:25,822 - Total sparsity: 74.94

2018-10-28 00:55:25,822 - --- validate (epoch=187)-----------
2018-10-28 00:55:25,823 - 10000 samples (128 per mini-batch)
2018-10-28 00:55:26,545 - Epoch: [187][   50/   78]    Loss 0.420201    Top1 87.062500    Top5 99.468750    
2018-10-28 00:55:26,940 - ==> Top1: 87.240    Top5: 99.510    Loss: 0.408

2018-10-28 00:55:26,940 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:55:26,941 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:55:26,950 - 

2018-10-28 00:55:26,950 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:55:28,121 - Epoch: [188][   50/  391]    Overall Loss 0.184154    Objective Loss 0.184154    Top1 93.718750    Top5 99.890625    LR 0.030000    Time 0.023378    
2018-10-28 00:55:29,233 - Epoch: [188][  100/  391]    Overall Loss 0.178238    Objective Loss 0.178238    Top1 93.695312    Top5 99.898438    LR 0.030000    Time 0.022795    
2018-10-28 00:55:30,347 - Epoch: [188][  150/  391]    Overall Loss 0.182138    Objective Loss 0.182138    Top1 93.463542    Top5 99.901042    LR 0.030000    Time 0.022612    
2018-10-28 00:55:31,458 - Epoch: [188][  200/  391]    Overall Loss 0.184758    Objective Loss 0.184758    Top1 93.355469    Top5 99.890625    LR 0.030000    Time 0.022509    
2018-10-28 00:55:32,570 - Epoch: [188][  250/  391]    Overall Loss 0.185916    Objective Loss 0.185916    Top1 93.353125    Top5 99.893750    LR 0.030000    Time 0.022449    
2018-10-28 00:55:33,682 - Epoch: [188][  300/  391]    Overall Loss 0.185852    Objective Loss 0.185852    Top1 93.354167    Top5 99.906250    LR 0.030000    Time 0.022411    
2018-10-28 00:55:34,793 - Epoch: [188][  350/  391]    Overall Loss 0.185754    Objective Loss 0.185754    Top1 93.330357    Top5 99.917411    LR 0.030000    Time 0.022380    
2018-10-28 00:55:35,784 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37885 | -0.00018 |    0.19119 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12007 | -0.00284 |    0.03800 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11626 | -0.00221 |    0.04157 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11999 | -0.00812 |    0.05118 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08690 | -0.00323 |    0.02506 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12737 | -0.00529 |    0.05005 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08506 |  0.00147 |    0.02325 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11873 | -0.00457 |    0.05644 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11072 | -0.00406 |    0.06308 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14622 | -0.00378 |    0.07123 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09018 | -0.00334 |    0.03589 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07366 |  0.00156 |    0.02641 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09478 | -0.00526 |    0.04136 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07700 | -0.00052 |    0.03443 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07636 | -0.00226 |    0.02933 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08227 | -0.00295 |    0.04084 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08020 | -0.00202 |    0.02888 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07727 | -0.00291 |    0.03160 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06522 | -0.00038 |    0.02934 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05068 | -0.00098 |    0.01618 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03377 |  0.00026 |    0.00804 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55071 | -0.03073 |    0.32341 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:55:35,784 - Total sparsity: 74.94

2018-10-28 00:55:35,784 - --- validate (epoch=188)-----------
2018-10-28 00:55:35,784 - 10000 samples (128 per mini-batch)
2018-10-28 00:55:36,508 - Epoch: [188][   50/   78]    Loss 0.391643    Top1 88.265625    Top5 99.421875    
2018-10-28 00:55:36,905 - ==> Top1: 88.220    Top5: 99.490    Loss: 0.380

2018-10-28 00:55:36,906 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:55:36,906 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:55:36,920 - 

2018-10-28 00:55:36,920 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:55:38,094 - Epoch: [189][   50/  391]    Overall Loss 0.173280    Objective Loss 0.173280    Top1 93.562500    Top5 99.953125    LR 0.030000    Time 0.023450    
2018-10-28 00:55:39,208 - Epoch: [189][  100/  391]    Overall Loss 0.176183    Objective Loss 0.176183    Top1 93.625000    Top5 99.929688    LR 0.030000    Time 0.022846    
2018-10-28 00:55:40,320 - Epoch: [189][  150/  391]    Overall Loss 0.178212    Objective Loss 0.178212    Top1 93.536458    Top5 99.927083    LR 0.030000    Time 0.022640    
2018-10-28 00:55:41,433 - Epoch: [189][  200/  391]    Overall Loss 0.181004    Objective Loss 0.181004    Top1 93.457031    Top5 99.910156    LR 0.030000    Time 0.022538    
2018-10-28 00:55:42,548 - Epoch: [189][  250/  391]    Overall Loss 0.182979    Objective Loss 0.182979    Top1 93.334375    Top5 99.912500    LR 0.030000    Time 0.022483    
2018-10-28 00:55:43,662 - Epoch: [189][  300/  391]    Overall Loss 0.185772    Objective Loss 0.185772    Top1 93.234375    Top5 99.898438    LR 0.030000    Time 0.022446    
2018-10-28 00:55:44,775 - Epoch: [189][  350/  391]    Overall Loss 0.185949    Objective Loss 0.185949    Top1 93.247768    Top5 99.901786    LR 0.030000    Time 0.022403    
2018-10-28 00:55:45,766 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37886 | -0.00151 |    0.19084 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12012 | -0.00297 |    0.03795 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11627 | -0.00195 |    0.04154 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12004 | -0.00787 |    0.05107 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08686 | -0.00335 |    0.02501 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12744 | -0.00550 |    0.04994 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08505 |  0.00167 |    0.02330 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11873 | -0.00464 |    0.05633 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11074 | -0.00420 |    0.06311 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14598 | -0.00443 |    0.07062 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09019 | -0.00333 |    0.03587 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07368 |  0.00142 |    0.02640 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09480 | -0.00547 |    0.04135 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07704 | -0.00068 |    0.03444 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07634 | -0.00232 |    0.02924 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08230 | -0.00298 |    0.04084 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08012 | -0.00188 |    0.02891 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07729 | -0.00297 |    0.03159 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06529 | -0.00045 |    0.02938 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05076 | -0.00099 |    0.01618 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03379 |  0.00028 |    0.00805 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.54989 | -0.03084 |    0.32291 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:55:45,767 - Total sparsity: 74.94

2018-10-28 00:55:45,767 - --- validate (epoch=189)-----------
2018-10-28 00:55:45,767 - 10000 samples (128 per mini-batch)
2018-10-28 00:55:46,486 - Epoch: [189][   50/   78]    Loss 0.414747    Top1 87.562500    Top5 99.515625    
2018-10-28 00:55:46,876 - ==> Top1: 87.550    Top5: 99.540    Loss: 0.416

2018-10-28 00:55:46,876 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:55:46,877 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:55:46,888 - 

2018-10-28 00:55:46,888 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:55:48,062 - Epoch: [190][   50/  391]    Overall Loss 0.177869    Objective Loss 0.177869    Top1 93.531250    Top5 99.906250    LR 0.030000    Time 0.023435    
2018-10-28 00:55:49,175 - Epoch: [190][  100/  391]    Overall Loss 0.174832    Objective Loss 0.174832    Top1 93.757812    Top5 99.929688    LR 0.030000    Time 0.022834    
2018-10-28 00:55:50,288 - Epoch: [190][  150/  391]    Overall Loss 0.176050    Objective Loss 0.176050    Top1 93.734375    Top5 99.921875    LR 0.030000    Time 0.022638    
2018-10-28 00:55:51,402 - Epoch: [190][  200/  391]    Overall Loss 0.179270    Objective Loss 0.179270    Top1 93.605469    Top5 99.921875    LR 0.030000    Time 0.022543    
2018-10-28 00:55:52,515 - Epoch: [190][  250/  391]    Overall Loss 0.180356    Objective Loss 0.180356    Top1 93.478125    Top5 99.925000    LR 0.030000    Time 0.022482    
2018-10-28 00:55:53,630 - Epoch: [190][  300/  391]    Overall Loss 0.181236    Objective Loss 0.181236    Top1 93.466146    Top5 99.919271    LR 0.030000    Time 0.022445    
2018-10-28 00:55:54,743 - Epoch: [190][  350/  391]    Overall Loss 0.183589    Objective Loss 0.183589    Top1 93.397321    Top5 99.910714    LR 0.030000    Time 0.022416    
2018-10-28 00:55:55,736 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37905 |  0.00066 |    0.19154 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12016 | -0.00321 |    0.03806 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11620 | -0.00179 |    0.04137 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12000 | -0.00813 |    0.05115 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08683 | -0.00349 |    0.02508 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12748 | -0.00536 |    0.04987 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08498 |  0.00194 |    0.02328 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11867 | -0.00447 |    0.05632 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11072 | -0.00407 |    0.06306 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14570 | -0.00423 |    0.07034 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09018 | -0.00330 |    0.03589 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07369 |  0.00158 |    0.02637 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09481 | -0.00538 |    0.04132 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07705 | -0.00070 |    0.03441 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07631 | -0.00227 |    0.02923 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08231 | -0.00311 |    0.04088 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.08001 | -0.00194 |    0.02876 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07730 | -0.00296 |    0.03158 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06536 | -0.00036 |    0.02939 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05083 | -0.00104 |    0.01621 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03380 |  0.00025 |    0.00805 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.54984 | -0.03085 |    0.32297 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:55:55,736 - Total sparsity: 74.94

2018-10-28 00:55:55,737 - --- validate (epoch=190)-----------
2018-10-28 00:55:55,737 - 10000 samples (128 per mini-batch)
2018-10-28 00:55:56,460 - Epoch: [190][   50/   78]    Loss 0.420734    Top1 87.453125    Top5 99.453125    
2018-10-28 00:55:56,847 - ==> Top1: 87.510    Top5: 99.510    Loss: 0.416

2018-10-28 00:55:56,848 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:55:56,848 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:55:56,858 - 

2018-10-28 00:55:56,859 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:55:58,029 - Epoch: [191][   50/  391]    Overall Loss 0.159521    Objective Loss 0.159521    Top1 94.203125    Top5 99.968750    LR 0.030000    Time 0.023369    
2018-10-28 00:55:59,140 - Epoch: [191][  100/  391]    Overall Loss 0.167961    Objective Loss 0.167961    Top1 93.812500    Top5 99.953125    LR 0.030000    Time 0.022779    
2018-10-28 00:56:00,253 - Epoch: [191][  150/  391]    Overall Loss 0.171022    Objective Loss 0.171022    Top1 93.744792    Top5 99.942708    LR 0.030000    Time 0.022600    
2018-10-28 00:56:01,366 - Epoch: [191][  200/  391]    Overall Loss 0.175615    Objective Loss 0.175615    Top1 93.558594    Top5 99.949219    LR 0.030000    Time 0.022507    
2018-10-28 00:56:02,477 - Epoch: [191][  250/  391]    Overall Loss 0.178287    Objective Loss 0.178287    Top1 93.515625    Top5 99.950000    LR 0.030000    Time 0.022445    
2018-10-28 00:56:03,589 - Epoch: [191][  300/  391]    Overall Loss 0.181080    Objective Loss 0.181080    Top1 93.458333    Top5 99.940104    LR 0.030000    Time 0.022407    
2018-10-28 00:56:04,699 - Epoch: [191][  350/  391]    Overall Loss 0.182518    Objective Loss 0.182518    Top1 93.386161    Top5 99.937500    LR 0.030000    Time 0.022373    
2018-10-28 00:56:05,693 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37863 |  0.00077 |    0.19120 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12022 | -0.00277 |    0.03797 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11616 | -0.00167 |    0.04151 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12001 | -0.00795 |    0.05102 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08679 | -0.00370 |    0.02512 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12756 | -0.00507 |    0.04996 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08492 |  0.00193 |    0.02321 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11860 | -0.00485 |    0.05641 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11071 | -0.00426 |    0.06307 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14555 | -0.00440 |    0.06990 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09018 | -0.00337 |    0.03585 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07370 |  0.00147 |    0.02641 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09481 | -0.00535 |    0.04131 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07706 | -0.00062 |    0.03440 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07630 | -0.00226 |    0.02924 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08232 | -0.00314 |    0.04090 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07992 | -0.00162 |    0.02860 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07731 | -0.00300 |    0.03157 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06541 | -0.00022 |    0.02937 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05089 | -0.00109 |    0.01625 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03380 |  0.00024 |    0.00805 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55033 | -0.03089 |    0.32322 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:56:05,693 - Total sparsity: 74.94

2018-10-28 00:56:05,693 - --- validate (epoch=191)-----------
2018-10-28 00:56:05,693 - 10000 samples (128 per mini-batch)
2018-10-28 00:56:06,413 - Epoch: [191][   50/   78]    Loss 0.396863    Top1 88.140625    Top5 99.468750    
2018-10-28 00:56:06,802 - ==> Top1: 88.020    Top5: 99.470    Loss: 0.399

2018-10-28 00:56:06,803 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:56:06,803 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:56:06,815 - 

2018-10-28 00:56:06,815 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:56:07,983 - Epoch: [192][   50/  391]    Overall Loss 0.176423    Objective Loss 0.176423    Top1 93.328125    Top5 99.859375    LR 0.030000    Time 0.023334    
2018-10-28 00:56:09,092 - Epoch: [192][  100/  391]    Overall Loss 0.180559    Objective Loss 0.180559    Top1 93.226562    Top5 99.906250    LR 0.030000    Time 0.022741    
2018-10-28 00:56:10,202 - Epoch: [192][  150/  391]    Overall Loss 0.181809    Objective Loss 0.181809    Top1 93.312500    Top5 99.911458    LR 0.030000    Time 0.022553    
2018-10-28 00:56:11,315 - Epoch: [192][  200/  391]    Overall Loss 0.186801    Objective Loss 0.186801    Top1 93.128906    Top5 99.910156    LR 0.030000    Time 0.022473    
2018-10-28 00:56:12,426 - Epoch: [192][  250/  391]    Overall Loss 0.184796    Objective Loss 0.184796    Top1 93.293750    Top5 99.912500    LR 0.030000    Time 0.022419    
2018-10-28 00:56:13,539 - Epoch: [192][  300/  391]    Overall Loss 0.183054    Objective Loss 0.183054    Top1 93.359375    Top5 99.916667    LR 0.030000    Time 0.022387    
2018-10-28 00:56:14,651 - Epoch: [192][  350/  391]    Overall Loss 0.183307    Objective Loss 0.183307    Top1 93.354911    Top5 99.910714    LR 0.030000    Time 0.022362    
2018-10-28 00:56:15,646 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37827 |  0.00048 |    0.19114 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12017 | -0.00322 |    0.03782 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11607 | -0.00193 |    0.04131 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11997 | -0.00866 |    0.05108 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08680 | -0.00337 |    0.02512 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12759 | -0.00494 |    0.04999 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08491 |  0.00181 |    0.02321 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11862 | -0.00433 |    0.05631 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11070 | -0.00442 |    0.06310 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14535 | -0.00412 |    0.06990 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09019 | -0.00336 |    0.03590 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07372 |  0.00145 |    0.02642 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09482 | -0.00546 |    0.04137 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07708 | -0.00054 |    0.03445 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07628 | -0.00231 |    0.02931 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08232 | -0.00311 |    0.04088 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07985 | -0.00165 |    0.02853 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07732 | -0.00294 |    0.03153 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06548 | -0.00037 |    0.02940 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05096 | -0.00105 |    0.01627 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03380 |  0.00024 |    0.00806 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.54976 | -0.03085 |    0.32294 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:56:15,646 - Total sparsity: 74.94

2018-10-28 00:56:15,646 - --- validate (epoch=192)-----------
2018-10-28 00:56:15,646 - 10000 samples (128 per mini-batch)
2018-10-28 00:56:16,367 - Epoch: [192][   50/   78]    Loss 0.384681    Top1 88.562500    Top5 99.656250    
2018-10-28 00:56:16,753 - ==> Top1: 88.590    Top5: 99.650    Loss: 0.383

2018-10-28 00:56:16,754 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:56:16,754 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:56:16,766 - 

2018-10-28 00:56:16,767 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:56:17,939 - Epoch: [193][   50/  391]    Overall Loss 0.170234    Objective Loss 0.170234    Top1 93.984375    Top5 99.953125    LR 0.030000    Time 0.023408    
2018-10-28 00:56:19,049 - Epoch: [193][  100/  391]    Overall Loss 0.175269    Objective Loss 0.175269    Top1 93.734375    Top5 99.945312    LR 0.030000    Time 0.022792    
2018-10-28 00:56:20,160 - Epoch: [193][  150/  391]    Overall Loss 0.180109    Objective Loss 0.180109    Top1 93.505208    Top5 99.937500    LR 0.030000    Time 0.022593    
2018-10-28 00:56:21,273 - Epoch: [193][  200/  391]    Overall Loss 0.179286    Objective Loss 0.179286    Top1 93.566406    Top5 99.933594    LR 0.030000    Time 0.022500    
2018-10-28 00:56:22,385 - Epoch: [193][  250/  391]    Overall Loss 0.181888    Objective Loss 0.181888    Top1 93.484375    Top5 99.931250    LR 0.030000    Time 0.022444    
2018-10-28 00:56:23,496 - Epoch: [193][  300/  391]    Overall Loss 0.184440    Objective Loss 0.184440    Top1 93.447917    Top5 99.927083    LR 0.030000    Time 0.022402    
2018-10-28 00:56:24,607 - Epoch: [193][  350/  391]    Overall Loss 0.186442    Objective Loss 0.186442    Top1 93.397321    Top5 99.921875    LR 0.030000    Time 0.022374    
2018-10-28 00:56:25,604 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37792 | -0.00116 |    0.19133 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12016 | -0.00321 |    0.03798 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11603 | -0.00186 |    0.04118 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12004 | -0.00829 |    0.05096 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08680 | -0.00341 |    0.02516 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12755 | -0.00501 |    0.05007 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08488 |  0.00165 |    0.02308 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11861 | -0.00451 |    0.05637 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11073 | -0.00447 |    0.06320 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14519 | -0.00458 |    0.07055 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09021 | -0.00351 |    0.03587 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07375 |  0.00153 |    0.02638 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09488 | -0.00524 |    0.04136 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07712 | -0.00055 |    0.03447 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07629 | -0.00238 |    0.02933 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08236 | -0.00303 |    0.04090 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07978 | -0.00181 |    0.02860 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07734 | -0.00306 |    0.03156 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06555 | -0.00033 |    0.02944 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05103 | -0.00106 |    0.01630 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03383 |  0.00025 |    0.00806 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.54901 | -0.03080 |    0.32251 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:56:25,604 - Total sparsity: 74.94

2018-10-28 00:56:25,605 - --- validate (epoch=193)-----------
2018-10-28 00:56:25,605 - 10000 samples (128 per mini-batch)
2018-10-28 00:56:26,329 - Epoch: [193][   50/   78]    Loss 0.390657    Top1 88.218750    Top5 99.531250    
2018-10-28 00:56:26,721 - ==> Top1: 88.030    Top5: 99.580    Loss: 0.394

2018-10-28 00:56:26,721 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:56:26,721 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:56:26,731 - 

2018-10-28 00:56:26,732 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:56:27,902 - Epoch: [194][   50/  391]    Overall Loss 0.161039    Objective Loss 0.161039    Top1 94.265625    Top5 99.953125    LR 0.030000    Time 0.023376    
2018-10-28 00:56:29,014 - Epoch: [194][  100/  391]    Overall Loss 0.169991    Objective Loss 0.169991    Top1 94.117188    Top5 99.953125    LR 0.030000    Time 0.022791    
2018-10-28 00:56:30,126 - Epoch: [194][  150/  391]    Overall Loss 0.174306    Objective Loss 0.174306    Top1 93.885417    Top5 99.932292    LR 0.030000    Time 0.022602    
2018-10-28 00:56:31,237 - Epoch: [194][  200/  391]    Overall Loss 0.177538    Objective Loss 0.177538    Top1 93.808594    Top5 99.917969    LR 0.030000    Time 0.022500    
2018-10-28 00:56:32,349 - Epoch: [194][  250/  391]    Overall Loss 0.179145    Objective Loss 0.179145    Top1 93.696875    Top5 99.918750    LR 0.030000    Time 0.022443    
2018-10-28 00:56:33,462 - Epoch: [194][  300/  391]    Overall Loss 0.181417    Objective Loss 0.181417    Top1 93.531250    Top5 99.916667    LR 0.030000    Time 0.022406    
2018-10-28 00:56:34,573 - Epoch: [194][  350/  391]    Overall Loss 0.183161    Objective Loss 0.183161    Top1 93.486607    Top5 99.919643    LR 0.030000    Time 0.022365    
2018-10-28 00:56:35,562 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37811 | -0.00084 |    0.19138 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12012 | -0.00332 |    0.03804 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11601 | -0.00155 |    0.04117 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12001 | -0.00918 |    0.05116 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08679 | -0.00334 |    0.02509 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12753 | -0.00453 |    0.04995 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08481 |  0.00170 |    0.02324 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11853 | -0.00450 |    0.05636 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11067 | -0.00464 |    0.06322 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14494 | -0.00435 |    0.07024 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09020 | -0.00342 |    0.03588 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07375 |  0.00153 |    0.02640 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09487 | -0.00504 |    0.04136 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07712 | -0.00053 |    0.03445 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07627 | -0.00222 |    0.02932 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08235 | -0.00320 |    0.04093 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07970 | -0.00161 |    0.02865 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07733 | -0.00297 |    0.03157 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06560 | -0.00038 |    0.02946 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05110 | -0.00102 |    0.01632 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03385 |  0.00028 |    0.00807 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.54927 | -0.03108 |    0.32272 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:56:35,563 - Total sparsity: 74.94

2018-10-28 00:56:35,563 - --- validate (epoch=194)-----------
2018-10-28 00:56:35,563 - 10000 samples (128 per mini-batch)
2018-10-28 00:56:36,279 - Epoch: [194][   50/   78]    Loss 0.385835    Top1 88.078125    Top5 99.578125    
2018-10-28 00:56:36,669 - ==> Top1: 88.270    Top5: 99.630    Loss: 0.376

2018-10-28 00:56:36,670 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:56:36,670 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:56:36,679 - 

2018-10-28 00:56:36,680 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:56:37,851 - Epoch: [195][   50/  391]    Overall Loss 0.180556    Objective Loss 0.180556    Top1 93.453125    Top5 99.921875    LR 0.030000    Time 0.023391    
2018-10-28 00:56:38,963 - Epoch: [195][  100/  391]    Overall Loss 0.176138    Objective Loss 0.176138    Top1 93.679688    Top5 99.953125    LR 0.030000    Time 0.022799    
2018-10-28 00:56:40,075 - Epoch: [195][  150/  391]    Overall Loss 0.174133    Objective Loss 0.174133    Top1 93.755208    Top5 99.937500    LR 0.030000    Time 0.022602    
2018-10-28 00:56:41,185 - Epoch: [195][  200/  391]    Overall Loss 0.177992    Objective Loss 0.177992    Top1 93.664062    Top5 99.925781    LR 0.030000    Time 0.022496    
2018-10-28 00:56:42,297 - Epoch: [195][  250/  391]    Overall Loss 0.181268    Objective Loss 0.181268    Top1 93.575000    Top5 99.931250    LR 0.030000    Time 0.022439    
2018-10-28 00:56:43,408 - Epoch: [195][  300/  391]    Overall Loss 0.184195    Objective Loss 0.184195    Top1 93.463542    Top5 99.937500    LR 0.030000    Time 0.022400    
2018-10-28 00:56:44,520 - Epoch: [195][  350/  391]    Overall Loss 0.184210    Objective Loss 0.184210    Top1 93.450893    Top5 99.946429    LR 0.030000    Time 0.022371    
2018-10-28 00:56:45,517 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37787 |  0.00164 |    0.19106 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12013 | -0.00328 |    0.03802 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11602 | -0.00161 |    0.04120 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12012 | -0.00872 |    0.05124 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08676 | -0.00357 |    0.02511 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12749 | -0.00476 |    0.04986 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08476 |  0.00154 |    0.02320 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11851 | -0.00384 |    0.05619 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11069 | -0.00458 |    0.06313 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14469 | -0.00452 |    0.07010 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09020 | -0.00340 |    0.03585 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07375 |  0.00171 |    0.02637 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09487 | -0.00538 |    0.04135 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07713 | -0.00028 |    0.03458 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07625 | -0.00234 |    0.02934 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08238 | -0.00310 |    0.04095 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07964 | -0.00179 |    0.02874 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07735 | -0.00292 |    0.03153 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06566 | -0.00035 |    0.02952 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05117 | -0.00107 |    0.01633 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03386 |  0.00029 |    0.00808 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.54912 | -0.03112 |    0.32274 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:56:45,517 - Total sparsity: 74.94

2018-10-28 00:56:45,517 - --- validate (epoch=195)-----------
2018-10-28 00:56:45,517 - 10000 samples (128 per mini-batch)
2018-10-28 00:56:46,230 - Epoch: [195][   50/   78]    Loss 0.442639    Top1 87.328125    Top5 99.234375    
2018-10-28 00:56:46,615 - ==> Top1: 87.100    Top5: 99.360    Loss: 0.452

2018-10-28 00:56:46,615 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:56:46,616 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:56:46,631 - 

2018-10-28 00:56:46,631 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:56:47,801 - Epoch: [196][   50/  391]    Overall Loss 0.173654    Objective Loss 0.173654    Top1 94.015625    Top5 99.953125    LR 0.030000    Time 0.023358    
2018-10-28 00:56:48,911 - Epoch: [196][  100/  391]    Overall Loss 0.174559    Objective Loss 0.174559    Top1 93.945312    Top5 99.953125    LR 0.030000    Time 0.022763    
2018-10-28 00:56:50,022 - Epoch: [196][  150/  391]    Overall Loss 0.179925    Objective Loss 0.179925    Top1 93.692708    Top5 99.947917    LR 0.030000    Time 0.022572    
2018-10-28 00:56:51,130 - Epoch: [196][  200/  391]    Overall Loss 0.180615    Objective Loss 0.180615    Top1 93.648438    Top5 99.941406    LR 0.030000    Time 0.022465    
2018-10-28 00:56:52,243 - Epoch: [196][  250/  391]    Overall Loss 0.181462    Objective Loss 0.181462    Top1 93.603125    Top5 99.934375    LR 0.030000    Time 0.022416    
2018-10-28 00:56:53,355 - Epoch: [196][  300/  391]    Overall Loss 0.181711    Objective Loss 0.181711    Top1 93.606771    Top5 99.929688    LR 0.030000    Time 0.022385    
2018-10-28 00:56:54,467 - Epoch: [196][  350/  391]    Overall Loss 0.181753    Objective Loss 0.181753    Top1 93.569196    Top5 99.933036    LR 0.030000    Time 0.022361    
2018-10-28 00:56:55,456 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37819 |  0.00039 |    0.19119 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12024 | -0.00305 |    0.03811 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11602 | -0.00210 |    0.04119 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12010 | -0.00878 |    0.05111 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08675 | -0.00365 |    0.02516 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12745 | -0.00483 |    0.04995 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08473 |  0.00149 |    0.02300 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11846 | -0.00430 |    0.05619 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11069 | -0.00435 |    0.06303 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14458 | -0.00386 |    0.06958 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09023 | -0.00348 |    0.03586 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07376 |  0.00159 |    0.02640 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09491 | -0.00519 |    0.04138 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07715 | -0.00053 |    0.03456 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07624 | -0.00230 |    0.02935 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08240 | -0.00307 |    0.04096 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07955 | -0.00201 |    0.02864 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07737 | -0.00301 |    0.03154 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06572 | -0.00041 |    0.02957 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05125 | -0.00100 |    0.01632 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03388 |  0.00029 |    0.00807 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.54939 | -0.03098 |    0.32282 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:56:55,457 - Total sparsity: 74.94

2018-10-28 00:56:55,457 - --- validate (epoch=196)-----------
2018-10-28 00:56:55,457 - 10000 samples (128 per mini-batch)
2018-10-28 00:56:56,176 - Epoch: [196][   50/   78]    Loss 0.420204    Top1 87.234375    Top5 99.453125    
2018-10-28 00:56:56,565 - ==> Top1: 87.240    Top5: 99.490    Loss: 0.423

2018-10-28 00:56:56,565 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:56:56,565 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:56:56,576 - 

2018-10-28 00:56:56,576 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:56:57,747 - Epoch: [197][   50/  391]    Overall Loss 0.160385    Objective Loss 0.160385    Top1 94.453125    Top5 99.906250    LR 0.030000    Time 0.023382    
2018-10-28 00:56:58,861 - Epoch: [197][  100/  391]    Overall Loss 0.162994    Objective Loss 0.162994    Top1 94.320312    Top5 99.921875    LR 0.030000    Time 0.022817    
2018-10-28 00:56:59,974 - Epoch: [197][  150/  391]    Overall Loss 0.170764    Objective Loss 0.170764    Top1 93.947917    Top5 99.911458    LR 0.030000    Time 0.022624    
2018-10-28 00:57:01,090 - Epoch: [197][  200/  391]    Overall Loss 0.173720    Objective Loss 0.173720    Top1 93.882812    Top5 99.910156    LR 0.030000    Time 0.022540    
2018-10-28 00:57:02,203 - Epoch: [197][  250/  391]    Overall Loss 0.177331    Objective Loss 0.177331    Top1 93.709375    Top5 99.900000    LR 0.030000    Time 0.022477    
2018-10-28 00:57:03,317 - Epoch: [197][  300/  391]    Overall Loss 0.177872    Objective Loss 0.177872    Top1 93.671875    Top5 99.903646    LR 0.030000    Time 0.022440    
2018-10-28 00:57:04,429 - Epoch: [197][  350/  391]    Overall Loss 0.178992    Objective Loss 0.178992    Top1 93.622768    Top5 99.906250    LR 0.030000    Time 0.022410    
2018-10-28 00:57:05,423 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37797 | -0.00014 |    0.19078 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12016 | -0.00382 |    0.03810 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11595 | -0.00208 |    0.04120 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.12000 | -0.00864 |    0.05100 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08665 | -0.00352 |    0.02509 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12745 | -0.00494 |    0.04975 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08464 |  0.00153 |    0.02317 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11839 | -0.00420 |    0.05629 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11066 | -0.00418 |    0.06304 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14443 | -0.00371 |    0.06960 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09024 | -0.00362 |    0.03589 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07375 |  0.00144 |    0.02638 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09494 | -0.00525 |    0.04142 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07713 | -0.00067 |    0.03458 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07620 | -0.00237 |    0.02928 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08241 | -0.00303 |    0.04096 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07946 | -0.00213 |    0.02853 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07738 | -0.00302 |    0.03160 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06577 | -0.00044 |    0.02961 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05131 | -0.00096 |    0.01635 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03390 |  0.00029 |    0.00809 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.54963 | -0.03099 |    0.32313 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:57:05,423 - Total sparsity: 74.94

2018-10-28 00:57:05,423 - --- validate (epoch=197)-----------
2018-10-28 00:57:05,423 - 10000 samples (128 per mini-batch)
2018-10-28 00:57:06,143 - Epoch: [197][   50/   78]    Loss 0.456354    Top1 86.859375    Top5 99.515625    
2018-10-28 00:57:06,532 - ==> Top1: 87.050    Top5: 99.530    Loss: 0.448

2018-10-28 00:57:06,533 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:57:06,533 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:57:06,543 - 

2018-10-28 00:57:06,543 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:57:07,715 - Epoch: [198][   50/  391]    Overall Loss 0.185409    Objective Loss 0.185409    Top1 93.406250    Top5 99.906250    LR 0.030000    Time 0.023400    
2018-10-28 00:57:08,825 - Epoch: [198][  100/  391]    Overall Loss 0.184296    Objective Loss 0.184296    Top1 93.414062    Top5 99.937500    LR 0.030000    Time 0.022792    
2018-10-28 00:57:09,937 - Epoch: [198][  150/  391]    Overall Loss 0.184447    Objective Loss 0.184447    Top1 93.302083    Top5 99.937500    LR 0.030000    Time 0.022600    
2018-10-28 00:57:11,049 - Epoch: [198][  200/  391]    Overall Loss 0.179497    Objective Loss 0.179497    Top1 93.554688    Top5 99.937500    LR 0.030000    Time 0.022502    
2018-10-28 00:57:12,161 - Epoch: [198][  250/  391]    Overall Loss 0.182630    Objective Loss 0.182630    Top1 93.471875    Top5 99.946875    LR 0.030000    Time 0.022447    
2018-10-28 00:57:13,275 - Epoch: [198][  300/  391]    Overall Loss 0.183450    Objective Loss 0.183450    Top1 93.507812    Top5 99.921875    LR 0.030000    Time 0.022412    
2018-10-28 00:57:14,387 - Epoch: [198][  350/  391]    Overall Loss 0.184794    Objective Loss 0.184794    Top1 93.404018    Top5 99.921875    LR 0.030000    Time 0.022385    
2018-10-28 00:57:15,377 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37805 |  0.00172 |    0.19119 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12019 | -0.00361 |    0.03808 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11598 | -0.00187 |    0.04148 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11994 | -0.00886 |    0.05127 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08663 | -0.00362 |    0.02509 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12753 | -0.00473 |    0.04974 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08462 |  0.00171 |    0.02322 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11841 | -0.00418 |    0.05632 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11070 | -0.00436 |    0.06298 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14431 | -0.00296 |    0.06940 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09031 | -0.00365 |    0.03592 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07377 |  0.00145 |    0.02638 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09499 | -0.00520 |    0.04140 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07712 | -0.00080 |    0.03449 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07620 | -0.00237 |    0.02930 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08245 | -0.00289 |    0.04098 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07943 | -0.00233 |    0.02852 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07741 | -0.00295 |    0.03159 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06584 | -0.00042 |    0.02960 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05140 | -0.00107 |    0.01640 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03392 |  0.00030 |    0.00810 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.54837 | -0.03099 |    0.32217 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:57:15,377 - Total sparsity: 74.94

2018-10-28 00:57:15,377 - --- validate (epoch=198)-----------
2018-10-28 00:57:15,378 - 10000 samples (128 per mini-batch)
2018-10-28 00:57:16,094 - Epoch: [198][   50/   78]    Loss 0.427162    Top1 87.703125    Top5 99.531250    
2018-10-28 00:57:16,483 - ==> Top1: 87.570    Top5: 99.570    Loss: 0.415

2018-10-28 00:57:16,484 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:57:16,484 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:57:16,498 - 

2018-10-28 00:57:16,498 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:57:17,674 - Epoch: [199][   50/  391]    Overall Loss 0.181917    Objective Loss 0.181917    Top1 93.500000    Top5 99.859375    LR 0.030000    Time 0.023474    
2018-10-28 00:57:18,785 - Epoch: [199][  100/  391]    Overall Loss 0.171679    Objective Loss 0.171679    Top1 93.984375    Top5 99.914062    LR 0.030000    Time 0.022835    
2018-10-28 00:57:19,898 - Epoch: [199][  150/  391]    Overall Loss 0.171105    Objective Loss 0.171105    Top1 94.010417    Top5 99.921875    LR 0.030000    Time 0.022637    
2018-10-28 00:57:21,010 - Epoch: [199][  200/  391]    Overall Loss 0.170731    Objective Loss 0.170731    Top1 94.015625    Top5 99.933594    LR 0.030000    Time 0.022530    
2018-10-28 00:57:22,124 - Epoch: [199][  250/  391]    Overall Loss 0.176801    Objective Loss 0.176801    Top1 93.806250    Top5 99.931250    LR 0.030000    Time 0.022472    
2018-10-28 00:57:23,235 - Epoch: [199][  300/  391]    Overall Loss 0.182046    Objective Loss 0.182046    Top1 93.578125    Top5 99.921875    LR 0.030000    Time 0.022428    
2018-10-28 00:57:24,349 - Epoch: [199][  350/  391]    Overall Loss 0.184155    Objective Loss 0.184155    Top1 93.517857    Top5 99.924107    LR 0.030000    Time 0.022401    
2018-10-28 00:57:25,340 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37816 |  0.00102 |    0.19098 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12022 | -0.00329 |    0.03808 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11600 | -0.00196 |    0.04133 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11998 | -0.00891 |    0.05105 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08663 | -0.00382 |    0.02522 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12759 | -0.00524 |    0.04997 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08459 |  0.00176 |    0.02328 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11842 | -0.00420 |    0.05640 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11074 | -0.00432 |    0.06310 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14414 | -0.00284 |    0.06938 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09034 | -0.00355 |    0.03593 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07378 |  0.00133 |    0.02641 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09499 | -0.00523 |    0.04141 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07710 | -0.00086 |    0.03443 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07621 | -0.00218 |    0.02930 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08248 | -0.00298 |    0.04094 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07939 | -0.00232 |    0.02845 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07744 | -0.00300 |    0.03162 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06592 | -0.00040 |    0.02968 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05149 | -0.00102 |    0.01642 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03395 |  0.00031 |    0.00810 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.54810 | -0.03085 |    0.32196 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:57:25,340 - Total sparsity: 74.94

2018-10-28 00:57:25,341 - --- validate (epoch=199)-----------
2018-10-28 00:57:25,341 - 10000 samples (128 per mini-batch)
2018-10-28 00:57:26,073 - Epoch: [199][   50/   78]    Loss 0.415591    Top1 88.234375    Top5 99.453125    
2018-10-28 00:57:26,468 - ==> Top1: 88.020    Top5: 99.540    Loss: 0.409

2018-10-28 00:57:26,468 - ==> Best Top1: 89.190   On Epoch: 110

2018-10-28 00:57:26,469 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:57:26,486 - 

2018-10-28 00:57:26,487 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:57:27,631 - Epoch: [200][   50/  391]    Overall Loss 0.161728    Objective Loss 0.161728    Top1 94.296875    Top5 99.953125    LR 0.003000    Time 0.022855    
2018-10-28 00:57:28,743 - Epoch: [200][  100/  391]    Overall Loss 0.154429    Objective Loss 0.154429    Top1 94.562500    Top5 99.960938    LR 0.003000    Time 0.022533    
2018-10-28 00:57:29,854 - Epoch: [200][  150/  391]    Overall Loss 0.152464    Objective Loss 0.152464    Top1 94.671875    Top5 99.947917    LR 0.003000    Time 0.022424    
2018-10-28 00:57:30,964 - Epoch: [200][  200/  391]    Overall Loss 0.148832    Objective Loss 0.148832    Top1 94.820312    Top5 99.953125    LR 0.003000    Time 0.022361    
2018-10-28 00:57:32,072 - Epoch: [200][  250/  391]    Overall Loss 0.148559    Objective Loss 0.148559    Top1 94.796875    Top5 99.953125    LR 0.003000    Time 0.022315    
2018-10-28 00:57:33,182 - Epoch: [200][  300/  391]    Overall Loss 0.145601    Objective Loss 0.145601    Top1 94.947917    Top5 99.955729    LR 0.003000    Time 0.022290    
2018-10-28 00:57:34,294 - Epoch: [200][  350/  391]    Overall Loss 0.143217    Objective Loss 0.143217    Top1 95.062500    Top5 99.957589    LR 0.003000    Time 0.022279    
2018-10-28 00:57:35,281 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37775 |  0.00135 |    0.19053 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.12009 | -0.00331 |    0.03806 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11587 | -0.00201 |    0.04126 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11985 | -0.00898 |    0.05100 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08654 | -0.00379 |    0.02519 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12745 | -0.00526 |    0.04989 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08451 |  0.00173 |    0.02327 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11829 | -0.00418 |    0.05633 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11062 | -0.00430 |    0.06304 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14399 | -0.00267 |    0.06932 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09024 | -0.00349 |    0.03587 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07370 |  0.00132 |    0.02638 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09489 | -0.00520 |    0.04136 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07702 | -0.00081 |    0.03440 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07613 | -0.00217 |    0.02927 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08239 | -0.00296 |    0.04090 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07930 | -0.00229 |    0.02843 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07736 | -0.00301 |    0.03159 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06585 | -0.00043 |    0.02965 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05143 | -0.00106 |    0.01640 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03391 |  0.00031 |    0.00809 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.54886 | -0.03090 |    0.32239 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:57:35,281 - Total sparsity: 74.94

2018-10-28 00:57:35,281 - --- validate (epoch=200)-----------
2018-10-28 00:57:35,281 - 10000 samples (128 per mini-batch)
2018-10-28 00:57:36,002 - Epoch: [200][   50/   78]    Loss 0.339649    Top1 90.234375    Top5 99.609375    
2018-10-28 00:57:36,393 - ==> Top1: 90.120    Top5: 99.680    Loss: 0.336

2018-10-28 00:57:36,394 - ==> Best Top1: 90.120   On Epoch: 200

2018-10-28 00:57:36,394 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:57:36,410 - 

2018-10-28 00:57:36,410 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:57:37,554 - Epoch: [201][   50/  391]    Overall Loss 0.123647    Objective Loss 0.123647    Top1 96.015625    Top5 99.984375    LR 0.003000    Time 0.022855    
2018-10-28 00:57:38,664 - Epoch: [201][  100/  391]    Overall Loss 0.120256    Objective Loss 0.120256    Top1 96.093750    Top5 99.976562    LR 0.003000    Time 0.022516    
2018-10-28 00:57:39,775 - Epoch: [201][  150/  391]    Overall Loss 0.124177    Objective Loss 0.124177    Top1 95.854167    Top5 99.979167    LR 0.003000    Time 0.022404    
2018-10-28 00:57:40,886 - Epoch: [201][  200/  391]    Overall Loss 0.126304    Objective Loss 0.126304    Top1 95.773438    Top5 99.972656    LR 0.003000    Time 0.022334    
2018-10-28 00:57:41,997 - Epoch: [201][  250/  391]    Overall Loss 0.125734    Objective Loss 0.125734    Top1 95.781250    Top5 99.971875    LR 0.003000    Time 0.022308    
2018-10-28 00:57:43,112 - Epoch: [201][  300/  391]    Overall Loss 0.124850    Objective Loss 0.124850    Top1 95.776042    Top5 99.976562    LR 0.003000    Time 0.022301    
2018-10-28 00:57:44,223 - Epoch: [201][  350/  391]    Overall Loss 0.124803    Objective Loss 0.124803    Top1 95.734375    Top5 99.975446    LR 0.003000    Time 0.022285    
2018-10-28 00:57:45,217 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37734 |  0.00115 |    0.19030 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11996 | -0.00331 |    0.03802 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11574 | -0.00203 |    0.04122 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11972 | -0.00893 |    0.05094 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08645 | -0.00376 |    0.02516 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12731 | -0.00524 |    0.04984 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08441 |  0.00170 |    0.02323 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11816 | -0.00419 |    0.05625 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11050 | -0.00431 |    0.06297 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14383 | -0.00268 |    0.06928 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09014 | -0.00348 |    0.03584 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07362 |  0.00133 |    0.02635 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09478 | -0.00516 |    0.04132 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07694 | -0.00080 |    0.03436 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07605 | -0.00218 |    0.02924 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08230 | -0.00294 |    0.04086 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07921 | -0.00227 |    0.02840 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07728 | -0.00301 |    0.03155 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06578 | -0.00043 |    0.02962 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05138 | -0.00108 |    0.01639 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03388 |  0.00031 |    0.00808 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.54977 | -0.03095 |    0.32288 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:57:45,217 - Total sparsity: 74.94

2018-10-28 00:57:45,217 - --- validate (epoch=201)-----------
2018-10-28 00:57:45,217 - 10000 samples (128 per mini-batch)
2018-10-28 00:57:45,937 - Epoch: [201][   50/   78]    Loss 0.333936    Top1 90.296875    Top5 99.656250    
2018-10-28 00:57:46,328 - ==> Top1: 90.160    Top5: 99.700    Loss: 0.331

2018-10-28 00:57:46,329 - ==> Best Top1: 90.160   On Epoch: 201

2018-10-28 00:57:46,329 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:57:46,350 - 

2018-10-28 00:57:46,350 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:57:47,495 - Epoch: [202][   50/  391]    Overall Loss 0.129111    Objective Loss 0.129111    Top1 95.437500    Top5 99.937500    LR 0.003000    Time 0.022859    
2018-10-28 00:57:48,606 - Epoch: [202][  100/  391]    Overall Loss 0.122178    Objective Loss 0.122178    Top1 95.781250    Top5 99.953125    LR 0.003000    Time 0.022530    
2018-10-28 00:57:49,716 - Epoch: [202][  150/  391]    Overall Loss 0.122062    Objective Loss 0.122062    Top1 95.796875    Top5 99.958333    LR 0.003000    Time 0.022412    
2018-10-28 00:57:50,827 - Epoch: [202][  200/  391]    Overall Loss 0.122056    Objective Loss 0.122056    Top1 95.828125    Top5 99.964844    LR 0.003000    Time 0.022356    
2018-10-28 00:57:51,938 - Epoch: [202][  250/  391]    Overall Loss 0.121673    Objective Loss 0.121673    Top1 95.878125    Top5 99.965625    LR 0.003000    Time 0.022324    
2018-10-28 00:57:53,050 - Epoch: [202][  300/  391]    Overall Loss 0.121545    Objective Loss 0.121545    Top1 95.843750    Top5 99.963542    LR 0.003000    Time 0.022306    
2018-10-28 00:57:54,160 - Epoch: [202][  350/  391]    Overall Loss 0.121273    Objective Loss 0.121273    Top1 95.848214    Top5 99.964286    LR 0.003000    Time 0.022288    
2018-10-28 00:57:55,149 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37693 |  0.00084 |    0.19010 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11983 | -0.00324 |    0.03798 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11562 | -0.00199 |    0.04116 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11959 | -0.00885 |    0.05090 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08636 | -0.00373 |    0.02511 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12717 | -0.00523 |    0.04974 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08432 |  0.00168 |    0.02319 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11803 | -0.00421 |    0.05617 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11038 | -0.00428 |    0.06290 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14367 | -0.00263 |    0.06923 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.09005 | -0.00347 |    0.03581 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07354 |  0.00134 |    0.02630 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09468 | -0.00518 |    0.04127 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07685 | -0.00079 |    0.03432 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07596 | -0.00219 |    0.02920 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08221 | -0.00296 |    0.04082 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07913 | -0.00224 |    0.02838 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07719 | -0.00300 |    0.03152 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06571 | -0.00043 |    0.02958 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05132 | -0.00109 |    0.01637 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03384 |  0.00031 |    0.00807 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55064 | -0.03096 |    0.32339 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:57:55,149 - Total sparsity: 74.94

2018-10-28 00:57:55,149 - --- validate (epoch=202)-----------
2018-10-28 00:57:55,150 - 10000 samples (128 per mini-batch)
2018-10-28 00:57:55,873 - Epoch: [202][   50/   78]    Loss 0.339436    Top1 90.171875    Top5 99.703125    
2018-10-28 00:57:56,265 - ==> Top1: 89.950    Top5: 99.730    Loss: 0.335

2018-10-28 00:57:56,266 - ==> Best Top1: 90.160   On Epoch: 201

2018-10-28 00:57:56,266 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:57:56,276 - 

2018-10-28 00:57:56,276 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:57:57,449 - Epoch: [203][   50/  391]    Overall Loss 0.120036    Objective Loss 0.120036    Top1 95.906250    Top5 100.000000    LR 0.003000    Time 0.023431    
2018-10-28 00:57:58,561 - Epoch: [203][  100/  391]    Overall Loss 0.118918    Objective Loss 0.118918    Top1 95.914062    Top5 99.984375    LR 0.003000    Time 0.022825    
2018-10-28 00:57:59,672 - Epoch: [203][  150/  391]    Overall Loss 0.117127    Objective Loss 0.117127    Top1 96.005208    Top5 99.984375    LR 0.003000    Time 0.022612    
2018-10-28 00:58:00,787 - Epoch: [203][  200/  391]    Overall Loss 0.117281    Objective Loss 0.117281    Top1 95.917969    Top5 99.980469    LR 0.003000    Time 0.022527    
2018-10-28 00:58:01,900 - Epoch: [203][  250/  391]    Overall Loss 0.117690    Objective Loss 0.117690    Top1 95.940625    Top5 99.978125    LR 0.003000    Time 0.022469    
2018-10-28 00:58:03,012 - Epoch: [203][  300/  391]    Overall Loss 0.117909    Objective Loss 0.117909    Top1 95.989583    Top5 99.976562    LR 0.003000    Time 0.022416    
2018-10-28 00:58:04,124 - Epoch: [203][  350/  391]    Overall Loss 0.117900    Objective Loss 0.117900    Top1 96.011161    Top5 99.970982    LR 0.003000    Time 0.022385    
2018-10-28 00:58:05,118 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37651 |  0.00081 |    0.19005 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11970 | -0.00321 |    0.03795 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11549 | -0.00205 |    0.04110 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11946 | -0.00886 |    0.05084 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08626 | -0.00372 |    0.02507 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12704 | -0.00511 |    0.04968 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08423 |  0.00172 |    0.02315 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11791 | -0.00418 |    0.05609 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11026 | -0.00427 |    0.06282 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14351 | -0.00269 |    0.06913 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08995 | -0.00344 |    0.03577 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07346 |  0.00134 |    0.02627 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09458 | -0.00516 |    0.04122 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07677 | -0.00080 |    0.03429 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07588 | -0.00219 |    0.02917 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08212 | -0.00294 |    0.04077 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07904 | -0.00225 |    0.02835 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07711 | -0.00300 |    0.03149 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06564 | -0.00044 |    0.02955 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05126 | -0.00111 |    0.01635 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03380 |  0.00031 |    0.00806 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55145 | -0.03100 |    0.32383 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:58:05,119 - Total sparsity: 74.94

2018-10-28 00:58:05,119 - --- validate (epoch=203)-----------
2018-10-28 00:58:05,119 - 10000 samples (128 per mini-batch)
2018-10-28 00:58:05,843 - Epoch: [203][   50/   78]    Loss 0.336288    Top1 90.468750    Top5 99.656250    
2018-10-28 00:58:06,237 - ==> Top1: 90.300    Top5: 99.700    Loss: 0.335

2018-10-28 00:58:06,237 - ==> Best Top1: 90.300   On Epoch: 203

2018-10-28 00:58:06,238 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:58:06,253 - 

2018-10-28 00:58:06,254 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:58:07,397 - Epoch: [204][   50/  391]    Overall Loss 0.107002    Objective Loss 0.107002    Top1 96.312500    Top5 99.906250    LR 0.003000    Time 0.022842    
2018-10-28 00:58:08,510 - Epoch: [204][  100/  391]    Overall Loss 0.110186    Objective Loss 0.110186    Top1 96.140625    Top5 99.945312    LR 0.003000    Time 0.022534    
2018-10-28 00:58:09,622 - Epoch: [204][  150/  391]    Overall Loss 0.113451    Objective Loss 0.113451    Top1 96.119792    Top5 99.958333    LR 0.003000    Time 0.022430    
2018-10-28 00:58:10,736 - Epoch: [204][  200/  391]    Overall Loss 0.113685    Objective Loss 0.113685    Top1 96.160156    Top5 99.960938    LR 0.003000    Time 0.022386    
2018-10-28 00:58:11,849 - Epoch: [204][  250/  391]    Overall Loss 0.114122    Objective Loss 0.114122    Top1 96.090625    Top5 99.968750    LR 0.003000    Time 0.022352    
2018-10-28 00:58:12,960 - Epoch: [204][  300/  391]    Overall Loss 0.113945    Objective Loss 0.113945    Top1 96.104167    Top5 99.966146    LR 0.003000    Time 0.022326    
2018-10-28 00:58:14,071 - Epoch: [204][  350/  391]    Overall Loss 0.113764    Objective Loss 0.113764    Top1 96.116071    Top5 99.968750    LR 0.003000    Time 0.022308    
2018-10-28 00:58:15,063 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37610 |  0.00097 |    0.18974 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11956 | -0.00319 |    0.03791 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11536 | -0.00198 |    0.04104 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11933 | -0.00882 |    0.05074 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08617 | -0.00367 |    0.02505 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12690 | -0.00504 |    0.04962 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08414 |  0.00174 |    0.02312 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11778 | -0.00419 |    0.05602 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11014 | -0.00425 |    0.06276 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14335 | -0.00267 |    0.06909 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08985 | -0.00343 |    0.03573 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07338 |  0.00136 |    0.02624 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09447 | -0.00516 |    0.04117 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07669 | -0.00077 |    0.03424 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07580 | -0.00219 |    0.02914 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08203 | -0.00295 |    0.04073 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07895 | -0.00225 |    0.02833 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07703 | -0.00299 |    0.03145 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06557 | -0.00044 |    0.02952 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05121 | -0.00112 |    0.01633 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03377 |  0.00031 |    0.00805 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55225 | -0.03100 |    0.32429 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:58:15,063 - Total sparsity: 74.94

2018-10-28 00:58:15,063 - --- validate (epoch=204)-----------
2018-10-28 00:58:15,063 - 10000 samples (128 per mini-batch)
2018-10-28 00:58:15,783 - Epoch: [204][   50/   78]    Loss 0.337205    Top1 90.265625    Top5 99.687500    
2018-10-28 00:58:16,174 - ==> Top1: 90.230    Top5: 99.730    Loss: 0.333

2018-10-28 00:58:16,175 - ==> Best Top1: 90.300   On Epoch: 203

2018-10-28 00:58:16,175 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:58:16,185 - 

2018-10-28 00:58:16,186 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:58:17,358 - Epoch: [205][   50/  391]    Overall Loss 0.108114    Objective Loss 0.108114    Top1 96.156250    Top5 100.000000    LR 0.003000    Time 0.023407    
2018-10-28 00:58:18,470 - Epoch: [205][  100/  391]    Overall Loss 0.113016    Objective Loss 0.113016    Top1 96.054688    Top5 99.992188    LR 0.003000    Time 0.022814    
2018-10-28 00:58:19,582 - Epoch: [205][  150/  391]    Overall Loss 0.111692    Objective Loss 0.111692    Top1 96.151042    Top5 99.994792    LR 0.003000    Time 0.022615    
2018-10-28 00:58:20,695 - Epoch: [205][  200/  391]    Overall Loss 0.111599    Objective Loss 0.111599    Top1 96.191406    Top5 99.980469    LR 0.003000    Time 0.022520    
2018-10-28 00:58:21,808 - Epoch: [205][  250/  391]    Overall Loss 0.112204    Objective Loss 0.112204    Top1 96.181250    Top5 99.978125    LR 0.003000    Time 0.022459    
2018-10-28 00:58:22,918 - Epoch: [205][  300/  391]    Overall Loss 0.113239    Objective Loss 0.113239    Top1 96.117188    Top5 99.981771    LR 0.003000    Time 0.022415    
2018-10-28 00:58:24,030 - Epoch: [205][  350/  391]    Overall Loss 0.113884    Objective Loss 0.113884    Top1 96.109375    Top5 99.982143    LR 0.003000    Time 0.022384    
2018-10-28 00:58:25,019 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37569 |  0.00113 |    0.18958 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11943 | -0.00316 |    0.03787 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11524 | -0.00197 |    0.04098 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11920 | -0.00879 |    0.05071 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08608 | -0.00367 |    0.02503 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12676 | -0.00509 |    0.04955 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08404 |  0.00172 |    0.02309 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11765 | -0.00419 |    0.05594 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.11002 | -0.00425 |    0.06269 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14319 | -0.00265 |    0.06900 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08975 | -0.00342 |    0.03569 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07330 |  0.00137 |    0.02620 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09437 | -0.00516 |    0.04114 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07660 | -0.00078 |    0.03421 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07572 | -0.00220 |    0.02911 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08195 | -0.00294 |    0.04068 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07887 | -0.00226 |    0.02830 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07694 | -0.00296 |    0.03142 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06550 | -0.00045 |    0.02949 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05115 | -0.00113 |    0.01632 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03373 |  0.00031 |    0.00804 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55297 | -0.03103 |    0.32470 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:58:25,019 - Total sparsity: 74.94

2018-10-28 00:58:25,019 - --- validate (epoch=205)-----------
2018-10-28 00:58:25,019 - 10000 samples (128 per mini-batch)
2018-10-28 00:58:25,744 - Epoch: [205][   50/   78]    Loss 0.343264    Top1 90.250000    Top5 99.640625    
2018-10-28 00:58:26,135 - ==> Top1: 90.150    Top5: 99.690    Loss: 0.339

2018-10-28 00:58:26,136 - ==> Best Top1: 90.300   On Epoch: 203

2018-10-28 00:58:26,136 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:58:26,147 - 

2018-10-28 00:58:26,147 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:58:27,319 - Epoch: [206][   50/  391]    Overall Loss 0.104352    Objective Loss 0.104352    Top1 96.734375    Top5 99.968750    LR 0.003000    Time 0.023399    
2018-10-28 00:58:28,431 - Epoch: [206][  100/  391]    Overall Loss 0.106044    Objective Loss 0.106044    Top1 96.421875    Top5 99.976562    LR 0.003000    Time 0.022808    
2018-10-28 00:58:29,545 - Epoch: [206][  150/  391]    Overall Loss 0.106798    Objective Loss 0.106798    Top1 96.421875    Top5 99.973958    LR 0.003000    Time 0.022622    
2018-10-28 00:58:30,659 - Epoch: [206][  200/  391]    Overall Loss 0.105482    Objective Loss 0.105482    Top1 96.460938    Top5 99.980469    LR 0.003000    Time 0.022529    
2018-10-28 00:58:31,771 - Epoch: [206][  250/  391]    Overall Loss 0.106653    Objective Loss 0.106653    Top1 96.362500    Top5 99.984375    LR 0.003000    Time 0.022465    
2018-10-28 00:58:32,883 - Epoch: [206][  300/  391]    Overall Loss 0.105232    Objective Loss 0.105232    Top1 96.419271    Top5 99.986979    LR 0.003000    Time 0.022423    
2018-10-28 00:58:33,995 - Epoch: [206][  350/  391]    Overall Loss 0.106485    Objective Loss 0.106485    Top1 96.388393    Top5 99.982143    LR 0.003000    Time 0.022394    
2018-10-28 00:58:34,983 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37528 |  0.00017 |    0.18940 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11930 | -0.00315 |    0.03784 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11511 | -0.00197 |    0.04093 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11907 | -0.00879 |    0.05065 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08598 | -0.00368 |    0.02499 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12662 | -0.00509 |    0.04946 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08395 |  0.00171 |    0.02306 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11752 | -0.00420 |    0.05586 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10989 | -0.00427 |    0.06263 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14303 | -0.00272 |    0.06892 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08965 | -0.00341 |    0.03564 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07322 |  0.00137 |    0.02617 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09427 | -0.00516 |    0.04109 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07652 | -0.00078 |    0.03417 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07563 | -0.00220 |    0.02907 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08186 | -0.00294 |    0.04064 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07878 | -0.00226 |    0.02825 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07686 | -0.00296 |    0.03138 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06543 | -0.00045 |    0.02946 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05110 | -0.00114 |    0.01630 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03369 |  0.00031 |    0.00803 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55380 | -0.03102 |    0.32516 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:58:34,983 - Total sparsity: 74.94

2018-10-28 00:58:34,983 - --- validate (epoch=206)-----------
2018-10-28 00:58:34,983 - 10000 samples (128 per mini-batch)
2018-10-28 00:58:35,696 - Epoch: [206][   50/   78]    Loss 0.338485    Top1 90.437500    Top5 99.687500    
2018-10-28 00:58:36,082 - ==> Top1: 90.200    Top5: 99.720    Loss: 0.337

2018-10-28 00:58:36,083 - ==> Best Top1: 90.300   On Epoch: 203

2018-10-28 00:58:36,083 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:58:36,093 - 

2018-10-28 00:58:36,094 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:58:37,264 - Epoch: [207][   50/  391]    Overall Loss 0.101030    Objective Loss 0.101030    Top1 96.578125    Top5 99.968750    LR 0.003000    Time 0.023360    
2018-10-28 00:58:38,375 - Epoch: [207][  100/  391]    Overall Loss 0.104498    Objective Loss 0.104498    Top1 96.476562    Top5 99.960938    LR 0.003000    Time 0.022779    
2018-10-28 00:58:39,487 - Epoch: [207][  150/  391]    Overall Loss 0.106916    Objective Loss 0.106916    Top1 96.437500    Top5 99.963542    LR 0.003000    Time 0.022589    
2018-10-28 00:58:40,598 - Epoch: [207][  200/  391]    Overall Loss 0.106291    Objective Loss 0.106291    Top1 96.433594    Top5 99.968750    LR 0.003000    Time 0.022491    
2018-10-28 00:58:41,711 - Epoch: [207][  250/  391]    Overall Loss 0.107403    Objective Loss 0.107403    Top1 96.375000    Top5 99.965625    LR 0.003000    Time 0.022425    
2018-10-28 00:58:42,823 - Epoch: [207][  300/  391]    Overall Loss 0.107293    Objective Loss 0.107293    Top1 96.382812    Top5 99.963542    LR 0.003000    Time 0.022389    
2018-10-28 00:58:43,937 - Epoch: [207][  350/  391]    Overall Loss 0.108855    Objective Loss 0.108855    Top1 96.314732    Top5 99.957589    LR 0.003000    Time 0.022370    
2018-10-28 00:58:44,930 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37487 |  0.00057 |    0.18910 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11917 | -0.00315 |    0.03779 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11498 | -0.00196 |    0.04089 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11894 | -0.00874 |    0.05059 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08589 | -0.00369 |    0.02496 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12648 | -0.00510 |    0.04940 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08386 |  0.00169 |    0.02301 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11739 | -0.00420 |    0.05579 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10977 | -0.00427 |    0.06256 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14288 | -0.00273 |    0.06883 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08956 | -0.00338 |    0.03560 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07314 |  0.00137 |    0.02613 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09416 | -0.00517 |    0.04104 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07644 | -0.00077 |    0.03414 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07555 | -0.00219 |    0.02903 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08177 | -0.00294 |    0.04060 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07869 | -0.00222 |    0.02821 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07678 | -0.00296 |    0.03135 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06536 | -0.00045 |    0.02943 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05104 | -0.00114 |    0.01628 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03366 |  0.00031 |    0.00802 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55451 | -0.03104 |    0.32552 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:58:44,930 - Total sparsity: 74.94

2018-10-28 00:58:44,930 - --- validate (epoch=207)-----------
2018-10-28 00:58:44,930 - 10000 samples (128 per mini-batch)
2018-10-28 00:58:45,637 - Epoch: [207][   50/   78]    Loss 0.341782    Top1 90.265625    Top5 99.656250    
2018-10-28 00:58:46,016 - ==> Top1: 90.100    Top5: 99.700    Loss: 0.339

2018-10-28 00:58:46,017 - ==> Best Top1: 90.300   On Epoch: 203

2018-10-28 00:58:46,017 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:58:46,027 - 

2018-10-28 00:58:46,027 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:58:47,198 - Epoch: [208][   50/  391]    Overall Loss 0.103783    Objective Loss 0.103783    Top1 96.406250    Top5 99.968750    LR 0.003000    Time 0.023372    
2018-10-28 00:58:48,306 - Epoch: [208][  100/  391]    Overall Loss 0.103141    Objective Loss 0.103141    Top1 96.460938    Top5 99.968750    LR 0.003000    Time 0.022756    
2018-10-28 00:58:49,419 - Epoch: [208][  150/  391]    Overall Loss 0.102580    Objective Loss 0.102580    Top1 96.520833    Top5 99.973958    LR 0.003000    Time 0.022583    
2018-10-28 00:58:50,531 - Epoch: [208][  200/  391]    Overall Loss 0.103170    Objective Loss 0.103170    Top1 96.546875    Top5 99.976562    LR 0.003000    Time 0.022490    
2018-10-28 00:58:51,642 - Epoch: [208][  250/  391]    Overall Loss 0.103582    Objective Loss 0.103582    Top1 96.481250    Top5 99.978125    LR 0.003000    Time 0.022432    
2018-10-28 00:58:52,754 - Epoch: [208][  300/  391]    Overall Loss 0.104889    Objective Loss 0.104889    Top1 96.463542    Top5 99.979167    LR 0.003000    Time 0.022382    
2018-10-28 00:58:53,869 - Epoch: [208][  350/  391]    Overall Loss 0.105113    Objective Loss 0.105113    Top1 96.457589    Top5 99.973214    LR 0.003000    Time 0.022368    
2018-10-28 00:58:54,862 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37446 |  0.00061 |    0.18881 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11904 | -0.00311 |    0.03774 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11486 | -0.00193 |    0.04083 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11882 | -0.00871 |    0.05054 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08579 | -0.00370 |    0.02493 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12635 | -0.00504 |    0.04935 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08377 |  0.00171 |    0.02299 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11726 | -0.00421 |    0.05573 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10965 | -0.00430 |    0.06249 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14272 | -0.00268 |    0.06880 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08946 | -0.00338 |    0.03556 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07306 |  0.00136 |    0.02610 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09406 | -0.00516 |    0.04099 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07635 | -0.00078 |    0.03410 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07547 | -0.00219 |    0.02900 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08168 | -0.00294 |    0.04055 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07861 | -0.00222 |    0.02818 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07669 | -0.00295 |    0.03132 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06528 | -0.00045 |    0.02939 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05099 | -0.00115 |    0.01626 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03362 |  0.00032 |    0.00801 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55526 | -0.03105 |    0.32593 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:58:54,862 - Total sparsity: 74.94

2018-10-28 00:58:54,863 - --- validate (epoch=208)-----------
2018-10-28 00:58:54,863 - 10000 samples (128 per mini-batch)
2018-10-28 00:58:55,582 - Epoch: [208][   50/   78]    Loss 0.341009    Top1 90.281250    Top5 99.656250    
2018-10-28 00:58:55,969 - ==> Top1: 90.180    Top5: 99.720    Loss: 0.338

2018-10-28 00:58:55,970 - ==> Best Top1: 90.300   On Epoch: 203

2018-10-28 00:58:55,970 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:58:55,980 - 

2018-10-28 00:58:55,981 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:58:57,152 - Epoch: [209][   50/  391]    Overall Loss 0.106247    Objective Loss 0.106247    Top1 96.140625    Top5 99.984375    LR 0.003000    Time 0.023396    
2018-10-28 00:58:58,263 - Epoch: [209][  100/  391]    Overall Loss 0.109827    Objective Loss 0.109827    Top1 96.109375    Top5 99.984375    LR 0.003000    Time 0.022784    
2018-10-28 00:58:59,374 - Epoch: [209][  150/  391]    Overall Loss 0.108668    Objective Loss 0.108668    Top1 96.156250    Top5 99.984375    LR 0.003000    Time 0.022590    
2018-10-28 00:59:00,490 - Epoch: [209][  200/  391]    Overall Loss 0.110016    Objective Loss 0.110016    Top1 96.058594    Top5 99.984375    LR 0.003000    Time 0.022514    
2018-10-28 00:59:01,601 - Epoch: [209][  250/  391]    Overall Loss 0.107849    Objective Loss 0.107849    Top1 96.171875    Top5 99.984375    LR 0.003000    Time 0.022451    
2018-10-28 00:59:02,714 - Epoch: [209][  300/  391]    Overall Loss 0.107018    Objective Loss 0.107018    Top1 96.231771    Top5 99.979167    LR 0.003000    Time 0.022415    
2018-10-28 00:59:03,827 - Epoch: [209][  350/  391]    Overall Loss 0.106263    Objective Loss 0.106263    Top1 96.256696    Top5 99.977679    LR 0.003000    Time 0.022389    
2018-10-28 00:59:04,817 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37406 |  0.00057 |    0.18884 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11891 | -0.00308 |    0.03771 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11473 | -0.00196 |    0.04081 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11869 | -0.00867 |    0.05048 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08570 | -0.00366 |    0.02489 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12621 | -0.00498 |    0.04931 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08368 |  0.00173 |    0.02297 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11714 | -0.00414 |    0.05565 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10953 | -0.00432 |    0.06242 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14256 | -0.00266 |    0.06871 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08936 | -0.00337 |    0.03553 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07298 |  0.00135 |    0.02607 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09396 | -0.00514 |    0.04094 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07627 | -0.00078 |    0.03407 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07539 | -0.00219 |    0.02897 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08159 | -0.00293 |    0.04051 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07852 | -0.00223 |    0.02815 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07661 | -0.00294 |    0.03128 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06521 | -0.00046 |    0.02937 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05093 | -0.00116 |    0.01624 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03358 |  0.00032 |    0.00800 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55594 | -0.03102 |    0.32633 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:59:04,817 - Total sparsity: 74.94

2018-10-28 00:59:04,817 - --- validate (epoch=209)-----------
2018-10-28 00:59:04,817 - 10000 samples (128 per mini-batch)
2018-10-28 00:59:05,591 - Epoch: [209][   50/   78]    Loss 0.346152    Top1 90.171875    Top5 99.687500    
2018-10-28 00:59:05,990 - ==> Top1: 90.080    Top5: 99.720    Loss: 0.344

2018-10-28 00:59:05,990 - ==> Best Top1: 90.300   On Epoch: 203

2018-10-28 00:59:05,990 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:59:06,001 - 

2018-10-28 00:59:06,001 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:59:07,174 - Epoch: [210][   50/  391]    Overall Loss 0.106451    Objective Loss 0.106451    Top1 96.390625    Top5 100.000000    LR 0.003000    Time 0.023420    
2018-10-28 00:59:08,286 - Epoch: [210][  100/  391]    Overall Loss 0.103965    Objective Loss 0.103965    Top1 96.515625    Top5 99.976562    LR 0.003000    Time 0.022820    
2018-10-28 00:59:09,398 - Epoch: [210][  150/  391]    Overall Loss 0.105243    Objective Loss 0.105243    Top1 96.406250    Top5 99.973958    LR 0.003000    Time 0.022614    
2018-10-28 00:59:10,511 - Epoch: [210][  200/  391]    Overall Loss 0.104934    Objective Loss 0.104934    Top1 96.429688    Top5 99.976562    LR 0.003000    Time 0.022518    
2018-10-28 00:59:11,624 - Epoch: [210][  250/  391]    Overall Loss 0.104551    Objective Loss 0.104551    Top1 96.418750    Top5 99.971875    LR 0.003000    Time 0.022461    
2018-10-28 00:59:12,737 - Epoch: [210][  300/  391]    Overall Loss 0.104122    Objective Loss 0.104122    Top1 96.442708    Top5 99.971354    LR 0.003000    Time 0.022423    
2018-10-28 00:59:13,849 - Epoch: [210][  350/  391]    Overall Loss 0.102952    Objective Loss 0.102952    Top1 96.506696    Top5 99.973214    LR 0.003000    Time 0.022393    
2018-10-28 00:59:14,842 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37365 |  0.00106 |    0.18853 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11878 | -0.00308 |    0.03765 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11461 | -0.00201 |    0.04077 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11856 | -0.00865 |    0.05044 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08561 | -0.00363 |    0.02487 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12607 | -0.00506 |    0.04926 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08359 |  0.00176 |    0.02292 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11701 | -0.00411 |    0.05560 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10941 | -0.00434 |    0.06235 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14241 | -0.00272 |    0.06865 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08927 | -0.00336 |    0.03548 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07290 |  0.00135 |    0.02604 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09386 | -0.00514 |    0.04090 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07619 | -0.00078 |    0.03402 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07530 | -0.00218 |    0.02894 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08150 | -0.00293 |    0.04046 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07843 | -0.00224 |    0.02813 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07653 | -0.00293 |    0.03125 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06514 | -0.00046 |    0.02933 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05088 | -0.00116 |    0.01622 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03355 |  0.00033 |    0.00799 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55663 | -0.03103 |    0.32669 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:59:14,842 - Total sparsity: 74.94

2018-10-28 00:59:14,842 - --- validate (epoch=210)-----------
2018-10-28 00:59:14,842 - 10000 samples (128 per mini-batch)
2018-10-28 00:59:15,565 - Epoch: [210][   50/   78]    Loss 0.349569    Top1 90.031250    Top5 99.609375    
2018-10-28 00:59:15,957 - ==> Top1: 90.100    Top5: 99.660    Loss: 0.343

2018-10-28 00:59:15,957 - ==> Best Top1: 90.300   On Epoch: 203

2018-10-28 00:59:15,958 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:59:15,971 - 

2018-10-28 00:59:15,972 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:59:17,142 - Epoch: [211][   50/  391]    Overall Loss 0.104478    Objective Loss 0.104478    Top1 96.390625    Top5 99.984375    LR 0.003000    Time 0.023376    
2018-10-28 00:59:18,254 - Epoch: [211][  100/  391]    Overall Loss 0.102422    Objective Loss 0.102422    Top1 96.546875    Top5 99.968750    LR 0.003000    Time 0.022795    
2018-10-28 00:59:19,365 - Epoch: [211][  150/  391]    Overall Loss 0.100460    Objective Loss 0.100460    Top1 96.578125    Top5 99.968750    LR 0.003000    Time 0.022595    
2018-10-28 00:59:20,479 - Epoch: [211][  200/  391]    Overall Loss 0.100525    Objective Loss 0.100525    Top1 96.585938    Top5 99.972656    LR 0.003000    Time 0.022489    
2018-10-28 00:59:21,593 - Epoch: [211][  250/  391]    Overall Loss 0.102149    Objective Loss 0.102149    Top1 96.559375    Top5 99.971875    LR 0.003000    Time 0.022442    
2018-10-28 00:59:22,706 - Epoch: [211][  300/  391]    Overall Loss 0.102238    Objective Loss 0.102238    Top1 96.541667    Top5 99.973958    LR 0.003000    Time 0.022408    
2018-10-28 00:59:23,818 - Epoch: [211][  350/  391]    Overall Loss 0.103017    Objective Loss 0.103017    Top1 96.488839    Top5 99.973214    LR 0.003000    Time 0.022379    
2018-10-28 00:59:24,813 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37324 |  0.00068 |    0.18831 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11866 | -0.00304 |    0.03761 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11448 | -0.00201 |    0.04072 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11843 | -0.00863 |    0.05039 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08551 | -0.00364 |    0.02485 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12594 | -0.00494 |    0.04917 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08349 |  0.00177 |    0.02289 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11688 | -0.00410 |    0.05555 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10930 | -0.00431 |    0.06228 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14225 | -0.00267 |    0.06858 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08917 | -0.00337 |    0.03544 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07282 |  0.00134 |    0.02601 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09376 | -0.00512 |    0.04085 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07611 | -0.00077 |    0.03398 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07522 | -0.00218 |    0.02891 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08141 | -0.00293 |    0.04042 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07835 | -0.00223 |    0.02810 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07644 | -0.00293 |    0.03121 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06507 | -0.00046 |    0.02930 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05082 | -0.00118 |    0.01620 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03351 |  0.00032 |    0.00798 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55725 | -0.03105 |    0.32706 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:59:24,813 - Total sparsity: 74.94

2018-10-28 00:59:24,813 - --- validate (epoch=211)-----------
2018-10-28 00:59:24,813 - 10000 samples (128 per mini-batch)
2018-10-28 00:59:25,535 - Epoch: [211][   50/   78]    Loss 0.351126    Top1 90.062500    Top5 99.656250    
2018-10-28 00:59:25,924 - ==> Top1: 90.080    Top5: 99.730    Loss: 0.348

2018-10-28 00:59:25,925 - ==> Best Top1: 90.300   On Epoch: 203

2018-10-28 00:59:25,925 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:59:25,935 - 

2018-10-28 00:59:25,935 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:59:27,107 - Epoch: [212][   50/  391]    Overall Loss 0.103107    Objective Loss 0.103107    Top1 96.609375    Top5 99.984375    LR 0.003000    Time 0.023394    
2018-10-28 00:59:28,217 - Epoch: [212][  100/  391]    Overall Loss 0.101285    Objective Loss 0.101285    Top1 96.617188    Top5 99.992188    LR 0.003000    Time 0.022785    
2018-10-28 00:59:29,330 - Epoch: [212][  150/  391]    Overall Loss 0.102305    Objective Loss 0.102305    Top1 96.598958    Top5 99.994792    LR 0.003000    Time 0.022606    
2018-10-28 00:59:30,446 - Epoch: [212][  200/  391]    Overall Loss 0.101911    Objective Loss 0.101911    Top1 96.605469    Top5 99.988281    LR 0.003000    Time 0.022525    
2018-10-28 00:59:31,560 - Epoch: [212][  250/  391]    Overall Loss 0.100225    Objective Loss 0.100225    Top1 96.634375    Top5 99.981250    LR 0.003000    Time 0.022470    
2018-10-28 00:59:32,673 - Epoch: [212][  300/  391]    Overall Loss 0.099592    Objective Loss 0.099592    Top1 96.671875    Top5 99.976562    LR 0.003000    Time 0.022432    
2018-10-28 00:59:33,785 - Epoch: [212][  350/  391]    Overall Loss 0.100567    Objective Loss 0.100567    Top1 96.640625    Top5 99.970982    LR 0.003000    Time 0.022401    
2018-10-28 00:59:34,777 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37283 |  0.00006 |    0.18804 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11853 | -0.00299 |    0.03755 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11436 | -0.00197 |    0.04068 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11830 | -0.00860 |    0.05033 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08542 | -0.00363 |    0.02482 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12580 | -0.00495 |    0.04914 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08340 |  0.00176 |    0.02284 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11675 | -0.00417 |    0.05548 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10918 | -0.00431 |    0.06221 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14209 | -0.00264 |    0.06846 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08907 | -0.00338 |    0.03540 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07274 |  0.00135 |    0.02598 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09366 | -0.00510 |    0.04080 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07602 | -0.00076 |    0.03395 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07514 | -0.00218 |    0.02888 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08133 | -0.00292 |    0.04037 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07826 | -0.00222 |    0.02807 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07636 | -0.00292 |    0.03118 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06500 | -0.00046 |    0.02927 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05077 | -0.00119 |    0.01619 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03347 |  0.00033 |    0.00798 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55797 | -0.03107 |    0.32743 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:59:34,777 - Total sparsity: 74.94

2018-10-28 00:59:34,777 - --- validate (epoch=212)-----------
2018-10-28 00:59:34,778 - 10000 samples (128 per mini-batch)
2018-10-28 00:59:35,496 - Epoch: [212][   50/   78]    Loss 0.346483    Top1 90.156250    Top5 99.656250    
2018-10-28 00:59:35,885 - ==> Top1: 90.140    Top5: 99.670    Loss: 0.343

2018-10-28 00:59:35,886 - ==> Best Top1: 90.300   On Epoch: 203

2018-10-28 00:59:35,886 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:59:35,895 - 

2018-10-28 00:59:35,896 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:59:37,065 - Epoch: [213][   50/  391]    Overall Loss 0.094524    Objective Loss 0.094524    Top1 96.843750    Top5 100.000000    LR 0.003000    Time 0.023350    
2018-10-28 00:59:38,177 - Epoch: [213][  100/  391]    Overall Loss 0.096679    Objective Loss 0.096679    Top1 96.804688    Top5 99.976562    LR 0.003000    Time 0.022781    
2018-10-28 00:59:39,287 - Epoch: [213][  150/  391]    Overall Loss 0.099029    Objective Loss 0.099029    Top1 96.677083    Top5 99.973958    LR 0.003000    Time 0.022576    
2018-10-28 00:59:40,399 - Epoch: [213][  200/  391]    Overall Loss 0.100031    Objective Loss 0.100031    Top1 96.605469    Top5 99.976562    LR 0.003000    Time 0.022486    
2018-10-28 00:59:41,511 - Epoch: [213][  250/  391]    Overall Loss 0.099948    Objective Loss 0.099948    Top1 96.606250    Top5 99.978125    LR 0.003000    Time 0.022433    
2018-10-28 00:59:42,625 - Epoch: [213][  300/  391]    Overall Loss 0.099563    Objective Loss 0.099563    Top1 96.588542    Top5 99.981771    LR 0.003000    Time 0.022401    
2018-10-28 00:59:43,739 - Epoch: [213][  350/  391]    Overall Loss 0.100660    Objective Loss 0.100660    Top1 96.513393    Top5 99.984375    LR 0.003000    Time 0.022379    
2018-10-28 00:59:44,730 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37243 |  0.00030 |    0.18783 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11840 | -0.00293 |    0.03754 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11423 | -0.00198 |    0.04061 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11817 | -0.00864 |    0.05027 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08533 | -0.00366 |    0.02479 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12567 | -0.00488 |    0.04908 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08331 |  0.00175 |    0.02282 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11662 | -0.00421 |    0.05542 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10906 | -0.00431 |    0.06213 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14194 | -0.00270 |    0.06843 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08897 | -0.00338 |    0.03536 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07266 |  0.00134 |    0.02594 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09356 | -0.00510 |    0.04074 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07594 | -0.00078 |    0.03392 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07506 | -0.00219 |    0.02884 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08124 | -0.00291 |    0.04033 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07818 | -0.00223 |    0.02803 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07628 | -0.00292 |    0.03114 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06493 | -0.00047 |    0.02924 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05071 | -0.00119 |    0.01617 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03344 |  0.00033 |    0.00797 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55863 | -0.03107 |    0.32779 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:59:44,731 - Total sparsity: 74.94

2018-10-28 00:59:44,731 - --- validate (epoch=213)-----------
2018-10-28 00:59:44,731 - 10000 samples (128 per mini-batch)
2018-10-28 00:59:45,457 - Epoch: [213][   50/   78]    Loss 0.343948    Top1 90.468750    Top5 99.640625    
2018-10-28 00:59:45,849 - ==> Top1: 90.440    Top5: 99.680    Loss: 0.341

2018-10-28 00:59:45,849 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 00:59:45,850 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:59:45,872 - 

2018-10-28 00:59:45,872 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:59:47,018 - Epoch: [214][   50/  391]    Overall Loss 0.097852    Objective Loss 0.097852    Top1 96.703125    Top5 99.953125    LR 0.003000    Time 0.022880    
2018-10-28 00:59:48,133 - Epoch: [214][  100/  391]    Overall Loss 0.094818    Objective Loss 0.094818    Top1 96.789062    Top5 99.976562    LR 0.003000    Time 0.022573    
2018-10-28 00:59:49,246 - Epoch: [214][  150/  391]    Overall Loss 0.095880    Objective Loss 0.095880    Top1 96.770833    Top5 99.968750    LR 0.003000    Time 0.022459    
2018-10-28 00:59:50,357 - Epoch: [214][  200/  391]    Overall Loss 0.096075    Objective Loss 0.096075    Top1 96.765625    Top5 99.964844    LR 0.003000    Time 0.022397    
2018-10-28 00:59:51,468 - Epoch: [214][  250/  391]    Overall Loss 0.097245    Objective Loss 0.097245    Top1 96.731250    Top5 99.968750    LR 0.003000    Time 0.022355    
2018-10-28 00:59:52,580 - Epoch: [214][  300/  391]    Overall Loss 0.096280    Objective Loss 0.096280    Top1 96.789062    Top5 99.971354    LR 0.003000    Time 0.022331    
2018-10-28 00:59:53,693 - Epoch: [214][  350/  391]    Overall Loss 0.096612    Objective Loss 0.096612    Top1 96.767857    Top5 99.975446    LR 0.003000    Time 0.022307    
2018-10-28 00:59:54,686 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37202 |  0.00071 |    0.18757 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11827 | -0.00296 |    0.03749 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11411 | -0.00199 |    0.04058 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11805 | -0.00859 |    0.05021 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08523 | -0.00365 |    0.02475 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12553 | -0.00487 |    0.04904 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08322 |  0.00174 |    0.02279 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11650 | -0.00423 |    0.05535 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10894 | -0.00430 |    0.06206 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14178 | -0.00268 |    0.06832 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08888 | -0.00339 |    0.03531 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07258 |  0.00135 |    0.02591 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09346 | -0.00511 |    0.04070 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07586 | -0.00079 |    0.03389 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07498 | -0.00218 |    0.02881 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08115 | -0.00292 |    0.04028 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07809 | -0.00219 |    0.02800 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07620 | -0.00291 |    0.03111 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06486 | -0.00046 |    0.02921 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05066 | -0.00120 |    0.01615 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03340 |  0.00032 |    0.00796 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55933 | -0.03107 |    0.32820 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 00:59:54,686 - Total sparsity: 74.94

2018-10-28 00:59:54,686 - --- validate (epoch=214)-----------
2018-10-28 00:59:54,686 - 10000 samples (128 per mini-batch)
2018-10-28 00:59:55,410 - Epoch: [214][   50/   78]    Loss 0.347802    Top1 90.218750    Top5 99.593750    
2018-10-28 00:59:55,798 - ==> Top1: 90.130    Top5: 99.660    Loss: 0.344

2018-10-28 00:59:55,798 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 00:59:55,798 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 00:59:55,808 - 

2018-10-28 00:59:55,808 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 00:59:56,980 - Epoch: [215][   50/  391]    Overall Loss 0.101966    Objective Loss 0.101966    Top1 96.328125    Top5 100.000000    LR 0.003000    Time 0.023403    
2018-10-28 00:59:58,093 - Epoch: [215][  100/  391]    Overall Loss 0.102904    Objective Loss 0.102904    Top1 96.359375    Top5 99.992188    LR 0.003000    Time 0.022817    
2018-10-28 00:59:59,208 - Epoch: [215][  150/  391]    Overall Loss 0.099408    Objective Loss 0.099408    Top1 96.463542    Top5 99.989583    LR 0.003000    Time 0.022634    
2018-10-28 01:00:00,326 - Epoch: [215][  200/  391]    Overall Loss 0.097443    Objective Loss 0.097443    Top1 96.632812    Top5 99.984375    LR 0.003000    Time 0.022559    
2018-10-28 01:00:01,438 - Epoch: [215][  250/  391]    Overall Loss 0.097461    Objective Loss 0.097461    Top1 96.671875    Top5 99.978125    LR 0.003000    Time 0.022491    
2018-10-28 01:00:02,551 - Epoch: [215][  300/  391]    Overall Loss 0.097015    Objective Loss 0.097015    Top1 96.703125    Top5 99.981771    LR 0.003000    Time 0.022447    
2018-10-28 01:00:03,663 - Epoch: [215][  350/  391]    Overall Loss 0.097143    Objective Loss 0.097143    Top1 96.676339    Top5 99.982143    LR 0.003000    Time 0.022413    
2018-10-28 01:00:04,659 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37162 |  0.00099 |    0.18739 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11814 | -0.00294 |    0.03743 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11398 | -0.00205 |    0.04055 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11792 | -0.00857 |    0.05015 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08514 | -0.00365 |    0.02472 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12539 | -0.00489 |    0.04896 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08313 |  0.00177 |    0.02274 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11637 | -0.00419 |    0.05528 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10882 | -0.00427 |    0.06199 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14162 | -0.00274 |    0.06823 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08878 | -0.00337 |    0.03528 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07250 |  0.00137 |    0.02588 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09336 | -0.00511 |    0.04066 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07578 | -0.00080 |    0.03386 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07490 | -0.00217 |    0.02878 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08106 | -0.00292 |    0.04024 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07801 | -0.00217 |    0.02795 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07612 | -0.00291 |    0.03107 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06479 | -0.00046 |    0.02918 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05060 | -0.00119 |    0.01614 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03336 |  0.00033 |    0.00795 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.55994 | -0.03105 |    0.32852 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:00:04,659 - Total sparsity: 74.94

2018-10-28 01:00:04,659 - --- validate (epoch=215)-----------
2018-10-28 01:00:04,660 - 10000 samples (128 per mini-batch)
2018-10-28 01:00:05,385 - Epoch: [215][   50/   78]    Loss 0.348842    Top1 90.359375    Top5 99.593750    
2018-10-28 01:00:05,779 - ==> Top1: 90.290    Top5: 99.640    Loss: 0.345

2018-10-28 01:00:05,780 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:00:05,780 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:00:05,791 - 

2018-10-28 01:00:05,791 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:00:06,963 - Epoch: [216][   50/  391]    Overall Loss 0.102744    Objective Loss 0.102744    Top1 96.671875    Top5 99.968750    LR 0.003000    Time 0.023407    
2018-10-28 01:00:08,076 - Epoch: [216][  100/  391]    Overall Loss 0.100499    Objective Loss 0.100499    Top1 96.695312    Top5 99.968750    LR 0.003000    Time 0.022817    
2018-10-28 01:00:09,186 - Epoch: [216][  150/  391]    Overall Loss 0.100367    Objective Loss 0.100367    Top1 96.708333    Top5 99.963542    LR 0.003000    Time 0.022608    
2018-10-28 01:00:10,296 - Epoch: [216][  200/  391]    Overall Loss 0.097767    Objective Loss 0.097767    Top1 96.832031    Top5 99.972656    LR 0.003000    Time 0.022499    
2018-10-28 01:00:11,408 - Epoch: [216][  250/  391]    Overall Loss 0.098419    Objective Loss 0.098419    Top1 96.743750    Top5 99.975000    LR 0.003000    Time 0.022441    
2018-10-28 01:00:12,521 - Epoch: [216][  300/  391]    Overall Loss 0.097599    Objective Loss 0.097599    Top1 96.768229    Top5 99.976562    LR 0.003000    Time 0.022406    
2018-10-28 01:00:13,633 - Epoch: [216][  350/  391]    Overall Loss 0.097427    Objective Loss 0.097427    Top1 96.776786    Top5 99.977679    LR 0.003000    Time 0.022379    
2018-10-28 01:00:14,624 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37121 |  0.00089 |    0.18721 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11801 | -0.00289 |    0.03738 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11386 | -0.00203 |    0.04050 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11779 | -0.00856 |    0.05010 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08505 | -0.00364 |    0.02468 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12526 | -0.00488 |    0.04892 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08304 |  0.00179 |    0.02272 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11625 | -0.00417 |    0.05520 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10871 | -0.00425 |    0.06192 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14147 | -0.00265 |    0.06815 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08869 | -0.00336 |    0.03524 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07243 |  0.00138 |    0.02585 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09326 | -0.00506 |    0.04061 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07570 | -0.00078 |    0.03383 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07481 | -0.00217 |    0.02875 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08097 | -0.00292 |    0.04019 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07792 | -0.00218 |    0.02792 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07603 | -0.00291 |    0.03104 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06473 | -0.00047 |    0.02914 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05055 | -0.00120 |    0.01612 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03333 |  0.00033 |    0.00794 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56055 | -0.03103 |    0.32888 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:00:14,624 - Total sparsity: 74.94

2018-10-28 01:00:14,625 - --- validate (epoch=216)-----------
2018-10-28 01:00:14,625 - 10000 samples (128 per mini-batch)
2018-10-28 01:00:15,342 - Epoch: [216][   50/   78]    Loss 0.353865    Top1 90.250000    Top5 99.656250    
2018-10-28 01:00:15,730 - ==> Top1: 90.260    Top5: 99.700    Loss: 0.349

2018-10-28 01:00:15,731 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:00:15,731 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:00:15,741 - 

2018-10-28 01:00:15,742 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:00:16,915 - Epoch: [217][   50/  391]    Overall Loss 0.092941    Objective Loss 0.092941    Top1 96.937500    Top5 100.000000    LR 0.003000    Time 0.023421    
2018-10-28 01:00:18,027 - Epoch: [217][  100/  391]    Overall Loss 0.094771    Objective Loss 0.094771    Top1 96.812500    Top5 99.976562    LR 0.003000    Time 0.022822    
2018-10-28 01:00:19,139 - Epoch: [217][  150/  391]    Overall Loss 0.096181    Objective Loss 0.096181    Top1 96.786458    Top5 99.968750    LR 0.003000    Time 0.022621    
2018-10-28 01:00:20,254 - Epoch: [217][  200/  391]    Overall Loss 0.097610    Objective Loss 0.097610    Top1 96.691406    Top5 99.972656    LR 0.003000    Time 0.022534    
2018-10-28 01:00:21,369 - Epoch: [217][  250/  391]    Overall Loss 0.096434    Objective Loss 0.096434    Top1 96.734375    Top5 99.978125    LR 0.003000    Time 0.022480    
2018-10-28 01:00:22,483 - Epoch: [217][  300/  391]    Overall Loss 0.097185    Objective Loss 0.097185    Top1 96.692708    Top5 99.973958    LR 0.003000    Time 0.022444    
2018-10-28 01:00:23,596 - Epoch: [217][  350/  391]    Overall Loss 0.097395    Objective Loss 0.097395    Top1 96.691964    Top5 99.975446    LR 0.003000    Time 0.022412    
2018-10-28 01:00:24,592 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37081 |  0.00072 |    0.18705 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11789 | -0.00292 |    0.03735 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11373 | -0.00201 |    0.04046 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11766 | -0.00857 |    0.05005 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08496 | -0.00362 |    0.02466 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12512 | -0.00484 |    0.04884 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08295 |  0.00181 |    0.02272 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11612 | -0.00417 |    0.05514 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10859 | -0.00424 |    0.06186 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14132 | -0.00260 |    0.06808 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08859 | -0.00337 |    0.03519 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07235 |  0.00137 |    0.02581 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09316 | -0.00507 |    0.04056 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07562 | -0.00076 |    0.03381 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07473 | -0.00218 |    0.02871 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08089 | -0.00293 |    0.04014 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07784 | -0.00217 |    0.02789 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07595 | -0.00291 |    0.03100 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06466 | -0.00047 |    0.02912 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05050 | -0.00120 |    0.01610 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03329 |  0.00032 |    0.00793 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56108 | -0.03105 |    0.32916 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:00:24,592 - Total sparsity: 74.94

2018-10-28 01:00:24,592 - --- validate (epoch=217)-----------
2018-10-28 01:00:24,592 - 10000 samples (128 per mini-batch)
2018-10-28 01:00:25,305 - Epoch: [217][   50/   78]    Loss 0.351029    Top1 90.265625    Top5 99.625000    
2018-10-28 01:00:25,696 - ==> Top1: 90.280    Top5: 99.680    Loss: 0.345

2018-10-28 01:00:25,697 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:00:25,697 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:00:25,707 - 

2018-10-28 01:00:25,708 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:00:26,879 - Epoch: [218][   50/  391]    Overall Loss 0.095027    Objective Loss 0.095027    Top1 96.640625    Top5 99.984375    LR 0.003000    Time 0.023388    
2018-10-28 01:00:27,991 - Epoch: [218][  100/  391]    Overall Loss 0.100156    Objective Loss 0.100156    Top1 96.539062    Top5 99.992188    LR 0.003000    Time 0.022800    
2018-10-28 01:00:29,103 - Epoch: [218][  150/  391]    Overall Loss 0.096928    Objective Loss 0.096928    Top1 96.604167    Top5 99.994792    LR 0.003000    Time 0.022608    
2018-10-28 01:00:30,217 - Epoch: [218][  200/  391]    Overall Loss 0.096102    Objective Loss 0.096102    Top1 96.601562    Top5 99.996094    LR 0.003000    Time 0.022517    
2018-10-28 01:00:31,330 - Epoch: [218][  250/  391]    Overall Loss 0.096584    Objective Loss 0.096584    Top1 96.600000    Top5 99.993750    LR 0.003000    Time 0.022461    
2018-10-28 01:00:32,442 - Epoch: [218][  300/  391]    Overall Loss 0.097823    Objective Loss 0.097823    Top1 96.588542    Top5 99.989583    LR 0.003000    Time 0.022421    
2018-10-28 01:00:33,554 - Epoch: [218][  350/  391]    Overall Loss 0.098033    Objective Loss 0.098033    Top1 96.609375    Top5 99.984375    LR 0.003000    Time 0.022393    
2018-10-28 01:00:34,545 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37041 |  0.00050 |    0.18681 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11776 | -0.00292 |    0.03732 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11361 | -0.00201 |    0.04042 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11754 | -0.00853 |    0.04999 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08487 | -0.00358 |    0.02462 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12498 | -0.00493 |    0.04882 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08286 |  0.00178 |    0.02267 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11599 | -0.00416 |    0.05508 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10847 | -0.00420 |    0.06179 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14116 | -0.00262 |    0.06800 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08850 | -0.00335 |    0.03515 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07227 |  0.00138 |    0.02579 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09306 | -0.00505 |    0.04052 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07553 | -0.00076 |    0.03378 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07465 | -0.00218 |    0.02868 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08080 | -0.00293 |    0.04011 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07775 | -0.00219 |    0.02784 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07587 | -0.00288 |    0.03097 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06459 | -0.00048 |    0.02908 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05044 | -0.00120 |    0.01608 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03325 |  0.00033 |    0.00792 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56160 | -0.03103 |    0.32942 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:00:34,545 - Total sparsity: 74.94

2018-10-28 01:00:34,545 - --- validate (epoch=218)-----------
2018-10-28 01:00:34,546 - 10000 samples (128 per mini-batch)
2018-10-28 01:00:35,269 - Epoch: [218][   50/   78]    Loss 0.355655    Top1 90.234375    Top5 99.656250    
2018-10-28 01:00:35,661 - ==> Top1: 90.010    Top5: 99.720    Loss: 0.352

2018-10-28 01:00:35,665 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:00:35,665 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:00:35,676 - 

2018-10-28 01:00:35,676 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:00:36,846 - Epoch: [219][   50/  391]    Overall Loss 0.097650    Objective Loss 0.097650    Top1 96.578125    Top5 99.968750    LR 0.003000    Time 0.023363    
2018-10-28 01:00:37,958 - Epoch: [219][  100/  391]    Overall Loss 0.097862    Objective Loss 0.097862    Top1 96.617188    Top5 99.968750    LR 0.003000    Time 0.022790    
2018-10-28 01:00:39,071 - Epoch: [219][  150/  391]    Overall Loss 0.098716    Objective Loss 0.098716    Top1 96.604167    Top5 99.963542    LR 0.003000    Time 0.022604    
2018-10-28 01:00:40,183 - Epoch: [219][  200/  391]    Overall Loss 0.097621    Objective Loss 0.097621    Top1 96.691406    Top5 99.964844    LR 0.003000    Time 0.022505    
2018-10-28 01:00:41,295 - Epoch: [219][  250/  391]    Overall Loss 0.097076    Objective Loss 0.097076    Top1 96.684375    Top5 99.968750    LR 0.003000    Time 0.022447    
2018-10-28 01:00:42,406 - Epoch: [219][  300/  391]    Overall Loss 0.096597    Objective Loss 0.096597    Top1 96.708333    Top5 99.966146    LR 0.003000    Time 0.022406    
2018-10-28 01:00:43,520 - Epoch: [219][  350/  391]    Overall Loss 0.095481    Objective Loss 0.095481    Top1 96.750000    Top5 99.964286    LR 0.003000    Time 0.022382    
2018-10-28 01:00:44,510 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.37000 |  0.00024 |    0.18662 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11763 | -0.00282 |    0.03730 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11349 | -0.00201 |    0.04040 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11742 | -0.00837 |    0.04997 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08477 | -0.00360 |    0.02459 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12485 | -0.00497 |    0.04874 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08277 |  0.00176 |    0.02264 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11587 | -0.00416 |    0.05502 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10835 | -0.00422 |    0.06172 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14101 | -0.00262 |    0.06789 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08840 | -0.00333 |    0.03512 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07219 |  0.00138 |    0.02575 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09296 | -0.00507 |    0.04047 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07545 | -0.00077 |    0.03375 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07457 | -0.00219 |    0.02864 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08071 | -0.00293 |    0.04006 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07767 | -0.00216 |    0.02781 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07579 | -0.00287 |    0.03093 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06452 | -0.00048 |    0.02905 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05039 | -0.00119 |    0.01607 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03322 |  0.00033 |    0.00791 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56218 | -0.03105 |    0.32974 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:00:44,510 - Total sparsity: 74.94

2018-10-28 01:00:44,510 - --- validate (epoch=219)-----------
2018-10-28 01:00:44,510 - 10000 samples (128 per mini-batch)
2018-10-28 01:00:45,231 - Epoch: [219][   50/   78]    Loss 0.355079    Top1 90.359375    Top5 99.609375    
2018-10-28 01:00:45,624 - ==> Top1: 90.190    Top5: 99.670    Loss: 0.349

2018-10-28 01:00:45,625 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:00:45,625 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:00:45,641 - 

2018-10-28 01:00:45,642 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:00:46,785 - Epoch: [220][   50/  391]    Overall Loss 0.098177    Objective Loss 0.098177    Top1 96.671875    Top5 99.968750    LR 0.003000    Time 0.022826    
2018-10-28 01:00:47,897 - Epoch: [220][  100/  391]    Overall Loss 0.096103    Objective Loss 0.096103    Top1 96.695312    Top5 99.984375    LR 0.003000    Time 0.022525    
2018-10-28 01:00:49,008 - Epoch: [220][  150/  391]    Overall Loss 0.097240    Objective Loss 0.097240    Top1 96.609375    Top5 99.989583    LR 0.003000    Time 0.022415    
2018-10-28 01:00:50,121 - Epoch: [220][  200/  391]    Overall Loss 0.096949    Objective Loss 0.096949    Top1 96.656250    Top5 99.988281    LR 0.003000    Time 0.022366    
2018-10-28 01:00:51,232 - Epoch: [220][  250/  391]    Overall Loss 0.095948    Objective Loss 0.095948    Top1 96.687500    Top5 99.990625    LR 0.003000    Time 0.022335    
2018-10-28 01:00:52,346 - Epoch: [220][  300/  391]    Overall Loss 0.094236    Objective Loss 0.094236    Top1 96.729167    Top5 99.992188    LR 0.003000    Time 0.022322    
2018-10-28 01:00:53,459 - Epoch: [220][  350/  391]    Overall Loss 0.093995    Objective Loss 0.093995    Top1 96.756696    Top5 99.993304    LR 0.003000    Time 0.022308    
2018-10-28 01:00:54,452 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36960 |  0.00067 |    0.18644 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11750 | -0.00292 |    0.03725 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11336 | -0.00199 |    0.04034 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11729 | -0.00844 |    0.04991 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08468 | -0.00354 |    0.02455 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12471 | -0.00498 |    0.04871 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08268 |  0.00178 |    0.02262 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11574 | -0.00418 |    0.05494 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10824 | -0.00421 |    0.06165 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14085 | -0.00259 |    0.06784 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08831 | -0.00333 |    0.03509 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07211 |  0.00138 |    0.02572 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09286 | -0.00507 |    0.04042 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07537 | -0.00075 |    0.03370 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07449 | -0.00219 |    0.02861 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08063 | -0.00293 |    0.04002 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07758 | -0.00212 |    0.02778 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07571 | -0.00286 |    0.03090 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06445 | -0.00047 |    0.02902 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05033 | -0.00120 |    0.01605 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03318 |  0.00033 |    0.00791 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56276 | -0.03104 |    0.33005 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:00:54,452 - Total sparsity: 74.94

2018-10-28 01:00:54,452 - --- validate (epoch=220)-----------
2018-10-28 01:00:54,453 - 10000 samples (128 per mini-batch)
2018-10-28 01:00:55,177 - Epoch: [220][   50/   78]    Loss 0.360326    Top1 90.062500    Top5 99.640625    
2018-10-28 01:00:55,571 - ==> Top1: 90.100    Top5: 99.690    Loss: 0.354

2018-10-28 01:00:55,572 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:00:55,572 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:00:55,582 - 

2018-10-28 01:00:55,582 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:00:56,755 - Epoch: [221][   50/  391]    Overall Loss 0.095490    Objective Loss 0.095490    Top1 96.625000    Top5 99.984375    LR 0.003000    Time 0.023424    
2018-10-28 01:00:57,869 - Epoch: [221][  100/  391]    Overall Loss 0.093679    Objective Loss 0.093679    Top1 96.875000    Top5 99.992188    LR 0.003000    Time 0.022834    
2018-10-28 01:00:58,981 - Epoch: [221][  150/  391]    Overall Loss 0.093837    Objective Loss 0.093837    Top1 96.822917    Top5 99.994792    LR 0.003000    Time 0.022630    
2018-10-28 01:01:00,095 - Epoch: [221][  200/  391]    Overall Loss 0.091954    Objective Loss 0.091954    Top1 96.921875    Top5 99.996094    LR 0.003000    Time 0.022532    
2018-10-28 01:01:01,207 - Epoch: [221][  250/  391]    Overall Loss 0.091714    Objective Loss 0.091714    Top1 96.925000    Top5 99.993750    LR 0.003000    Time 0.022468    
2018-10-28 01:01:02,320 - Epoch: [221][  300/  391]    Overall Loss 0.091944    Objective Loss 0.091944    Top1 96.898438    Top5 99.992188    LR 0.003000    Time 0.022429    
2018-10-28 01:01:03,433 - Epoch: [221][  350/  391]    Overall Loss 0.092216    Objective Loss 0.092216    Top1 96.897321    Top5 99.984375    LR 0.003000    Time 0.022400    
2018-10-28 01:01:04,422 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36921 |  0.00092 |    0.18636 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11738 | -0.00292 |    0.03721 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11324 | -0.00192 |    0.04031 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11716 | -0.00846 |    0.04985 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08459 | -0.00351 |    0.02453 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12458 | -0.00495 |    0.04863 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08259 |  0.00177 |    0.02261 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11562 | -0.00418 |    0.05488 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10812 | -0.00424 |    0.06160 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14070 | -0.00267 |    0.06779 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08821 | -0.00331 |    0.03505 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07204 |  0.00136 |    0.02570 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09276 | -0.00506 |    0.04038 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07529 | -0.00078 |    0.03367 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07441 | -0.00218 |    0.02858 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08054 | -0.00292 |    0.03997 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07750 | -0.00213 |    0.02775 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07563 | -0.00285 |    0.03086 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06438 | -0.00047 |    0.02899 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05028 | -0.00119 |    0.01603 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03315 |  0.00033 |    0.00790 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56330 | -0.03103 |    0.33035 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:01:04,422 - Total sparsity: 74.94

2018-10-28 01:01:04,422 - --- validate (epoch=221)-----------
2018-10-28 01:01:04,422 - 10000 samples (128 per mini-batch)
2018-10-28 01:01:05,149 - Epoch: [221][   50/   78]    Loss 0.359234    Top1 90.218750    Top5 99.531250    
2018-10-28 01:01:05,538 - ==> Top1: 90.190    Top5: 99.590    Loss: 0.353

2018-10-28 01:01:05,539 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:01:05,539 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:01:05,553 - 

2018-10-28 01:01:05,553 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:01:06,725 - Epoch: [222][   50/  391]    Overall Loss 0.092224    Objective Loss 0.092224    Top1 96.656250    Top5 99.984375    LR 0.003000    Time 0.023404    
2018-10-28 01:01:07,835 - Epoch: [222][  100/  391]    Overall Loss 0.087313    Objective Loss 0.087313    Top1 96.992188    Top5 99.992188    LR 0.003000    Time 0.022790    
2018-10-28 01:01:08,948 - Epoch: [222][  150/  391]    Overall Loss 0.094001    Objective Loss 0.094001    Top1 96.677083    Top5 99.994792    LR 0.003000    Time 0.022599    
2018-10-28 01:01:10,062 - Epoch: [222][  200/  391]    Overall Loss 0.094187    Objective Loss 0.094187    Top1 96.699219    Top5 99.992188    LR 0.003000    Time 0.022514    
2018-10-28 01:01:11,176 - Epoch: [222][  250/  391]    Overall Loss 0.095828    Objective Loss 0.095828    Top1 96.618750    Top5 99.990625    LR 0.003000    Time 0.022460    
2018-10-28 01:01:12,290 - Epoch: [222][  300/  391]    Overall Loss 0.096671    Objective Loss 0.096671    Top1 96.619792    Top5 99.989583    LR 0.003000    Time 0.022428    
2018-10-28 01:01:13,404 - Epoch: [222][  350/  391]    Overall Loss 0.096854    Objective Loss 0.096854    Top1 96.616071    Top5 99.986607    LR 0.003000    Time 0.022403    
2018-10-28 01:01:14,396 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36881 |  0.00047 |    0.18614 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11725 | -0.00293 |    0.03716 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11312 | -0.00192 |    0.04028 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11703 | -0.00848 |    0.04980 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08450 | -0.00349 |    0.02450 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12444 | -0.00498 |    0.04856 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08250 |  0.00178 |    0.02257 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11549 | -0.00419 |    0.05482 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10800 | -0.00425 |    0.06154 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14054 | -0.00269 |    0.06772 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08812 | -0.00329 |    0.03502 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07196 |  0.00138 |    0.02566 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09266 | -0.00504 |    0.04034 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07521 | -0.00077 |    0.03363 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07433 | -0.00217 |    0.02855 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08045 | -0.00290 |    0.03992 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07741 | -0.00213 |    0.02771 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07555 | -0.00283 |    0.03083 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06431 | -0.00047 |    0.02896 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05023 | -0.00119 |    0.01602 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03311 |  0.00034 |    0.00789 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56380 | -0.03104 |    0.33061 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:01:14,396 - Total sparsity: 74.94

2018-10-28 01:01:14,396 - --- validate (epoch=222)-----------
2018-10-28 01:01:14,396 - 10000 samples (128 per mini-batch)
2018-10-28 01:01:15,115 - Epoch: [222][   50/   78]    Loss 0.360532    Top1 90.015625    Top5 99.546875    
2018-10-28 01:01:15,505 - ==> Top1: 90.020    Top5: 99.640    Loss: 0.355

2018-10-28 01:01:15,505 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:01:15,505 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:01:15,516 - 

2018-10-28 01:01:15,516 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:01:16,687 - Epoch: [223][   50/  391]    Overall Loss 0.101596    Objective Loss 0.101596    Top1 96.593750    Top5 100.000000    LR 0.003000    Time 0.023379    
2018-10-28 01:01:17,799 - Epoch: [223][  100/  391]    Overall Loss 0.098640    Objective Loss 0.098640    Top1 96.648438    Top5 100.000000    LR 0.003000    Time 0.022796    
2018-10-28 01:01:18,910 - Epoch: [223][  150/  391]    Overall Loss 0.098094    Objective Loss 0.098094    Top1 96.630208    Top5 99.989583    LR 0.003000    Time 0.022598    
2018-10-28 01:01:20,021 - Epoch: [223][  200/  391]    Overall Loss 0.096212    Objective Loss 0.096212    Top1 96.667969    Top5 99.988281    LR 0.003000    Time 0.022494    
2018-10-28 01:01:21,132 - Epoch: [223][  250/  391]    Overall Loss 0.094217    Objective Loss 0.094217    Top1 96.731250    Top5 99.987500    LR 0.003000    Time 0.022437    
2018-10-28 01:01:22,242 - Epoch: [223][  300/  391]    Overall Loss 0.095433    Objective Loss 0.095433    Top1 96.643229    Top5 99.981771    LR 0.003000    Time 0.022394    
2018-10-28 01:01:23,352 - Epoch: [223][  350/  391]    Overall Loss 0.096719    Objective Loss 0.096719    Top1 96.647321    Top5 99.979911    LR 0.003000    Time 0.022362    
2018-10-28 01:01:24,344 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36841 |  0.00029 |    0.18583 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11712 | -0.00291 |    0.03711 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11300 | -0.00193 |    0.04022 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11691 | -0.00846 |    0.04975 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08441 | -0.00353 |    0.02448 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12431 | -0.00506 |    0.04851 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08241 |  0.00175 |    0.02254 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11537 | -0.00416 |    0.05477 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10789 | -0.00428 |    0.06147 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14039 | -0.00278 |    0.06764 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08803 | -0.00326 |    0.03498 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07188 |  0.00137 |    0.02564 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09257 | -0.00499 |    0.04029 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07513 | -0.00076 |    0.03360 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07425 | -0.00216 |    0.02851 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08037 | -0.00291 |    0.03988 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07733 | -0.00211 |    0.02767 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07547 | -0.00284 |    0.03079 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06425 | -0.00046 |    0.02893 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05017 | -0.00120 |    0.01600 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03307 |  0.00034 |    0.00788 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56424 | -0.03103 |    0.33085 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:01:24,344 - Total sparsity: 74.94

2018-10-28 01:01:24,344 - --- validate (epoch=223)-----------
2018-10-28 01:01:24,344 - 10000 samples (128 per mini-batch)
2018-10-28 01:01:25,059 - Epoch: [223][   50/   78]    Loss 0.360136    Top1 90.171875    Top5 99.609375    
2018-10-28 01:01:25,443 - ==> Top1: 90.050    Top5: 99.660    Loss: 0.357

2018-10-28 01:01:25,443 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:01:25,444 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:01:25,453 - 

2018-10-28 01:01:25,454 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:01:26,624 - Epoch: [224][   50/  391]    Overall Loss 0.098688    Objective Loss 0.098688    Top1 96.468750    Top5 99.984375    LR 0.003000    Time 0.023364    
2018-10-28 01:01:27,747 - Epoch: [224][  100/  391]    Overall Loss 0.097644    Objective Loss 0.097644    Top1 96.593750    Top5 99.976562    LR 0.003000    Time 0.022901    
2018-10-28 01:01:28,875 - Epoch: [224][  150/  391]    Overall Loss 0.094938    Objective Loss 0.094938    Top1 96.713542    Top5 99.979167    LR 0.003000    Time 0.022776    
2018-10-28 01:01:30,052 - Epoch: [224][  200/  391]    Overall Loss 0.094878    Objective Loss 0.094878    Top1 96.710938    Top5 99.980469    LR 0.003000    Time 0.022961    
2018-10-28 01:01:31,166 - Epoch: [224][  250/  391]    Overall Loss 0.092991    Objective Loss 0.092991    Top1 96.790625    Top5 99.984375    LR 0.003000    Time 0.022819    
2018-10-28 01:01:32,276 - Epoch: [224][  300/  391]    Overall Loss 0.093668    Objective Loss 0.093668    Top1 96.744792    Top5 99.986979    LR 0.003000    Time 0.022712    
2018-10-28 01:01:33,386 - Epoch: [224][  350/  391]    Overall Loss 0.093325    Objective Loss 0.093325    Top1 96.745536    Top5 99.984375    LR 0.003000    Time 0.022635    
2018-10-28 01:01:34,376 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36802 |  0.00005 |    0.18566 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11700 | -0.00292 |    0.03705 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11288 | -0.00192 |    0.04017 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11678 | -0.00841 |    0.04966 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08432 | -0.00352 |    0.02444 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12418 | -0.00499 |    0.04845 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08233 |  0.00177 |    0.02251 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11525 | -0.00414 |    0.05469 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10777 | -0.00428 |    0.06139 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14023 | -0.00278 |    0.06760 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08793 | -0.00326 |    0.03494 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07181 |  0.00137 |    0.02561 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09247 | -0.00498 |    0.04024 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07505 | -0.00077 |    0.03356 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07417 | -0.00216 |    0.02848 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08028 | -0.00290 |    0.03984 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07724 | -0.00209 |    0.02763 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07539 | -0.00282 |    0.03076 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06418 | -0.00045 |    0.02890 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05012 | -0.00120 |    0.01598 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03304 |  0.00034 |    0.00787 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56475 | -0.03101 |    0.33110 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:01:34,377 - Total sparsity: 74.94

2018-10-28 01:01:34,377 - --- validate (epoch=224)-----------
2018-10-28 01:01:34,377 - 10000 samples (128 per mini-batch)
2018-10-28 01:01:35,103 - Epoch: [224][   50/   78]    Loss 0.359918    Top1 90.125000    Top5 99.593750    
2018-10-28 01:01:35,496 - ==> Top1: 90.180    Top5: 99.660    Loss: 0.356

2018-10-28 01:01:35,497 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:01:35,497 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:01:35,507 - 

2018-10-28 01:01:35,508 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:01:36,678 - Epoch: [225][   50/  391]    Overall Loss 0.085079    Objective Loss 0.085079    Top1 97.140625    Top5 99.921875    LR 0.003000    Time 0.023376    
2018-10-28 01:01:37,790 - Epoch: [225][  100/  391]    Overall Loss 0.089563    Objective Loss 0.089563    Top1 96.953125    Top5 99.960938    LR 0.003000    Time 0.022791    
2018-10-28 01:01:38,900 - Epoch: [225][  150/  391]    Overall Loss 0.091232    Objective Loss 0.091232    Top1 96.848958    Top5 99.973958    LR 0.003000    Time 0.022588    
2018-10-28 01:01:40,011 - Epoch: [225][  200/  391]    Overall Loss 0.091544    Objective Loss 0.091544    Top1 96.851562    Top5 99.980469    LR 0.003000    Time 0.022488    
2018-10-28 01:01:41,120 - Epoch: [225][  250/  391]    Overall Loss 0.092384    Objective Loss 0.092384    Top1 96.803125    Top5 99.981250    LR 0.003000    Time 0.022421    
2018-10-28 01:01:42,231 - Epoch: [225][  300/  391]    Overall Loss 0.093678    Objective Loss 0.093678    Top1 96.760417    Top5 99.981771    LR 0.003000    Time 0.022385    
2018-10-28 01:01:43,345 - Epoch: [225][  350/  391]    Overall Loss 0.094445    Objective Loss 0.094445    Top1 96.723214    Top5 99.982143    LR 0.003000    Time 0.022365    
2018-10-28 01:01:44,338 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36762 |  0.00025 |    0.18530 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11687 | -0.00298 |    0.03700 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11276 | -0.00189 |    0.04013 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11666 | -0.00842 |    0.04964 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08423 | -0.00351 |    0.02440 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12404 | -0.00500 |    0.04837 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08224 |  0.00179 |    0.02250 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11512 | -0.00418 |    0.05464 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10766 | -0.00428 |    0.06133 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.14008 | -0.00281 |    0.06754 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08784 | -0.00323 |    0.03490 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07173 |  0.00137 |    0.02558 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09237 | -0.00497 |    0.04019 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07497 | -0.00075 |    0.03352 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07409 | -0.00218 |    0.02845 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08020 | -0.00290 |    0.03980 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07716 | -0.00213 |    0.02761 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07531 | -0.00282 |    0.03072 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06411 | -0.00045 |    0.02887 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05007 | -0.00120 |    0.01596 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03300 |  0.00034 |    0.00786 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56523 | -0.03101 |    0.33137 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:01:44,339 - Total sparsity: 74.94

2018-10-28 01:01:44,339 - --- validate (epoch=225)-----------
2018-10-28 01:01:44,339 - 10000 samples (128 per mini-batch)
2018-10-28 01:01:45,071 - Epoch: [225][   50/   78]    Loss 0.357505    Top1 90.359375    Top5 99.593750    
2018-10-28 01:01:45,465 - ==> Top1: 90.230    Top5: 99.670    Loss: 0.357

2018-10-28 01:01:45,466 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:01:45,466 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:01:45,477 - 

2018-10-28 01:01:45,477 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:01:46,650 - Epoch: [226][   50/  391]    Overall Loss 0.088420    Objective Loss 0.088420    Top1 97.078125    Top5 99.968750    LR 0.003000    Time 0.023420    
2018-10-28 01:01:47,772 - Epoch: [226][  100/  391]    Overall Loss 0.090515    Objective Loss 0.090515    Top1 96.937500    Top5 99.984375    LR 0.003000    Time 0.022918    
2018-10-28 01:01:48,890 - Epoch: [226][  150/  391]    Overall Loss 0.092685    Objective Loss 0.092685    Top1 96.854167    Top5 99.989583    LR 0.003000    Time 0.022723    
2018-10-28 01:01:50,011 - Epoch: [226][  200/  391]    Overall Loss 0.090990    Objective Loss 0.090990    Top1 96.890625    Top5 99.992188    LR 0.003000    Time 0.022636    
2018-10-28 01:01:51,155 - Epoch: [226][  250/  391]    Overall Loss 0.091548    Objective Loss 0.091548    Top1 96.887500    Top5 99.987500    LR 0.003000    Time 0.022683    
2018-10-28 01:01:52,271 - Epoch: [226][  300/  391]    Overall Loss 0.091267    Objective Loss 0.091267    Top1 96.906250    Top5 99.986979    LR 0.003000    Time 0.022616    
2018-10-28 01:01:53,383 - Epoch: [226][  350/  391]    Overall Loss 0.090829    Objective Loss 0.090829    Top1 96.933036    Top5 99.984375    LR 0.003000    Time 0.022559    
2018-10-28 01:01:54,379 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36722 |  0.00055 |    0.18509 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11675 | -0.00296 |    0.03695 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11263 | -0.00184 |    0.04008 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11653 | -0.00842 |    0.04956 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08414 | -0.00352 |    0.02436 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12391 | -0.00492 |    0.04830 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08215 |  0.00176 |    0.02248 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11500 | -0.00417 |    0.05457 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10754 | -0.00426 |    0.06126 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13993 | -0.00274 |    0.06747 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08775 | -0.00322 |    0.03486 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07165 |  0.00137 |    0.02555 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09227 | -0.00495 |    0.04014 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07489 | -0.00074 |    0.03348 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07401 | -0.00218 |    0.02842 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08011 | -0.00289 |    0.03976 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07708 | -0.00210 |    0.02758 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07523 | -0.00282 |    0.03068 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06404 | -0.00044 |    0.02884 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.05002 | -0.00119 |    0.01594 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03297 |  0.00034 |    0.00786 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56572 | -0.03102 |    0.33163 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:01:54,379 - Total sparsity: 74.94

2018-10-28 01:01:54,379 - --- validate (epoch=226)-----------
2018-10-28 01:01:54,379 - 10000 samples (128 per mini-batch)
2018-10-28 01:01:55,104 - Epoch: [226][   50/   78]    Loss 0.359508    Top1 90.328125    Top5 99.562500    
2018-10-28 01:01:55,495 - ==> Top1: 90.140    Top5: 99.640    Loss: 0.359

2018-10-28 01:01:55,496 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:01:55,496 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:01:55,506 - 

2018-10-28 01:01:55,507 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:01:56,678 - Epoch: [227][   50/  391]    Overall Loss 0.091508    Objective Loss 0.091508    Top1 96.953125    Top5 99.984375    LR 0.003000    Time 0.023399    
2018-10-28 01:01:57,790 - Epoch: [227][  100/  391]    Overall Loss 0.087804    Objective Loss 0.087804    Top1 97.070312    Top5 99.984375    LR 0.003000    Time 0.022803    
2018-10-28 01:01:58,902 - Epoch: [227][  150/  391]    Overall Loss 0.087312    Objective Loss 0.087312    Top1 97.093750    Top5 99.989583    LR 0.003000    Time 0.022605    
2018-10-28 01:02:00,014 - Epoch: [227][  200/  391]    Overall Loss 0.087611    Objective Loss 0.087611    Top1 97.125000    Top5 99.988281    LR 0.003000    Time 0.022508    
2018-10-28 01:02:01,126 - Epoch: [227][  250/  391]    Overall Loss 0.089334    Objective Loss 0.089334    Top1 97.031250    Top5 99.990625    LR 0.003000    Time 0.022450    
2018-10-28 01:02:02,236 - Epoch: [227][  300/  391]    Overall Loss 0.090121    Objective Loss 0.090121    Top1 97.007812    Top5 99.992188    LR 0.003000    Time 0.022404    
2018-10-28 01:02:03,346 - Epoch: [227][  350/  391]    Overall Loss 0.089695    Objective Loss 0.089695    Top1 97.002232    Top5 99.991071    LR 0.003000    Time 0.022372    
2018-10-28 01:02:04,357 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36683 |  0.00054 |    0.18485 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11662 | -0.00297 |    0.03692 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11251 | -0.00184 |    0.04003 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11641 | -0.00843 |    0.04950 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08405 | -0.00353 |    0.02432 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12378 | -0.00488 |    0.04825 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08206 |  0.00170 |    0.02247 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11488 | -0.00413 |    0.05452 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10743 | -0.00422 |    0.06118 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13977 | -0.00277 |    0.06740 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08765 | -0.00321 |    0.03483 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07158 |  0.00137 |    0.02552 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09218 | -0.00493 |    0.04011 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07481 | -0.00074 |    0.03344 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07393 | -0.00217 |    0.02839 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.08003 | -0.00288 |    0.03972 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07699 | -0.00208 |    0.02756 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07515 | -0.00282 |    0.03065 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06398 | -0.00043 |    0.02880 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04996 | -0.00119 |    0.01592 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03293 |  0.00034 |    0.00785 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56622 | -0.03098 |    0.33192 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:02:04,357 - Total sparsity: 74.94

2018-10-28 01:02:04,357 - --- validate (epoch=227)-----------
2018-10-28 01:02:04,357 - 10000 samples (128 per mini-batch)
2018-10-28 01:02:05,083 - Epoch: [227][   50/   78]    Loss 0.361494    Top1 90.265625    Top5 99.546875    
2018-10-28 01:02:05,477 - ==> Top1: 90.100    Top5: 99.600    Loss: 0.357

2018-10-28 01:02:05,477 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:02:05,478 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:02:05,492 - 

2018-10-28 01:02:05,492 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:02:06,701 - Epoch: [228][   50/  391]    Overall Loss 0.090535    Objective Loss 0.090535    Top1 97.265625    Top5 99.984375    LR 0.003000    Time 0.024126    
2018-10-28 01:02:07,813 - Epoch: [228][  100/  391]    Overall Loss 0.087716    Objective Loss 0.087716    Top1 97.210938    Top5 99.992188    LR 0.003000    Time 0.023171    
2018-10-28 01:02:08,925 - Epoch: [228][  150/  391]    Overall Loss 0.090006    Objective Loss 0.090006    Top1 97.083333    Top5 99.989583    LR 0.003000    Time 0.022851    
2018-10-28 01:02:10,036 - Epoch: [228][  200/  391]    Overall Loss 0.090532    Objective Loss 0.090532    Top1 96.976562    Top5 99.992188    LR 0.003000    Time 0.022691    
2018-10-28 01:02:11,148 - Epoch: [228][  250/  391]    Overall Loss 0.091767    Objective Loss 0.091767    Top1 96.859375    Top5 99.987500    LR 0.003000    Time 0.022593    
2018-10-28 01:02:12,258 - Epoch: [228][  300/  391]    Overall Loss 0.090054    Objective Loss 0.090054    Top1 96.953125    Top5 99.986979    LR 0.003000    Time 0.022526    
2018-10-28 01:02:13,371 - Epoch: [228][  350/  391]    Overall Loss 0.091580    Objective Loss 0.091580    Top1 96.901786    Top5 99.986607    LR 0.003000    Time 0.022482    
2018-10-28 01:02:14,360 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36643 |  0.00063 |    0.18464 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11650 | -0.00289 |    0.03687 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11239 | -0.00179 |    0.04002 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11628 | -0.00842 |    0.04943 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08396 | -0.00349 |    0.02430 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12365 | -0.00484 |    0.04823 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08197 |  0.00172 |    0.02245 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11476 | -0.00409 |    0.05444 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10731 | -0.00422 |    0.06113 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13962 | -0.00273 |    0.06736 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08756 | -0.00320 |    0.03479 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07150 |  0.00137 |    0.02550 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09208 | -0.00490 |    0.04006 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07473 | -0.00072 |    0.03341 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07385 | -0.00217 |    0.02835 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07994 | -0.00288 |    0.03967 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07691 | -0.00205 |    0.02754 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07507 | -0.00283 |    0.03062 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06391 | -0.00043 |    0.02877 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04991 | -0.00119 |    0.01590 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03290 |  0.00034 |    0.00784 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56662 | -0.03098 |    0.33211 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:02:14,361 - Total sparsity: 74.94

2018-10-28 01:02:14,361 - --- validate (epoch=228)-----------
2018-10-28 01:02:14,361 - 10000 samples (128 per mini-batch)
2018-10-28 01:02:15,082 - Epoch: [228][   50/   78]    Loss 0.359568    Top1 90.359375    Top5 99.640625    
2018-10-28 01:02:15,472 - ==> Top1: 90.120    Top5: 99.670    Loss: 0.358

2018-10-28 01:02:15,473 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:02:15,473 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:02:15,483 - 

2018-10-28 01:02:15,484 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:02:16,655 - Epoch: [229][   50/  391]    Overall Loss 0.092005    Objective Loss 0.092005    Top1 96.734375    Top5 99.968750    LR 0.003000    Time 0.023385    
2018-10-28 01:02:17,764 - Epoch: [229][  100/  391]    Overall Loss 0.094150    Objective Loss 0.094150    Top1 96.835938    Top5 99.984375    LR 0.003000    Time 0.022775    
2018-10-28 01:02:18,874 - Epoch: [229][  150/  391]    Overall Loss 0.092771    Objective Loss 0.092771    Top1 96.854167    Top5 99.973958    LR 0.003000    Time 0.022574    
2018-10-28 01:02:19,985 - Epoch: [229][  200/  391]    Overall Loss 0.092562    Objective Loss 0.092562    Top1 96.875000    Top5 99.980469    LR 0.003000    Time 0.022458    
2018-10-28 01:02:21,097 - Epoch: [229][  250/  391]    Overall Loss 0.090710    Objective Loss 0.090710    Top1 96.934375    Top5 99.981250    LR 0.003000    Time 0.022409    
2018-10-28 01:02:22,209 - Epoch: [229][  300/  391]    Overall Loss 0.090577    Objective Loss 0.090577    Top1 96.927083    Top5 99.984375    LR 0.003000    Time 0.022379    
2018-10-28 01:02:23,320 - Epoch: [229][  350/  391]    Overall Loss 0.090505    Objective Loss 0.090505    Top1 96.946429    Top5 99.984375    LR 0.003000    Time 0.022353    
2018-10-28 01:02:24,311 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36604 |  0.00029 |    0.18438 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11637 | -0.00294 |    0.03682 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11227 | -0.00174 |    0.03996 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11616 | -0.00842 |    0.04941 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08387 | -0.00347 |    0.02427 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12352 | -0.00483 |    0.04816 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08189 |  0.00174 |    0.02243 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11464 | -0.00406 |    0.05438 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10720 | -0.00424 |    0.06106 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13947 | -0.00279 |    0.06732 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08747 | -0.00321 |    0.03476 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07143 |  0.00137 |    0.02547 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09198 | -0.00487 |    0.04003 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07466 | -0.00073 |    0.03338 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07377 | -0.00217 |    0.02832 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07986 | -0.00288 |    0.03963 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07682 | -0.00208 |    0.02752 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07499 | -0.00283 |    0.03059 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06384 | -0.00045 |    0.02874 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04986 | -0.00119 |    0.01589 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03286 |  0.00034 |    0.00783 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56708 | -0.03099 |    0.33236 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:02:24,311 - Total sparsity: 74.94

2018-10-28 01:02:24,311 - --- validate (epoch=229)-----------
2018-10-28 01:02:24,311 - 10000 samples (128 per mini-batch)
2018-10-28 01:02:25,035 - Epoch: [229][   50/   78]    Loss 0.361991    Top1 90.343750    Top5 99.671875    
2018-10-28 01:02:25,425 - ==> Top1: 90.180    Top5: 99.690    Loss: 0.360

2018-10-28 01:02:25,426 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:02:25,426 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:02:25,440 - 

2018-10-28 01:02:25,440 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:02:26,611 - Epoch: [230][   50/  391]    Overall Loss 0.084455    Objective Loss 0.084455    Top1 97.234375    Top5 99.984375    LR 0.003000    Time 0.023383    
2018-10-28 01:02:27,724 - Epoch: [230][  100/  391]    Overall Loss 0.086905    Objective Loss 0.086905    Top1 97.031250    Top5 99.976562    LR 0.003000    Time 0.022806    
2018-10-28 01:02:28,839 - Epoch: [230][  150/  391]    Overall Loss 0.087267    Objective Loss 0.087267    Top1 97.010417    Top5 99.979167    LR 0.003000    Time 0.022628    
2018-10-28 01:02:29,952 - Epoch: [230][  200/  391]    Overall Loss 0.087803    Objective Loss 0.087803    Top1 97.019531    Top5 99.980469    LR 0.003000    Time 0.022529    
2018-10-28 01:02:31,068 - Epoch: [230][  250/  391]    Overall Loss 0.088422    Objective Loss 0.088422    Top1 97.015625    Top5 99.978125    LR 0.003000    Time 0.022466    
2018-10-28 01:02:32,182 - Epoch: [230][  300/  391]    Overall Loss 0.089494    Objective Loss 0.089494    Top1 96.940104    Top5 99.973958    LR 0.003000    Time 0.022434    
2018-10-28 01:02:33,297 - Epoch: [230][  350/  391]    Overall Loss 0.089266    Objective Loss 0.089266    Top1 96.924107    Top5 99.973214    LR 0.003000    Time 0.022409    
2018-10-28 01:02:34,287 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36566 |  0.00026 |    0.18419 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11625 | -0.00292 |    0.03677 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11215 | -0.00183 |    0.03992 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11603 | -0.00845 |    0.04934 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08378 | -0.00348 |    0.02424 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12338 | -0.00485 |    0.04809 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08180 |  0.00175 |    0.02239 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11451 | -0.00407 |    0.05432 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10708 | -0.00423 |    0.06100 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13931 | -0.00288 |    0.06724 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08737 | -0.00320 |    0.03472 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07135 |  0.00136 |    0.02546 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09189 | -0.00486 |    0.03999 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07458 | -0.00074 |    0.03334 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07370 | -0.00217 |    0.02830 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07977 | -0.00287 |    0.03958 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07674 | -0.00206 |    0.02748 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07491 | -0.00283 |    0.03056 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06377 | -0.00046 |    0.02871 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04981 | -0.00119 |    0.01587 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03283 |  0.00034 |    0.00782 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56758 | -0.03095 |    0.33261 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:02:34,288 - Total sparsity: 74.94

2018-10-28 01:02:34,288 - --- validate (epoch=230)-----------
2018-10-28 01:02:34,288 - 10000 samples (128 per mini-batch)
2018-10-28 01:02:35,008 - Epoch: [230][   50/   78]    Loss 0.367221    Top1 90.203125    Top5 99.593750    
2018-10-28 01:02:35,401 - ==> Top1: 89.980    Top5: 99.660    Loss: 0.368

2018-10-28 01:02:35,401 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:02:35,401 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:02:35,412 - 

2018-10-28 01:02:35,412 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:02:36,584 - Epoch: [231][   50/  391]    Overall Loss 0.089469    Objective Loss 0.089469    Top1 96.953125    Top5 99.937500    LR 0.003000    Time 0.023402    
2018-10-28 01:02:37,698 - Epoch: [231][  100/  391]    Overall Loss 0.091230    Objective Loss 0.091230    Top1 96.906250    Top5 99.960938    LR 0.003000    Time 0.022823    
2018-10-28 01:02:38,811 - Epoch: [231][  150/  391]    Overall Loss 0.089422    Objective Loss 0.089422    Top1 96.921875    Top5 99.973958    LR 0.003000    Time 0.022629    
2018-10-28 01:02:39,924 - Epoch: [231][  200/  391]    Overall Loss 0.088904    Objective Loss 0.088904    Top1 96.980469    Top5 99.980469    LR 0.003000    Time 0.022529    
2018-10-28 01:02:41,036 - Epoch: [231][  250/  391]    Overall Loss 0.088201    Objective Loss 0.088201    Top1 96.996875    Top5 99.984375    LR 0.003000    Time 0.022468    
2018-10-28 01:02:42,148 - Epoch: [231][  300/  391]    Overall Loss 0.089018    Objective Loss 0.089018    Top1 96.971354    Top5 99.986979    LR 0.003000    Time 0.022425    
2018-10-28 01:02:43,260 - Epoch: [231][  350/  391]    Overall Loss 0.087509    Objective Loss 0.087509    Top1 97.033482    Top5 99.982143    LR 0.003000    Time 0.022396    
2018-10-28 01:02:44,254 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36526 |  0.00024 |    0.18394 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11612 | -0.00286 |    0.03671 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11203 | -0.00175 |    0.03987 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11591 | -0.00840 |    0.04928 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08369 | -0.00346 |    0.02421 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12325 | -0.00480 |    0.04801 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08171 |  0.00173 |    0.02237 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11439 | -0.00408 |    0.05427 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10697 | -0.00422 |    0.06094 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13916 | -0.00293 |    0.06720 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08728 | -0.00317 |    0.03468 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07127 |  0.00137 |    0.02543 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09179 | -0.00487 |    0.03996 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07450 | -0.00072 |    0.03331 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07362 | -0.00217 |    0.02826 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07969 | -0.00285 |    0.03954 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07666 | -0.00205 |    0.02744 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07483 | -0.00282 |    0.03053 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06371 | -0.00045 |    0.02868 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04975 | -0.00119 |    0.01586 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03279 |  0.00035 |    0.00782 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56802 | -0.03095 |    0.33287 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:02:44,254 - Total sparsity: 74.94

2018-10-28 01:02:44,255 - --- validate (epoch=231)-----------
2018-10-28 01:02:44,255 - 10000 samples (128 per mini-batch)
2018-10-28 01:02:44,978 - Epoch: [231][   50/   78]    Loss 0.365720    Top1 90.109375    Top5 99.625000    
2018-10-28 01:02:45,373 - ==> Top1: 90.080    Top5: 99.660    Loss: 0.363

2018-10-28 01:02:45,374 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:02:45,374 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:02:45,385 - 

2018-10-28 01:02:45,385 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:02:46,560 - Epoch: [232][   50/  391]    Overall Loss 0.082990    Objective Loss 0.082990    Top1 97.140625    Top5 99.984375    LR 0.003000    Time 0.023452    
2018-10-28 01:02:47,673 - Epoch: [232][  100/  391]    Overall Loss 0.085256    Objective Loss 0.085256    Top1 97.085938    Top5 99.984375    LR 0.003000    Time 0.022850    
2018-10-28 01:02:48,787 - Epoch: [232][  150/  391]    Overall Loss 0.084089    Objective Loss 0.084089    Top1 97.083333    Top5 99.984375    LR 0.003000    Time 0.022652    
2018-10-28 01:02:49,901 - Epoch: [232][  200/  391]    Overall Loss 0.084830    Objective Loss 0.084830    Top1 97.042969    Top5 99.984375    LR 0.003000    Time 0.022552    
2018-10-28 01:02:51,016 - Epoch: [232][  250/  391]    Overall Loss 0.085299    Objective Loss 0.085299    Top1 97.059375    Top5 99.978125    LR 0.003000    Time 0.022493    
2018-10-28 01:02:52,130 - Epoch: [232][  300/  391]    Overall Loss 0.085535    Objective Loss 0.085535    Top1 97.052083    Top5 99.976562    LR 0.003000    Time 0.022455    
2018-10-28 01:02:53,244 - Epoch: [232][  350/  391]    Overall Loss 0.086867    Objective Loss 0.086867    Top1 97.008929    Top5 99.977679    LR 0.003000    Time 0.022426    
2018-10-28 01:02:54,239 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36488 |  0.00023 |    0.18375 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11600 | -0.00290 |    0.03667 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11191 | -0.00175 |    0.03985 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11578 | -0.00849 |    0.04922 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08360 | -0.00348 |    0.02419 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12312 | -0.00477 |    0.04796 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08162 |  0.00173 |    0.02234 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11427 | -0.00407 |    0.05423 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10686 | -0.00418 |    0.06088 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13901 | -0.00290 |    0.06709 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08719 | -0.00312 |    0.03464 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07120 |  0.00136 |    0.02540 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09169 | -0.00486 |    0.03991 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07442 | -0.00073 |    0.03328 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07354 | -0.00215 |    0.02824 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07960 | -0.00284 |    0.03950 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07657 | -0.00206 |    0.02742 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07475 | -0.00281 |    0.03049 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06364 | -0.00044 |    0.02865 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04970 | -0.00119 |    0.01584 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03275 |  0.00035 |    0.00781 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56846 | -0.03093 |    0.33309 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:02:54,239 - Total sparsity: 74.94

2018-10-28 01:02:54,239 - --- validate (epoch=232)-----------
2018-10-28 01:02:54,239 - 10000 samples (128 per mini-batch)
2018-10-28 01:02:54,960 - Epoch: [232][   50/   78]    Loss 0.371320    Top1 89.906250    Top5 99.562500    
2018-10-28 01:02:55,352 - ==> Top1: 89.900    Top5: 99.620    Loss: 0.368

2018-10-28 01:02:55,353 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:02:55,353 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:02:55,369 - 

2018-10-28 01:02:55,370 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:02:56,513 - Epoch: [233][   50/  391]    Overall Loss 0.084925    Objective Loss 0.084925    Top1 97.062500    Top5 100.000000    LR 0.003000    Time 0.022834    
2018-10-28 01:02:57,625 - Epoch: [233][  100/  391]    Overall Loss 0.084443    Objective Loss 0.084443    Top1 97.070312    Top5 99.992188    LR 0.003000    Time 0.022523    
2018-10-28 01:02:58,736 - Epoch: [233][  150/  391]    Overall Loss 0.086304    Objective Loss 0.086304    Top1 97.052083    Top5 99.989583    LR 0.003000    Time 0.022414    
2018-10-28 01:02:59,849 - Epoch: [233][  200/  391]    Overall Loss 0.088848    Objective Loss 0.088848    Top1 96.949219    Top5 99.992188    LR 0.003000    Time 0.022368    
2018-10-28 01:03:00,962 - Epoch: [233][  250/  391]    Overall Loss 0.089797    Objective Loss 0.089797    Top1 96.881250    Top5 99.981250    LR 0.003000    Time 0.022342    
2018-10-28 01:03:02,074 - Epoch: [233][  300/  391]    Overall Loss 0.090183    Objective Loss 0.090183    Top1 96.867188    Top5 99.981771    LR 0.003000    Time 0.022319    
2018-10-28 01:03:03,185 - Epoch: [233][  350/  391]    Overall Loss 0.089978    Objective Loss 0.089978    Top1 96.888393    Top5 99.982143    LR 0.003000    Time 0.022302    
2018-10-28 01:03:04,177 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36449 | -0.00009 |    0.18356 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11588 | -0.00285 |    0.03663 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11179 | -0.00180 |    0.03978 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11566 | -0.00846 |    0.04917 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08351 | -0.00346 |    0.02416 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12299 | -0.00473 |    0.04790 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08154 |  0.00173 |    0.02231 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11415 | -0.00405 |    0.05417 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10675 | -0.00418 |    0.06082 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13886 | -0.00292 |    0.06699 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08710 | -0.00313 |    0.03460 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07112 |  0.00139 |    0.02537 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09160 | -0.00485 |    0.03985 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07434 | -0.00073 |    0.03325 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07346 | -0.00215 |    0.02821 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07952 | -0.00283 |    0.03946 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07649 | -0.00206 |    0.02739 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07467 | -0.00280 |    0.03046 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06357 | -0.00044 |    0.02862 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04965 | -0.00119 |    0.01582 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03272 |  0.00035 |    0.00780 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56884 | -0.03093 |    0.33329 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:03:04,177 - Total sparsity: 74.94

2018-10-28 01:03:04,177 - --- validate (epoch=233)-----------
2018-10-28 01:03:04,177 - 10000 samples (128 per mini-batch)
2018-10-28 01:03:04,905 - Epoch: [233][   50/   78]    Loss 0.363770    Top1 90.171875    Top5 99.609375    
2018-10-28 01:03:05,298 - ==> Top1: 90.100    Top5: 99.640    Loss: 0.361

2018-10-28 01:03:05,299 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:03:05,299 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:03:05,310 - 

2018-10-28 01:03:05,310 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:03:06,483 - Epoch: [234][   50/  391]    Overall Loss 0.081308    Objective Loss 0.081308    Top1 97.296875    Top5 99.984375    LR 0.003000    Time 0.023419    
2018-10-28 01:03:07,594 - Epoch: [234][  100/  391]    Overall Loss 0.087173    Objective Loss 0.087173    Top1 97.039062    Top5 99.992188    LR 0.003000    Time 0.022811    
2018-10-28 01:03:08,705 - Epoch: [234][  150/  391]    Overall Loss 0.084421    Objective Loss 0.084421    Top1 97.140625    Top5 99.984375    LR 0.003000    Time 0.022605    
2018-10-28 01:03:09,813 - Epoch: [234][  200/  391]    Overall Loss 0.088155    Objective Loss 0.088155    Top1 96.933594    Top5 99.984375    LR 0.003000    Time 0.022487    
2018-10-28 01:03:10,924 - Epoch: [234][  250/  391]    Overall Loss 0.088642    Objective Loss 0.088642    Top1 96.943750    Top5 99.981250    LR 0.003000    Time 0.022428    
2018-10-28 01:03:12,035 - Epoch: [234][  300/  391]    Overall Loss 0.087196    Objective Loss 0.087196    Top1 97.000000    Top5 99.981771    LR 0.003000    Time 0.022390    
2018-10-28 01:03:13,146 - Epoch: [234][  350/  391]    Overall Loss 0.088178    Objective Loss 0.088178    Top1 96.930804    Top5 99.984375    LR 0.003000    Time 0.022362    
2018-10-28 01:03:14,135 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36410 | -0.00015 |    0.18323 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11575 | -0.00284 |    0.03660 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11167 | -0.00184 |    0.03973 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11554 | -0.00843 |    0.04911 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08342 | -0.00346 |    0.02414 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12286 | -0.00472 |    0.04787 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08145 |  0.00174 |    0.02228 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11402 | -0.00410 |    0.05411 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10663 | -0.00419 |    0.06076 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13871 | -0.00296 |    0.06688 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08701 | -0.00313 |    0.03456 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07105 |  0.00138 |    0.02535 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09150 | -0.00483 |    0.03981 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07426 | -0.00073 |    0.03321 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07338 | -0.00214 |    0.02817 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07944 | -0.00282 |    0.03941 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07641 | -0.00206 |    0.02735 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07459 | -0.00279 |    0.03042 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06351 | -0.00044 |    0.02859 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04960 | -0.00119 |    0.01580 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03268 |  0.00034 |    0.00779 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56924 | -0.03095 |    0.33347 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:03:14,135 - Total sparsity: 74.94

2018-10-28 01:03:14,135 - --- validate (epoch=234)-----------
2018-10-28 01:03:14,136 - 10000 samples (128 per mini-batch)
2018-10-28 01:03:14,854 - Epoch: [234][   50/   78]    Loss 0.367750    Top1 90.140625    Top5 99.609375    
2018-10-28 01:03:15,241 - ==> Top1: 90.050    Top5: 99.670    Loss: 0.366

2018-10-28 01:03:15,242 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:03:15,242 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:03:15,253 - 

2018-10-28 01:03:15,253 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:03:16,425 - Epoch: [235][   50/  391]    Overall Loss 0.087796    Objective Loss 0.087796    Top1 97.000000    Top5 99.984375    LR 0.003000    Time 0.023395    
2018-10-28 01:03:17,536 - Epoch: [235][  100/  391]    Overall Loss 0.084999    Objective Loss 0.084999    Top1 97.148438    Top5 99.984375    LR 0.003000    Time 0.022798    
2018-10-28 01:03:18,647 - Epoch: [235][  150/  391]    Overall Loss 0.087693    Objective Loss 0.087693    Top1 97.020833    Top5 99.979167    LR 0.003000    Time 0.022598    
2018-10-28 01:03:19,758 - Epoch: [235][  200/  391]    Overall Loss 0.088713    Objective Loss 0.088713    Top1 96.992188    Top5 99.976562    LR 0.003000    Time 0.022497    
2018-10-28 01:03:20,871 - Epoch: [235][  250/  391]    Overall Loss 0.089375    Objective Loss 0.089375    Top1 96.968750    Top5 99.971875    LR 0.003000    Time 0.022428    
2018-10-28 01:03:21,984 - Epoch: [235][  300/  391]    Overall Loss 0.087470    Objective Loss 0.087470    Top1 97.046875    Top5 99.973958    LR 0.003000    Time 0.022395    
2018-10-28 01:03:23,097 - Epoch: [235][  350/  391]    Overall Loss 0.086916    Objective Loss 0.086916    Top1 97.080357    Top5 99.975446    LR 0.003000    Time 0.022372    
2018-10-28 01:03:24,091 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36370 |  0.00050 |    0.18307 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11563 | -0.00285 |    0.03654 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11155 | -0.00186 |    0.03968 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11541 | -0.00843 |    0.04905 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08333 | -0.00346 |    0.02411 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12273 | -0.00473 |    0.04780 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08136 |  0.00173 |    0.02226 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11390 | -0.00409 |    0.05404 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10652 | -0.00412 |    0.06068 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13856 | -0.00289 |    0.06678 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08691 | -0.00313 |    0.03453 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07097 |  0.00137 |    0.02532 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09141 | -0.00485 |    0.03976 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07418 | -0.00070 |    0.03318 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07330 | -0.00215 |    0.02814 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07935 | -0.00282 |    0.03937 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07632 | -0.00204 |    0.02732 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07452 | -0.00278 |    0.03039 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06344 | -0.00045 |    0.02856 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04955 | -0.00119 |    0.01579 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03265 |  0.00034 |    0.00778 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.56967 | -0.03090 |    0.33371 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:03:24,091 - Total sparsity: 74.94

2018-10-28 01:03:24,092 - --- validate (epoch=235)-----------
2018-10-28 01:03:24,092 - 10000 samples (128 per mini-batch)
2018-10-28 01:03:24,816 - Epoch: [235][   50/   78]    Loss 0.376531    Top1 90.125000    Top5 99.640625    
2018-10-28 01:03:25,209 - ==> Top1: 89.960    Top5: 99.700    Loss: 0.373

2018-10-28 01:03:25,210 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:03:25,210 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:03:25,219 - 

2018-10-28 01:03:25,219 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:03:26,390 - Epoch: [236][   50/  391]    Overall Loss 0.090434    Objective Loss 0.090434    Top1 96.703125    Top5 99.984375    LR 0.003000    Time 0.023385    
2018-10-28 01:03:27,502 - Epoch: [236][  100/  391]    Overall Loss 0.092083    Objective Loss 0.092083    Top1 96.695312    Top5 99.984375    LR 0.003000    Time 0.022799    
2018-10-28 01:03:28,613 - Epoch: [236][  150/  391]    Overall Loss 0.091721    Objective Loss 0.091721    Top1 96.739583    Top5 99.984375    LR 0.003000    Time 0.022602    
2018-10-28 01:03:29,727 - Epoch: [236][  200/  391]    Overall Loss 0.091020    Objective Loss 0.091020    Top1 96.750000    Top5 99.988281    LR 0.003000    Time 0.022513    
2018-10-28 01:03:30,841 - Epoch: [236][  250/  391]    Overall Loss 0.090365    Objective Loss 0.090365    Top1 96.793750    Top5 99.990625    LR 0.003000    Time 0.022463    
2018-10-28 01:03:31,954 - Epoch: [236][  300/  391]    Overall Loss 0.089539    Objective Loss 0.089539    Top1 96.856771    Top5 99.992188    LR 0.003000    Time 0.022424    
2018-10-28 01:03:33,064 - Epoch: [236][  350/  391]    Overall Loss 0.088628    Objective Loss 0.088628    Top1 96.926339    Top5 99.993304    LR 0.003000    Time 0.022387    
2018-10-28 01:03:34,059 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36332 |  0.00004 |    0.18287 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11550 | -0.00292 |    0.03648 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11143 | -0.00185 |    0.03965 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11529 | -0.00848 |    0.04900 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08324 | -0.00342 |    0.02408 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12260 | -0.00470 |    0.04778 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08127 |  0.00177 |    0.02219 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11378 | -0.00410 |    0.05398 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10641 | -0.00416 |    0.06063 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13840 | -0.00301 |    0.06676 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08682 | -0.00311 |    0.03449 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07090 |  0.00135 |    0.02530 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09131 | -0.00486 |    0.03972 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07411 | -0.00069 |    0.03314 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07323 | -0.00215 |    0.02811 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07927 | -0.00281 |    0.03933 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07624 | -0.00205 |    0.02729 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07444 | -0.00277 |    0.03036 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06337 | -0.00045 |    0.02853 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04950 | -0.00119 |    0.01577 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03261 |  0.00035 |    0.00777 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57009 | -0.03091 |    0.33393 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:03:34,059 - Total sparsity: 74.94

2018-10-28 01:03:34,059 - --- validate (epoch=236)-----------
2018-10-28 01:03:34,059 - 10000 samples (128 per mini-batch)
2018-10-28 01:03:34,783 - Epoch: [236][   50/   78]    Loss 0.375512    Top1 90.125000    Top5 99.625000    
2018-10-28 01:03:35,175 - ==> Top1: 90.020    Top5: 99.660    Loss: 0.373

2018-10-28 01:03:35,176 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:03:35,176 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:03:35,192 - 

2018-10-28 01:03:35,192 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:03:36,334 - Epoch: [237][   50/  391]    Overall Loss 0.089924    Objective Loss 0.089924    Top1 97.000000    Top5 100.000000    LR 0.003000    Time 0.022804    
2018-10-28 01:03:37,446 - Epoch: [237][  100/  391]    Overall Loss 0.086502    Objective Loss 0.086502    Top1 97.054688    Top5 99.984375    LR 0.003000    Time 0.022504    
2018-10-28 01:03:38,558 - Epoch: [237][  150/  391]    Overall Loss 0.089201    Objective Loss 0.089201    Top1 96.984375    Top5 99.979167    LR 0.003000    Time 0.022413    
2018-10-28 01:03:39,671 - Epoch: [237][  200/  391]    Overall Loss 0.088873    Objective Loss 0.088873    Top1 96.968750    Top5 99.976562    LR 0.003000    Time 0.022368    
2018-10-28 01:03:40,785 - Epoch: [237][  250/  391]    Overall Loss 0.088440    Objective Loss 0.088440    Top1 96.987500    Top5 99.981250    LR 0.003000    Time 0.022341    
2018-10-28 01:03:41,897 - Epoch: [237][  300/  391]    Overall Loss 0.088163    Objective Loss 0.088163    Top1 97.023438    Top5 99.981771    LR 0.003000    Time 0.022320    
2018-10-28 01:03:43,007 - Epoch: [237][  350/  391]    Overall Loss 0.087758    Objective Loss 0.087758    Top1 97.044643    Top5 99.979911    LR 0.003000    Time 0.022302    
2018-10-28 01:03:44,000 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36293 |  0.00038 |    0.18272 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11538 | -0.00293 |    0.03647 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11131 | -0.00181 |    0.03960 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11517 | -0.00840 |    0.04896 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08315 | -0.00340 |    0.02406 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12247 | -0.00466 |    0.04770 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08119 |  0.00176 |    0.02216 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11366 | -0.00406 |    0.05392 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10629 | -0.00416 |    0.06057 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13825 | -0.00306 |    0.06666 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08673 | -0.00311 |    0.03445 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07082 |  0.00136 |    0.02528 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09121 | -0.00485 |    0.03967 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07403 | -0.00068 |    0.03311 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07315 | -0.00214 |    0.02807 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07919 | -0.00278 |    0.03929 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07616 | -0.00205 |    0.02725 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07436 | -0.00276 |    0.03032 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06331 | -0.00044 |    0.02850 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04944 | -0.00118 |    0.01576 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03258 |  0.00035 |    0.00776 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57046 | -0.03090 |    0.33412 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:03:44,000 - Total sparsity: 74.94

2018-10-28 01:03:44,001 - --- validate (epoch=237)-----------
2018-10-28 01:03:44,001 - 10000 samples (128 per mini-batch)
2018-10-28 01:03:44,743 - Epoch: [237][   50/   78]    Loss 0.369406    Top1 90.359375    Top5 99.562500    
2018-10-28 01:03:45,136 - ==> Top1: 90.360    Top5: 99.620    Loss: 0.367

2018-10-28 01:03:45,136 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:03:45,137 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:03:45,151 - 

2018-10-28 01:03:45,151 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:03:46,319 - Epoch: [238][   50/  391]    Overall Loss 0.078444    Objective Loss 0.078444    Top1 97.171875    Top5 100.000000    LR 0.003000    Time 0.023317    
2018-10-28 01:03:47,428 - Epoch: [238][  100/  391]    Overall Loss 0.081877    Objective Loss 0.081877    Top1 97.109375    Top5 99.984375    LR 0.003000    Time 0.022738    
2018-10-28 01:03:48,538 - Epoch: [238][  150/  391]    Overall Loss 0.083341    Objective Loss 0.083341    Top1 97.125000    Top5 99.984375    LR 0.003000    Time 0.022551    
2018-10-28 01:03:49,649 - Epoch: [238][  200/  391]    Overall Loss 0.084584    Objective Loss 0.084584    Top1 97.132812    Top5 99.984375    LR 0.003000    Time 0.022460    
2018-10-28 01:03:50,763 - Epoch: [238][  250/  391]    Overall Loss 0.085382    Objective Loss 0.085382    Top1 97.103125    Top5 99.981250    LR 0.003000    Time 0.022420    
2018-10-28 01:03:51,874 - Epoch: [238][  300/  391]    Overall Loss 0.084204    Objective Loss 0.084204    Top1 97.130208    Top5 99.984375    LR 0.003000    Time 0.022384    
2018-10-28 01:03:52,988 - Epoch: [238][  350/  391]    Overall Loss 0.084386    Objective Loss 0.084386    Top1 97.100446    Top5 99.984375    LR 0.003000    Time 0.022365    
2018-10-28 01:03:53,980 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36254 |  0.00012 |    0.18245 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11526 | -0.00291 |    0.03645 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11119 | -0.00182 |    0.03956 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11505 | -0.00840 |    0.04890 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08306 | -0.00344 |    0.02405 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12234 | -0.00466 |    0.04764 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08110 |  0.00173 |    0.02214 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11354 | -0.00408 |    0.05386 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10618 | -0.00416 |    0.06051 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13810 | -0.00302 |    0.06665 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08664 | -0.00306 |    0.03442 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07075 |  0.00138 |    0.02525 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09112 | -0.00485 |    0.03963 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07395 | -0.00069 |    0.03308 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07307 | -0.00214 |    0.02804 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07910 | -0.00278 |    0.03925 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07607 | -0.00205 |    0.02722 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07428 | -0.00276 |    0.03029 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06324 | -0.00044 |    0.02847 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04939 | -0.00119 |    0.01574 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03254 |  0.00035 |    0.00775 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57091 | -0.03092 |    0.33440 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:03:53,980 - Total sparsity: 74.94

2018-10-28 01:03:53,981 - --- validate (epoch=238)-----------
2018-10-28 01:03:53,981 - 10000 samples (128 per mini-batch)
2018-10-28 01:03:54,703 - Epoch: [238][   50/   78]    Loss 0.375785    Top1 90.093750    Top5 99.562500    
2018-10-28 01:03:55,093 - ==> Top1: 90.090    Top5: 99.590    Loss: 0.373

2018-10-28 01:03:55,094 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:03:55,094 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:03:55,104 - 

2018-10-28 01:03:55,105 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:03:56,275 - Epoch: [239][   50/  391]    Overall Loss 0.076728    Objective Loss 0.076728    Top1 97.453125    Top5 100.000000    LR 0.003000    Time 0.023371    
2018-10-28 01:03:57,386 - Epoch: [239][  100/  391]    Overall Loss 0.083745    Objective Loss 0.083745    Top1 97.062500    Top5 100.000000    LR 0.003000    Time 0.022788    
2018-10-28 01:03:58,498 - Epoch: [239][  150/  391]    Overall Loss 0.083975    Objective Loss 0.083975    Top1 97.119792    Top5 99.994792    LR 0.003000    Time 0.022592    
2018-10-28 01:03:59,613 - Epoch: [239][  200/  391]    Overall Loss 0.085166    Objective Loss 0.085166    Top1 97.054688    Top5 99.996094    LR 0.003000    Time 0.022496    
2018-10-28 01:04:00,727 - Epoch: [239][  250/  391]    Overall Loss 0.084247    Objective Loss 0.084247    Top1 97.159375    Top5 99.993750    LR 0.003000    Time 0.022447    
2018-10-28 01:04:01,839 - Epoch: [239][  300/  391]    Overall Loss 0.085383    Objective Loss 0.085383    Top1 97.117188    Top5 99.992188    LR 0.003000    Time 0.022410    
2018-10-28 01:04:02,950 - Epoch: [239][  350/  391]    Overall Loss 0.084645    Objective Loss 0.084645    Top1 97.162946    Top5 99.991071    LR 0.003000    Time 0.022379    
2018-10-28 01:04:03,962 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36216 |  0.00058 |    0.18220 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11513 | -0.00287 |    0.03642 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11107 | -0.00187 |    0.03951 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11493 | -0.00830 |    0.04883 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08297 | -0.00346 |    0.02401 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12222 | -0.00461 |    0.04757 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08101 |  0.00173 |    0.02213 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11342 | -0.00406 |    0.05381 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10607 | -0.00417 |    0.06044 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13795 | -0.00308 |    0.06659 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08655 | -0.00306 |    0.03438 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07067 |  0.00137 |    0.02523 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09102 | -0.00484 |    0.03960 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07387 | -0.00068 |    0.03306 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07299 | -0.00212 |    0.02801 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07902 | -0.00278 |    0.03921 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07599 | -0.00203 |    0.02719 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07420 | -0.00276 |    0.03026 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06318 | -0.00043 |    0.02844 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04934 | -0.00119 |    0.01572 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03251 |  0.00035 |    0.00775 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57135 | -0.03092 |    0.33463 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:04:03,962 - Total sparsity: 74.94

2018-10-28 01:04:03,963 - --- validate (epoch=239)-----------
2018-10-28 01:04:03,963 - 10000 samples (128 per mini-batch)
2018-10-28 01:04:04,720 - Epoch: [239][   50/   78]    Loss 0.369305    Top1 90.312500    Top5 99.578125    
2018-10-28 01:04:05,120 - ==> Top1: 90.210    Top5: 99.630    Loss: 0.366

2018-10-28 01:04:05,120 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:04:05,120 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:04:05,131 - 

2018-10-28 01:04:05,132 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:04:06,326 - Epoch: [240][   50/  391]    Overall Loss 0.092158    Objective Loss 0.092158    Top1 96.703125    Top5 99.984375    LR 0.003000    Time 0.023843    
2018-10-28 01:04:07,444 - Epoch: [240][  100/  391]    Overall Loss 0.091220    Objective Loss 0.091220    Top1 96.812500    Top5 99.992188    LR 0.003000    Time 0.023095    
2018-10-28 01:04:08,561 - Epoch: [240][  150/  391]    Overall Loss 0.090361    Objective Loss 0.090361    Top1 96.796875    Top5 99.989583    LR 0.003000    Time 0.022833    
2018-10-28 01:04:09,679 - Epoch: [240][  200/  391]    Overall Loss 0.088394    Objective Loss 0.088394    Top1 96.949219    Top5 99.992188    LR 0.003000    Time 0.022706    
2018-10-28 01:04:10,831 - Epoch: [240][  250/  391]    Overall Loss 0.088257    Objective Loss 0.088257    Top1 96.968750    Top5 99.990625    LR 0.003000    Time 0.022751    
2018-10-28 01:04:12,004 - Epoch: [240][  300/  391]    Overall Loss 0.086493    Objective Loss 0.086493    Top1 97.036458    Top5 99.992188    LR 0.003000    Time 0.022864    
2018-10-28 01:04:13,136 - Epoch: [240][  350/  391]    Overall Loss 0.087628    Objective Loss 0.087628    Top1 97.031250    Top5 99.986607    LR 0.003000    Time 0.022829    
2018-10-28 01:04:14,159 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36178 |  0.00054 |    0.18207 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11501 | -0.00284 |    0.03635 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11095 | -0.00185 |    0.03945 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11480 | -0.00841 |    0.04879 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08289 | -0.00347 |    0.02398 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12209 | -0.00466 |    0.04753 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08093 |  0.00173 |    0.02209 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11330 | -0.00407 |    0.05374 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10596 | -0.00416 |    0.06038 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13780 | -0.00315 |    0.06649 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08646 | -0.00305 |    0.03434 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07060 |  0.00138 |    0.02522 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09093 | -0.00482 |    0.03955 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07380 | -0.00069 |    0.03301 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07292 | -0.00211 |    0.02798 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07894 | -0.00276 |    0.03916 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07591 | -0.00201 |    0.02716 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07413 | -0.00276 |    0.03023 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06311 | -0.00042 |    0.02841 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04929 | -0.00120 |    0.01571 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03247 |  0.00035 |    0.00774 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57169 | -0.03092 |    0.33479 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:04:14,159 - Total sparsity: 74.94

2018-10-28 01:04:14,159 - --- validate (epoch=240)-----------
2018-10-28 01:04:14,159 - 10000 samples (128 per mini-batch)
2018-10-28 01:04:14,913 - Epoch: [240][   50/   78]    Loss 0.372737    Top1 90.343750    Top5 99.562500    
2018-10-28 01:04:15,309 - ==> Top1: 90.180    Top5: 99.630    Loss: 0.368

2018-10-28 01:04:15,310 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:04:15,310 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:04:15,326 - 

2018-10-28 01:04:15,326 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:04:16,483 - Epoch: [241][   50/  391]    Overall Loss 0.083291    Objective Loss 0.083291    Top1 97.187500    Top5 99.984375    LR 0.003000    Time 0.023110    
2018-10-28 01:04:17,594 - Epoch: [241][  100/  391]    Overall Loss 0.083680    Objective Loss 0.083680    Top1 97.148438    Top5 99.984375    LR 0.003000    Time 0.022647    
2018-10-28 01:04:18,705 - Epoch: [241][  150/  391]    Overall Loss 0.085599    Objective Loss 0.085599    Top1 97.046875    Top5 99.989583    LR 0.003000    Time 0.022493    
2018-10-28 01:04:19,818 - Epoch: [241][  200/  391]    Overall Loss 0.085556    Objective Loss 0.085556    Top1 97.019531    Top5 99.984375    LR 0.003000    Time 0.022431    
2018-10-28 01:04:20,927 - Epoch: [241][  250/  391]    Overall Loss 0.085907    Objective Loss 0.085907    Top1 97.034375    Top5 99.987500    LR 0.003000    Time 0.022374    
2018-10-28 01:04:22,036 - Epoch: [241][  300/  391]    Overall Loss 0.086691    Objective Loss 0.086691    Top1 96.979167    Top5 99.986979    LR 0.003000    Time 0.022338    
2018-10-28 01:04:23,145 - Epoch: [241][  350/  391]    Overall Loss 0.087115    Objective Loss 0.087115    Top1 97.002232    Top5 99.986607    LR 0.003000    Time 0.022312    
2018-10-28 01:04:24,135 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36140 |  0.00011 |    0.18186 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11489 | -0.00287 |    0.03631 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11084 | -0.00191 |    0.03941 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11468 | -0.00837 |    0.04872 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08280 | -0.00343 |    0.02395 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12196 | -0.00464 |    0.04750 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08084 |  0.00174 |    0.02208 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11318 | -0.00406 |    0.05369 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10585 | -0.00416 |    0.06032 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13765 | -0.00319 |    0.06644 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08637 | -0.00305 |    0.03430 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07053 |  0.00138 |    0.02518 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09084 | -0.00484 |    0.03950 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07372 | -0.00069 |    0.03298 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07284 | -0.00211 |    0.02795 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07886 | -0.00277 |    0.03913 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07583 | -0.00202 |    0.02713 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07405 | -0.00275 |    0.03019 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06305 | -0.00043 |    0.02838 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04924 | -0.00119 |    0.01569 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03244 |  0.00035 |    0.00773 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57206 | -0.03092 |    0.33497 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:04:24,135 - Total sparsity: 74.94

2018-10-28 01:04:24,135 - --- validate (epoch=241)-----------
2018-10-28 01:04:24,135 - 10000 samples (128 per mini-batch)
2018-10-28 01:04:24,844 - Epoch: [241][   50/   78]    Loss 0.372246    Top1 90.062500    Top5 99.625000    
2018-10-28 01:04:25,235 - ==> Top1: 90.120    Top5: 99.650    Loss: 0.366

2018-10-28 01:04:25,236 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:04:25,236 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:04:25,247 - 

2018-10-28 01:04:25,247 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:04:26,417 - Epoch: [242][   50/  391]    Overall Loss 0.087133    Objective Loss 0.087133    Top1 96.812500    Top5 99.968750    LR 0.003000    Time 0.023359    
2018-10-28 01:04:27,530 - Epoch: [242][  100/  391]    Overall Loss 0.087916    Objective Loss 0.087916    Top1 96.796875    Top5 99.960938    LR 0.003000    Time 0.022790    
2018-10-28 01:04:28,640 - Epoch: [242][  150/  391]    Overall Loss 0.086965    Objective Loss 0.086965    Top1 96.802083    Top5 99.973958    LR 0.003000    Time 0.022588    
2018-10-28 01:04:29,752 - Epoch: [242][  200/  391]    Overall Loss 0.087473    Objective Loss 0.087473    Top1 96.808594    Top5 99.980469    LR 0.003000    Time 0.022477    
2018-10-28 01:04:30,865 - Epoch: [242][  250/  391]    Overall Loss 0.086448    Objective Loss 0.086448    Top1 96.837500    Top5 99.984375    LR 0.003000    Time 0.022427    
2018-10-28 01:04:31,977 - Epoch: [242][  300/  391]    Overall Loss 0.085356    Objective Loss 0.085356    Top1 96.851562    Top5 99.984375    LR 0.003000    Time 0.022394    
2018-10-28 01:04:33,089 - Epoch: [242][  350/  391]    Overall Loss 0.085484    Objective Loss 0.085484    Top1 96.881696    Top5 99.984375    LR 0.003000    Time 0.022368    
2018-10-28 01:04:34,082 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36102 |  0.00035 |    0.18171 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11477 | -0.00291 |    0.03629 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11072 | -0.00187 |    0.03937 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11457 | -0.00833 |    0.04869 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08271 | -0.00343 |    0.02392 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12183 | -0.00461 |    0.04740 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08076 |  0.00173 |    0.02206 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11306 | -0.00404 |    0.05363 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10573 | -0.00415 |    0.06026 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13750 | -0.00309 |    0.06630 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08628 | -0.00303 |    0.03427 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07045 |  0.00138 |    0.02514 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09074 | -0.00484 |    0.03947 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07364 | -0.00069 |    0.03294 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07276 | -0.00212 |    0.02793 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07877 | -0.00277 |    0.03908 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07575 | -0.00202 |    0.02712 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07397 | -0.00274 |    0.03017 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06298 | -0.00044 |    0.02834 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04919 | -0.00119 |    0.01567 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03241 |  0.00035 |    0.00773 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57247 | -0.03094 |    0.33520 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:04:34,082 - Total sparsity: 74.94

2018-10-28 01:04:34,082 - --- validate (epoch=242)-----------
2018-10-28 01:04:34,082 - 10000 samples (128 per mini-batch)
2018-10-28 01:04:34,809 - Epoch: [242][   50/   78]    Loss 0.378069    Top1 89.968750    Top5 99.593750    
2018-10-28 01:04:35,201 - ==> Top1: 89.850    Top5: 99.650    Loss: 0.372

2018-10-28 01:04:35,202 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:04:35,202 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:04:35,216 - 

2018-10-28 01:04:35,216 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:04:36,386 - Epoch: [243][   50/  391]    Overall Loss 0.081310    Objective Loss 0.081310    Top1 97.203125    Top5 100.000000    LR 0.003000    Time 0.023353    
2018-10-28 01:04:37,496 - Epoch: [243][  100/  391]    Overall Loss 0.082935    Objective Loss 0.082935    Top1 97.203125    Top5 99.992188    LR 0.003000    Time 0.022765    
2018-10-28 01:04:38,606 - Epoch: [243][  150/  391]    Overall Loss 0.081670    Objective Loss 0.081670    Top1 97.291667    Top5 99.989583    LR 0.003000    Time 0.022569    
2018-10-28 01:04:39,715 - Epoch: [243][  200/  391]    Overall Loss 0.081130    Objective Loss 0.081130    Top1 97.269531    Top5 99.992188    LR 0.003000    Time 0.022466    
2018-10-28 01:04:40,825 - Epoch: [243][  250/  391]    Overall Loss 0.082445    Objective Loss 0.082445    Top1 97.206250    Top5 99.990625    LR 0.003000    Time 0.022408    
2018-10-28 01:04:41,934 - Epoch: [243][  300/  391]    Overall Loss 0.082926    Objective Loss 0.082926    Top1 97.171875    Top5 99.992188    LR 0.003000    Time 0.022365    
2018-10-28 01:04:43,043 - Epoch: [243][  350/  391]    Overall Loss 0.084175    Objective Loss 0.084175    Top1 97.116071    Top5 99.993304    LR 0.003000    Time 0.022334    
2018-10-28 01:04:44,035 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36063 |  0.00095 |    0.18160 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11465 | -0.00295 |    0.03624 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11060 | -0.00182 |    0.03931 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11444 | -0.00834 |    0.04863 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08262 | -0.00343 |    0.02388 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12170 | -0.00458 |    0.04735 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08067 |  0.00177 |    0.02204 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11294 | -0.00407 |    0.05356 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10562 | -0.00418 |    0.06021 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13735 | -0.00321 |    0.06625 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08619 | -0.00301 |    0.03422 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07038 |  0.00138 |    0.02512 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09065 | -0.00480 |    0.03943 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07357 | -0.00067 |    0.03291 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07269 | -0.00210 |    0.02790 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07869 | -0.00278 |    0.03904 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07566 | -0.00202 |    0.02709 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07389 | -0.00274 |    0.03014 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06292 | -0.00043 |    0.02831 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04914 | -0.00120 |    0.01565 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03237 |  0.00035 |    0.00772 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57281 | -0.03091 |    0.33538 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:04:44,036 - Total sparsity: 74.94

2018-10-28 01:04:44,036 - --- validate (epoch=243)-----------
2018-10-28 01:04:44,036 - 10000 samples (128 per mini-batch)
2018-10-28 01:04:44,760 - Epoch: [243][   50/   78]    Loss 0.374029    Top1 90.031250    Top5 99.578125    
2018-10-28 01:04:45,152 - ==> Top1: 89.960    Top5: 99.620    Loss: 0.370

2018-10-28 01:04:45,153 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:04:45,153 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:04:45,163 - 

2018-10-28 01:04:45,163 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:04:46,334 - Epoch: [244][   50/  391]    Overall Loss 0.084171    Objective Loss 0.084171    Top1 97.140625    Top5 99.984375    LR 0.003000    Time 0.023388    
2018-10-28 01:04:47,446 - Epoch: [244][  100/  391]    Overall Loss 0.084367    Objective Loss 0.084367    Top1 97.156250    Top5 99.976562    LR 0.003000    Time 0.022799    
2018-10-28 01:04:48,557 - Epoch: [244][  150/  391]    Overall Loss 0.083775    Objective Loss 0.083775    Top1 97.208333    Top5 99.984375    LR 0.003000    Time 0.022599    
2018-10-28 01:04:49,671 - Epoch: [244][  200/  391]    Overall Loss 0.083720    Objective Loss 0.083720    Top1 97.191406    Top5 99.984375    LR 0.003000    Time 0.022509    
2018-10-28 01:04:50,784 - Epoch: [244][  250/  391]    Overall Loss 0.084259    Objective Loss 0.084259    Top1 97.178125    Top5 99.984375    LR 0.003000    Time 0.022455    
2018-10-28 01:04:51,896 - Epoch: [244][  300/  391]    Overall Loss 0.084600    Objective Loss 0.084600    Top1 97.127604    Top5 99.986979    LR 0.003000    Time 0.022417    
2018-10-28 01:04:53,008 - Epoch: [244][  350/  391]    Overall Loss 0.085190    Objective Loss 0.085190    Top1 97.073661    Top5 99.988839    LR 0.003000    Time 0.022387    
2018-10-28 01:04:53,996 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.36025 |  0.00031 |    0.18133 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11452 | -0.00292 |    0.03622 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11048 | -0.00186 |    0.03929 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11433 | -0.00824 |    0.04859 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08254 | -0.00338 |    0.02386 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12157 | -0.00458 |    0.04731 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08059 |  0.00176 |    0.02200 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11282 | -0.00405 |    0.05349 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10551 | -0.00418 |    0.06014 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13720 | -0.00320 |    0.06618 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08610 | -0.00301 |    0.03418 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07030 |  0.00137 |    0.02509 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09056 | -0.00478 |    0.03938 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07349 | -0.00066 |    0.03287 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07261 | -0.00208 |    0.02786 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07861 | -0.00276 |    0.03900 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07558 | -0.00204 |    0.02707 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07382 | -0.00272 |    0.03010 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06285 | -0.00043 |    0.02829 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04909 | -0.00119 |    0.01563 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03234 |  0.00036 |    0.00771 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57316 | -0.03090 |    0.33561 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:04:53,996 - Total sparsity: 74.94

2018-10-28 01:04:53,996 - --- validate (epoch=244)-----------
2018-10-28 01:04:53,996 - 10000 samples (128 per mini-batch)
2018-10-28 01:04:54,707 - Epoch: [244][   50/   78]    Loss 0.379317    Top1 90.109375    Top5 99.562500    
2018-10-28 01:04:55,093 - ==> Top1: 89.950    Top5: 99.620    Loss: 0.377

2018-10-28 01:04:55,093 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:04:55,094 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:04:55,107 - 

2018-10-28 01:04:55,108 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:04:56,276 - Epoch: [245][   50/  391]    Overall Loss 0.078615    Objective Loss 0.078615    Top1 97.171875    Top5 100.000000    LR 0.003000    Time 0.023328    
2018-10-28 01:04:57,386 - Epoch: [245][  100/  391]    Overall Loss 0.083493    Objective Loss 0.083493    Top1 97.007812    Top5 100.000000    LR 0.003000    Time 0.022756    
2018-10-28 01:04:58,498 - Epoch: [245][  150/  391]    Overall Loss 0.085205    Objective Loss 0.085205    Top1 97.005208    Top5 100.000000    LR 0.003000    Time 0.022570    
2018-10-28 01:04:59,611 - Epoch: [245][  200/  391]    Overall Loss 0.085072    Objective Loss 0.085072    Top1 97.027344    Top5 99.996094    LR 0.003000    Time 0.022490    
2018-10-28 01:05:00,725 - Epoch: [245][  250/  391]    Overall Loss 0.084964    Objective Loss 0.084964    Top1 97.043750    Top5 99.996875    LR 0.003000    Time 0.022441    
2018-10-28 01:05:01,834 - Epoch: [245][  300/  391]    Overall Loss 0.084340    Objective Loss 0.084340    Top1 97.104167    Top5 99.994792    LR 0.003000    Time 0.022394    
2018-10-28 01:05:02,945 - Epoch: [245][  350/  391]    Overall Loss 0.083818    Objective Loss 0.083818    Top1 97.167411    Top5 99.995536    LR 0.003000    Time 0.022355    
2018-10-28 01:05:03,934 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35987 |  0.00057 |    0.18107 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11440 | -0.00292 |    0.03619 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11037 | -0.00185 |    0.03923 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11421 | -0.00829 |    0.04855 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08245 | -0.00339 |    0.02384 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12145 | -0.00460 |    0.04723 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08050 |  0.00171 |    0.02197 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11270 | -0.00405 |    0.05343 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10540 | -0.00417 |    0.06008 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13705 | -0.00322 |    0.06610 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08601 | -0.00302 |    0.03415 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07023 |  0.00137 |    0.02507 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09046 | -0.00477 |    0.03934 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07341 | -0.00064 |    0.03284 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07253 | -0.00209 |    0.02783 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07853 | -0.00275 |    0.03896 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07550 | -0.00202 |    0.02703 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07374 | -0.00273 |    0.03006 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06279 | -0.00043 |    0.02826 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04904 | -0.00118 |    0.01562 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03230 |  0.00036 |    0.00770 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57351 | -0.03087 |    0.33579 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:05:03,934 - Total sparsity: 74.94

2018-10-28 01:05:03,934 - --- validate (epoch=245)-----------
2018-10-28 01:05:03,934 - 10000 samples (128 per mini-batch)
2018-10-28 01:05:04,655 - Epoch: [245][   50/   78]    Loss 0.380898    Top1 90.000000    Top5 99.578125    
2018-10-28 01:05:05,046 - ==> Top1: 89.940    Top5: 99.630    Loss: 0.379

2018-10-28 01:05:05,047 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:05:05,047 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:05:05,065 - 

2018-10-28 01:05:05,065 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:05:06,205 - Epoch: [246][   50/  391]    Overall Loss 0.086466    Objective Loss 0.086466    Top1 96.937500    Top5 100.000000    LR 0.003000    Time 0.022759    
2018-10-28 01:05:07,314 - Epoch: [246][  100/  391]    Overall Loss 0.088643    Objective Loss 0.088643    Top1 96.882812    Top5 100.000000    LR 0.003000    Time 0.022459    
2018-10-28 01:05:08,422 - Epoch: [246][  150/  391]    Overall Loss 0.086663    Objective Loss 0.086663    Top1 96.989583    Top5 99.994792    LR 0.003000    Time 0.022355    
2018-10-28 01:05:09,532 - Epoch: [246][  200/  391]    Overall Loss 0.086277    Objective Loss 0.086277    Top1 97.000000    Top5 99.992188    LR 0.003000    Time 0.022309    
2018-10-28 01:05:10,643 - Epoch: [246][  250/  391]    Overall Loss 0.084853    Objective Loss 0.084853    Top1 97.071875    Top5 99.993750    LR 0.003000    Time 0.022284    
2018-10-28 01:05:11,752 - Epoch: [246][  300/  391]    Overall Loss 0.083914    Objective Loss 0.083914    Top1 97.135417    Top5 99.992188    LR 0.003000    Time 0.022264    
2018-10-28 01:05:12,864 - Epoch: [246][  350/  391]    Overall Loss 0.084583    Objective Loss 0.084583    Top1 97.095982    Top5 99.986607    LR 0.003000    Time 0.022256    
2018-10-28 01:05:13,851 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35949 | -0.00001 |    0.18070 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11428 | -0.00283 |    0.03615 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11025 | -0.00179 |    0.03921 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11409 | -0.00826 |    0.04847 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08237 | -0.00336 |    0.02380 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12132 | -0.00457 |    0.04718 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08042 |  0.00175 |    0.02195 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11259 | -0.00399 |    0.05339 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10529 | -0.00417 |    0.06001 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13691 | -0.00316 |    0.06598 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08592 | -0.00301 |    0.03411 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07016 |  0.00137 |    0.02504 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09037 | -0.00474 |    0.03930 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07334 | -0.00065 |    0.03281 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07246 | -0.00210 |    0.02781 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07845 | -0.00274 |    0.03892 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07542 | -0.00202 |    0.02699 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07366 | -0.00272 |    0.03003 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06272 | -0.00043 |    0.02824 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04899 | -0.00118 |    0.01560 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03227 |  0.00036 |    0.00769 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57385 | -0.03087 |    0.33599 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:05:13,852 - Total sparsity: 74.94

2018-10-28 01:05:13,852 - --- validate (epoch=246)-----------
2018-10-28 01:05:13,852 - 10000 samples (128 per mini-batch)
2018-10-28 01:05:14,571 - Epoch: [246][   50/   78]    Loss 0.374634    Top1 90.046875    Top5 99.656250    
2018-10-28 01:05:14,960 - ==> Top1: 89.860    Top5: 99.680    Loss: 0.373

2018-10-28 01:05:14,961 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:05:14,961 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:05:14,972 - 

2018-10-28 01:05:14,972 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:05:16,143 - Epoch: [247][   50/  391]    Overall Loss 0.074822    Objective Loss 0.074822    Top1 97.531250    Top5 100.000000    LR 0.003000    Time 0.023381    
2018-10-28 01:05:17,254 - Epoch: [247][  100/  391]    Overall Loss 0.076025    Objective Loss 0.076025    Top1 97.367188    Top5 99.992188    LR 0.003000    Time 0.022788    
2018-10-28 01:05:18,366 - Epoch: [247][  150/  391]    Overall Loss 0.080584    Objective Loss 0.080584    Top1 97.250000    Top5 99.989583    LR 0.003000    Time 0.022594    
2018-10-28 01:05:19,478 - Epoch: [247][  200/  391]    Overall Loss 0.082878    Objective Loss 0.082878    Top1 97.125000    Top5 99.992188    LR 0.003000    Time 0.022499    
2018-10-28 01:05:20,588 - Epoch: [247][  250/  391]    Overall Loss 0.083942    Objective Loss 0.083942    Top1 97.103125    Top5 99.993750    LR 0.003000    Time 0.022437    
2018-10-28 01:05:21,701 - Epoch: [247][  300/  391]    Overall Loss 0.084612    Objective Loss 0.084612    Top1 97.083333    Top5 99.992188    LR 0.003000    Time 0.022402    
2018-10-28 01:05:22,813 - Epoch: [247][  350/  391]    Overall Loss 0.084429    Objective Loss 0.084429    Top1 97.084821    Top5 99.993304    LR 0.003000    Time 0.022374    
2018-10-28 01:05:23,802 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35911 | -0.00003 |    0.18056 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11416 | -0.00284 |    0.03607 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11013 | -0.00183 |    0.03918 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11397 | -0.00824 |    0.04845 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08228 | -0.00333 |    0.02376 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12119 | -0.00460 |    0.04713 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08033 |  0.00176 |    0.02194 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11247 | -0.00395 |    0.05332 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10518 | -0.00414 |    0.05995 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13676 | -0.00316 |    0.06590 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08583 | -0.00297 |    0.03406 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07009 |  0.00135 |    0.02501 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09028 | -0.00473 |    0.03925 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07326 | -0.00065 |    0.03277 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07238 | -0.00210 |    0.02778 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07836 | -0.00273 |    0.03888 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07534 | -0.00199 |    0.02697 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07359 | -0.00269 |    0.03000 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06266 | -0.00043 |    0.02821 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04894 | -0.00118 |    0.01558 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03223 |  0.00036 |    0.00768 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57420 | -0.03084 |    0.33616 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:05:23,802 - Total sparsity: 74.94

2018-10-28 01:05:23,802 - --- validate (epoch=247)-----------
2018-10-28 01:05:23,802 - 10000 samples (128 per mini-batch)
2018-10-28 01:05:24,533 - Epoch: [247][   50/   78]    Loss 0.379993    Top1 89.828125    Top5 99.593750    
2018-10-28 01:05:24,924 - ==> Top1: 89.850    Top5: 99.660    Loss: 0.374

2018-10-28 01:05:24,925 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:05:24,925 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:05:24,935 - 

2018-10-28 01:05:24,935 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:05:26,106 - Epoch: [248][   50/  391]    Overall Loss 0.078542    Objective Loss 0.078542    Top1 97.187500    Top5 99.953125    LR 0.003000    Time 0.023377    
2018-10-28 01:05:27,218 - Epoch: [248][  100/  391]    Overall Loss 0.079062    Objective Loss 0.079062    Top1 97.195312    Top5 99.968750    LR 0.003000    Time 0.022789    
2018-10-28 01:05:28,330 - Epoch: [248][  150/  391]    Overall Loss 0.080960    Objective Loss 0.080960    Top1 97.140625    Top5 99.973958    LR 0.003000    Time 0.022602    
2018-10-28 01:05:29,442 - Epoch: [248][  200/  391]    Overall Loss 0.082205    Objective Loss 0.082205    Top1 97.156250    Top5 99.976562    LR 0.003000    Time 0.022505    
2018-10-28 01:05:30,555 - Epoch: [248][  250/  391]    Overall Loss 0.083141    Objective Loss 0.083141    Top1 97.125000    Top5 99.981250    LR 0.003000    Time 0.022449    
2018-10-28 01:05:31,666 - Epoch: [248][  300/  391]    Overall Loss 0.082885    Objective Loss 0.082885    Top1 97.130208    Top5 99.984375    LR 0.003000    Time 0.022407    
2018-10-28 01:05:32,778 - Epoch: [248][  350/  391]    Overall Loss 0.083340    Objective Loss 0.083340    Top1 97.145089    Top5 99.982143    LR 0.003000    Time 0.022381    
2018-10-28 01:05:33,769 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35874 |  0.00022 |    0.18047 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11404 | -0.00287 |    0.03605 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.11002 | -0.00182 |    0.03911 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11384 | -0.00834 |    0.04837 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08219 | -0.00333 |    0.02374 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12107 | -0.00452 |    0.04707 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08025 |  0.00179 |    0.02191 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11235 | -0.00397 |    0.05325 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10507 | -0.00412 |    0.05988 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13661 | -0.00314 |    0.06579 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08575 | -0.00295 |    0.03403 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.07001 |  0.00135 |    0.02498 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09019 | -0.00474 |    0.03920 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07319 | -0.00065 |    0.03274 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07230 | -0.00210 |    0.02775 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07828 | -0.00273 |    0.03884 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07526 | -0.00198 |    0.02695 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07351 | -0.00270 |    0.02996 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06260 | -0.00043 |    0.02818 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04889 | -0.00118 |    0.01557 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03220 |  0.00036 |    0.00768 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57454 | -0.03082 |    0.33636 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:05:33,769 - Total sparsity: 74.94

2018-10-28 01:05:33,769 - --- validate (epoch=248)-----------
2018-10-28 01:05:33,769 - 10000 samples (128 per mini-batch)
2018-10-28 01:05:34,491 - Epoch: [248][   50/   78]    Loss 0.379128    Top1 90.015625    Top5 99.640625    
2018-10-28 01:05:34,885 - ==> Top1: 90.000    Top5: 99.680    Loss: 0.374

2018-10-28 01:05:34,885 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:05:34,886 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:05:34,895 - 

2018-10-28 01:05:34,895 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:05:36,067 - Epoch: [249][   50/  391]    Overall Loss 0.082084    Objective Loss 0.082084    Top1 97.296875    Top5 99.984375    LR 0.003000    Time 0.023389    
2018-10-28 01:05:37,177 - Epoch: [249][  100/  391]    Overall Loss 0.085468    Objective Loss 0.085468    Top1 97.140625    Top5 99.992188    LR 0.003000    Time 0.022790    
2018-10-28 01:05:38,287 - Epoch: [249][  150/  391]    Overall Loss 0.083807    Objective Loss 0.083807    Top1 97.192708    Top5 99.989583    LR 0.003000    Time 0.022582    
2018-10-28 01:05:39,397 - Epoch: [249][  200/  391]    Overall Loss 0.084022    Objective Loss 0.084022    Top1 97.152344    Top5 99.992188    LR 0.003000    Time 0.022481    
2018-10-28 01:05:40,507 - Epoch: [249][  250/  391]    Overall Loss 0.083549    Objective Loss 0.083549    Top1 97.178125    Top5 99.987500    LR 0.003000    Time 0.022419    
2018-10-28 01:05:41,618 - Epoch: [249][  300/  391]    Overall Loss 0.082655    Objective Loss 0.082655    Top1 97.192708    Top5 99.984375    LR 0.003000    Time 0.022369    
2018-10-28 01:05:42,728 - Epoch: [249][  350/  391]    Overall Loss 0.081974    Objective Loss 0.081974    Top1 97.220982    Top5 99.984375    LR 0.003000    Time 0.022342    
2018-10-28 01:05:43,720 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35835 |  0.00092 |    0.18034 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11392 | -0.00284 |    0.03600 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10990 | -0.00182 |    0.03907 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11373 | -0.00829 |    0.04834 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08210 | -0.00334 |    0.02371 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12094 | -0.00452 |    0.04706 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08016 |  0.00175 |    0.02189 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11223 | -0.00392 |    0.05319 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10496 | -0.00413 |    0.05981 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13646 | -0.00312 |    0.06576 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08566 | -0.00294 |    0.03399 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06994 |  0.00134 |    0.02496 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09009 | -0.00474 |    0.03914 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07311 | -0.00059 |    0.03270 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07223 | -0.00209 |    0.02772 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07820 | -0.00273 |    0.03880 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07518 | -0.00200 |    0.02692 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07344 | -0.00268 |    0.02993 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06253 | -0.00044 |    0.02815 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04884 | -0.00118 |    0.01556 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03216 |  0.00036 |    0.00767 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57491 | -0.03080 |    0.33656 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:05:43,720 - Total sparsity: 74.94

2018-10-28 01:05:43,720 - --- validate (epoch=249)-----------
2018-10-28 01:05:43,720 - 10000 samples (128 per mini-batch)
2018-10-28 01:05:44,450 - Epoch: [249][   50/   78]    Loss 0.384969    Top1 89.968750    Top5 99.578125    
2018-10-28 01:05:44,843 - ==> Top1: 89.990    Top5: 99.630    Loss: 0.379

2018-10-28 01:05:44,844 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:05:44,844 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:05:44,858 - 

2018-10-28 01:05:44,858 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:05:46,030 - Epoch: [250][   50/  391]    Overall Loss 0.085748    Objective Loss 0.085748    Top1 96.906250    Top5 99.984375    LR 0.001500    Time 0.023404    
2018-10-28 01:05:47,140 - Epoch: [250][  100/  391]    Overall Loss 0.078880    Objective Loss 0.078880    Top1 97.187500    Top5 99.992188    LR 0.001500    Time 0.022788    
2018-10-28 01:05:48,248 - Epoch: [250][  150/  391]    Overall Loss 0.079652    Objective Loss 0.079652    Top1 97.213542    Top5 99.989583    LR 0.001500    Time 0.022567    
2018-10-28 01:05:49,359 - Epoch: [250][  200/  391]    Overall Loss 0.078826    Objective Loss 0.078826    Top1 97.312500    Top5 99.992188    LR 0.001500    Time 0.022473    
2018-10-28 01:05:50,471 - Epoch: [250][  250/  391]    Overall Loss 0.078462    Objective Loss 0.078462    Top1 97.359375    Top5 99.990625    LR 0.001500    Time 0.022422    
2018-10-28 01:05:51,581 - Epoch: [250][  300/  391]    Overall Loss 0.078060    Objective Loss 0.078060    Top1 97.401042    Top5 99.989583    LR 0.001500    Time 0.022382    
2018-10-28 01:05:52,692 - Epoch: [250][  350/  391]    Overall Loss 0.077689    Objective Loss 0.077689    Top1 97.424107    Top5 99.991071    LR 0.001500    Time 0.022355    
2018-10-28 01:05:53,681 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35815 |  0.00068 |    0.18022 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11386 | -0.00285 |    0.03598 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10984 | -0.00179 |    0.03905 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11367 | -0.00825 |    0.04831 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08206 | -0.00333 |    0.02370 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12087 | -0.00452 |    0.04702 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08012 |  0.00175 |    0.02187 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11217 | -0.00391 |    0.05315 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10490 | -0.00413 |    0.05977 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13639 | -0.00312 |    0.06574 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08561 | -0.00294 |    0.03397 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06990 |  0.00134 |    0.02494 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.09004 | -0.00473 |    0.03912 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07307 | -0.00061 |    0.03269 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07219 | -0.00208 |    0.02770 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07816 | -0.00272 |    0.03878 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07513 | -0.00200 |    0.02690 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07340 | -0.00267 |    0.02992 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06250 | -0.00044 |    0.02814 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04881 | -0.00119 |    0.01555 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03215 |  0.00036 |    0.00766 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57511 | -0.03080 |    0.33666 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:05:53,681 - Total sparsity: 74.94

2018-10-28 01:05:53,681 - --- validate (epoch=250)-----------
2018-10-28 01:05:53,682 - 10000 samples (128 per mini-batch)
2018-10-28 01:05:54,407 - Epoch: [250][   50/   78]    Loss 0.386493    Top1 89.843750    Top5 99.609375    
2018-10-28 01:05:54,799 - ==> Top1: 89.900    Top5: 99.650    Loss: 0.379

2018-10-28 01:05:54,799 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:05:54,800 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:05:54,816 - 

2018-10-28 01:05:54,816 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:05:55,957 - Epoch: [251][   50/  391]    Overall Loss 0.076366    Objective Loss 0.076366    Top1 97.328125    Top5 100.000000    LR 0.001500    Time 0.022790    
2018-10-28 01:05:57,067 - Epoch: [251][  100/  391]    Overall Loss 0.080831    Objective Loss 0.080831    Top1 97.179688    Top5 100.000000    LR 0.001500    Time 0.022484    
2018-10-28 01:05:58,179 - Epoch: [251][  150/  391]    Overall Loss 0.079983    Objective Loss 0.079983    Top1 97.244792    Top5 100.000000    LR 0.001500    Time 0.022390    
2018-10-28 01:05:59,289 - Epoch: [251][  200/  391]    Overall Loss 0.080306    Objective Loss 0.080306    Top1 97.210938    Top5 99.992188    LR 0.001500    Time 0.022342    
2018-10-28 01:06:00,401 - Epoch: [251][  250/  391]    Overall Loss 0.080726    Objective Loss 0.080726    Top1 97.209375    Top5 99.987500    LR 0.001500    Time 0.022314    
2018-10-28 01:06:01,512 - Epoch: [251][  300/  391]    Overall Loss 0.081308    Objective Loss 0.081308    Top1 97.203125    Top5 99.989583    LR 0.001500    Time 0.022293    
2018-10-28 01:06:02,623 - Epoch: [251][  350/  391]    Overall Loss 0.081077    Objective Loss 0.081077    Top1 97.196429    Top5 99.991071    LR 0.001500    Time 0.022280    
2018-10-28 01:06:03,612 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35796 |  0.00022 |    0.18003 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11380 | -0.00285 |    0.03598 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10978 | -0.00178 |    0.03903 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11360 | -0.00826 |    0.04827 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08201 | -0.00332 |    0.02369 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12080 | -0.00452 |    0.04701 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08007 |  0.00174 |    0.02185 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11211 | -0.00390 |    0.05313 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10485 | -0.00411 |    0.05975 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13631 | -0.00309 |    0.06568 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08556 | -0.00293 |    0.03395 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06986 |  0.00135 |    0.02493 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08999 | -0.00472 |    0.03909 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07303 | -0.00061 |    0.03267 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07215 | -0.00208 |    0.02768 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07812 | -0.00270 |    0.03876 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07509 | -0.00198 |    0.02689 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07336 | -0.00267 |    0.02990 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06246 | -0.00044 |    0.02812 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04878 | -0.00118 |    0.01554 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03213 |  0.00036 |    0.00766 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57526 | -0.03081 |    0.33674 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:06:03,612 - Total sparsity: 74.94

2018-10-28 01:06:03,613 - --- validate (epoch=251)-----------
2018-10-28 01:06:03,613 - 10000 samples (128 per mini-batch)
2018-10-28 01:06:04,383 - Epoch: [251][   50/   78]    Loss 0.386486    Top1 89.703125    Top5 99.609375    
2018-10-28 01:06:04,771 - ==> Top1: 89.720    Top5: 99.660    Loss: 0.382

2018-10-28 01:06:04,772 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:06:04,772 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:06:04,784 - 

2018-10-28 01:06:04,784 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:06:05,955 - Epoch: [252][   50/  391]    Overall Loss 0.076063    Objective Loss 0.076063    Top1 97.421875    Top5 100.000000    LR 0.001500    Time 0.023378    
2018-10-28 01:06:07,067 - Epoch: [252][  100/  391]    Overall Loss 0.076426    Objective Loss 0.076426    Top1 97.390625    Top5 99.984375    LR 0.001500    Time 0.022793    
2018-10-28 01:06:08,179 - Epoch: [252][  150/  391]    Overall Loss 0.077553    Objective Loss 0.077553    Top1 97.385417    Top5 99.989583    LR 0.001500    Time 0.022605    
2018-10-28 01:06:09,291 - Epoch: [252][  200/  391]    Overall Loss 0.077889    Objective Loss 0.077889    Top1 97.394531    Top5 99.988281    LR 0.001500    Time 0.022507    
2018-10-28 01:06:10,403 - Epoch: [252][  250/  391]    Overall Loss 0.078737    Objective Loss 0.078737    Top1 97.371875    Top5 99.987500    LR 0.001500    Time 0.022449    
2018-10-28 01:06:11,517 - Epoch: [252][  300/  391]    Overall Loss 0.079651    Objective Loss 0.079651    Top1 97.325521    Top5 99.989583    LR 0.001500    Time 0.022414    
2018-10-28 01:06:12,630 - Epoch: [252][  350/  391]    Overall Loss 0.079782    Objective Loss 0.079782    Top1 97.301339    Top5 99.991071    LR 0.001500    Time 0.022390    
2018-10-28 01:06:13,627 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35776 |  0.00037 |    0.17992 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11373 | -0.00285 |    0.03596 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10972 | -0.00178 |    0.03901 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11354 | -0.00827 |    0.04824 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08197 | -0.00331 |    0.02367 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12074 | -0.00451 |    0.04697 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.08003 |  0.00175 |    0.02183 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11205 | -0.00390 |    0.05309 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10479 | -0.00410 |    0.05971 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13623 | -0.00309 |    0.06564 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08551 | -0.00294 |    0.03393 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06982 |  0.00135 |    0.02491 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08995 | -0.00470 |    0.03907 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07299 | -0.00061 |    0.03265 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07211 | -0.00209 |    0.02766 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07807 | -0.00270 |    0.03874 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07505 | -0.00198 |    0.02688 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07332 | -0.00267 |    0.02988 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06243 | -0.00044 |    0.02810 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04876 | -0.00118 |    0.01553 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03211 |  0.00036 |    0.00766 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57545 | -0.03080 |    0.33684 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:06:13,627 - Total sparsity: 74.94

2018-10-28 01:06:13,627 - --- validate (epoch=252)-----------
2018-10-28 01:06:13,627 - 10000 samples (128 per mini-batch)
2018-10-28 01:06:14,355 - Epoch: [252][   50/   78]    Loss 0.384262    Top1 90.078125    Top5 99.546875    
2018-10-28 01:06:14,745 - ==> Top1: 90.080    Top5: 99.610    Loss: 0.379

2018-10-28 01:06:14,746 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:06:14,746 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:06:14,757 - 

2018-10-28 01:06:14,757 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:06:15,927 - Epoch: [253][   50/  391]    Overall Loss 0.077834    Objective Loss 0.077834    Top1 97.359375    Top5 99.984375    LR 0.001500    Time 0.023362    
2018-10-28 01:06:17,036 - Epoch: [253][  100/  391]    Overall Loss 0.076825    Objective Loss 0.076825    Top1 97.406250    Top5 99.984375    LR 0.001500    Time 0.022755    
2018-10-28 01:06:18,145 - Epoch: [253][  150/  391]    Overall Loss 0.079359    Objective Loss 0.079359    Top1 97.338542    Top5 99.989583    LR 0.001500    Time 0.022555    
2018-10-28 01:06:19,256 - Epoch: [253][  200/  391]    Overall Loss 0.079097    Objective Loss 0.079097    Top1 97.339844    Top5 99.992188    LR 0.001500    Time 0.022465    
2018-10-28 01:06:20,367 - Epoch: [253][  250/  391]    Overall Loss 0.077621    Objective Loss 0.077621    Top1 97.381250    Top5 99.993750    LR 0.001500    Time 0.022412    
2018-10-28 01:06:21,478 - Epoch: [253][  300/  391]    Overall Loss 0.078237    Objective Loss 0.078237    Top1 97.343750    Top5 99.992188    LR 0.001500    Time 0.022376    
2018-10-28 01:06:22,588 - Epoch: [253][  350/  391]    Overall Loss 0.078511    Objective Loss 0.078511    Top1 97.310268    Top5 99.993304    LR 0.001500    Time 0.022348    
2018-10-28 01:06:23,578 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35756 |  0.00039 |    0.17976 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11367 | -0.00284 |    0.03593 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10966 | -0.00180 |    0.03899 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11348 | -0.00825 |    0.04820 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08192 | -0.00333 |    0.02366 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12067 | -0.00446 |    0.04695 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07998 |  0.00173 |    0.02183 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11199 | -0.00389 |    0.05305 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10473 | -0.00408 |    0.05967 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13616 | -0.00312 |    0.06561 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08547 | -0.00295 |    0.03392 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06978 |  0.00136 |    0.02490 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08990 | -0.00471 |    0.03905 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07295 | -0.00059 |    0.03263 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07207 | -0.00209 |    0.02765 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07803 | -0.00270 |    0.03871 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07501 | -0.00199 |    0.02686 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07328 | -0.00267 |    0.02986 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06239 | -0.00044 |    0.02809 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04873 | -0.00118 |    0.01552 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03209 |  0.00036 |    0.00765 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57566 | -0.03079 |    0.33696 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:06:23,579 - Total sparsity: 74.94

2018-10-28 01:06:23,579 - --- validate (epoch=253)-----------
2018-10-28 01:06:23,579 - 10000 samples (128 per mini-batch)
2018-10-28 01:06:24,302 - Epoch: [253][   50/   78]    Loss 0.379925    Top1 90.062500    Top5 99.578125    
2018-10-28 01:06:24,695 - ==> Top1: 90.020    Top5: 99.620    Loss: 0.379

2018-10-28 01:06:24,696 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:06:24,696 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:06:24,707 - 

2018-10-28 01:06:24,707 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:06:25,879 - Epoch: [254][   50/  391]    Overall Loss 0.078389    Objective Loss 0.078389    Top1 97.359375    Top5 99.984375    LR 0.001500    Time 0.023386    
2018-10-28 01:06:26,990 - Epoch: [254][  100/  391]    Overall Loss 0.078002    Objective Loss 0.078002    Top1 97.382812    Top5 99.984375    LR 0.001500    Time 0.022789    
2018-10-28 01:06:28,100 - Epoch: [254][  150/  391]    Overall Loss 0.079513    Objective Loss 0.079513    Top1 97.302083    Top5 99.989583    LR 0.001500    Time 0.022589    
2018-10-28 01:06:29,210 - Epoch: [254][  200/  391]    Overall Loss 0.080409    Objective Loss 0.080409    Top1 97.261719    Top5 99.992188    LR 0.001500    Time 0.022486    
2018-10-28 01:06:30,322 - Epoch: [254][  250/  391]    Overall Loss 0.079496    Objective Loss 0.079496    Top1 97.268750    Top5 99.990625    LR 0.001500    Time 0.022428    
2018-10-28 01:06:31,431 - Epoch: [254][  300/  391]    Overall Loss 0.079040    Objective Loss 0.079040    Top1 97.296875    Top5 99.992188    LR 0.001500    Time 0.022384    
2018-10-28 01:06:32,541 - Epoch: [254][  350/  391]    Overall Loss 0.079448    Objective Loss 0.079448    Top1 97.254464    Top5 99.988839    LR 0.001500    Time 0.022354    
2018-10-28 01:06:33,530 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35736 |  0.00033 |    0.17968 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11361 | -0.00285 |    0.03591 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10959 | -0.00180 |    0.03897 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11341 | -0.00823 |    0.04817 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08188 | -0.00333 |    0.02365 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12061 | -0.00446 |    0.04693 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07994 |  0.00174 |    0.02182 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11192 | -0.00394 |    0.05303 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10467 | -0.00408 |    0.05963 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13608 | -0.00313 |    0.06556 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08542 | -0.00293 |    0.03390 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06975 |  0.00136 |    0.02488 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08985 | -0.00471 |    0.03903 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07291 | -0.00059 |    0.03261 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07203 | -0.00209 |    0.02764 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07799 | -0.00271 |    0.03869 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07496 | -0.00199 |    0.02685 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07323 | -0.00267 |    0.02984 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06236 | -0.00044 |    0.02807 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04870 | -0.00117 |    0.01551 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03207 |  0.00036 |    0.00765 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57583 | -0.03079 |    0.33705 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:06:33,530 - Total sparsity: 74.94

2018-10-28 01:06:33,531 - --- validate (epoch=254)-----------
2018-10-28 01:06:33,531 - 10000 samples (128 per mini-batch)
2018-10-28 01:06:34,254 - Epoch: [254][   50/   78]    Loss 0.382973    Top1 90.093750    Top5 99.640625    
2018-10-28 01:06:34,646 - ==> Top1: 90.000    Top5: 99.680    Loss: 0.380

2018-10-28 01:06:34,647 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:06:34,647 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:06:34,663 - 

2018-10-28 01:06:34,664 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:06:35,805 - Epoch: [255][   50/  391]    Overall Loss 0.077489    Objective Loss 0.077489    Top1 97.578125    Top5 99.968750    LR 0.001500    Time 0.022788    
2018-10-28 01:06:36,913 - Epoch: [255][  100/  391]    Overall Loss 0.074211    Objective Loss 0.074211    Top1 97.531250    Top5 99.984375    LR 0.001500    Time 0.022464    
2018-10-28 01:06:38,023 - Epoch: [255][  150/  391]    Overall Loss 0.074960    Objective Loss 0.074960    Top1 97.473958    Top5 99.989583    LR 0.001500    Time 0.022368    
2018-10-28 01:06:39,136 - Epoch: [255][  200/  391]    Overall Loss 0.074385    Objective Loss 0.074385    Top1 97.492188    Top5 99.992188    LR 0.001500    Time 0.022334    
2018-10-28 01:06:40,249 - Epoch: [255][  250/  391]    Overall Loss 0.075404    Objective Loss 0.075404    Top1 97.481250    Top5 99.987500    LR 0.001500    Time 0.022314    
2018-10-28 01:06:41,363 - Epoch: [255][  300/  391]    Overall Loss 0.076449    Objective Loss 0.076449    Top1 97.447917    Top5 99.989583    LR 0.001500    Time 0.022302    
2018-10-28 01:06:42,475 - Epoch: [255][  350/  391]    Overall Loss 0.076597    Objective Loss 0.076597    Top1 97.419643    Top5 99.991071    LR 0.001500    Time 0.022291    
2018-10-28 01:06:43,464 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35716 |  0.00056 |    0.17952 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11354 | -0.00282 |    0.03589 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10953 | -0.00177 |    0.03895 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11335 | -0.00821 |    0.04814 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08183 | -0.00334 |    0.02363 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12054 | -0.00445 |    0.04690 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07989 |  0.00175 |    0.02180 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11186 | -0.00391 |    0.05301 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10461 | -0.00409 |    0.05960 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13600 | -0.00319 |    0.06550 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08537 | -0.00292 |    0.03388 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06971 |  0.00136 |    0.02487 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08980 | -0.00468 |    0.03901 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07287 | -0.00060 |    0.03259 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07199 | -0.00208 |    0.02762 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07794 | -0.00270 |    0.03867 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07492 | -0.00199 |    0.02684 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07319 | -0.00266 |    0.02982 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06233 | -0.00044 |    0.02806 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04868 | -0.00118 |    0.01550 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03206 |  0.00036 |    0.00764 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57605 | -0.03078 |    0.33716 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:06:43,464 - Total sparsity: 74.94

2018-10-28 01:06:43,465 - --- validate (epoch=255)-----------
2018-10-28 01:06:43,465 - 10000 samples (128 per mini-batch)
2018-10-28 01:06:44,186 - Epoch: [255][   50/   78]    Loss 0.384251    Top1 89.718750    Top5 99.640625    
2018-10-28 01:06:44,578 - ==> Top1: 89.910    Top5: 99.680    Loss: 0.382

2018-10-28 01:06:44,579 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:06:44,579 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:06:44,597 - 

2018-10-28 01:06:44,597 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:06:45,739 - Epoch: [256][   50/  391]    Overall Loss 0.075451    Objective Loss 0.075451    Top1 97.312500    Top5 99.984375    LR 0.001500    Time 0.022813    
2018-10-28 01:06:46,850 - Epoch: [256][  100/  391]    Overall Loss 0.075311    Objective Loss 0.075311    Top1 97.359375    Top5 99.992188    LR 0.001500    Time 0.022501    
2018-10-28 01:06:47,961 - Epoch: [256][  150/  391]    Overall Loss 0.074465    Objective Loss 0.074465    Top1 97.458333    Top5 99.994792    LR 0.001500    Time 0.022399    
2018-10-28 01:06:49,071 - Epoch: [256][  200/  391]    Overall Loss 0.075870    Objective Loss 0.075870    Top1 97.382812    Top5 99.996094    LR 0.001500    Time 0.022343    
2018-10-28 01:06:50,182 - Epoch: [256][  250/  391]    Overall Loss 0.076648    Objective Loss 0.076648    Top1 97.334375    Top5 99.993750    LR 0.001500    Time 0.022315    
2018-10-28 01:06:51,293 - Epoch: [256][  300/  391]    Overall Loss 0.076162    Objective Loss 0.076162    Top1 97.382812    Top5 99.994792    LR 0.001500    Time 0.022294    
2018-10-28 01:06:52,403 - Epoch: [256][  350/  391]    Overall Loss 0.076045    Objective Loss 0.076045    Top1 97.377232    Top5 99.988839    LR 0.001500    Time 0.022275    
2018-10-28 01:06:53,398 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35696 |  0.00064 |    0.17940 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11348 | -0.00280 |    0.03586 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10947 | -0.00175 |    0.03892 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11329 | -0.00821 |    0.04810 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08178 | -0.00334 |    0.02363 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12047 | -0.00443 |    0.04687 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07985 |  0.00175 |    0.02179 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11180 | -0.00393 |    0.05299 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10456 | -0.00407 |    0.05957 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13593 | -0.00318 |    0.06547 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08533 | -0.00293 |    0.03386 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06967 |  0.00136 |    0.02485 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08975 | -0.00468 |    0.03898 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07283 | -0.00061 |    0.03257 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07195 | -0.00209 |    0.02761 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07790 | -0.00270 |    0.03865 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07488 | -0.00198 |    0.02682 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07315 | -0.00265 |    0.02981 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06229 | -0.00044 |    0.02804 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04865 | -0.00118 |    0.01549 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03204 |  0.00036 |    0.00764 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57625 | -0.03077 |    0.33728 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:06:53,398 - Total sparsity: 74.94

2018-10-28 01:06:53,398 - --- validate (epoch=256)-----------
2018-10-28 01:06:53,398 - 10000 samples (128 per mini-batch)
2018-10-28 01:06:54,123 - Epoch: [256][   50/   78]    Loss 0.383631    Top1 90.046875    Top5 99.593750    
2018-10-28 01:06:54,515 - ==> Top1: 89.990    Top5: 99.640    Loss: 0.380

2018-10-28 01:06:54,516 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:06:54,516 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:06:54,526 - 

2018-10-28 01:06:54,527 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:06:55,704 - Epoch: [257][   50/  391]    Overall Loss 0.081095    Objective Loss 0.081095    Top1 97.171875    Top5 100.000000    LR 0.001500    Time 0.023510    
2018-10-28 01:06:56,814 - Epoch: [257][  100/  391]    Overall Loss 0.081790    Objective Loss 0.081790    Top1 97.195312    Top5 100.000000    LR 0.001500    Time 0.022843    
2018-10-28 01:06:57,926 - Epoch: [257][  150/  391]    Overall Loss 0.080377    Objective Loss 0.080377    Top1 97.255208    Top5 100.000000    LR 0.001500    Time 0.022630    
2018-10-28 01:06:59,036 - Epoch: [257][  200/  391]    Overall Loss 0.080232    Objective Loss 0.080232    Top1 97.222656    Top5 99.996094    LR 0.001500    Time 0.022518    
2018-10-28 01:07:00,148 - Epoch: [257][  250/  391]    Overall Loss 0.079700    Objective Loss 0.079700    Top1 97.253125    Top5 99.996875    LR 0.001500    Time 0.022457    
2018-10-28 01:07:01,259 - Epoch: [257][  300/  391]    Overall Loss 0.079364    Objective Loss 0.079364    Top1 97.278646    Top5 99.994792    LR 0.001500    Time 0.022413    
2018-10-28 01:07:02,370 - Epoch: [257][  350/  391]    Overall Loss 0.079083    Objective Loss 0.079083    Top1 97.299107    Top5 99.995536    LR 0.001500    Time 0.022382    
2018-10-28 01:07:03,363 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35676 |  0.00013 |    0.17930 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11342 | -0.00281 |    0.03584 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10941 | -0.00175 |    0.03889 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11323 | -0.00822 |    0.04810 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08174 | -0.00332 |    0.02360 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12041 | -0.00441 |    0.04685 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07981 |  0.00172 |    0.02178 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11173 | -0.00392 |    0.05296 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10450 | -0.00406 |    0.05953 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13585 | -0.00311 |    0.06543 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08528 | -0.00292 |    0.03384 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06963 |  0.00135 |    0.02484 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08970 | -0.00466 |    0.03896 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07279 | -0.00059 |    0.03256 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07191 | -0.00209 |    0.02759 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07786 | -0.00270 |    0.03863 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07484 | -0.00198 |    0.02681 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07311 | -0.00265 |    0.02979 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06226 | -0.00044 |    0.02803 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04862 | -0.00118 |    0.01548 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03202 |  0.00036 |    0.00763 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57640 | -0.03078 |    0.33735 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:07:03,364 - Total sparsity: 74.94

2018-10-28 01:07:03,364 - --- validate (epoch=257)-----------
2018-10-28 01:07:03,364 - 10000 samples (128 per mini-batch)
2018-10-28 01:07:04,095 - Epoch: [257][   50/   78]    Loss 0.379907    Top1 89.984375    Top5 99.656250    
2018-10-28 01:07:04,486 - ==> Top1: 90.060    Top5: 99.690    Loss: 0.376

2018-10-28 01:07:04,487 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:07:04,487 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:07:04,502 - 

2018-10-28 01:07:04,502 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:07:05,672 - Epoch: [258][   50/  391]    Overall Loss 0.079495    Objective Loss 0.079495    Top1 97.406250    Top5 100.000000    LR 0.001500    Time 0.023351    
2018-10-28 01:07:06,782 - Epoch: [258][  100/  391]    Overall Loss 0.075718    Objective Loss 0.075718    Top1 97.468750    Top5 99.976562    LR 0.001500    Time 0.022760    
2018-10-28 01:07:07,892 - Epoch: [258][  150/  391]    Overall Loss 0.075859    Objective Loss 0.075859    Top1 97.427083    Top5 99.984375    LR 0.001500    Time 0.022565    
2018-10-28 01:07:09,000 - Epoch: [258][  200/  391]    Overall Loss 0.077376    Objective Loss 0.077376    Top1 97.386719    Top5 99.980469    LR 0.001500    Time 0.022462    
2018-10-28 01:07:10,111 - Epoch: [258][  250/  391]    Overall Loss 0.077401    Objective Loss 0.077401    Top1 97.328125    Top5 99.984375    LR 0.001500    Time 0.022405    
2018-10-28 01:07:11,220 - Epoch: [258][  300/  391]    Overall Loss 0.076544    Objective Loss 0.076544    Top1 97.408854    Top5 99.981771    LR 0.001500    Time 0.022366    
2018-10-28 01:07:12,330 - Epoch: [258][  350/  391]    Overall Loss 0.076758    Objective Loss 0.076758    Top1 97.395089    Top5 99.982143    LR 0.001500    Time 0.022338    
2018-10-28 01:07:13,323 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35656 |  0.00010 |    0.17920 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11335 | -0.00281 |    0.03583 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10935 | -0.00173 |    0.03888 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11316 | -0.00823 |    0.04806 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08169 | -0.00331 |    0.02358 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12034 | -0.00443 |    0.04683 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07976 |  0.00172 |    0.02177 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11167 | -0.00393 |    0.05292 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10444 | -0.00405 |    0.05949 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13577 | -0.00313 |    0.06540 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08523 | -0.00291 |    0.03383 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06959 |  0.00135 |    0.02483 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08965 | -0.00465 |    0.03894 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07275 | -0.00060 |    0.03254 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07187 | -0.00208 |    0.02758 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07781 | -0.00269 |    0.03860 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07480 | -0.00198 |    0.02679 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07307 | -0.00264 |    0.02977 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06222 | -0.00045 |    0.02801 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04860 | -0.00118 |    0.01548 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03200 |  0.00036 |    0.00763 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57660 | -0.03077 |    0.33746 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:07:13,323 - Total sparsity: 74.94

2018-10-28 01:07:13,323 - --- validate (epoch=258)-----------
2018-10-28 01:07:13,323 - 10000 samples (128 per mini-batch)
2018-10-28 01:07:14,045 - Epoch: [258][   50/   78]    Loss 0.381198    Top1 90.046875    Top5 99.671875    
2018-10-28 01:07:14,439 - ==> Top1: 90.000    Top5: 99.690    Loss: 0.378

2018-10-28 01:07:14,439 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:07:14,440 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:07:14,450 - 

2018-10-28 01:07:14,451 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:07:15,619 - Epoch: [259][   50/  391]    Overall Loss 0.076313    Objective Loss 0.076313    Top1 97.218750    Top5 100.000000    LR 0.001500    Time 0.023334    
2018-10-28 01:07:16,730 - Epoch: [259][  100/  391]    Overall Loss 0.076013    Objective Loss 0.076013    Top1 97.296875    Top5 99.992188    LR 0.001500    Time 0.022761    
2018-10-28 01:07:17,842 - Epoch: [259][  150/  391]    Overall Loss 0.077663    Objective Loss 0.077663    Top1 97.234375    Top5 99.989583    LR 0.001500    Time 0.022581    
2018-10-28 01:07:18,954 - Epoch: [259][  200/  391]    Overall Loss 0.078652    Objective Loss 0.078652    Top1 97.207031    Top5 99.992188    LR 0.001500    Time 0.022487    
2018-10-28 01:07:20,066 - Epoch: [259][  250/  391]    Overall Loss 0.078803    Objective Loss 0.078803    Top1 97.212500    Top5 99.990625    LR 0.001500    Time 0.022435    
2018-10-28 01:07:21,178 - Epoch: [259][  300/  391]    Overall Loss 0.079359    Objective Loss 0.079359    Top1 97.236979    Top5 99.989583    LR 0.001500    Time 0.022398    
2018-10-28 01:07:22,290 - Epoch: [259][  350/  391]    Overall Loss 0.078290    Objective Loss 0.078290    Top1 97.274554    Top5 99.991071    LR 0.001500    Time 0.022372    
2018-10-28 01:07:23,282 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35636 |  0.00038 |    0.17916 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11329 | -0.00282 |    0.03582 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10929 | -0.00171 |    0.03886 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11310 | -0.00817 |    0.04804 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08165 | -0.00329 |    0.02356 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12027 | -0.00441 |    0.04679 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07972 |  0.00173 |    0.02175 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11161 | -0.00393 |    0.05288 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10438 | -0.00405 |    0.05946 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13570 | -0.00318 |    0.06537 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08518 | -0.00290 |    0.03380 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06955 |  0.00134 |    0.02481 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08960 | -0.00466 |    0.03891 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07271 | -0.00058 |    0.03253 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07183 | -0.00207 |    0.02756 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07777 | -0.00269 |    0.03858 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07475 | -0.00198 |    0.02677 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07303 | -0.00263 |    0.02976 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06219 | -0.00045 |    0.02800 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04857 | -0.00118 |    0.01547 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03198 |  0.00037 |    0.00763 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57678 | -0.03075 |    0.33755 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:07:23,283 - Total sparsity: 74.94

2018-10-28 01:07:23,283 - --- validate (epoch=259)-----------
2018-10-28 01:07:23,283 - 10000 samples (128 per mini-batch)
2018-10-28 01:07:24,004 - Epoch: [259][   50/   78]    Loss 0.382849    Top1 89.968750    Top5 99.671875    
2018-10-28 01:07:24,396 - ==> Top1: 90.000    Top5: 99.700    Loss: 0.380

2018-10-28 01:07:24,397 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:07:24,397 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:07:24,409 - 

2018-10-28 01:07:24,409 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:07:25,578 - Epoch: [260][   50/  391]    Overall Loss 0.074956    Objective Loss 0.074956    Top1 97.359375    Top5 99.984375    LR 0.001500    Time 0.023355    
2018-10-28 01:07:26,689 - Epoch: [260][  100/  391]    Overall Loss 0.076309    Objective Loss 0.076309    Top1 97.359375    Top5 99.992188    LR 0.001500    Time 0.022771    
2018-10-28 01:07:27,801 - Epoch: [260][  150/  391]    Overall Loss 0.075629    Objective Loss 0.075629    Top1 97.364583    Top5 99.994792    LR 0.001500    Time 0.022583    
2018-10-28 01:07:28,914 - Epoch: [260][  200/  391]    Overall Loss 0.077366    Objective Loss 0.077366    Top1 97.292969    Top5 99.992188    LR 0.001500    Time 0.022495    
2018-10-28 01:07:30,025 - Epoch: [260][  250/  391]    Overall Loss 0.076971    Objective Loss 0.076971    Top1 97.281250    Top5 99.993750    LR 0.001500    Time 0.022436    
2018-10-28 01:07:31,137 - Epoch: [260][  300/  391]    Overall Loss 0.077433    Objective Loss 0.077433    Top1 97.265625    Top5 99.989583    LR 0.001500    Time 0.022398    
2018-10-28 01:07:32,245 - Epoch: [260][  350/  391]    Overall Loss 0.077108    Objective Loss 0.077108    Top1 97.308036    Top5 99.988839    LR 0.001500    Time 0.022361    
2018-10-28 01:07:33,239 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35617 |  0.00015 |    0.17905 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11323 | -0.00281 |    0.03580 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10923 | -0.00172 |    0.03885 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11304 | -0.00819 |    0.04801 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08160 | -0.00331 |    0.02355 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12021 | -0.00442 |    0.04676 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07967 |  0.00173 |    0.02174 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11155 | -0.00390 |    0.05286 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10433 | -0.00402 |    0.05942 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13562 | -0.00319 |    0.06534 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08514 | -0.00290 |    0.03379 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06951 |  0.00134 |    0.02480 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08955 | -0.00464 |    0.03889 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07267 | -0.00059 |    0.03251 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07179 | -0.00206 |    0.02755 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07773 | -0.00269 |    0.03856 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07471 | -0.00197 |    0.02676 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07299 | -0.00263 |    0.02974 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06215 | -0.00045 |    0.02798 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04854 | -0.00118 |    0.01546 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03197 |  0.00037 |    0.00762 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57694 | -0.03074 |    0.33763 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:07:33,239 - Total sparsity: 74.94

2018-10-28 01:07:33,239 - --- validate (epoch=260)-----------
2018-10-28 01:07:33,239 - 10000 samples (128 per mini-batch)
2018-10-28 01:07:33,969 - Epoch: [260][   50/   78]    Loss 0.378539    Top1 90.187500    Top5 99.656250    
2018-10-28 01:07:34,363 - ==> Top1: 90.260    Top5: 99.690    Loss: 0.376

2018-10-28 01:07:34,363 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:07:34,364 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:07:34,380 - 

2018-10-28 01:07:34,380 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:07:35,521 - Epoch: [261][   50/  391]    Overall Loss 0.071135    Objective Loss 0.071135    Top1 97.593750    Top5 100.000000    LR 0.001500    Time 0.022782    
2018-10-28 01:07:36,631 - Epoch: [261][  100/  391]    Overall Loss 0.076630    Objective Loss 0.076630    Top1 97.359375    Top5 100.000000    LR 0.001500    Time 0.022478    
2018-10-28 01:07:37,744 - Epoch: [261][  150/  391]    Overall Loss 0.076757    Objective Loss 0.076757    Top1 97.317708    Top5 100.000000    LR 0.001500    Time 0.022395    
2018-10-28 01:07:38,851 - Epoch: [261][  200/  391]    Overall Loss 0.076664    Objective Loss 0.076664    Top1 97.316406    Top5 100.000000    LR 0.001500    Time 0.022327    
2018-10-28 01:07:39,964 - Epoch: [261][  250/  391]    Overall Loss 0.077408    Objective Loss 0.077408    Top1 97.278125    Top5 99.987500    LR 0.001500    Time 0.022308    
2018-10-28 01:07:41,075 - Epoch: [261][  300/  391]    Overall Loss 0.076848    Objective Loss 0.076848    Top1 97.294271    Top5 99.989583    LR 0.001500    Time 0.022290    
2018-10-28 01:07:42,187 - Epoch: [261][  350/  391]    Overall Loss 0.077207    Objective Loss 0.077207    Top1 97.292411    Top5 99.991071    LR 0.001500    Time 0.022277    
2018-10-28 01:07:43,181 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35597 |  0.00011 |    0.17897 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11317 | -0.00281 |    0.03577 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10917 | -0.00170 |    0.03881 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11297 | -0.00819 |    0.04798 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08156 | -0.00331 |    0.02355 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12014 | -0.00444 |    0.04675 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07963 |  0.00173 |    0.02173 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11149 | -0.00393 |    0.05283 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10427 | -0.00403 |    0.05938 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13554 | -0.00317 |    0.06532 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08509 | -0.00290 |    0.03377 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06948 |  0.00134 |    0.02478 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08950 | -0.00463 |    0.03887 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07263 | -0.00058 |    0.03249 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07175 | -0.00206 |    0.02753 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07768 | -0.00267 |    0.03854 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07467 | -0.00194 |    0.02674 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07295 | -0.00263 |    0.02972 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06212 | -0.00045 |    0.02797 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04852 | -0.00118 |    0.01545 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03195 |  0.00037 |    0.00762 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57713 | -0.03074 |    0.33772 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:07:43,181 - Total sparsity: 74.94

2018-10-28 01:07:43,181 - --- validate (epoch=261)-----------
2018-10-28 01:07:43,181 - 10000 samples (128 per mini-batch)
2018-10-28 01:07:43,907 - Epoch: [261][   50/   78]    Loss 0.382363    Top1 90.171875    Top5 99.671875    
2018-10-28 01:07:44,300 - ==> Top1: 90.150    Top5: 99.710    Loss: 0.381

2018-10-28 01:07:44,301 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:07:44,301 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:07:44,313 - 

2018-10-28 01:07:44,313 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:07:45,483 - Epoch: [262][   50/  391]    Overall Loss 0.078869    Objective Loss 0.078869    Top1 97.359375    Top5 99.984375    LR 0.001500    Time 0.023364    
2018-10-28 01:07:46,594 - Epoch: [262][  100/  391]    Overall Loss 0.080230    Objective Loss 0.080230    Top1 97.265625    Top5 99.984375    LR 0.001500    Time 0.022774    
2018-10-28 01:07:47,705 - Epoch: [262][  150/  391]    Overall Loss 0.077788    Objective Loss 0.077788    Top1 97.333333    Top5 99.989583    LR 0.001500    Time 0.022581    
2018-10-28 01:07:48,817 - Epoch: [262][  200/  391]    Overall Loss 0.076680    Objective Loss 0.076680    Top1 97.394531    Top5 99.984375    LR 0.001500    Time 0.022470    
2018-10-28 01:07:49,927 - Epoch: [262][  250/  391]    Overall Loss 0.077577    Objective Loss 0.077577    Top1 97.343750    Top5 99.987500    LR 0.001500    Time 0.022414    
2018-10-28 01:07:51,038 - Epoch: [262][  300/  391]    Overall Loss 0.076684    Objective Loss 0.076684    Top1 97.372396    Top5 99.989583    LR 0.001500    Time 0.022377    
2018-10-28 01:07:52,150 - Epoch: [262][  350/  391]    Overall Loss 0.076718    Objective Loss 0.076718    Top1 97.401786    Top5 99.991071    LR 0.001500    Time 0.022354    
2018-10-28 01:07:53,148 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35577 |  0.00034 |    0.17883 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11310 | -0.00280 |    0.03576 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10911 | -0.00173 |    0.03879 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11291 | -0.00820 |    0.04795 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08151 | -0.00332 |    0.02354 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12007 | -0.00443 |    0.04671 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07958 |  0.00171 |    0.02171 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11142 | -0.00391 |    0.05280 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10421 | -0.00403 |    0.05934 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13547 | -0.00314 |    0.06528 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08504 | -0.00290 |    0.03376 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06944 |  0.00133 |    0.02477 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08945 | -0.00464 |    0.03884 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07259 | -0.00059 |    0.03247 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07171 | -0.00206 |    0.02752 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07764 | -0.00267 |    0.03851 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07463 | -0.00194 |    0.02673 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07291 | -0.00263 |    0.02971 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06209 | -0.00045 |    0.02795 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04849 | -0.00118 |    0.01544 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03193 |  0.00037 |    0.00761 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57732 | -0.03074 |    0.33781 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:07:53,148 - Total sparsity: 74.94

2018-10-28 01:07:53,148 - --- validate (epoch=262)-----------
2018-10-28 01:07:53,148 - 10000 samples (128 per mini-batch)
2018-10-28 01:07:53,856 - Epoch: [262][   50/   78]    Loss 0.383856    Top1 90.125000    Top5 99.640625    
2018-10-28 01:07:54,238 - ==> Top1: 90.150    Top5: 99.690    Loss: 0.382

2018-10-28 01:07:54,238 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:07:54,239 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:07:54,249 - 

2018-10-28 01:07:54,250 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:07:55,419 - Epoch: [263][   50/  391]    Overall Loss 0.074597    Objective Loss 0.074597    Top1 97.625000    Top5 100.000000    LR 0.001500    Time 0.023355    
2018-10-28 01:07:56,528 - Epoch: [263][  100/  391]    Overall Loss 0.078355    Objective Loss 0.078355    Top1 97.437500    Top5 100.000000    LR 0.001500    Time 0.022758    
2018-10-28 01:07:57,639 - Epoch: [263][  150/  391]    Overall Loss 0.075916    Objective Loss 0.075916    Top1 97.546875    Top5 99.994792    LR 0.001500    Time 0.022565    
2018-10-28 01:07:58,749 - Epoch: [263][  200/  391]    Overall Loss 0.076382    Objective Loss 0.076382    Top1 97.492188    Top5 99.996094    LR 0.001500    Time 0.022471    
2018-10-28 01:07:59,861 - Epoch: [263][  250/  391]    Overall Loss 0.077091    Objective Loss 0.077091    Top1 97.425000    Top5 99.990625    LR 0.001500    Time 0.022417    
2018-10-28 01:08:00,973 - Epoch: [263][  300/  391]    Overall Loss 0.077663    Objective Loss 0.077663    Top1 97.385417    Top5 99.989583    LR 0.001500    Time 0.022385    
2018-10-28 01:08:02,082 - Epoch: [263][  350/  391]    Overall Loss 0.078007    Objective Loss 0.078007    Top1 97.383929    Top5 99.979911    LR 0.001500    Time 0.022352    
2018-10-28 01:08:03,069 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35557 |  0.00036 |    0.17884 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11304 | -0.00280 |    0.03573 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10905 | -0.00170 |    0.03878 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11285 | -0.00815 |    0.04792 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08147 | -0.00331 |    0.02352 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.12000 | -0.00442 |    0.04671 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07954 |  0.00170 |    0.02170 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11136 | -0.00393 |    0.05277 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10415 | -0.00402 |    0.05931 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13539 | -0.00311 |    0.06526 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08500 | -0.00290 |    0.03373 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06940 |  0.00133 |    0.02476 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08940 | -0.00464 |    0.03881 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07255 | -0.00059 |    0.03244 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07167 | -0.00206 |    0.02750 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07760 | -0.00267 |    0.03849 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07459 | -0.00195 |    0.02672 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07287 | -0.00262 |    0.02969 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06205 | -0.00045 |    0.02794 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04846 | -0.00118 |    0.01543 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03191 |  0.00037 |    0.00761 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57747 | -0.03073 |    0.33791 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:08:03,069 - Total sparsity: 74.94

2018-10-28 01:08:03,069 - --- validate (epoch=263)-----------
2018-10-28 01:08:03,070 - 10000 samples (128 per mini-batch)
2018-10-28 01:08:03,793 - Epoch: [263][   50/   78]    Loss 0.382923    Top1 89.937500    Top5 99.640625    
2018-10-28 01:08:04,183 - ==> Top1: 90.030    Top5: 99.690    Loss: 0.380

2018-10-28 01:08:04,183 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:08:04,183 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:08:04,193 - 

2018-10-28 01:08:04,193 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:08:05,366 - Epoch: [264][   50/  391]    Overall Loss 0.075068    Objective Loss 0.075068    Top1 97.421875    Top5 100.000000    LR 0.001500    Time 0.023418    
2018-10-28 01:08:06,476 - Epoch: [264][  100/  391]    Overall Loss 0.077756    Objective Loss 0.077756    Top1 97.250000    Top5 100.000000    LR 0.001500    Time 0.022798    
2018-10-28 01:08:07,587 - Epoch: [264][  150/  391]    Overall Loss 0.079252    Objective Loss 0.079252    Top1 97.307292    Top5 99.989583    LR 0.001500    Time 0.022592    
2018-10-28 01:08:08,698 - Epoch: [264][  200/  391]    Overall Loss 0.077648    Objective Loss 0.077648    Top1 97.347656    Top5 99.992188    LR 0.001500    Time 0.022492    
2018-10-28 01:08:09,808 - Epoch: [264][  250/  391]    Overall Loss 0.078843    Objective Loss 0.078843    Top1 97.265625    Top5 99.990625    LR 0.001500    Time 0.022432    
2018-10-28 01:08:10,919 - Epoch: [264][  300/  391]    Overall Loss 0.078823    Objective Loss 0.078823    Top1 97.255208    Top5 99.989583    LR 0.001500    Time 0.022391    
2018-10-28 01:08:12,031 - Epoch: [264][  350/  391]    Overall Loss 0.077842    Objective Loss 0.077842    Top1 97.319196    Top5 99.991071    LR 0.001500    Time 0.022365    
2018-10-28 01:08:13,018 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35538 |  0.00030 |    0.17874 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11298 | -0.00281 |    0.03570 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10899 | -0.00171 |    0.03875 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11279 | -0.00814 |    0.04787 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08142 | -0.00331 |    0.02352 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11994 | -0.00441 |    0.04667 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07950 |  0.00169 |    0.02168 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11130 | -0.00391 |    0.05272 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10409 | -0.00402 |    0.05928 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13531 | -0.00316 |    0.06524 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08495 | -0.00290 |    0.03371 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06936 |  0.00133 |    0.02474 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08936 | -0.00462 |    0.03880 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07251 | -0.00059 |    0.03243 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07163 | -0.00205 |    0.02749 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07756 | -0.00267 |    0.03847 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07454 | -0.00195 |    0.02671 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07283 | -0.00262 |    0.02967 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06202 | -0.00045 |    0.02792 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04844 | -0.00117 |    0.01543 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03189 |  0.00037 |    0.00760 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57764 | -0.03072 |    0.33798 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:08:13,018 - Total sparsity: 74.94

2018-10-28 01:08:13,018 - --- validate (epoch=264)-----------
2018-10-28 01:08:13,018 - 10000 samples (128 per mini-batch)
2018-10-28 01:08:13,744 - Epoch: [264][   50/   78]    Loss 0.382782    Top1 90.156250    Top5 99.593750    
2018-10-28 01:08:14,139 - ==> Top1: 90.100    Top5: 99.650    Loss: 0.382

2018-10-28 01:08:14,140 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:08:14,140 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:08:14,150 - 

2018-10-28 01:08:14,151 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:08:15,324 - Epoch: [265][   50/  391]    Overall Loss 0.083473    Objective Loss 0.083473    Top1 96.984375    Top5 100.000000    LR 0.001500    Time 0.023433    
2018-10-28 01:08:16,433 - Epoch: [265][  100/  391]    Overall Loss 0.082605    Objective Loss 0.082605    Top1 97.117188    Top5 100.000000    LR 0.001500    Time 0.022788    
2018-10-28 01:08:17,544 - Epoch: [265][  150/  391]    Overall Loss 0.077687    Objective Loss 0.077687    Top1 97.333333    Top5 100.000000    LR 0.001500    Time 0.022590    
2018-10-28 01:08:18,654 - Epoch: [265][  200/  391]    Overall Loss 0.076924    Objective Loss 0.076924    Top1 97.386719    Top5 99.996094    LR 0.001500    Time 0.022489    
2018-10-28 01:08:19,765 - Epoch: [265][  250/  391]    Overall Loss 0.077678    Objective Loss 0.077678    Top1 97.359375    Top5 99.993750    LR 0.001500    Time 0.022431    
2018-10-28 01:08:20,879 - Epoch: [265][  300/  391]    Overall Loss 0.076967    Objective Loss 0.076967    Top1 97.393229    Top5 99.992188    LR 0.001500    Time 0.022400    
2018-10-28 01:08:21,989 - Epoch: [265][  350/  391]    Overall Loss 0.076698    Objective Loss 0.076698    Top1 97.410714    Top5 99.991071    LR 0.001500    Time 0.022367    
2018-10-28 01:08:22,979 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35518 |  0.00055 |    0.17859 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11291 | -0.00284 |    0.03569 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10893 | -0.00171 |    0.03871 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11273 | -0.00813 |    0.04786 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08138 | -0.00330 |    0.02351 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11987 | -0.00438 |    0.04665 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07945 |  0.00170 |    0.02167 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11124 | -0.00389 |    0.05269 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10404 | -0.00403 |    0.05924 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13524 | -0.00319 |    0.06518 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08490 | -0.00291 |    0.03369 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06932 |  0.00133 |    0.02473 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08931 | -0.00461 |    0.03878 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07247 | -0.00061 |    0.03241 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07159 | -0.00205 |    0.02747 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07751 | -0.00266 |    0.03845 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07450 | -0.00193 |    0.02669 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07279 | -0.00262 |    0.02966 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06198 | -0.00045 |    0.02791 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04841 | -0.00117 |    0.01542 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03188 |  0.00037 |    0.00760 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57782 | -0.03071 |    0.33807 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:08:22,979 - Total sparsity: 74.94

2018-10-28 01:08:22,979 - --- validate (epoch=265)-----------
2018-10-28 01:08:22,979 - 10000 samples (128 per mini-batch)
2018-10-28 01:08:23,703 - Epoch: [265][   50/   78]    Loss 0.383899    Top1 90.093750    Top5 99.578125    
2018-10-28 01:08:24,093 - ==> Top1: 90.050    Top5: 99.650    Loss: 0.382

2018-10-28 01:08:24,094 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:08:24,094 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:08:24,114 - 

2018-10-28 01:08:24,115 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:08:25,258 - Epoch: [266][   50/  391]    Overall Loss 0.068888    Objective Loss 0.068888    Top1 97.484375    Top5 99.984375    LR 0.001500    Time 0.022833    
2018-10-28 01:08:26,368 - Epoch: [266][  100/  391]    Overall Loss 0.073458    Objective Loss 0.073458    Top1 97.429688    Top5 99.992188    LR 0.001500    Time 0.022504    
2018-10-28 01:08:27,479 - Epoch: [266][  150/  391]    Overall Loss 0.072916    Objective Loss 0.072916    Top1 97.484375    Top5 99.994792    LR 0.001500    Time 0.022395    
2018-10-28 01:08:28,590 - Epoch: [266][  200/  391]    Overall Loss 0.074985    Objective Loss 0.074985    Top1 97.371094    Top5 99.992188    LR 0.001500    Time 0.022346    
2018-10-28 01:08:29,701 - Epoch: [266][  250/  391]    Overall Loss 0.077032    Objective Loss 0.077032    Top1 97.331250    Top5 99.990625    LR 0.001500    Time 0.022318    
2018-10-28 01:08:30,813 - Epoch: [266][  300/  391]    Overall Loss 0.075933    Objective Loss 0.075933    Top1 97.403646    Top5 99.992188    LR 0.001500    Time 0.022299    
2018-10-28 01:08:31,924 - Epoch: [266][  350/  391]    Overall Loss 0.076950    Objective Loss 0.076950    Top1 97.386161    Top5 99.993304    LR 0.001500    Time 0.022283    
2018-10-28 01:08:32,916 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35498 |  0.00046 |    0.17849 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11285 | -0.00285 |    0.03566 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10887 | -0.00170 |    0.03868 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11266 | -0.00814 |    0.04783 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08133 | -0.00329 |    0.02349 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11981 | -0.00439 |    0.04662 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07941 |  0.00171 |    0.02167 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11118 | -0.00389 |    0.05265 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10398 | -0.00402 |    0.05921 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13516 | -0.00310 |    0.06514 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08486 | -0.00291 |    0.03367 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06928 |  0.00133 |    0.02471 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08926 | -0.00459 |    0.03875 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07243 | -0.00060 |    0.03239 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07155 | -0.00205 |    0.02745 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07747 | -0.00265 |    0.03843 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07446 | -0.00195 |    0.02668 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07275 | -0.00261 |    0.02964 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06195 | -0.00045 |    0.02789 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04838 | -0.00117 |    0.01541 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03186 |  0.00037 |    0.00759 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57798 | -0.03071 |    0.33816 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:08:32,917 - Total sparsity: 74.94

2018-10-28 01:08:32,917 - --- validate (epoch=266)-----------
2018-10-28 01:08:32,917 - 10000 samples (128 per mini-batch)
2018-10-28 01:08:33,645 - Epoch: [266][   50/   78]    Loss 0.381122    Top1 90.234375    Top5 99.578125    
2018-10-28 01:08:34,038 - ==> Top1: 90.250    Top5: 99.650    Loss: 0.379

2018-10-28 01:08:34,039 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:08:34,040 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:08:34,053 - 

2018-10-28 01:08:34,053 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:08:35,224 - Epoch: [267][   50/  391]    Overall Loss 0.077586    Objective Loss 0.077586    Top1 97.171875    Top5 99.984375    LR 0.001500    Time 0.023377    
2018-10-28 01:08:36,334 - Epoch: [267][  100/  391]    Overall Loss 0.078609    Objective Loss 0.078609    Top1 97.179688    Top5 99.984375    LR 0.001500    Time 0.022772    
2018-10-28 01:08:37,442 - Epoch: [267][  150/  391]    Overall Loss 0.079290    Objective Loss 0.079290    Top1 97.171875    Top5 99.979167    LR 0.001500    Time 0.022560    
2018-10-28 01:08:38,553 - Epoch: [267][  200/  391]    Overall Loss 0.079116    Objective Loss 0.079116    Top1 97.226562    Top5 99.984375    LR 0.001500    Time 0.022450    
2018-10-28 01:08:39,663 - Epoch: [267][  250/  391]    Overall Loss 0.078512    Objective Loss 0.078512    Top1 97.228125    Top5 99.987500    LR 0.001500    Time 0.022393    
2018-10-28 01:08:40,773 - Epoch: [267][  300/  391]    Overall Loss 0.078104    Objective Loss 0.078104    Top1 97.276042    Top5 99.981771    LR 0.001500    Time 0.022359    
2018-10-28 01:08:41,883 - Epoch: [267][  350/  391]    Overall Loss 0.077432    Objective Loss 0.077432    Top1 97.303571    Top5 99.982143    LR 0.001500    Time 0.022330    
2018-10-28 01:08:42,870 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35478 |  0.00057 |    0.17842 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11279 | -0.00281 |    0.03565 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10880 | -0.00173 |    0.03867 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11261 | -0.00809 |    0.04780 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08129 | -0.00330 |    0.02347 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11974 | -0.00436 |    0.04660 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07936 |  0.00170 |    0.02165 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11112 | -0.00389 |    0.05262 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10392 | -0.00403 |    0.05918 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13509 | -0.00310 |    0.06513 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08481 | -0.00289 |    0.03365 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06925 |  0.00133 |    0.02470 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08921 | -0.00457 |    0.03873 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07239 | -0.00062 |    0.03237 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07151 | -0.00205 |    0.02744 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07743 | -0.00266 |    0.03841 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07442 | -0.00195 |    0.02667 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07271 | -0.00261 |    0.02963 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06192 | -0.00046 |    0.02788 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04836 | -0.00118 |    0.01540 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03184 |  0.00037 |    0.00759 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57815 | -0.03070 |    0.33825 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:08:42,870 - Total sparsity: 74.94

2018-10-28 01:08:42,870 - --- validate (epoch=267)-----------
2018-10-28 01:08:42,871 - 10000 samples (128 per mini-batch)
2018-10-28 01:08:43,597 - Epoch: [267][   50/   78]    Loss 0.387016    Top1 90.000000    Top5 99.578125    
2018-10-28 01:08:43,990 - ==> Top1: 90.110    Top5: 99.640    Loss: 0.384

2018-10-28 01:08:43,991 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:08:43,991 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:08:44,002 - 

2018-10-28 01:08:44,002 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:08:45,171 - Epoch: [268][   50/  391]    Overall Loss 0.076535    Objective Loss 0.076535    Top1 97.281250    Top5 100.000000    LR 0.001500    Time 0.023340    
2018-10-28 01:08:46,281 - Epoch: [268][  100/  391]    Overall Loss 0.074178    Objective Loss 0.074178    Top1 97.437500    Top5 99.984375    LR 0.001500    Time 0.022758    
2018-10-28 01:08:47,393 - Epoch: [268][  150/  391]    Overall Loss 0.073941    Objective Loss 0.073941    Top1 97.494792    Top5 99.989583    LR 0.001500    Time 0.022575    
2018-10-28 01:08:48,502 - Epoch: [268][  200/  391]    Overall Loss 0.073489    Objective Loss 0.073489    Top1 97.453125    Top5 99.992188    LR 0.001500    Time 0.022472    
2018-10-28 01:08:49,613 - Epoch: [268][  250/  391]    Overall Loss 0.074678    Objective Loss 0.074678    Top1 97.425000    Top5 99.993750    LR 0.001500    Time 0.022402    
2018-10-28 01:08:50,722 - Epoch: [268][  300/  391]    Overall Loss 0.075342    Objective Loss 0.075342    Top1 97.403646    Top5 99.994792    LR 0.001500    Time 0.022360    
2018-10-28 01:08:51,831 - Epoch: [268][  350/  391]    Overall Loss 0.075086    Objective Loss 0.075086    Top1 97.417411    Top5 99.995536    LR 0.001500    Time 0.022330    
2018-10-28 01:08:52,825 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35458 |  0.00074 |    0.17827 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11273 | -0.00278 |    0.03563 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10874 | -0.00173 |    0.03864 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11255 | -0.00806 |    0.04777 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08124 | -0.00327 |    0.02346 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11967 | -0.00438 |    0.04657 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07932 |  0.00171 |    0.02164 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11105 | -0.00389 |    0.05259 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10386 | -0.00398 |    0.05915 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13501 | -0.00309 |    0.06506 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08476 | -0.00287 |    0.03363 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06921 |  0.00133 |    0.02469 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08916 | -0.00457 |    0.03871 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07235 | -0.00061 |    0.03235 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07147 | -0.00206 |    0.02742 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07739 | -0.00266 |    0.03839 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07438 | -0.00195 |    0.02665 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07267 | -0.00260 |    0.02961 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06188 | -0.00047 |    0.02786 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04833 | -0.00118 |    0.01539 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03182 |  0.00037 |    0.00759 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57832 | -0.03069 |    0.33834 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:08:52,825 - Total sparsity: 74.94

2018-10-28 01:08:52,825 - --- validate (epoch=268)-----------
2018-10-28 01:08:52,825 - 10000 samples (128 per mini-batch)
2018-10-28 01:08:53,551 - Epoch: [268][   50/   78]    Loss 0.387388    Top1 90.046875    Top5 99.625000    
2018-10-28 01:08:53,943 - ==> Top1: 90.100    Top5: 99.670    Loss: 0.383

2018-10-28 01:08:53,944 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:08:53,944 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:08:53,953 - 

2018-10-28 01:08:53,954 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:08:55,124 - Epoch: [269][   50/  391]    Overall Loss 0.078654    Objective Loss 0.078654    Top1 97.203125    Top5 100.000000    LR 0.001500    Time 0.023366    
2018-10-28 01:08:56,234 - Epoch: [269][  100/  391]    Overall Loss 0.074502    Objective Loss 0.074502    Top1 97.421875    Top5 100.000000    LR 0.001500    Time 0.022767    
2018-10-28 01:08:57,344 - Epoch: [269][  150/  391]    Overall Loss 0.072501    Objective Loss 0.072501    Top1 97.494792    Top5 99.994792    LR 0.001500    Time 0.022569    
2018-10-28 01:08:58,452 - Epoch: [269][  200/  391]    Overall Loss 0.074441    Objective Loss 0.074441    Top1 97.437500    Top5 99.996094    LR 0.001500    Time 0.022463    
2018-10-28 01:08:59,562 - Epoch: [269][  250/  391]    Overall Loss 0.073181    Objective Loss 0.073181    Top1 97.496875    Top5 99.996875    LR 0.001500    Time 0.022404    
2018-10-28 01:09:00,672 - Epoch: [269][  300/  391]    Overall Loss 0.073821    Objective Loss 0.073821    Top1 97.476562    Top5 99.992188    LR 0.001500    Time 0.022366    
2018-10-28 01:09:01,783 - Epoch: [269][  350/  391]    Overall Loss 0.074344    Objective Loss 0.074344    Top1 97.459821    Top5 99.993304    LR 0.001500    Time 0.022342    
2018-10-28 01:09:02,780 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35439 |  0.00046 |    0.17818 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11266 | -0.00278 |    0.03561 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10868 | -0.00173 |    0.03862 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11248 | -0.00807 |    0.04773 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08120 | -0.00328 |    0.02344 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11961 | -0.00435 |    0.04654 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07928 |  0.00170 |    0.02162 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11099 | -0.00390 |    0.05256 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10381 | -0.00399 |    0.05911 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13494 | -0.00306 |    0.06501 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08472 | -0.00286 |    0.03362 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06917 |  0.00134 |    0.02467 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08911 | -0.00456 |    0.03868 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07231 | -0.00060 |    0.03233 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07143 | -0.00205 |    0.02740 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07734 | -0.00265 |    0.03837 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07434 | -0.00195 |    0.02664 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07263 | -0.00260 |    0.02959 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06185 | -0.00046 |    0.02784 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04830 | -0.00118 |    0.01538 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03181 |  0.00037 |    0.00758 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57851 | -0.03069 |    0.33845 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:09:02,780 - Total sparsity: 74.94

2018-10-28 01:09:02,780 - --- validate (epoch=269)-----------
2018-10-28 01:09:02,780 - 10000 samples (128 per mini-batch)
2018-10-28 01:09:03,500 - Epoch: [269][   50/   78]    Loss 0.388580    Top1 89.812500    Top5 99.562500    
2018-10-28 01:09:03,890 - ==> Top1: 90.030    Top5: 99.640    Loss: 0.384

2018-10-28 01:09:03,890 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:09:03,890 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:09:03,904 - 

2018-10-28 01:09:03,905 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:09:05,075 - Epoch: [270][   50/  391]    Overall Loss 0.077855    Objective Loss 0.077855    Top1 97.343750    Top5 100.000000    LR 0.001500    Time 0.023379    
2018-10-28 01:09:06,186 - Epoch: [270][  100/  391]    Overall Loss 0.078567    Objective Loss 0.078567    Top1 97.289062    Top5 100.000000    LR 0.001500    Time 0.022784    
2018-10-28 01:09:07,297 - Epoch: [270][  150/  391]    Overall Loss 0.076065    Objective Loss 0.076065    Top1 97.468750    Top5 100.000000    LR 0.001500    Time 0.022585    
2018-10-28 01:09:08,407 - Epoch: [270][  200/  391]    Overall Loss 0.075963    Objective Loss 0.075963    Top1 97.484375    Top5 100.000000    LR 0.001500    Time 0.022482    
2018-10-28 01:09:09,517 - Epoch: [270][  250/  391]    Overall Loss 0.075803    Objective Loss 0.075803    Top1 97.471875    Top5 99.996875    LR 0.001500    Time 0.022422    
2018-10-28 01:09:10,627 - Epoch: [270][  300/  391]    Overall Loss 0.077118    Objective Loss 0.077118    Top1 97.406250    Top5 99.997396    LR 0.001500    Time 0.022379    
2018-10-28 01:09:11,737 - Epoch: [270][  350/  391]    Overall Loss 0.076980    Objective Loss 0.076980    Top1 97.386161    Top5 99.995536    LR 0.001500    Time 0.022350    
2018-10-28 01:09:12,726 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35419 |  0.00018 |    0.17813 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11260 | -0.00280 |    0.03560 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10862 | -0.00173 |    0.03859 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11242 | -0.00807 |    0.04770 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08115 | -0.00327 |    0.02344 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11954 | -0.00434 |    0.04652 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07923 |  0.00170 |    0.02160 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11093 | -0.00388 |    0.05253 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10375 | -0.00398 |    0.05907 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13486 | -0.00304 |    0.06495 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08467 | -0.00286 |    0.03360 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06913 |  0.00134 |    0.02465 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08906 | -0.00459 |    0.03866 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07227 | -0.00060 |    0.03231 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07139 | -0.00205 |    0.02738 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07730 | -0.00265 |    0.03835 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07429 | -0.00194 |    0.02663 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07259 | -0.00260 |    0.02958 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06182 | -0.00046 |    0.02782 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04828 | -0.00119 |    0.01537 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03179 |  0.00037 |    0.00758 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57868 | -0.03068 |    0.33852 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:09:12,726 - Total sparsity: 74.94

2018-10-28 01:09:12,726 - --- validate (epoch=270)-----------
2018-10-28 01:09:12,726 - 10000 samples (128 per mini-batch)
2018-10-28 01:09:13,460 - Epoch: [270][   50/   78]    Loss 0.388088    Top1 89.906250    Top5 99.578125    
2018-10-28 01:09:13,851 - ==> Top1: 90.040    Top5: 99.630    Loss: 0.385

2018-10-28 01:09:13,851 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:09:13,852 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:09:13,868 - 

2018-10-28 01:09:13,868 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:09:15,008 - Epoch: [271][   50/  391]    Overall Loss 0.076400    Objective Loss 0.076400    Top1 97.718750    Top5 99.984375    LR 0.001500    Time 0.022776    
2018-10-28 01:09:16,118 - Epoch: [271][  100/  391]    Overall Loss 0.072911    Objective Loss 0.072911    Top1 97.710938    Top5 99.984375    LR 0.001500    Time 0.022471    
2018-10-28 01:09:17,229 - Epoch: [271][  150/  391]    Overall Loss 0.072849    Objective Loss 0.072849    Top1 97.661458    Top5 99.989583    LR 0.001500    Time 0.022380    
2018-10-28 01:09:18,339 - Epoch: [271][  200/  391]    Overall Loss 0.073642    Objective Loss 0.073642    Top1 97.562500    Top5 99.988281    LR 0.001500    Time 0.022325    
2018-10-28 01:09:19,450 - Epoch: [271][  250/  391]    Overall Loss 0.073785    Objective Loss 0.073785    Top1 97.559375    Top5 99.990625    LR 0.001500    Time 0.022301    
2018-10-28 01:09:20,562 - Epoch: [271][  300/  391]    Overall Loss 0.074217    Objective Loss 0.074217    Top1 97.533854    Top5 99.986979    LR 0.001500    Time 0.022287    
2018-10-28 01:09:21,675 - Epoch: [271][  350/  391]    Overall Loss 0.074032    Objective Loss 0.074032    Top1 97.555804    Top5 99.988839    LR 0.001500    Time 0.022277    
2018-10-28 01:09:22,664 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35400 |  0.00046 |    0.17800 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11254 | -0.00281 |    0.03558 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10856 | -0.00173 |    0.03856 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11236 | -0.00809 |    0.04769 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08111 | -0.00327 |    0.02342 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11948 | -0.00434 |    0.04648 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07919 |  0.00170 |    0.02159 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11087 | -0.00388 |    0.05250 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10369 | -0.00398 |    0.05904 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13479 | -0.00297 |    0.06494 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08462 | -0.00287 |    0.03358 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06909 |  0.00135 |    0.02464 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08902 | -0.00458 |    0.03864 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07223 | -0.00058 |    0.03229 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07135 | -0.00205 |    0.02737 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07726 | -0.00265 |    0.03833 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07425 | -0.00194 |    0.02660 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07255 | -0.00260 |    0.02956 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06178 | -0.00046 |    0.02781 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04825 | -0.00118 |    0.01537 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03177 |  0.00037 |    0.00757 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57887 | -0.03068 |    0.33861 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:09:22,664 - Total sparsity: 74.94

2018-10-28 01:09:22,665 - --- validate (epoch=271)-----------
2018-10-28 01:09:22,665 - 10000 samples (128 per mini-batch)
2018-10-28 01:09:23,390 - Epoch: [271][   50/   78]    Loss 0.392996    Top1 89.890625    Top5 99.578125    
2018-10-28 01:09:23,784 - ==> Top1: 89.980    Top5: 99.640    Loss: 0.390

2018-10-28 01:09:23,785 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:09:23,785 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:09:23,802 - 

2018-10-28 01:09:23,802 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:09:25,029 - Epoch: [272][   50/  391]    Overall Loss 0.074073    Objective Loss 0.074073    Top1 97.265625    Top5 100.000000    LR 0.001500    Time 0.024502    
2018-10-28 01:09:26,223 - Epoch: [272][  100/  391]    Overall Loss 0.074920    Objective Loss 0.074920    Top1 97.382812    Top5 99.992188    LR 0.001500    Time 0.024172    
2018-10-28 01:09:27,419 - Epoch: [272][  150/  391]    Overall Loss 0.075835    Objective Loss 0.075835    Top1 97.354167    Top5 99.989583    LR 0.001500    Time 0.024084    
2018-10-28 01:09:28,608 - Epoch: [272][  200/  391]    Overall Loss 0.073655    Objective Loss 0.073655    Top1 97.417969    Top5 99.992188    LR 0.001500    Time 0.024001    
2018-10-28 01:09:29,808 - Epoch: [272][  250/  391]    Overall Loss 0.074136    Objective Loss 0.074136    Top1 97.403125    Top5 99.987500    LR 0.001500    Time 0.023993    
2018-10-28 01:09:31,010 - Epoch: [272][  300/  391]    Overall Loss 0.073867    Objective Loss 0.073867    Top1 97.424479    Top5 99.989583    LR 0.001500    Time 0.023995    
2018-10-28 01:09:32,205 - Epoch: [272][  350/  391]    Overall Loss 0.074769    Objective Loss 0.074769    Top1 97.408482    Top5 99.986607    LR 0.001500    Time 0.023978    
2018-10-28 01:09:33,265 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35380 |  0.00043 |    0.17786 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11248 | -0.00280 |    0.03555 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10850 | -0.00176 |    0.03855 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11229 | -0.00811 |    0.04767 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08106 | -0.00328 |    0.02341 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11941 | -0.00434 |    0.04646 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07914 |  0.00171 |    0.02158 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11081 | -0.00391 |    0.05248 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10363 | -0.00399 |    0.05900 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13471 | -0.00302 |    0.06491 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08458 | -0.00287 |    0.03355 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06906 |  0.00133 |    0.02463 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08897 | -0.00458 |    0.03862 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07219 | -0.00060 |    0.03227 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07131 | -0.00205 |    0.02735 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07722 | -0.00264 |    0.03831 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07421 | -0.00195 |    0.02659 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07251 | -0.00259 |    0.02954 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06175 | -0.00046 |    0.02780 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04823 | -0.00118 |    0.01536 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03175 |  0.00037 |    0.00757 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57906 | -0.03068 |    0.33871 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:09:33,265 - Total sparsity: 74.94

2018-10-28 01:09:33,265 - --- validate (epoch=272)-----------
2018-10-28 01:09:33,266 - 10000 samples (128 per mini-batch)
2018-10-28 01:09:34,021 - Epoch: [272][   50/   78]    Loss 0.389409    Top1 89.921875    Top5 99.625000    
2018-10-28 01:09:34,429 - ==> Top1: 90.030    Top5: 99.690    Loss: 0.387

2018-10-28 01:09:34,430 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:09:34,430 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:09:34,446 - 

2018-10-28 01:09:34,447 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:09:35,591 - Epoch: [273][   50/  391]    Overall Loss 0.069033    Objective Loss 0.069033    Top1 97.718750    Top5 100.000000    LR 0.001500    Time 0.022849    
2018-10-28 01:09:36,703 - Epoch: [273][  100/  391]    Overall Loss 0.072826    Objective Loss 0.072826    Top1 97.531250    Top5 99.992188    LR 0.001500    Time 0.022535    
2018-10-28 01:09:37,816 - Epoch: [273][  150/  391]    Overall Loss 0.074131    Objective Loss 0.074131    Top1 97.411458    Top5 99.994792    LR 0.001500    Time 0.022434    
2018-10-28 01:09:38,929 - Epoch: [273][  200/  391]    Overall Loss 0.073572    Objective Loss 0.073572    Top1 97.433594    Top5 99.996094    LR 0.001500    Time 0.022383    
2018-10-28 01:09:40,042 - Epoch: [273][  250/  391]    Overall Loss 0.073666    Objective Loss 0.073666    Top1 97.450000    Top5 99.996875    LR 0.001500    Time 0.022354    
2018-10-28 01:09:41,154 - Epoch: [273][  300/  391]    Overall Loss 0.074654    Objective Loss 0.074654    Top1 97.403646    Top5 99.994792    LR 0.001500    Time 0.022330    
2018-10-28 01:09:42,267 - Epoch: [273][  350/  391]    Overall Loss 0.075418    Objective Loss 0.075418    Top1 97.381696    Top5 99.995536    LR 0.001500    Time 0.022316    
2018-10-28 01:09:43,264 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35360 |  0.00045 |    0.17774 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11241 | -0.00278 |    0.03553 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10844 | -0.00174 |    0.03853 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11223 | -0.00811 |    0.04763 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08102 | -0.00327 |    0.02339 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11934 | -0.00435 |    0.04642 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07910 |  0.00170 |    0.02156 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11075 | -0.00388 |    0.05244 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10358 | -0.00398 |    0.05897 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13463 | -0.00303 |    0.06489 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08453 | -0.00288 |    0.03353 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06902 |  0.00133 |    0.02462 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08892 | -0.00457 |    0.03859 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07215 | -0.00058 |    0.03225 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07127 | -0.00205 |    0.02734 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07717 | -0.00265 |    0.03829 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07417 | -0.00196 |    0.02657 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07247 | -0.00258 |    0.02953 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06171 | -0.00046 |    0.02778 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04820 | -0.00118 |    0.01535 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03173 |  0.00037 |    0.00756 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57922 | -0.03068 |    0.33881 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:09:43,265 - Total sparsity: 74.94

2018-10-28 01:09:43,265 - --- validate (epoch=273)-----------
2018-10-28 01:09:43,265 - 10000 samples (128 per mini-batch)
2018-10-28 01:09:43,993 - Epoch: [273][   50/   78]    Loss 0.388737    Top1 90.125000    Top5 99.578125    
2018-10-28 01:09:44,387 - ==> Top1: 90.190    Top5: 99.640    Loss: 0.387

2018-10-28 01:09:44,388 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:09:44,388 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:09:44,399 - 

2018-10-28 01:09:44,399 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:09:45,572 - Epoch: [274][   50/  391]    Overall Loss 0.080375    Objective Loss 0.080375    Top1 97.140625    Top5 100.000000    LR 0.001500    Time 0.023425    
2018-10-28 01:09:46,687 - Epoch: [274][  100/  391]    Overall Loss 0.078496    Objective Loss 0.078496    Top1 97.289062    Top5 100.000000    LR 0.001500    Time 0.022845    
2018-10-28 01:09:47,802 - Epoch: [274][  150/  391]    Overall Loss 0.076134    Objective Loss 0.076134    Top1 97.390625    Top5 100.000000    LR 0.001500    Time 0.022657    
2018-10-28 01:09:48,917 - Epoch: [274][  200/  391]    Overall Loss 0.077676    Objective Loss 0.077676    Top1 97.375000    Top5 99.988281    LR 0.001500    Time 0.022541    
2018-10-28 01:09:50,032 - Epoch: [274][  250/  391]    Overall Loss 0.077543    Objective Loss 0.077543    Top1 97.378125    Top5 99.990625    LR 0.001500    Time 0.022487    
2018-10-28 01:09:51,145 - Epoch: [274][  300/  391]    Overall Loss 0.076824    Objective Loss 0.076824    Top1 97.390625    Top5 99.989583    LR 0.001500    Time 0.022446    
2018-10-28 01:09:52,259 - Epoch: [274][  350/  391]    Overall Loss 0.076995    Objective Loss 0.076995    Top1 97.406250    Top5 99.991071    LR 0.001500    Time 0.022418    
2018-10-28 01:09:53,249 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35341 |  0.00057 |    0.17768 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11235 | -0.00276 |    0.03551 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10838 | -0.00172 |    0.03850 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11217 | -0.00809 |    0.04761 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08097 | -0.00330 |    0.02338 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11928 | -0.00433 |    0.04639 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07906 |  0.00169 |    0.02156 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11069 | -0.00387 |    0.05241 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10352 | -0.00395 |    0.05894 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13456 | -0.00301 |    0.06488 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08448 | -0.00288 |    0.03352 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06898 |  0.00134 |    0.02461 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08887 | -0.00456 |    0.03857 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07211 | -0.00059 |    0.03223 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07123 | -0.00204 |    0.02732 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07713 | -0.00263 |    0.03826 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07413 | -0.00194 |    0.02656 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07243 | -0.00259 |    0.02951 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06168 | -0.00046 |    0.02777 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04817 | -0.00118 |    0.01534 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03172 |  0.00037 |    0.00756 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57938 | -0.03068 |    0.33888 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:09:53,249 - Total sparsity: 74.94

2018-10-28 01:09:53,249 - --- validate (epoch=274)-----------
2018-10-28 01:09:53,249 - 10000 samples (128 per mini-batch)
2018-10-28 01:09:53,998 - Epoch: [274][   50/   78]    Loss 0.389272    Top1 90.109375    Top5 99.609375    
2018-10-28 01:09:54,390 - ==> Top1: 90.090    Top5: 99.650    Loss: 0.388

2018-10-28 01:09:54,391 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:09:54,391 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:09:54,402 - 

2018-10-28 01:09:54,402 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:09:55,572 - Epoch: [275][   50/  391]    Overall Loss 0.074669    Objective Loss 0.074669    Top1 97.500000    Top5 99.984375    LR 0.001500    Time 0.023351    
2018-10-28 01:09:56,683 - Epoch: [275][  100/  391]    Overall Loss 0.072674    Objective Loss 0.072674    Top1 97.546875    Top5 99.984375    LR 0.001500    Time 0.022773    
2018-10-28 01:09:57,794 - Epoch: [275][  150/  391]    Overall Loss 0.073764    Objective Loss 0.073764    Top1 97.567708    Top5 99.979167    LR 0.001500    Time 0.022581    
2018-10-28 01:09:58,906 - Epoch: [275][  200/  391]    Overall Loss 0.073518    Objective Loss 0.073518    Top1 97.574219    Top5 99.984375    LR 0.001500    Time 0.022489    
2018-10-28 01:10:00,018 - Epoch: [275][  250/  391]    Overall Loss 0.072917    Objective Loss 0.072917    Top1 97.596875    Top5 99.987500    LR 0.001500    Time 0.022421    
2018-10-28 01:10:01,136 - Epoch: [275][  300/  391]    Overall Loss 0.072532    Objective Loss 0.072532    Top1 97.598958    Top5 99.989583    LR 0.001500    Time 0.022405    
2018-10-28 01:10:02,249 - Epoch: [275][  350/  391]    Overall Loss 0.073106    Objective Loss 0.073106    Top1 97.584821    Top5 99.984375    LR 0.001500    Time 0.022380    
2018-10-28 01:10:03,239 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35321 |  0.00046 |    0.17756 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11229 | -0.00275 |    0.03550 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10832 | -0.00172 |    0.03849 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11210 | -0.00811 |    0.04759 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08092 | -0.00330 |    0.02337 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11921 | -0.00433 |    0.04636 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07901 |  0.00170 |    0.02153 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11062 | -0.00388 |    0.05238 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10346 | -0.00396 |    0.05890 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13448 | -0.00305 |    0.06485 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08444 | -0.00288 |    0.03349 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06894 |  0.00134 |    0.02460 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08882 | -0.00454 |    0.03855 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07207 | -0.00058 |    0.03221 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07119 | -0.00204 |    0.02731 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07709 | -0.00263 |    0.03824 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07408 | -0.00193 |    0.02654 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07239 | -0.00259 |    0.02950 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06165 | -0.00046 |    0.02775 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04815 | -0.00118 |    0.01533 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03170 |  0.00037 |    0.00756 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57955 | -0.03068 |    0.33896 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:10:03,239 - Total sparsity: 74.94

2018-10-28 01:10:03,239 - --- validate (epoch=275)-----------
2018-10-28 01:10:03,239 - 10000 samples (128 per mini-batch)
2018-10-28 01:10:03,973 - Epoch: [275][   50/   78]    Loss 0.388890    Top1 90.000000    Top5 99.562500    
2018-10-28 01:10:04,367 - ==> Top1: 90.050    Top5: 99.610    Loss: 0.386

2018-10-28 01:10:04,367 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:10:04,367 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:10:04,377 - 

2018-10-28 01:10:04,378 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:10:05,547 - Epoch: [276][   50/  391]    Overall Loss 0.070678    Objective Loss 0.070678    Top1 97.546875    Top5 100.000000    LR 0.001500    Time 0.023350    
2018-10-28 01:10:06,658 - Epoch: [276][  100/  391]    Overall Loss 0.071106    Objective Loss 0.071106    Top1 97.578125    Top5 100.000000    LR 0.001500    Time 0.022775    
2018-10-28 01:10:07,769 - Epoch: [276][  150/  391]    Overall Loss 0.072644    Objective Loss 0.072644    Top1 97.468750    Top5 99.994792    LR 0.001500    Time 0.022582    
2018-10-28 01:10:08,881 - Epoch: [276][  200/  391]    Overall Loss 0.073682    Objective Loss 0.073682    Top1 97.433594    Top5 99.992188    LR 0.001500    Time 0.022486    
2018-10-28 01:10:09,992 - Epoch: [276][  250/  391]    Overall Loss 0.075615    Objective Loss 0.075615    Top1 97.396875    Top5 99.990625    LR 0.001500    Time 0.022429    
2018-10-28 01:10:11,103 - Epoch: [276][  300/  391]    Overall Loss 0.075091    Objective Loss 0.075091    Top1 97.401042    Top5 99.989583    LR 0.001500    Time 0.022390    
2018-10-28 01:10:12,214 - Epoch: [276][  350/  391]    Overall Loss 0.074044    Objective Loss 0.074044    Top1 97.435268    Top5 99.991071    LR 0.001500    Time 0.022361    
2018-10-28 01:10:13,202 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35302 |  0.00048 |    0.17740 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11223 | -0.00277 |    0.03550 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10826 | -0.00174 |    0.03847 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11204 | -0.00811 |    0.04755 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08088 | -0.00329 |    0.02336 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11914 | -0.00435 |    0.04634 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07897 |  0.00169 |    0.02152 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11056 | -0.00388 |    0.05235 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10341 | -0.00397 |    0.05887 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13441 | -0.00304 |    0.06482 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08439 | -0.00288 |    0.03347 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06890 |  0.00134 |    0.02459 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08877 | -0.00452 |    0.03852 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07203 | -0.00059 |    0.03219 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07115 | -0.00204 |    0.02729 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07705 | -0.00262 |    0.03822 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07404 | -0.00194 |    0.02653 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07235 | -0.00259 |    0.02948 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06161 | -0.00046 |    0.02773 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04812 | -0.00118 |    0.01532 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03168 |  0.00037 |    0.00755 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57971 | -0.03068 |    0.33905 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:10:13,202 - Total sparsity: 74.94

2018-10-28 01:10:13,203 - --- validate (epoch=276)-----------
2018-10-28 01:10:13,203 - 10000 samples (128 per mini-batch)
2018-10-28 01:10:13,929 - Epoch: [276][   50/   78]    Loss 0.386103    Top1 90.015625    Top5 99.593750    
2018-10-28 01:10:14,326 - ==> Top1: 90.040    Top5: 99.660    Loss: 0.382

2018-10-28 01:10:14,327 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:10:14,327 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:10:14,343 - 

2018-10-28 01:10:14,343 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:10:15,490 - Epoch: [277][   50/  391]    Overall Loss 0.072541    Objective Loss 0.072541    Top1 97.578125    Top5 100.000000    LR 0.001500    Time 0.022908    
2018-10-28 01:10:16,603 - Epoch: [277][  100/  391]    Overall Loss 0.074464    Objective Loss 0.074464    Top1 97.460938    Top5 99.992188    LR 0.001500    Time 0.022573    
2018-10-28 01:10:17,717 - Epoch: [277][  150/  391]    Overall Loss 0.074916    Objective Loss 0.074916    Top1 97.447917    Top5 99.979167    LR 0.001500    Time 0.022462    
2018-10-28 01:10:18,828 - Epoch: [277][  200/  391]    Overall Loss 0.073802    Objective Loss 0.073802    Top1 97.496094    Top5 99.984375    LR 0.001500    Time 0.022398    
2018-10-28 01:10:19,940 - Epoch: [277][  250/  391]    Overall Loss 0.073703    Objective Loss 0.073703    Top1 97.515625    Top5 99.984375    LR 0.001500    Time 0.022362    
2018-10-28 01:10:21,054 - Epoch: [277][  300/  391]    Overall Loss 0.074338    Objective Loss 0.074338    Top1 97.471354    Top5 99.984375    LR 0.001500    Time 0.022342    
2018-10-28 01:10:22,167 - Epoch: [277][  350/  391]    Overall Loss 0.074137    Objective Loss 0.074137    Top1 97.482143    Top5 99.986607    LR 0.001500    Time 0.022326    
2018-10-28 01:10:23,159 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35282 |  0.00043 |    0.17732 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11216 | -0.00279 |    0.03548 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10820 | -0.00173 |    0.03844 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11198 | -0.00809 |    0.04753 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08084 | -0.00327 |    0.02334 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11908 | -0.00434 |    0.04631 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07893 |  0.00168 |    0.02151 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11050 | -0.00384 |    0.05232 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10335 | -0.00395 |    0.05883 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13433 | -0.00306 |    0.06477 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08434 | -0.00288 |    0.03345 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06886 |  0.00134 |    0.02457 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08873 | -0.00452 |    0.03850 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07199 | -0.00058 |    0.03217 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07111 | -0.00204 |    0.02728 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07700 | -0.00262 |    0.03820 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07400 | -0.00193 |    0.02651 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07231 | -0.00259 |    0.02947 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06158 | -0.00046 |    0.02772 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04809 | -0.00117 |    0.01532 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03166 |  0.00037 |    0.00755 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.57989 | -0.03066 |    0.33916 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:10:23,159 - Total sparsity: 74.94

2018-10-28 01:10:23,159 - --- validate (epoch=277)-----------
2018-10-28 01:10:23,159 - 10000 samples (128 per mini-batch)
2018-10-28 01:10:23,887 - Epoch: [277][   50/   78]    Loss 0.392322    Top1 90.109375    Top5 99.578125    
2018-10-28 01:10:24,278 - ==> Top1: 90.050    Top5: 99.650    Loss: 0.388

2018-10-28 01:10:24,278 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:10:24,279 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:10:24,292 - 

2018-10-28 01:10:24,293 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:10:25,464 - Epoch: [278][   50/  391]    Overall Loss 0.074345    Objective Loss 0.074345    Top1 97.671875    Top5 100.000000    LR 0.001500    Time 0.023383    
2018-10-28 01:10:26,576 - Epoch: [278][  100/  391]    Overall Loss 0.068764    Objective Loss 0.068764    Top1 97.820312    Top5 100.000000    LR 0.001500    Time 0.022800    
2018-10-28 01:10:27,689 - Epoch: [278][  150/  391]    Overall Loss 0.071082    Objective Loss 0.071082    Top1 97.708333    Top5 99.994792    LR 0.001500    Time 0.022609    
2018-10-28 01:10:28,802 - Epoch: [278][  200/  391]    Overall Loss 0.072005    Objective Loss 0.072005    Top1 97.617188    Top5 99.996094    LR 0.001500    Time 0.022517    
2018-10-28 01:10:29,915 - Epoch: [278][  250/  391]    Overall Loss 0.073165    Objective Loss 0.073165    Top1 97.571875    Top5 99.993750    LR 0.001500    Time 0.022459    
2018-10-28 01:10:31,027 - Epoch: [278][  300/  391]    Overall Loss 0.074014    Objective Loss 0.074014    Top1 97.526042    Top5 99.994792    LR 0.001500    Time 0.022419    
2018-10-28 01:10:32,139 - Epoch: [278][  350/  391]    Overall Loss 0.074754    Objective Loss 0.074754    Top1 97.502232    Top5 99.993304    LR 0.001500    Time 0.022391    
2018-10-28 01:10:33,132 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35262 |  0.00054 |    0.17720 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11210 | -0.00282 |    0.03546 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10814 | -0.00174 |    0.03843 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11192 | -0.00811 |    0.04750 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08079 | -0.00326 |    0.02333 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11901 | -0.00434 |    0.04630 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07888 |  0.00170 |    0.02151 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11044 | -0.00383 |    0.05230 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10329 | -0.00394 |    0.05880 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13425 | -0.00307 |    0.06473 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08430 | -0.00287 |    0.03343 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06883 |  0.00134 |    0.02456 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08868 | -0.00452 |    0.03848 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07195 | -0.00059 |    0.03216 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07107 | -0.00203 |    0.02726 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07696 | -0.00261 |    0.03818 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07396 | -0.00193 |    0.02649 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07227 | -0.00258 |    0.02945 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06155 | -0.00046 |    0.02770 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04807 | -0.00117 |    0.01531 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03165 |  0.00037 |    0.00754 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.58004 | -0.03067 |    0.33924 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:10:33,132 - Total sparsity: 74.94

2018-10-28 01:10:33,132 - --- validate (epoch=278)-----------
2018-10-28 01:10:33,132 - 10000 samples (128 per mini-batch)
2018-10-28 01:10:33,853 - Epoch: [278][   50/   78]    Loss 0.393115    Top1 89.953125    Top5 99.546875    
2018-10-28 01:10:34,244 - ==> Top1: 89.960    Top5: 99.620    Loss: 0.391

2018-10-28 01:10:34,245 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:10:34,245 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:10:34,259 - 

2018-10-28 01:10:34,259 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:10:35,433 - Epoch: [279][   50/  391]    Overall Loss 0.069960    Objective Loss 0.069960    Top1 97.593750    Top5 99.984375    LR 0.001500    Time 0.023427    
2018-10-28 01:10:36,544 - Epoch: [279][  100/  391]    Overall Loss 0.070699    Objective Loss 0.070699    Top1 97.570312    Top5 99.992188    LR 0.001500    Time 0.022814    
2018-10-28 01:10:37,655 - Epoch: [279][  150/  391]    Overall Loss 0.073491    Objective Loss 0.073491    Top1 97.421875    Top5 99.984375    LR 0.001500    Time 0.022608    
2018-10-28 01:10:38,767 - Epoch: [279][  200/  391]    Overall Loss 0.075649    Objective Loss 0.075649    Top1 97.355469    Top5 99.988281    LR 0.001500    Time 0.022492    
2018-10-28 01:10:39,879 - Epoch: [279][  250/  391]    Overall Loss 0.075529    Objective Loss 0.075529    Top1 97.396875    Top5 99.984375    LR 0.001500    Time 0.022435    
2018-10-28 01:10:40,992 - Epoch: [279][  300/  391]    Overall Loss 0.075236    Objective Loss 0.075236    Top1 97.434896    Top5 99.984375    LR 0.001500    Time 0.022401    
2018-10-28 01:10:42,104 - Epoch: [279][  350/  391]    Overall Loss 0.075123    Objective Loss 0.075123    Top1 97.424107    Top5 99.982143    LR 0.001500    Time 0.022376    
2018-10-28 01:10:43,098 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35243 |  0.00061 |    0.17706 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11204 | -0.00283 |    0.03544 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10808 | -0.00172 |    0.03841 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11186 | -0.00808 |    0.04745 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08075 | -0.00327 |    0.02332 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11895 | -0.00432 |    0.04626 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07884 |  0.00171 |    0.02150 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11038 | -0.00385 |    0.05226 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10324 | -0.00394 |    0.05877 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13418 | -0.00307 |    0.06469 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08425 | -0.00286 |    0.03341 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06879 |  0.00134 |    0.02454 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08863 | -0.00451 |    0.03845 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07191 | -0.00059 |    0.03214 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07103 | -0.00203 |    0.02724 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07692 | -0.00261 |    0.03815 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07392 | -0.00193 |    0.02647 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07224 | -0.00258 |    0.02943 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06151 | -0.00046 |    0.02769 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04804 | -0.00117 |    0.01530 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03163 |  0.00037 |    0.00754 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.58020 | -0.03065 |    0.33933 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:10:43,098 - Total sparsity: 74.94

2018-10-28 01:10:43,098 - --- validate (epoch=279)-----------
2018-10-28 01:10:43,098 - 10000 samples (128 per mini-batch)
2018-10-28 01:10:43,823 - Epoch: [279][   50/   78]    Loss 0.387543    Top1 90.015625    Top5 99.609375    
2018-10-28 01:10:44,217 - ==> Top1: 90.060    Top5: 99.660    Loss: 0.384

2018-10-28 01:10:44,217 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:10:44,217 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:10:44,228 - 

2018-10-28 01:10:44,229 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:10:45,402 - Epoch: [280][   50/  391]    Overall Loss 0.069439    Objective Loss 0.069439    Top1 97.734375    Top5 100.000000    LR 0.001500    Time 0.023441    
2018-10-28 01:10:46,513 - Epoch: [280][  100/  391]    Overall Loss 0.071841    Objective Loss 0.071841    Top1 97.648438    Top5 99.992188    LR 0.001500    Time 0.022817    
2018-10-28 01:10:47,624 - Epoch: [280][  150/  391]    Overall Loss 0.072199    Objective Loss 0.072199    Top1 97.598958    Top5 99.989583    LR 0.001500    Time 0.022605    
2018-10-28 01:10:48,734 - Epoch: [280][  200/  391]    Overall Loss 0.072675    Objective Loss 0.072675    Top1 97.562500    Top5 99.988281    LR 0.001500    Time 0.022500    
2018-10-28 01:10:49,845 - Epoch: [280][  250/  391]    Overall Loss 0.073686    Objective Loss 0.073686    Top1 97.543750    Top5 99.990625    LR 0.001500    Time 0.022437    
2018-10-28 01:10:50,955 - Epoch: [280][  300/  391]    Overall Loss 0.073838    Objective Loss 0.073838    Top1 97.518229    Top5 99.992188    LR 0.001500    Time 0.022396    
2018-10-28 01:10:52,064 - Epoch: [280][  350/  391]    Overall Loss 0.073483    Objective Loss 0.073483    Top1 97.535714    Top5 99.993304    LR 0.001500    Time 0.022360    
2018-10-28 01:10:53,053 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35223 |  0.00046 |    0.17702 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11198 | -0.00282 |    0.03544 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10802 | -0.00175 |    0.03839 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11180 | -0.00805 |    0.04743 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08070 | -0.00326 |    0.02331 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11888 | -0.00429 |    0.04624 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07879 |  0.00171 |    0.02148 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11032 | -0.00385 |    0.05223 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10318 | -0.00392 |    0.05874 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13410 | -0.00309 |    0.06468 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08420 | -0.00287 |    0.03339 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06875 |  0.00135 |    0.02453 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08858 | -0.00450 |    0.03843 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07187 | -0.00059 |    0.03213 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07099 | -0.00202 |    0.02723 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07688 | -0.00260 |    0.03813 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07388 | -0.00191 |    0.02645 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07220 | -0.00258 |    0.02941 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06148 | -0.00045 |    0.02767 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04802 | -0.00117 |    0.01529 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03161 |  0.00037 |    0.00753 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.58034 | -0.03064 |    0.33941 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:10:53,053 - Total sparsity: 74.94

2018-10-28 01:10:53,053 - --- validate (epoch=280)-----------
2018-10-28 01:10:53,053 - 10000 samples (128 per mini-batch)
2018-10-28 01:10:53,770 - Epoch: [280][   50/   78]    Loss 0.391533    Top1 90.000000    Top5 99.562500    
2018-10-28 01:10:54,151 - ==> Top1: 90.020    Top5: 99.620    Loss: 0.388

2018-10-28 01:10:54,152 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:10:54,152 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:10:54,161 - 

2018-10-28 01:10:54,162 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:10:55,336 - Epoch: [281][   50/  391]    Overall Loss 0.084336    Objective Loss 0.084336    Top1 97.125000    Top5 99.984375    LR 0.001500    Time 0.023438    
2018-10-28 01:10:56,447 - Epoch: [281][  100/  391]    Overall Loss 0.077696    Objective Loss 0.077696    Top1 97.429688    Top5 99.984375    LR 0.001500    Time 0.022822    
2018-10-28 01:10:57,563 - Epoch: [281][  150/  391]    Overall Loss 0.077521    Objective Loss 0.077521    Top1 97.421875    Top5 99.984375    LR 0.001500    Time 0.022645    
2018-10-28 01:10:58,676 - Epoch: [281][  200/  391]    Overall Loss 0.075212    Objective Loss 0.075212    Top1 97.464844    Top5 99.980469    LR 0.001500    Time 0.022540    
2018-10-28 01:10:59,787 - Epoch: [281][  250/  391]    Overall Loss 0.074243    Objective Loss 0.074243    Top1 97.471875    Top5 99.984375    LR 0.001500    Time 0.022473    
2018-10-28 01:11:00,903 - Epoch: [281][  300/  391]    Overall Loss 0.073970    Objective Loss 0.073970    Top1 97.458333    Top5 99.986979    LR 0.001500    Time 0.022443    
2018-10-28 01:11:02,014 - Epoch: [281][  350/  391]    Overall Loss 0.073982    Objective Loss 0.073982    Top1 97.477679    Top5 99.988839    LR 0.001500    Time 0.022408    
2018-10-28 01:11:03,002 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35204 |  0.00043 |    0.17686 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11192 | -0.00280 |    0.03541 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10796 | -0.00177 |    0.03836 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11174 | -0.00805 |    0.04740 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08066 | -0.00322 |    0.02330 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11882 | -0.00430 |    0.04623 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07875 |  0.00172 |    0.02146 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11026 | -0.00385 |    0.05220 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10312 | -0.00393 |    0.05870 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13403 | -0.00308 |    0.06462 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08416 | -0.00288 |    0.03337 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06871 |  0.00135 |    0.02451 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08853 | -0.00451 |    0.03841 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07183 | -0.00058 |    0.03212 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07096 | -0.00203 |    0.02721 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07683 | -0.00261 |    0.03811 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07384 | -0.00190 |    0.02643 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07216 | -0.00257 |    0.02939 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06145 | -0.00045 |    0.02766 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04799 | -0.00117 |    0.01528 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03159 |  0.00037 |    0.00753 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.58049 | -0.03064 |    0.33948 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:11:03,002 - Total sparsity: 74.94

2018-10-28 01:11:03,002 - --- validate (epoch=281)-----------
2018-10-28 01:11:03,002 - 10000 samples (128 per mini-batch)
2018-10-28 01:11:03,733 - Epoch: [281][   50/   78]    Loss 0.390652    Top1 89.875000    Top5 99.531250    
2018-10-28 01:11:04,122 - ==> Top1: 90.040    Top5: 99.600    Loss: 0.389

2018-10-28 01:11:04,123 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:11:04,123 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:11:04,134 - 

2018-10-28 01:11:04,134 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:11:05,306 - Epoch: [282][   50/  391]    Overall Loss 0.065448    Objective Loss 0.065448    Top1 97.921875    Top5 100.000000    LR 0.001500    Time 0.023397    
2018-10-28 01:11:06,418 - Epoch: [282][  100/  391]    Overall Loss 0.069124    Objective Loss 0.069124    Top1 97.679688    Top5 100.000000    LR 0.001500    Time 0.022814    
2018-10-28 01:11:07,532 - Epoch: [282][  150/  391]    Overall Loss 0.068736    Objective Loss 0.068736    Top1 97.734375    Top5 99.994792    LR 0.001500    Time 0.022626    
2018-10-28 01:11:08,647 - Epoch: [282][  200/  391]    Overall Loss 0.070904    Objective Loss 0.070904    Top1 97.609375    Top5 99.992188    LR 0.001500    Time 0.022535    
2018-10-28 01:11:09,758 - Epoch: [282][  250/  391]    Overall Loss 0.072386    Objective Loss 0.072386    Top1 97.515625    Top5 99.990625    LR 0.001500    Time 0.022468    
2018-10-28 01:11:10,870 - Epoch: [282][  300/  391]    Overall Loss 0.072942    Objective Loss 0.072942    Top1 97.486979    Top5 99.992188    LR 0.001500    Time 0.022424    
2018-10-28 01:11:11,984 - Epoch: [282][  350/  391]    Overall Loss 0.073310    Objective Loss 0.073310    Top1 97.497768    Top5 99.991071    LR 0.001500    Time 0.022400    
2018-10-28 01:11:12,975 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35184 |  0.00038 |    0.17682 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11185 | -0.00279 |    0.03540 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10790 | -0.00175 |    0.03833 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11168 | -0.00801 |    0.04737 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08061 | -0.00320 |    0.02329 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11875 | -0.00426 |    0.04620 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07871 |  0.00174 |    0.02145 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11020 | -0.00385 |    0.05217 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10307 | -0.00391 |    0.05867 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13395 | -0.00305 |    0.06458 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08411 | -0.00287 |    0.03335 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06868 |  0.00135 |    0.02450 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08848 | -0.00450 |    0.03838 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07179 | -0.00058 |    0.03210 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07092 | -0.00203 |    0.02720 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07679 | -0.00261 |    0.03809 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07379 | -0.00190 |    0.02642 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07212 | -0.00256 |    0.02937 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06141 | -0.00046 |    0.02764 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04796 | -0.00117 |    0.01528 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03158 |  0.00037 |    0.00753 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.58063 | -0.03063 |    0.33954 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:11:12,975 - Total sparsity: 74.94

2018-10-28 01:11:12,975 - --- validate (epoch=282)-----------
2018-10-28 01:11:12,975 - 10000 samples (128 per mini-batch)
2018-10-28 01:11:13,699 - Epoch: [282][   50/   78]    Loss 0.395633    Top1 89.906250    Top5 99.593750    
2018-10-28 01:11:14,089 - ==> Top1: 89.940    Top5: 99.630    Loss: 0.391

2018-10-28 01:11:14,090 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:11:14,090 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:11:14,106 - 

2018-10-28 01:11:14,106 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:11:15,252 - Epoch: [283][   50/  391]    Overall Loss 0.066418    Objective Loss 0.066418    Top1 97.781250    Top5 100.000000    LR 0.001500    Time 0.022875    
2018-10-28 01:11:16,364 - Epoch: [283][  100/  391]    Overall Loss 0.070603    Objective Loss 0.070603    Top1 97.632812    Top5 99.984375    LR 0.001500    Time 0.022550    
2018-10-28 01:11:17,476 - Epoch: [283][  150/  391]    Overall Loss 0.067836    Objective Loss 0.067836    Top1 97.734375    Top5 99.989583    LR 0.001500    Time 0.022437    
2018-10-28 01:11:18,590 - Epoch: [283][  200/  391]    Overall Loss 0.070568    Objective Loss 0.070568    Top1 97.593750    Top5 99.984375    LR 0.001500    Time 0.022390    
2018-10-28 01:11:19,703 - Epoch: [283][  250/  391]    Overall Loss 0.071204    Objective Loss 0.071204    Top1 97.590625    Top5 99.987500    LR 0.001500    Time 0.022357    
2018-10-28 01:11:20,817 - Epoch: [283][  300/  391]    Overall Loss 0.072453    Objective Loss 0.072453    Top1 97.518229    Top5 99.989583    LR 0.001500    Time 0.022341    
2018-10-28 01:11:21,931 - Epoch: [283][  350/  391]    Overall Loss 0.073881    Objective Loss 0.073881    Top1 97.495536    Top5 99.988839    LR 0.001500    Time 0.022328    
2018-10-28 01:11:22,921 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35165 |  0.00048 |    0.17682 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11179 | -0.00280 |    0.03538 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10784 | -0.00174 |    0.03831 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11161 | -0.00801 |    0.04735 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08057 | -0.00320 |    0.02327 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11869 | -0.00426 |    0.04616 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07866 |  0.00172 |    0.02144 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11014 | -0.00385 |    0.05214 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10301 | -0.00391 |    0.05865 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13388 | -0.00306 |    0.06456 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08407 | -0.00286 |    0.03333 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06864 |  0.00135 |    0.02448 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08844 | -0.00449 |    0.03837 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07175 | -0.00059 |    0.03208 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07088 | -0.00203 |    0.02719 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07675 | -0.00259 |    0.03807 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07375 | -0.00190 |    0.02641 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07208 | -0.00256 |    0.02935 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06138 | -0.00045 |    0.02763 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04794 | -0.00117 |    0.01527 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03156 |  0.00037 |    0.00752 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.58078 | -0.03062 |    0.33963 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:11:22,922 - Total sparsity: 74.94

2018-10-28 01:11:22,922 - --- validate (epoch=283)-----------
2018-10-28 01:11:22,922 - 10000 samples (128 per mini-batch)
2018-10-28 01:11:23,649 - Epoch: [283][   50/   78]    Loss 0.392797    Top1 89.812500    Top5 99.562500    
2018-10-28 01:11:24,043 - ==> Top1: 89.900    Top5: 99.620    Loss: 0.391

2018-10-28 01:11:24,043 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:11:24,043 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:11:24,054 - 

2018-10-28 01:11:24,054 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:11:25,227 - Epoch: [284][   50/  391]    Overall Loss 0.071189    Objective Loss 0.071189    Top1 97.609375    Top5 100.000000    LR 0.001500    Time 0.023416    
2018-10-28 01:11:26,338 - Epoch: [284][  100/  391]    Overall Loss 0.073783    Objective Loss 0.073783    Top1 97.492188    Top5 100.000000    LR 0.001500    Time 0.022810    
2018-10-28 01:11:27,452 - Epoch: [284][  150/  391]    Overall Loss 0.073476    Objective Loss 0.073476    Top1 97.494792    Top5 99.994792    LR 0.001500    Time 0.022623    
2018-10-28 01:11:28,564 - Epoch: [284][  200/  391]    Overall Loss 0.072590    Objective Loss 0.072590    Top1 97.527344    Top5 99.992188    LR 0.001500    Time 0.022503    
2018-10-28 01:11:29,679 - Epoch: [284][  250/  391]    Overall Loss 0.072476    Objective Loss 0.072476    Top1 97.543750    Top5 99.990625    LR 0.001500    Time 0.022455    
2018-10-28 01:11:30,793 - Epoch: [284][  300/  391]    Overall Loss 0.072868    Objective Loss 0.072868    Top1 97.515625    Top5 99.989583    LR 0.001500    Time 0.022421    
2018-10-28 01:11:31,905 - Epoch: [284][  350/  391]    Overall Loss 0.072340    Objective Loss 0.072340    Top1 97.542411    Top5 99.991071    LR 0.001500    Time 0.022393    
2018-10-28 01:11:32,897 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35145 |  0.00043 |    0.17669 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11173 | -0.00279 |    0.03536 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10778 | -0.00179 |    0.03828 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11155 | -0.00802 |    0.04731 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08052 | -0.00321 |    0.02325 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11862 | -0.00425 |    0.04613 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07862 |  0.00171 |    0.02143 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11008 | -0.00382 |    0.05211 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10295 | -0.00392 |    0.05862 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13380 | -0.00310 |    0.06452 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08402 | -0.00286 |    0.03331 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06860 |  0.00136 |    0.02447 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08839 | -0.00449 |    0.03835 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07172 | -0.00059 |    0.03206 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07084 | -0.00204 |    0.02717 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07671 | -0.00259 |    0.03804 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07371 | -0.00190 |    0.02639 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07204 | -0.00256 |    0.02934 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06135 | -0.00045 |    0.02761 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04791 | -0.00116 |    0.01526 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03154 |  0.00038 |    0.00752 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.58095 | -0.03061 |    0.33971 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:11:32,897 - Total sparsity: 74.94

2018-10-28 01:11:32,898 - --- validate (epoch=284)-----------
2018-10-28 01:11:32,898 - 10000 samples (128 per mini-batch)
2018-10-28 01:11:33,620 - Epoch: [284][   50/   78]    Loss 0.391149    Top1 89.843750    Top5 99.593750    
2018-10-28 01:11:34,008 - ==> Top1: 90.030    Top5: 99.640    Loss: 0.390

2018-10-28 01:11:34,009 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:11:34,009 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:11:34,020 - 

2018-10-28 01:11:34,020 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:11:35,193 - Epoch: [285][   50/  391]    Overall Loss 0.071790    Objective Loss 0.071790    Top1 97.640625    Top5 100.000000    LR 0.001500    Time 0.023431    
2018-10-28 01:11:36,306 - Epoch: [285][  100/  391]    Overall Loss 0.072231    Objective Loss 0.072231    Top1 97.679688    Top5 99.984375    LR 0.001500    Time 0.022832    
2018-10-28 01:11:37,418 - Epoch: [285][  150/  391]    Overall Loss 0.074236    Objective Loss 0.074236    Top1 97.578125    Top5 99.989583    LR 0.001500    Time 0.022625    
2018-10-28 01:11:38,529 - Epoch: [285][  200/  391]    Overall Loss 0.075579    Objective Loss 0.075579    Top1 97.554688    Top5 99.984375    LR 0.001500    Time 0.022518    
2018-10-28 01:11:39,642 - Epoch: [285][  250/  391]    Overall Loss 0.075099    Objective Loss 0.075099    Top1 97.559375    Top5 99.984375    LR 0.001500    Time 0.022445    
2018-10-28 01:11:40,754 - Epoch: [285][  300/  391]    Overall Loss 0.073845    Objective Loss 0.073845    Top1 97.575521    Top5 99.986979    LR 0.001500    Time 0.022408    
2018-10-28 01:11:41,867 - Epoch: [285][  350/  391]    Overall Loss 0.073997    Objective Loss 0.073997    Top1 97.549107    Top5 99.984375    LR 0.001500    Time 0.022381    
2018-10-28 01:11:42,857 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35126 |  0.00043 |    0.17653 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11167 | -0.00279 |    0.03535 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10772 | -0.00175 |    0.03826 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11149 | -0.00800 |    0.04729 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08048 | -0.00322 |    0.02324 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11856 | -0.00423 |    0.04610 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07858 |  0.00171 |    0.02142 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.11002 | -0.00380 |    0.05209 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10289 | -0.00392 |    0.05857 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13373 | -0.00308 |    0.06452 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08397 | -0.00285 |    0.03328 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06856 |  0.00134 |    0.02446 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08834 | -0.00448 |    0.03834 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07168 | -0.00060 |    0.03204 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07080 | -0.00203 |    0.02715 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07667 | -0.00259 |    0.03802 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07367 | -0.00190 |    0.02638 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07200 | -0.00255 |    0.02933 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06131 | -0.00046 |    0.02760 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04789 | -0.00117 |    0.01525 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03152 |  0.00038 |    0.00751 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.58110 | -0.03060 |    0.33979 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:11:42,857 - Total sparsity: 74.94

2018-10-28 01:11:42,857 - --- validate (epoch=285)-----------
2018-10-28 01:11:42,857 - 10000 samples (128 per mini-batch)
2018-10-28 01:11:43,579 - Epoch: [285][   50/   78]    Loss 0.394909    Top1 89.953125    Top5 99.593750    
2018-10-28 01:11:43,971 - ==> Top1: 89.950    Top5: 99.640    Loss: 0.393

2018-10-28 01:11:43,972 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:11:43,972 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:11:43,982 - 

2018-10-28 01:11:43,982 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:11:45,156 - Epoch: [286][   50/  391]    Overall Loss 0.068730    Objective Loss 0.068730    Top1 97.781250    Top5 100.000000    LR 0.001500    Time 0.023431    
2018-10-28 01:11:46,271 - Epoch: [286][  100/  391]    Overall Loss 0.070786    Objective Loss 0.070786    Top1 97.703125    Top5 99.992188    LR 0.001500    Time 0.022857    
2018-10-28 01:11:47,383 - Epoch: [286][  150/  391]    Overall Loss 0.071757    Objective Loss 0.071757    Top1 97.661458    Top5 99.989583    LR 0.001500    Time 0.022639    
2018-10-28 01:11:48,496 - Epoch: [286][  200/  391]    Overall Loss 0.072965    Objective Loss 0.072965    Top1 97.621094    Top5 99.988281    LR 0.001500    Time 0.022539    
2018-10-28 01:11:49,610 - Epoch: [286][  250/  391]    Overall Loss 0.072995    Objective Loss 0.072995    Top1 97.590625    Top5 99.984375    LR 0.001500    Time 0.022481    
2018-10-28 01:11:50,723 - Epoch: [286][  300/  391]    Overall Loss 0.072342    Objective Loss 0.072342    Top1 97.619792    Top5 99.986979    LR 0.001500    Time 0.022429    
2018-10-28 01:11:51,837 - Epoch: [286][  350/  391]    Overall Loss 0.072904    Objective Loss 0.072904    Top1 97.580357    Top5 99.986607    LR 0.001500    Time 0.022402    
2018-10-28 01:11:52,826 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35107 |  0.00031 |    0.17642 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11161 | -0.00279 |    0.03532 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10766 | -0.00175 |    0.03823 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11143 | -0.00800 |    0.04727 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08044 | -0.00320 |    0.02322 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11849 | -0.00427 |    0.04608 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07853 |  0.00171 |    0.02140 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.10995 | -0.00380 |    0.05207 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10284 | -0.00391 |    0.05854 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13365 | -0.00309 |    0.06446 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08393 | -0.00285 |    0.03326 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06852 |  0.00135 |    0.02444 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08829 | -0.00449 |    0.03830 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07164 | -0.00060 |    0.03203 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07076 | -0.00203 |    0.02714 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07662 | -0.00259 |    0.03800 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07363 | -0.00191 |    0.02636 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07196 | -0.00255 |    0.02931 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06128 | -0.00045 |    0.02758 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04786 | -0.00117 |    0.01524 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03151 |  0.00038 |    0.00751 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.58127 | -0.03059 |    0.33988 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:11:52,826 - Total sparsity: 74.94

2018-10-28 01:11:52,826 - --- validate (epoch=286)-----------
2018-10-28 01:11:52,827 - 10000 samples (128 per mini-batch)
2018-10-28 01:11:53,549 - Epoch: [286][   50/   78]    Loss 0.392108    Top1 89.953125    Top5 99.578125    
2018-10-28 01:11:53,935 - ==> Top1: 90.100    Top5: 99.640    Loss: 0.391

2018-10-28 01:11:53,936 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:11:53,936 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:11:53,947 - 

2018-10-28 01:11:53,948 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:11:55,119 - Epoch: [287][   50/  391]    Overall Loss 0.062976    Objective Loss 0.062976    Top1 97.734375    Top5 100.000000    LR 0.001500    Time 0.023391    
2018-10-28 01:11:56,231 - Epoch: [287][  100/  391]    Overall Loss 0.067614    Objective Loss 0.067614    Top1 97.601562    Top5 99.992188    LR 0.001500    Time 0.022803    
2018-10-28 01:11:57,342 - Epoch: [287][  150/  391]    Overall Loss 0.068753    Objective Loss 0.068753    Top1 97.578125    Top5 99.994792    LR 0.001500    Time 0.022600    
2018-10-28 01:11:58,453 - Epoch: [287][  200/  391]    Overall Loss 0.070472    Objective Loss 0.070472    Top1 97.511719    Top5 99.996094    LR 0.001500    Time 0.022499    
2018-10-28 01:11:59,566 - Epoch: [287][  250/  391]    Overall Loss 0.071000    Objective Loss 0.071000    Top1 97.528125    Top5 99.996875    LR 0.001500    Time 0.022446    
2018-10-28 01:12:00,679 - Epoch: [287][  300/  391]    Overall Loss 0.072123    Objective Loss 0.072123    Top1 97.500000    Top5 99.994792    LR 0.001500    Time 0.022413    
2018-10-28 01:12:01,789 - Epoch: [287][  350/  391]    Overall Loss 0.072275    Objective Loss 0.072275    Top1 97.513393    Top5 99.993304    LR 0.001500    Time 0.022379    
2018-10-28 01:12:02,781 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35087 |  0.00032 |    0.17633 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11154 | -0.00278 |    0.03531 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10760 | -0.00177 |    0.03822 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11137 | -0.00801 |    0.04724 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08039 | -0.00321 |    0.02321 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11843 | -0.00421 |    0.04607 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07849 |  0.00170 |    0.02140 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.10989 | -0.00383 |    0.05203 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10278 | -0.00389 |    0.05851 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13358 | -0.00310 |    0.06440 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08388 | -0.00284 |    0.03325 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06849 |  0.00136 |    0.02443 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08824 | -0.00449 |    0.03828 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07160 | -0.00061 |    0.03201 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07072 | -0.00202 |    0.02712 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07658 | -0.00259 |    0.03798 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07359 | -0.00192 |    0.02635 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07192 | -0.00255 |    0.02929 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06125 | -0.00045 |    0.02757 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04783 | -0.00117 |    0.01523 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03149 |  0.00038 |    0.00751 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.58143 | -0.03059 |    0.33997 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:12:02,781 - Total sparsity: 74.94

2018-10-28 01:12:02,781 - --- validate (epoch=287)-----------
2018-10-28 01:12:02,781 - 10000 samples (128 per mini-batch)
2018-10-28 01:12:03,504 - Epoch: [287][   50/   78]    Loss 0.392638    Top1 89.890625    Top5 99.578125    
2018-10-28 01:12:03,899 - ==> Top1: 89.840    Top5: 99.610    Loss: 0.391

2018-10-28 01:12:03,900 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:12:03,900 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:12:03,921 - 

2018-10-28 01:12:03,921 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:12:05,067 - Epoch: [288][   50/  391]    Overall Loss 0.066485    Objective Loss 0.066485    Top1 97.812500    Top5 100.000000    LR 0.001500    Time 0.022896    
2018-10-28 01:12:06,179 - Epoch: [288][  100/  391]    Overall Loss 0.066546    Objective Loss 0.066546    Top1 97.828125    Top5 99.992188    LR 0.001500    Time 0.022544    
2018-10-28 01:12:07,291 - Epoch: [288][  150/  391]    Overall Loss 0.071244    Objective Loss 0.071244    Top1 97.552083    Top5 99.994792    LR 0.001500    Time 0.022437    
2018-10-28 01:12:08,404 - Epoch: [288][  200/  391]    Overall Loss 0.071088    Objective Loss 0.071088    Top1 97.593750    Top5 99.996094    LR 0.001500    Time 0.022387    
2018-10-28 01:12:09,516 - Epoch: [288][  250/  391]    Overall Loss 0.071827    Objective Loss 0.071827    Top1 97.531250    Top5 99.993750    LR 0.001500    Time 0.022354    
2018-10-28 01:12:10,628 - Epoch: [288][  300/  391]    Overall Loss 0.072187    Objective Loss 0.072187    Top1 97.539062    Top5 99.994792    LR 0.001500    Time 0.022330    
2018-10-28 01:12:11,740 - Epoch: [288][  350/  391]    Overall Loss 0.072352    Objective Loss 0.072352    Top1 97.526786    Top5 99.995536    LR 0.001500    Time 0.022314    
2018-10-28 01:12:12,730 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35068 |  0.00021 |    0.17616 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11148 | -0.00279 |    0.03527 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10754 | -0.00177 |    0.03821 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11131 | -0.00800 |    0.04721 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08035 | -0.00320 |    0.02319 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11836 | -0.00420 |    0.04605 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07845 |  0.00169 |    0.02138 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.10983 | -0.00384 |    0.05200 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10273 | -0.00389 |    0.05848 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13350 | -0.00311 |    0.06437 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08384 | -0.00284 |    0.03324 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06845 |  0.00136 |    0.02441 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08819 | -0.00448 |    0.03826 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07156 | -0.00061 |    0.03199 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07068 | -0.00202 |    0.02711 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07654 | -0.00259 |    0.03796 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07355 | -0.00191 |    0.02633 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07188 | -0.00255 |    0.02928 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06121 | -0.00046 |    0.02756 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04781 | -0.00117 |    0.01522 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03147 |  0.00038 |    0.00750 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.58160 | -0.03059 |    0.34006 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:12:12,730 - Total sparsity: 74.94

2018-10-28 01:12:12,730 - --- validate (epoch=288)-----------
2018-10-28 01:12:12,730 - 10000 samples (128 per mini-batch)
2018-10-28 01:12:13,441 - Epoch: [288][   50/   78]    Loss 0.391077    Top1 89.984375    Top5 99.562500    
2018-10-28 01:12:13,821 - ==> Top1: 90.090    Top5: 99.630    Loss: 0.389

2018-10-28 01:12:13,821 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:12:13,822 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:12:13,834 - 

2018-10-28 01:12:13,834 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:12:15,010 - Epoch: [289][   50/  391]    Overall Loss 0.070338    Objective Loss 0.070338    Top1 97.546875    Top5 99.984375    LR 0.001500    Time 0.023485    
2018-10-28 01:12:16,123 - Epoch: [289][  100/  391]    Overall Loss 0.070661    Objective Loss 0.070661    Top1 97.601562    Top5 99.976562    LR 0.001500    Time 0.022864    
2018-10-28 01:12:17,239 - Epoch: [289][  150/  391]    Overall Loss 0.069176    Objective Loss 0.069176    Top1 97.666667    Top5 99.984375    LR 0.001500    Time 0.022674    
2018-10-28 01:12:18,353 - Epoch: [289][  200/  391]    Overall Loss 0.069706    Objective Loss 0.069706    Top1 97.644531    Top5 99.984375    LR 0.001500    Time 0.022567    
2018-10-28 01:12:19,464 - Epoch: [289][  250/  391]    Overall Loss 0.070524    Objective Loss 0.070524    Top1 97.628125    Top5 99.984375    LR 0.001500    Time 0.022492    
2018-10-28 01:12:20,575 - Epoch: [289][  300/  391]    Overall Loss 0.071485    Objective Loss 0.071485    Top1 97.572917    Top5 99.986979    LR 0.001500    Time 0.022443    
2018-10-28 01:12:21,685 - Epoch: [289][  350/  391]    Overall Loss 0.071127    Objective Loss 0.071127    Top1 97.593750    Top5 99.988839    LR 0.001500    Time 0.022404    
2018-10-28 01:12:22,677 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35048 |  0.00043 |    0.17603 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11142 | -0.00277 |    0.03524 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10748 | -0.00173 |    0.03817 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11124 | -0.00801 |    0.04717 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08030 | -0.00319 |    0.02318 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11829 | -0.00421 |    0.04604 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07840 |  0.00168 |    0.02136 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.10977 | -0.00380 |    0.05198 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10267 | -0.00388 |    0.05845 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13342 | -0.00312 |    0.06432 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08379 | -0.00285 |    0.03322 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06841 |  0.00137 |    0.02440 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08815 | -0.00449 |    0.03824 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07152 | -0.00061 |    0.03197 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07064 | -0.00201 |    0.02710 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07650 | -0.00259 |    0.03794 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07350 | -0.00191 |    0.02632 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07184 | -0.00255 |    0.02927 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06118 | -0.00046 |    0.02754 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04778 | -0.00116 |    0.01521 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03145 |  0.00038 |    0.00750 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.58177 | -0.03057 |    0.34016 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:12:22,677 - Total sparsity: 74.94

2018-10-28 01:12:22,678 - --- validate (epoch=289)-----------
2018-10-28 01:12:22,678 - 10000 samples (128 per mini-batch)
2018-10-28 01:12:23,403 - Epoch: [289][   50/   78]    Loss 0.392355    Top1 90.093750    Top5 99.609375    
2018-10-28 01:12:23,797 - ==> Top1: 90.160    Top5: 99.650    Loss: 0.391

2018-10-28 01:12:23,797 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:12:23,798 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:12:23,813 - 

2018-10-28 01:12:23,814 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:12:24,986 - Epoch: [290][   50/  391]    Overall Loss 0.070636    Objective Loss 0.070636    Top1 97.500000    Top5 100.000000    LR 0.001500    Time 0.023414    
2018-10-28 01:12:26,099 - Epoch: [290][  100/  391]    Overall Loss 0.070939    Objective Loss 0.070939    Top1 97.500000    Top5 100.000000    LR 0.001500    Time 0.022822    
2018-10-28 01:12:27,209 - Epoch: [290][  150/  391]    Overall Loss 0.072370    Objective Loss 0.072370    Top1 97.473958    Top5 99.994792    LR 0.001500    Time 0.022607    
2018-10-28 01:12:28,321 - Epoch: [290][  200/  391]    Overall Loss 0.071111    Objective Loss 0.071111    Top1 97.539062    Top5 99.996094    LR 0.001500    Time 0.022508    
2018-10-28 01:12:29,434 - Epoch: [290][  250/  391]    Overall Loss 0.071337    Objective Loss 0.071337    Top1 97.534375    Top5 99.996875    LR 0.001500    Time 0.022452    
2018-10-28 01:12:30,547 - Epoch: [290][  300/  391]    Overall Loss 0.072500    Objective Loss 0.072500    Top1 97.520833    Top5 99.994792    LR 0.001500    Time 0.022417    
2018-10-28 01:12:31,661 - Epoch: [290][  350/  391]    Overall Loss 0.071771    Objective Loss 0.071771    Top1 97.591518    Top5 99.993304    LR 0.001500    Time 0.022394    
2018-10-28 01:12:32,655 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35029 |  0.00038 |    0.17600 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11136 | -0.00280 |    0.03522 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10742 | -0.00179 |    0.03816 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11118 | -0.00799 |    0.04714 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08026 | -0.00318 |    0.02317 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11823 | -0.00421 |    0.04600 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07836 |  0.00171 |    0.02135 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.10971 | -0.00384 |    0.05195 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10261 | -0.00387 |    0.05841 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13335 | -0.00312 |    0.06429 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08374 | -0.00285 |    0.03320 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06837 |  0.00138 |    0.02438 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08810 | -0.00449 |    0.03821 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07148 | -0.00061 |    0.03196 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07060 | -0.00201 |    0.02708 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07646 | -0.00259 |    0.03791 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07346 | -0.00192 |    0.02631 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07180 | -0.00256 |    0.02925 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06115 | -0.00046 |    0.02752 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04776 | -0.00116 |    0.01520 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03144 |  0.00038 |    0.00749 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.58191 | -0.03057 |    0.34024 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:12:32,655 - Total sparsity: 74.94

2018-10-28 01:12:32,655 - --- validate (epoch=290)-----------
2018-10-28 01:12:32,656 - 10000 samples (128 per mini-batch)
2018-10-28 01:12:33,377 - Epoch: [290][   50/   78]    Loss 0.394194    Top1 89.765625    Top5 99.640625    
2018-10-28 01:12:33,784 - ==> Top1: 89.930    Top5: 99.670    Loss: 0.391

2018-10-28 01:12:33,785 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:12:33,785 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:12:33,796 - 

2018-10-28 01:12:33,796 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:12:34,969 - Epoch: [291][   50/  391]    Overall Loss 0.072562    Objective Loss 0.072562    Top1 97.578125    Top5 100.000000    LR 0.001500    Time 0.023424    
2018-10-28 01:12:36,082 - Epoch: [291][  100/  391]    Overall Loss 0.068970    Objective Loss 0.068970    Top1 97.625000    Top5 99.992188    LR 0.001500    Time 0.022825    
2018-10-28 01:12:37,195 - Epoch: [291][  150/  391]    Overall Loss 0.070257    Objective Loss 0.070257    Top1 97.598958    Top5 99.994792    LR 0.001500    Time 0.022629    
2018-10-28 01:12:38,308 - Epoch: [291][  200/  391]    Overall Loss 0.073007    Objective Loss 0.073007    Top1 97.488281    Top5 99.992188    LR 0.001500    Time 0.022530    
2018-10-28 01:12:39,421 - Epoch: [291][  250/  391]    Overall Loss 0.073117    Objective Loss 0.073117    Top1 97.475000    Top5 99.993750    LR 0.001500    Time 0.022472    
2018-10-28 01:12:40,534 - Epoch: [291][  300/  391]    Overall Loss 0.073133    Objective Loss 0.073133    Top1 97.471354    Top5 99.992188    LR 0.001500    Time 0.022431    
2018-10-28 01:12:41,647 - Epoch: [291][  350/  391]    Overall Loss 0.073228    Objective Loss 0.073228    Top1 97.446429    Top5 99.993304    LR 0.001500    Time 0.022404    
2018-10-28 01:12:42,644 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.35010 |  0.00042 |    0.17590 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11130 | -0.00279 |    0.03520 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10736 | -0.00177 |    0.03814 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11112 | -0.00800 |    0.04711 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08021 | -0.00319 |    0.02314 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11816 | -0.00423 |    0.04595 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07831 |  0.00170 |    0.02134 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.10965 | -0.00381 |    0.05191 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10256 | -0.00388 |    0.05839 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13327 | -0.00314 |    0.06423 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08370 | -0.00286 |    0.03318 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06834 |  0.00138 |    0.02437 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08805 | -0.00447 |    0.03819 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07144 | -0.00061 |    0.03194 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07056 | -0.00200 |    0.02706 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07641 | -0.00258 |    0.03789 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07342 | -0.00191 |    0.02628 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07176 | -0.00255 |    0.02923 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06111 | -0.00045 |    0.02751 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04773 | -0.00117 |    0.01519 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03142 |  0.00038 |    0.00749 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.58204 | -0.03057 |    0.34030 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:12:42,644 - Total sparsity: 74.94

2018-10-28 01:12:42,644 - --- validate (epoch=291)-----------
2018-10-28 01:12:42,645 - 10000 samples (128 per mini-batch)
2018-10-28 01:12:43,361 - Epoch: [291][   50/   78]    Loss 0.390295    Top1 90.062500    Top5 99.562500    
2018-10-28 01:12:43,751 - ==> Top1: 90.110    Top5: 99.610    Loss: 0.390

2018-10-28 01:12:43,752 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:12:43,752 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:12:43,773 - 

2018-10-28 01:12:43,774 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:12:44,918 - Epoch: [292][   50/  391]    Overall Loss 0.075325    Objective Loss 0.075325    Top1 97.468750    Top5 99.984375    LR 0.001500    Time 0.022855    
2018-10-28 01:12:46,031 - Epoch: [292][  100/  391]    Overall Loss 0.075088    Objective Loss 0.075088    Top1 97.453125    Top5 99.984375    LR 0.001500    Time 0.022542    
2018-10-28 01:12:47,143 - Epoch: [292][  150/  391]    Overall Loss 0.074567    Objective Loss 0.074567    Top1 97.489583    Top5 99.989583    LR 0.001500    Time 0.022433    
2018-10-28 01:12:48,254 - Epoch: [292][  200/  391]    Overall Loss 0.074694    Objective Loss 0.074694    Top1 97.468750    Top5 99.988281    LR 0.001500    Time 0.022375    
2018-10-28 01:12:49,368 - Epoch: [292][  250/  391]    Overall Loss 0.072551    Objective Loss 0.072551    Top1 97.581250    Top5 99.990625    LR 0.001500    Time 0.022349    
2018-10-28 01:12:50,479 - Epoch: [292][  300/  391]    Overall Loss 0.072153    Objective Loss 0.072153    Top1 97.557292    Top5 99.989583    LR 0.001500    Time 0.022326    
2018-10-28 01:12:51,593 - Epoch: [292][  350/  391]    Overall Loss 0.072975    Objective Loss 0.072975    Top1 97.555804    Top5 99.988839    LR 0.001500    Time 0.022304    
2018-10-28 01:12:52,582 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.34990 |  0.00016 |    0.17586 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11124 | -0.00277 |    0.03518 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10730 | -0.00181 |    0.03811 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11106 | -0.00795 |    0.04708 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08017 | -0.00321 |    0.02314 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11810 | -0.00419 |    0.04592 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07827 |  0.00170 |    0.02134 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.10959 | -0.00382 |    0.05188 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10250 | -0.00388 |    0.05836 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13320 | -0.00319 |    0.06418 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08365 | -0.00285 |    0.03316 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06830 |  0.00138 |    0.02436 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08800 | -0.00448 |    0.03817 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07140 | -0.00061 |    0.03192 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07053 | -0.00200 |    0.02704 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07637 | -0.00259 |    0.03787 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07338 | -0.00192 |    0.02626 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07172 | -0.00256 |    0.02922 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06108 | -0.00046 |    0.02749 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04770 | -0.00116 |    0.01518 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03140 |  0.00038 |    0.00749 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.58220 | -0.03057 |    0.34037 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:12:52,582 - Total sparsity: 74.94

2018-10-28 01:12:52,583 - --- validate (epoch=292)-----------
2018-10-28 01:12:52,583 - 10000 samples (128 per mini-batch)
2018-10-28 01:12:53,305 - Epoch: [292][   50/   78]    Loss 0.394862    Top1 89.859375    Top5 99.593750    
2018-10-28 01:12:53,697 - ==> Top1: 89.960    Top5: 99.630    Loss: 0.392

2018-10-28 01:12:53,697 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:12:53,698 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:12:53,715 - 

2018-10-28 01:12:53,716 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:12:54,877 - Epoch: [293][   50/  391]    Overall Loss 0.069542    Objective Loss 0.069542    Top1 97.546875    Top5 100.000000    LR 0.001500    Time 0.023186    
2018-10-28 01:12:55,989 - Epoch: [293][  100/  391]    Overall Loss 0.071733    Objective Loss 0.071733    Top1 97.531250    Top5 99.992188    LR 0.001500    Time 0.022703    
2018-10-28 01:12:57,101 - Epoch: [293][  150/  391]    Overall Loss 0.071593    Objective Loss 0.071593    Top1 97.541667    Top5 99.994792    LR 0.001500    Time 0.022539    
2018-10-28 01:12:58,214 - Epoch: [293][  200/  391]    Overall Loss 0.071632    Objective Loss 0.071632    Top1 97.550781    Top5 99.996094    LR 0.001500    Time 0.022461    
2018-10-28 01:12:59,326 - Epoch: [293][  250/  391]    Overall Loss 0.071728    Objective Loss 0.071728    Top1 97.531250    Top5 99.993750    LR 0.001500    Time 0.022413    
2018-10-28 01:13:00,442 - Epoch: [293][  300/  391]    Overall Loss 0.071760    Objective Loss 0.071760    Top1 97.510417    Top5 99.989583    LR 0.001500    Time 0.022393    
2018-10-28 01:13:01,552 - Epoch: [293][  350/  391]    Overall Loss 0.071428    Objective Loss 0.071428    Top1 97.542411    Top5 99.988839    LR 0.001500    Time 0.022361    
2018-10-28 01:13:02,545 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.34971 |  0.00015 |    0.17579 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11118 | -0.00274 |    0.03517 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10724 | -0.00177 |    0.03809 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11100 | -0.00794 |    0.04706 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08012 | -0.00322 |    0.02313 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11803 | -0.00421 |    0.04589 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07823 |  0.00171 |    0.02132 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.10953 | -0.00382 |    0.05184 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10244 | -0.00389 |    0.05833 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13312 | -0.00319 |    0.06413 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08361 | -0.00286 |    0.03314 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06826 |  0.00139 |    0.02435 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08795 | -0.00448 |    0.03815 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07136 | -0.00060 |    0.03191 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07049 | -0.00201 |    0.02703 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07633 | -0.00257 |    0.03785 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07334 | -0.00191 |    0.02624 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07168 | -0.00255 |    0.02920 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06105 | -0.00046 |    0.02748 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04768 | -0.00116 |    0.01518 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03138 |  0.00038 |    0.00748 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.58236 | -0.03056 |    0.34046 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:13:02,545 - Total sparsity: 74.94

2018-10-28 01:13:02,545 - --- validate (epoch=293)-----------
2018-10-28 01:13:02,545 - 10000 samples (128 per mini-batch)
2018-10-28 01:13:03,272 - Epoch: [293][   50/   78]    Loss 0.394407    Top1 89.921875    Top5 99.609375    
2018-10-28 01:13:03,673 - ==> Top1: 90.010    Top5: 99.650    Loss: 0.393

2018-10-28 01:13:03,674 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:13:03,674 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:13:03,686 - 

2018-10-28 01:13:03,686 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:13:04,856 - Epoch: [294][   50/  391]    Overall Loss 0.077720    Objective Loss 0.077720    Top1 97.250000    Top5 100.000000    LR 0.001500    Time 0.023364    
2018-10-28 01:13:05,967 - Epoch: [294][  100/  391]    Overall Loss 0.071503    Objective Loss 0.071503    Top1 97.500000    Top5 100.000000    LR 0.001500    Time 0.022773    
2018-10-28 01:13:07,077 - Epoch: [294][  150/  391]    Overall Loss 0.071383    Objective Loss 0.071383    Top1 97.578125    Top5 100.000000    LR 0.001500    Time 0.022579    
2018-10-28 01:13:08,191 - Epoch: [294][  200/  391]    Overall Loss 0.070922    Objective Loss 0.070922    Top1 97.578125    Top5 100.000000    LR 0.001500    Time 0.022493    
2018-10-28 01:13:09,301 - Epoch: [294][  250/  391]    Overall Loss 0.072075    Objective Loss 0.072075    Top1 97.512500    Top5 100.000000    LR 0.001500    Time 0.022431    
2018-10-28 01:13:10,412 - Epoch: [294][  300/  391]    Overall Loss 0.072859    Objective Loss 0.072859    Top1 97.471354    Top5 99.994792    LR 0.001500    Time 0.022391    
2018-10-28 01:13:11,521 - Epoch: [294][  350/  391]    Overall Loss 0.073355    Objective Loss 0.073355    Top1 97.466518    Top5 99.993304    LR 0.001500    Time 0.022357    
2018-10-28 01:13:12,513 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.34952 |  0.00040 |    0.17562 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11112 | -0.00275 |    0.03515 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10718 | -0.00180 |    0.03806 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11094 | -0.00794 |    0.04703 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08008 | -0.00322 |    0.02310 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11797 | -0.00422 |    0.04588 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07819 |  0.00168 |    0.02131 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.10947 | -0.00381 |    0.05181 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10239 | -0.00393 |    0.05830 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13305 | -0.00317 |    0.06412 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08356 | -0.00285 |    0.03312 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06822 |  0.00138 |    0.02433 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08791 | -0.00448 |    0.03813 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07132 | -0.00060 |    0.03188 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07045 | -0.00201 |    0.02702 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07629 | -0.00256 |    0.03783 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07330 | -0.00191 |    0.02622 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07164 | -0.00255 |    0.02918 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06101 | -0.00045 |    0.02746 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04765 | -0.00116 |    0.01517 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03137 |  0.00038 |    0.00748 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.58249 | -0.03054 |    0.34052 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:13:12,513 - Total sparsity: 74.94

2018-10-28 01:13:12,513 - --- validate (epoch=294)-----------
2018-10-28 01:13:12,513 - 10000 samples (128 per mini-batch)
2018-10-28 01:13:13,253 - Epoch: [294][   50/   78]    Loss 0.397676    Top1 90.015625    Top5 99.625000    
2018-10-28 01:13:13,705 - ==> Top1: 89.990    Top5: 99.650    Loss: 0.396

2018-10-28 01:13:13,705 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:13:13,706 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:13:13,714 - 

2018-10-28 01:13:13,714 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:13:14,886 - Epoch: [295][   50/  391]    Overall Loss 0.066889    Objective Loss 0.066889    Top1 97.625000    Top5 99.984375    LR 0.001500    Time 0.023396    
2018-10-28 01:13:15,997 - Epoch: [295][  100/  391]    Overall Loss 0.070425    Objective Loss 0.070425    Top1 97.664062    Top5 99.976562    LR 0.001500    Time 0.022802    
2018-10-28 01:13:17,108 - Epoch: [295][  150/  391]    Overall Loss 0.068622    Objective Loss 0.068622    Top1 97.729167    Top5 99.979167    LR 0.001500    Time 0.022596    
2018-10-28 01:13:18,219 - Epoch: [295][  200/  391]    Overall Loss 0.068498    Objective Loss 0.068498    Top1 97.691406    Top5 99.984375    LR 0.001500    Time 0.022495    
2018-10-28 01:13:19,329 - Epoch: [295][  250/  391]    Overall Loss 0.069795    Objective Loss 0.069795    Top1 97.625000    Top5 99.984375    LR 0.001500    Time 0.022430    
2018-10-28 01:13:20,440 - Epoch: [295][  300/  391]    Overall Loss 0.070478    Objective Loss 0.070478    Top1 97.585938    Top5 99.986979    LR 0.001500    Time 0.022393    
2018-10-28 01:13:21,553 - Epoch: [295][  350/  391]    Overall Loss 0.070603    Objective Loss 0.070603    Top1 97.578125    Top5 99.988839    LR 0.001500    Time 0.022370    
2018-10-28 01:13:22,549 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.34933 |  0.00014 |    0.17555 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11105 | -0.00276 |    0.03513 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10712 | -0.00180 |    0.03803 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11088 | -0.00796 |    0.04701 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.08003 | -0.00321 |    0.02308 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11790 | -0.00426 |    0.04586 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07814 |  0.00166 |    0.02130 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.10941 | -0.00381 |    0.05178 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10233 | -0.00394 |    0.05827 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13298 | -0.00317 |    0.06410 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08351 | -0.00285 |    0.03310 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06819 |  0.00138 |    0.02431 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08786 | -0.00448 |    0.03811 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07129 | -0.00060 |    0.03187 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07041 | -0.00200 |    0.02700 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07625 | -0.00255 |    0.03780 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07326 | -0.00190 |    0.02620 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07161 | -0.00255 |    0.02917 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06098 | -0.00045 |    0.02745 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04763 | -0.00116 |    0.01516 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03135 |  0.00038 |    0.00747 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.58263 | -0.03054 |    0.34059 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:13:22,550 - Total sparsity: 74.94

2018-10-28 01:13:22,550 - --- validate (epoch=295)-----------
2018-10-28 01:13:22,550 - 10000 samples (128 per mini-batch)
2018-10-28 01:13:23,273 - Epoch: [295][   50/   78]    Loss 0.399515    Top1 90.031250    Top5 99.625000    
2018-10-28 01:13:23,665 - ==> Top1: 90.010    Top5: 99.640    Loss: 0.399

2018-10-28 01:13:23,666 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:13:23,666 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:13:23,678 - 

2018-10-28 01:13:23,678 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:13:24,849 - Epoch: [296][   50/  391]    Overall Loss 0.073490    Objective Loss 0.073490    Top1 97.390625    Top5 100.000000    LR 0.001500    Time 0.023384    
2018-10-28 01:13:25,960 - Epoch: [296][  100/  391]    Overall Loss 0.072817    Objective Loss 0.072817    Top1 97.468750    Top5 99.992188    LR 0.001500    Time 0.022791    
2018-10-28 01:13:27,073 - Epoch: [296][  150/  391]    Overall Loss 0.072682    Objective Loss 0.072682    Top1 97.447917    Top5 99.984375    LR 0.001500    Time 0.022605    
2018-10-28 01:13:28,186 - Epoch: [296][  200/  391]    Overall Loss 0.072069    Objective Loss 0.072069    Top1 97.421875    Top5 99.984375    LR 0.001500    Time 0.022513    
2018-10-28 01:13:29,300 - Epoch: [296][  250/  391]    Overall Loss 0.071183    Objective Loss 0.071183    Top1 97.525000    Top5 99.987500    LR 0.001500    Time 0.022459    
2018-10-28 01:13:30,413 - Epoch: [296][  300/  391]    Overall Loss 0.070975    Objective Loss 0.070975    Top1 97.533854    Top5 99.989583    LR 0.001500    Time 0.022421    
2018-10-28 01:13:31,525 - Epoch: [296][  350/  391]    Overall Loss 0.071188    Objective Loss 0.071188    Top1 97.540179    Top5 99.991071    LR 0.001500    Time 0.022391    
2018-10-28 01:13:32,518 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.34913 |  0.00047 |    0.17552 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11099 | -0.00274 |    0.03510 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10707 | -0.00177 |    0.03801 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11082 | -0.00798 |    0.04698 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.07999 | -0.00322 |    0.02305 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11784 | -0.00425 |    0.04585 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07810 |  0.00168 |    0.02129 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.10935 | -0.00380 |    0.05175 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10227 | -0.00393 |    0.05825 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13290 | -0.00317 |    0.06407 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08347 | -0.00284 |    0.03308 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06815 |  0.00138 |    0.02430 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08781 | -0.00446 |    0.03809 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07125 | -0.00061 |    0.03186 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07037 | -0.00200 |    0.02699 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07621 | -0.00255 |    0.03778 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07322 | -0.00189 |    0.02619 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07157 | -0.00254 |    0.02915 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06095 | -0.00045 |    0.02743 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04760 | -0.00116 |    0.01515 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03133 |  0.00038 |    0.00747 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.58278 | -0.03054 |    0.34065 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:13:32,518 - Total sparsity: 74.94

2018-10-28 01:13:32,518 - --- validate (epoch=296)-----------
2018-10-28 01:13:32,518 - 10000 samples (128 per mini-batch)
2018-10-28 01:13:33,245 - Epoch: [296][   50/   78]    Loss 0.390984    Top1 90.000000    Top5 99.609375    
2018-10-28 01:13:33,637 - ==> Top1: 90.140    Top5: 99.630    Loss: 0.388

2018-10-28 01:13:33,638 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:13:33,638 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:13:33,648 - 

2018-10-28 01:13:33,649 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:13:34,820 - Epoch: [297][   50/  391]    Overall Loss 0.068559    Objective Loss 0.068559    Top1 97.546875    Top5 99.984375    LR 0.001500    Time 0.023401    
2018-10-28 01:13:35,934 - Epoch: [297][  100/  391]    Overall Loss 0.071677    Objective Loss 0.071677    Top1 97.445312    Top5 99.992188    LR 0.001500    Time 0.022820    
2018-10-28 01:13:37,048 - Epoch: [297][  150/  391]    Overall Loss 0.072973    Objective Loss 0.072973    Top1 97.432292    Top5 99.984375    LR 0.001500    Time 0.022633    
2018-10-28 01:13:38,161 - Epoch: [297][  200/  391]    Overall Loss 0.074199    Objective Loss 0.074199    Top1 97.359375    Top5 99.988281    LR 0.001500    Time 0.022532    
2018-10-28 01:13:39,274 - Epoch: [297][  250/  391]    Overall Loss 0.074116    Objective Loss 0.074116    Top1 97.384375    Top5 99.990625    LR 0.001500    Time 0.022475    
2018-10-28 01:13:40,388 - Epoch: [297][  300/  391]    Overall Loss 0.073186    Objective Loss 0.073186    Top1 97.434896    Top5 99.989583    LR 0.001500    Time 0.022436    
2018-10-28 01:13:41,503 - Epoch: [297][  350/  391]    Overall Loss 0.072564    Objective Loss 0.072564    Top1 97.466518    Top5 99.991071    LR 0.001500    Time 0.022413    
2018-10-28 01:13:42,497 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.34894 |  0.00073 |    0.17540 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11093 | -0.00274 |    0.03507 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10701 | -0.00176 |    0.03799 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11076 | -0.00796 |    0.04694 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.07995 | -0.00320 |    0.02304 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11777 | -0.00420 |    0.04581 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07806 |  0.00169 |    0.02128 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.10929 | -0.00377 |    0.05172 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10222 | -0.00393 |    0.05821 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13283 | -0.00321 |    0.06402 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08342 | -0.00283 |    0.03306 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06811 |  0.00139 |    0.02428 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08776 | -0.00446 |    0.03807 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07121 | -0.00061 |    0.03183 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07033 | -0.00199 |    0.02697 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07616 | -0.00255 |    0.03776 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07318 | -0.00189 |    0.02618 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07153 | -0.00255 |    0.02914 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06091 | -0.00044 |    0.02742 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04757 | -0.00117 |    0.01514 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03131 |  0.00038 |    0.00746 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.58289 | -0.03052 |    0.34073 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:13:42,497 - Total sparsity: 74.94

2018-10-28 01:13:42,497 - --- validate (epoch=297)-----------
2018-10-28 01:13:42,498 - 10000 samples (128 per mini-batch)
2018-10-28 01:13:43,217 - Epoch: [297][   50/   78]    Loss 0.398437    Top1 90.046875    Top5 99.609375    
2018-10-28 01:13:43,607 - ==> Top1: 90.080    Top5: 99.660    Loss: 0.398

2018-10-28 01:13:43,608 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:13:43,608 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:13:43,617 - 

2018-10-28 01:13:43,618 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:13:44,787 - Epoch: [298][   50/  391]    Overall Loss 0.079213    Objective Loss 0.079213    Top1 97.281250    Top5 99.984375    LR 0.001500    Time 0.023355    
2018-10-28 01:13:45,898 - Epoch: [298][  100/  391]    Overall Loss 0.076706    Objective Loss 0.076706    Top1 97.367188    Top5 99.984375    LR 0.001500    Time 0.022769    
2018-10-28 01:13:47,010 - Epoch: [298][  150/  391]    Overall Loss 0.076549    Objective Loss 0.076549    Top1 97.296875    Top5 99.989583    LR 0.001500    Time 0.022585    
2018-10-28 01:13:48,121 - Epoch: [298][  200/  391]    Overall Loss 0.074424    Objective Loss 0.074424    Top1 97.367188    Top5 99.992188    LR 0.001500    Time 0.022488    
2018-10-28 01:13:49,233 - Epoch: [298][  250/  391]    Overall Loss 0.073331    Objective Loss 0.073331    Top1 97.440625    Top5 99.990625    LR 0.001500    Time 0.022433    
2018-10-28 01:13:50,345 - Epoch: [298][  300/  391]    Overall Loss 0.072136    Objective Loss 0.072136    Top1 97.486979    Top5 99.989583    LR 0.001500    Time 0.022396    
2018-10-28 01:13:51,456 - Epoch: [298][  350/  391]    Overall Loss 0.072174    Objective Loss 0.072174    Top1 97.508929    Top5 99.991071    LR 0.001500    Time 0.022367    
2018-10-28 01:13:52,443 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.34875 |  0.00036 |    0.17527 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11087 | -0.00274 |    0.03505 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10695 | -0.00177 |    0.03796 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11070 | -0.00798 |    0.04692 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.07990 | -0.00319 |    0.02303 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11771 | -0.00420 |    0.04578 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07801 |  0.00171 |    0.02127 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.10923 | -0.00375 |    0.05168 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10216 | -0.00393 |    0.05817 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13275 | -0.00314 |    0.06398 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08338 | -0.00282 |    0.03304 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06807 |  0.00139 |    0.02427 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08772 | -0.00444 |    0.03804 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07117 | -0.00060 |    0.03181 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07029 | -0.00199 |    0.02695 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07612 | -0.00256 |    0.03774 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07314 | -0.00188 |    0.02616 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07149 | -0.00254 |    0.02912 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06088 | -0.00044 |    0.02740 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04755 | -0.00116 |    0.01514 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03130 |  0.00039 |    0.00746 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.58303 | -0.03052 |    0.34078 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:13:52,443 - Total sparsity: 74.94

2018-10-28 01:13:52,443 - --- validate (epoch=298)-----------
2018-10-28 01:13:52,443 - 10000 samples (128 per mini-batch)
2018-10-28 01:13:53,169 - Epoch: [298][   50/   78]    Loss 0.395333    Top1 90.187500    Top5 99.625000    
2018-10-28 01:13:53,563 - ==> Top1: 90.140    Top5: 99.650    Loss: 0.394

2018-10-28 01:13:53,563 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:13:53,564 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:13:53,577 - 

2018-10-28 01:13:53,578 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-28 01:13:54,751 - Epoch: [299][   50/  391]    Overall Loss 0.065467    Objective Loss 0.065467    Top1 97.703125    Top5 100.000000    LR 0.001500    Time 0.023436    
2018-10-28 01:13:55,864 - Epoch: [299][  100/  391]    Overall Loss 0.065621    Objective Loss 0.065621    Top1 97.718750    Top5 100.000000    LR 0.001500    Time 0.022835    
2018-10-28 01:13:56,978 - Epoch: [299][  150/  391]    Overall Loss 0.066573    Objective Loss 0.066573    Top1 97.682292    Top5 99.989583    LR 0.001500    Time 0.022639    
2018-10-28 01:13:58,092 - Epoch: [299][  200/  391]    Overall Loss 0.067355    Objective Loss 0.067355    Top1 97.710938    Top5 99.992188    LR 0.001500    Time 0.022544    
2018-10-28 01:13:59,205 - Epoch: [299][  250/  391]    Overall Loss 0.069008    Objective Loss 0.069008    Top1 97.665625    Top5 99.990625    LR 0.001500    Time 0.022482    
2018-10-28 01:14:00,322 - Epoch: [299][  300/  391]    Overall Loss 0.070386    Objective Loss 0.070386    Top1 97.630208    Top5 99.989583    LR 0.001500    Time 0.022456    
2018-10-28 01:14:01,436 - Epoch: [299][  350/  391]    Overall Loss 0.070461    Objective Loss 0.070461    Top1 97.627232    Top5 99.991071    LR 0.001500    Time 0.022427    
2018-10-28 01:14:02,428 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            172 |    0.00000 |    0.00000 |  0.00000 | 10.41667 |  6.25000 |   60.18519 | 0.34855 |  0.00034 |    0.17518 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            427 |    0.00000 |    0.00000 |  6.25000 | 25.39062 |  0.00000 |   81.46701 | 0.11081 | -0.00276 |    0.03504 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            470 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   79.60069 | 0.10689 | -0.00178 |    0.03795 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            678 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   70.57292 | 0.11064 | -0.00793 |    0.04691 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.89062 | 0.07986 | -0.00317 |    0.02301 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            548 |    0.00000 |    0.00000 |  0.00000 | 18.35938 |  0.00000 |   76.21528 | 0.11764 | -0.00421 |    0.04575 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            258 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.80208 | 0.07797 |  0.00169 |    0.02125 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1629 |    0.00000 |    0.00000 |  0.00000 |  6.64062 |  0.00000 |   64.64844 | 0.10917 | -0.00374 |    0.05167 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4652 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   49.52257 | 0.10210 | -0.00395 |    0.05814 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            167 |    0.00000 |    0.00000 |  0.00000 | 67.38281 |  0.00000 |   67.38281 | 0.13268 | -0.00317 |    0.06398 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2224 |    0.00000 |    0.00000 |  0.00000 | 12.89062 |  0.00000 |   75.86806 | 0.08333 | -0.00282 |    0.03302 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1746 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.05469 | 0.06804 |  0.00138 |    0.02426 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2651 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   71.23481 | 0.08767 | -0.00441 |    0.03802 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2857 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   68.99957 | 0.07113 | -0.00061 |    0.03180 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3970 |    0.00000 |    0.00000 |  0.00000 | 19.04297 |  0.00000 |   78.46137 | 0.07025 | -0.00199 |    0.02694 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          14100 |    0.00000 |    0.00000 |  0.00000 |  2.29492 |  0.00000 |   61.75130 | 0.07608 | -0.00255 |    0.03772 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            360 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  0.00000 |   82.42188 | 0.07309 | -0.00188 |    0.02614 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9431 |    0.00000 |    0.00000 |  0.00000 | 10.27832 |  0.00000 |   74.41678 | 0.07145 | -0.00254 |    0.02910 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          11562 |    0.00000 |    0.00000 |  0.00000 |  7.91016 |  0.00000 |   68.63607 | 0.06085 | -0.00044 |    0.02739 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5952 |    0.00000 |    0.00000 |  0.00000 | 30.29785 |  0.00000 |   83.85417 | 0.04752 | -0.00116 |    0.01513 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 52.17285 |  0.00000 |   90.68197 | 0.03128 |  0.00039 |    0.00745 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            313 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   51.09375 | 0.58315 | -0.03053 |    0.34084 |
| 22 | Total sparsity:                     | -              |        270896 |          67881 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   74.94204 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-28 01:14:02,428 - Total sparsity: 74.94

2018-10-28 01:14:02,428 - --- validate (epoch=299)-----------
2018-10-28 01:14:02,428 - 10000 samples (128 per mini-batch)
2018-10-28 01:14:03,149 - Epoch: [299][   50/   78]    Loss 0.396897    Top1 89.984375    Top5 99.593750    
2018-10-28 01:14:03,538 - ==> Top1: 90.020    Top5: 99.630    Loss: 0.395

2018-10-28 01:14:03,539 - ==> Best Top1: 90.440   On Epoch: 213

2018-10-28 01:14:03,539 - Saving checkpoint to: logs/2018.10.28-002406/checkpoint.pth.tar
2018-10-28 01:14:03,557 - --- test ---------------------
2018-10-28 01:14:03,557 - 10000 samples (128 per mini-batch)
2018-10-28 01:14:04,283 - Test: [   50/   78]    Loss 0.396897    Top1 89.984375    Top5 99.593750    
2018-10-28 01:14:04,677 - ==> Top1: 90.020    Top5: 99.630    Loss: 0.395

2018-10-28 01:14:04,680 - 
2018-10-28 01:14:04,681 - Log file for this run: /home/ccma/Chilung/1022/distiller/examples/classifier_compression/logs/2018.10.28-002406/2018.10.28-002406.log
