2018-10-27 22:46:28,913 - Log file for this run: /home/ccma/Chilung/1022/distiller/examples/classifier_compression/logs/2018.10.27-224628/2018.10.27-224628.log
2018-10-27 22:46:28,913 - Number of CPUs: 8
2018-10-27 22:46:28,944 - Number of GPUs: 1
2018-10-27 22:46:28,944 - CUDA version: 8.0.61
2018-10-27 22:46:28,945 - CUDNN version: 7102
2018-10-27 22:46:28,945 - Kernel: 4.13.0-38-generic
2018-10-27 22:46:28,945 - Python: 3.5.2 (default, Nov 23 2017, 16:37:01) 
[GCC 5.4.0 20160609]
2018-10-27 22:46:28,945 - PyTorch: 0.4.0
2018-10-27 22:46:28,945 - Numpy: 1.14.3
2018-10-27 22:46:28,963 - Git is dirty
2018-10-27 22:46:28,964 - Active Git branch: master
2018-10-27 22:46:28,967 - Git commit: 8bf95d12172fb6e82a00ce40007953e23d9648c7
2018-10-27 22:46:28,967 - App args: ['compress_classifier.py', '--arch', 'resnet20_cifar', '--lr', '0.3', '-p', '50', '../../../data.cifar10', '-b', '128', '-j', '1', '--vs', '0', '--deterministic', '--epochs', '300', '--compress=../../../resnet20_cifar_baseline/resnet20_cifar_baseline.yaml']
2018-10-27 22:46:28,968 - ==> using cifar10 dataset
2018-10-27 22:46:28,968 - => creating resnet20_cifar model for CIFAR10
2018-10-27 22:46:31,815 - Optimizer Type: <class 'torch.optim.sgd.SGD'>
2018-10-27 22:46:31,815 - Optimizer Args: {'dampening': 0, 'weight_decay': 0.0001, 'momentum': 0.9, 'nesterov': False, 'lr': 0.3}
2018-10-27 22:46:33,220 - Dataset sizes:
	training=50000
	validation=10000
	test=10000
2018-10-27 22:46:33,221 - Reading compression schedule from: ../../../resnet20_cifar_baseline/resnet20_cifar_baseline.yaml
2018-10-27 22:46:33,260 - Schedule contents:
{
  "pruners": {
    "pruner1": {
      "class": "SensitivityPruner",
      "sensitivities": {
        "module.conv1.weight": 0.7,
        "module.layer1.0.conv1.weight": 0.7,
        "module.layer1.0.conv2.weight": 0.75,
        "module.layer1.1.conv1.weight": 0.65,
        "module.layer1.1.conv2.weight": 0.75,
        "module.layer1.2.conv1.weight": 0.45,
        "module.layer1.2.conv2.weight": 0.6,
        "module.layer2.0.conv1.weight": 0.4,
        "module.layer2.0.conv2.weight": 0.45,
        "module.layer2.0.downsample.0.weight": 0.6,
        "module.layer2.1.conv1.weight": 0.55,
        "module.layer2.1.conv2.weight": 0.65,
        "module.layer2.2.conv1.weight": 0.5,
        "module.layer2.2.conv2.weight": 0.6,
        "module.layer3.0.conv1.weight": 0.5,
        "module.layer3.0.conv2.weight": 0.4,
        "module.layer3.0.downsample.0.weight": 0.65,
        "module.layer3.1.conv1.weight": 0.45,
        "module.layer3.1.conv2.weight": 0.55,
        "module.layer3.2.conv1.weight": 0.5,
        "module.layer3.2.conv2.weight": 0.8,
        "module.fc.weight": 0.85
      }
    }
  },
  "lr_schedulers": {
    "training_lr": {
      "class": "MultiStepMultiGammaLR",
      "milestones": [
        100,
        200,
        250
      ],
      "gammas": [
        0.1,
        0.1,
        0.5
      ]
    }
  },
  "policies": [
    {
      "pruner": {
        "instance_name": "pruner1"
      },
      "starting_epoch": 0,
      "ending_epoch": 38,
      "frequency": 2
    },
    {
      "lr_scheduler": {
        "instance_name": "training_lr"
      },
      "starting_epoch": 0,
      "ending_epoch": 301,
      "frequency": 1
    }
  ]
}
2018-10-27 22:46:33,262 - 

2018-10-27 22:46:33,272 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:46:34,488 - Epoch: [0][   50/  391]    Overall Loss 2.135167    Objective Loss 2.135167    Top1 20.281250    Top5 73.687500    LR 0.300000    Time 0.024285    
2018-10-27 22:46:35,620 - Epoch: [0][  100/  391]    Overall Loss 1.987160    Objective Loss 1.987160    Top1 24.671875    Top5 79.273438    LR 0.300000    Time 0.023443    
2018-10-27 22:46:36,750 - Epoch: [0][  150/  391]    Overall Loss 1.904643    Objective Loss 1.904643    Top1 27.630208    Top5 81.796875    LR 0.300000    Time 0.023159    
2018-10-27 22:46:37,882 - Epoch: [0][  200/  391]    Overall Loss 1.853157    Objective Loss 1.853157    Top1 29.824219    Top5 83.226562    LR 0.300000    Time 0.023021    
2018-10-27 22:46:39,018 - Epoch: [0][  250/  391]    Overall Loss 1.800956    Objective Loss 1.800956    Top1 32.106250    Top5 84.581250    LR 0.300000    Time 0.022955    
2018-10-27 22:46:40,155 - Epoch: [0][  300/  391]    Overall Loss 1.755486    Objective Loss 1.755486    Top1 34.044271    Top5 85.627604    LR 0.300000    Time 0.022916    
2018-10-27 22:46:41,293 - Epoch: [0][  350/  391]    Overall Loss 1.716196    Objective Loss 1.716196    Top1 35.613839    Top5 86.497768    LR 0.300000    Time 0.022890    
2018-10-27 22:46:42,310 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            212 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   50.92593 | 0.40363 |  0.00302 |    0.21738 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |           1099 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   52.30035 | 0.15929 | -0.01753 |    0.08788 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |           1028 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   55.38194 | 0.14908 |  0.00001 |    0.08023 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |           1210 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   47.48264 | 0.14236 | -0.00403 |    0.08431 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |           1054 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   54.25347 | 0.13063 | -0.00382 |    0.07342 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |           1456 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   36.80556 | 0.13034 | -0.00545 |    0.08630 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |           1245 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   45.96354 | 0.12548 | -0.00152 |    0.07732 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           3198 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   30.59896 | 0.10234 | -0.00376 |    0.06880 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           6029 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   34.58116 | 0.09245 | -0.00302 |    0.06145 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            285 |    0.00000 |    0.00000 |  0.00000 | 44.33594 |  0.00000 |   44.33594 | 0.25307 |  0.01035 |    0.16083 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           5249 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   43.04470 | 0.09016 | -0.00314 |    0.05658 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           4811 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   47.79731 | 0.08821 | -0.00380 |    0.05352 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           5689 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   38.27040 | 0.08833 | -0.00076 |    0.05804 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           5099 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   44.67231 | 0.08519 | -0.00660 |    0.05416 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |          11446 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   37.90148 | 0.06886 | -0.00073 |    0.04449 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          25429 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   31.01942 | 0.06431 | -0.00445 |    0.04392 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |           1062 |    0.00000 |    0.00000 |  0.00000 | 48.14453 |  0.00000 |   48.14453 | 0.17396 | -0.00298 |    0.10810 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          24005 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   34.88227 | 0.06179 | -0.00138 |    0.04133 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          21515 |    0.00000 |    0.00000 |  0.00000 |  0.04883 |  0.00000 |   41.63683 | 0.06097 | -0.00152 |    0.03912 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          22703 |    0.00000 |    0.00000 |  0.00000 |  0.07324 |  0.00000 |   38.41417 | 0.05795 | -0.00007 |    0.03862 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |          15631 |    0.00000 |    0.00000 |  0.00000 |  0.56152 |  0.00000 |   57.59820 | 0.05627 | -0.00309 |    0.03171 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            323 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   49.53125 | 0.29850 | -0.05936 |    0.16984 |
| 22 | Total sparsity:                     | -              |        270896 |         159778 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   41.01869 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:46:42,311 - Total sparsity: 41.02

2018-10-27 22:46:42,311 - --- validate (epoch=0)-----------
2018-10-27 22:46:42,311 - 10000 samples (128 per mini-batch)
2018-10-27 22:46:43,034 - Epoch: [0][   50/   78]    Loss 1.569615    Top1 43.765625    Top5 90.921875    
2018-10-27 22:46:43,426 - ==> Top1: 43.650    Top5: 90.580    Loss: 1.576

2018-10-27 22:46:43,426 - ==> Best Top1: 43.650   On Epoch: 0

2018-10-27 22:46:43,427 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:46:43,530 - 

2018-10-27 22:46:43,530 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:46:44,675 - Epoch: [1][   50/  391]    Overall Loss 1.390032    Objective Loss 1.390032    Top1 49.890625    Top5 93.453125    LR 0.300000    Time 0.022878    
2018-10-27 22:46:45,806 - Epoch: [1][  100/  391]    Overall Loss 1.336576    Objective Loss 1.336576    Top1 51.617187    Top5 93.851562    LR 0.300000    Time 0.022728    
2018-10-27 22:46:46,939 - Epoch: [1][  150/  391]    Overall Loss 1.300230    Objective Loss 1.300230    Top1 52.890625    Top5 94.244792    LR 0.300000    Time 0.022698    
2018-10-27 22:46:48,059 - Epoch: [1][  200/  391]    Overall Loss 1.274476    Objective Loss 1.274476    Top1 53.976562    Top5 94.515625    LR 0.300000    Time 0.022618    
2018-10-27 22:46:49,175 - Epoch: [1][  250/  391]    Overall Loss 1.245546    Objective Loss 1.245546    Top1 55.012500    Top5 94.743750    LR 0.300000    Time 0.022552    
2018-10-27 22:46:50,301 - Epoch: [1][  300/  391]    Overall Loss 1.225858    Objective Loss 1.225858    Top1 55.653646    Top5 94.903646    LR 0.300000    Time 0.022545    
2018-10-27 22:46:51,426 - Epoch: [1][  350/  391]    Overall Loss 1.204629    Objective Loss 1.204629    Top1 56.415179    Top5 95.093750    LR 0.300000    Time 0.022532    
2018-10-27 22:46:52,425 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            212 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   50.92593 | 0.46084 | -0.00443 |    0.23560 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |           1099 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   52.30035 | 0.17358 | -0.01586 |    0.09240 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |           1028 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   55.38194 | 0.16583 | -0.00249 |    0.08588 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |           1210 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   47.48264 | 0.15904 | -0.00530 |    0.08967 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |           1054 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   54.25347 | 0.14233 | -0.00727 |    0.07612 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |           1456 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   36.80556 | 0.14237 | -0.00801 |    0.09084 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |           1245 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   45.96354 | 0.13543 | -0.00404 |    0.07905 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           3198 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   30.59896 | 0.12874 | -0.00618 |    0.08409 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           6029 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   34.58116 | 0.11095 | -0.00492 |    0.07131 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            285 |    0.00000 |    0.00000 |  0.00000 | 44.33594 |  0.00000 |   44.33594 | 0.28435 |  0.00766 |    0.17443 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           5249 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   43.04470 | 0.10651 | -0.00458 |    0.06378 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           4811 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   47.79731 | 0.09978 | -0.00546 |    0.05819 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           5689 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   38.27040 | 0.10160 | -0.00239 |    0.06402 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           5099 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   44.67231 | 0.09295 | -0.00996 |    0.05682 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |          11446 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   37.90148 | 0.08521 | -0.00086 |    0.05356 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          25429 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   31.01942 | 0.07413 | -0.00520 |    0.04901 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |           1062 |    0.00000 |    0.00000 |  0.00000 | 48.14453 |  0.00000 |   48.14453 | 0.18371 | -0.00578 |    0.10918 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          24005 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   34.88227 | 0.06885 | -0.00251 |    0.04456 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          21515 |    0.00000 |    0.00000 |  0.00000 |  0.04883 |  0.00000 |   41.63683 | 0.06489 | -0.00281 |    0.04004 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          22703 |    0.00000 |    0.00000 |  0.00000 |  0.07324 |  0.00000 |   38.41417 | 0.05969 | -0.00048 |    0.03827 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |          15631 |    0.00000 |    0.00000 |  0.00000 |  0.56152 |  0.00000 |   57.59820 | 0.05692 | -0.00291 |    0.03075 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            323 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   49.53125 | 0.38080 | -0.07665 |    0.22195 |
| 22 | Total sparsity:                     | -              |        270896 |         159778 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   41.01869 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:46:52,425 - Total sparsity: 41.02

2018-10-27 22:46:52,425 - --- validate (epoch=1)-----------
2018-10-27 22:46:52,426 - 10000 samples (128 per mini-batch)
2018-10-27 22:46:53,157 - Epoch: [1][   50/   78]    Loss 1.425859    Top1 56.375000    Top5 95.062500    
2018-10-27 22:46:53,553 - ==> Top1: 56.220    Top5: 95.250    Loss: 1.431

2018-10-27 22:46:53,554 - ==> Best Top1: 56.220   On Epoch: 1

2018-10-27 22:46:53,554 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:46:53,570 - 

2018-10-27 22:46:53,572 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:46:54,763 - Epoch: [2][   50/  391]    Overall Loss 1.003155    Objective Loss 1.003155    Top1 64.968750    Top5 97.140625    LR 0.300000    Time 0.023802    
2018-10-27 22:46:55,908 - Epoch: [2][  100/  391]    Overall Loss 1.001989    Objective Loss 1.001989    Top1 64.781250    Top5 97.062500    LR 0.300000    Time 0.023330    
2018-10-27 22:46:57,055 - Epoch: [2][  150/  391]    Overall Loss 0.979074    Objective Loss 0.979074    Top1 65.343750    Top5 97.109375    LR 0.300000    Time 0.023194    
2018-10-27 22:46:58,208 - Epoch: [2][  200/  391]    Overall Loss 0.959329    Objective Loss 0.959329    Top1 66.125000    Top5 97.121094    LR 0.300000    Time 0.023153    
2018-10-27 22:46:59,361 - Epoch: [2][  250/  391]    Overall Loss 0.951700    Objective Loss 0.951700    Top1 66.421875    Top5 97.206250    LR 0.300000    Time 0.023128    
2018-10-27 22:47:00,515 - Epoch: [2][  300/  391]    Overall Loss 0.941387    Objective Loss 0.941387    Top1 66.776042    Top5 97.234375    LR 0.300000    Time 0.023119    
2018-10-27 22:47:01,656 - Epoch: [2][  350/  391]    Overall Loss 0.930456    Objective Loss 0.930456    Top1 67.154018    Top5 97.292411    LR 0.300000    Time 0.023071    
2018-10-27 22:47:02,673 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            188 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   56.48148 | 0.48834 | -0.00915 |    0.24313 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            818 |    0.00000 |    0.00000 |  0.00000 |  4.29688 |  0.00000 |   64.49653 | 0.17802 | -0.01466 |    0.08470 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            724 |    0.00000 |    0.00000 |  0.00000 |  4.29688 |  0.00000 |   68.57639 | 0.17309 | -0.00172 |    0.07925 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            854 |    0.00000 |    0.00000 |  0.00000 |  1.17188 |  0.00000 |   62.93403 | 0.16406 | -0.00572 |    0.08086 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            721 |    0.00000 |    0.00000 |  0.00000 |  4.29688 |  0.00000 |   68.70660 | 0.14449 | -0.00729 |    0.06775 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |           1156 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   49.82639 | 0.14790 | -0.00845 |    0.08589 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            885 |    0.00000 |    0.00000 |  0.00000 |  1.56250 |  0.00000 |   61.58854 | 0.13888 | -0.00449 |    0.07106 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2637 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   42.77344 | 0.14600 | -0.00703 |    0.08830 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4721 |    0.00000 |    0.00000 |  0.00000 |  0.29297 |  0.00000 |   48.77387 | 0.12090 | -0.00478 |    0.07070 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            206 |    0.00000 |    0.00000 |  0.00000 | 59.76562 |  0.00000 |   59.76562 | 0.29165 |  0.01404 |    0.16114 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           3935 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   57.30252 | 0.11269 | -0.00508 |    0.06071 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           3401 |    0.00000 |    0.00000 |  0.00000 |  1.85547 |  0.00000 |   63.09679 | 0.10337 | -0.00475 |    0.05333 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           4338 |    0.00000 |    0.00000 |  0.00000 |  0.58594 |  0.00000 |   52.92969 | 0.10777 | -0.00356 |    0.06091 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           3688 |    0.00000 |    0.00000 |  0.00000 |  1.17188 |  0.00000 |   59.98264 | 0.09621 | -0.00915 |    0.05188 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           9031 |    0.00000 |    0.00000 |  0.00000 |  0.09766 |  0.00000 |   51.00369 | 0.09548 | -0.00142 |    0.05491 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          20232 |    0.00000 |    0.00000 |  0.00000 |  0.07324 |  0.00000 |   45.11719 | 0.08245 | -0.00628 |    0.04977 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            752 |    0.00000 |    0.00000 |  0.00000 | 63.28125 |  0.00000 |   63.28125 | 0.18119 | -0.00677 |    0.09566 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          18400 |    0.00000 |    0.00000 |  0.00000 |  0.21973 |  0.00000 |   50.08681 | 0.07517 | -0.00272 |    0.04369 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          15623 |    0.00000 |    0.00000 |  0.00000 |  0.80566 |  0.00000 |   57.61990 | 0.06775 | -0.00315 |    0.03687 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          16765 |    0.00000 |    0.00000 |  0.00000 |  0.73242 |  0.00000 |   54.52203 | 0.06120 | -0.00033 |    0.03488 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |          10231 |    0.00000 |    0.00000 |  0.00000 |  5.39551 |  0.00000 |   72.24664 | 0.05583 | -0.00267 |    0.02588 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            298 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   53.43750 | 0.41765 | -0.08040 |    0.23949 |
| 22 | Total sparsity:                     | -              |        270896 |         119604 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   55.84874 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:47:02,673 - Total sparsity: 55.85

2018-10-27 22:47:02,673 - --- validate (epoch=2)-----------
2018-10-27 22:47:02,673 - 10000 samples (128 per mini-batch)
2018-10-27 22:47:03,399 - Epoch: [2][   50/   78]    Loss 0.999244    Top1 67.250000    Top5 97.140625    
2018-10-27 22:47:03,795 - ==> Top1: 67.400    Top5: 97.310    Loss: 0.993

2018-10-27 22:47:03,796 - ==> Best Top1: 67.400   On Epoch: 2

2018-10-27 22:47:03,796 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:47:03,813 - 

2018-10-27 22:47:03,813 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:47:04,966 - Epoch: [3][   50/  391]    Overall Loss 0.822287    Objective Loss 0.822287    Top1 70.734375    Top5 97.921875    LR 0.300000    Time 0.023023    
2018-10-27 22:47:06,098 - Epoch: [3][  100/  391]    Overall Loss 0.828107    Objective Loss 0.828107    Top1 70.601562    Top5 97.851562    LR 0.300000    Time 0.022818    
2018-10-27 22:47:07,239 - Epoch: [3][  150/  391]    Overall Loss 0.814896    Objective Loss 0.814896    Top1 71.281250    Top5 97.760417    LR 0.300000    Time 0.022813    
2018-10-27 22:47:08,365 - Epoch: [3][  200/  391]    Overall Loss 0.811471    Objective Loss 0.811471    Top1 71.437500    Top5 97.863281    LR 0.300000    Time 0.022730    
2018-10-27 22:47:09,490 - Epoch: [3][  250/  391]    Overall Loss 0.808604    Objective Loss 0.808604    Top1 71.665625    Top5 97.865625    LR 0.300000    Time 0.022681    
2018-10-27 22:47:10,618 - Epoch: [3][  300/  391]    Overall Loss 0.802388    Objective Loss 0.802388    Top1 71.963542    Top5 97.927083    LR 0.300000    Time 0.022642    
2018-10-27 22:47:11,744 - Epoch: [3][  350/  391]    Overall Loss 0.798040    Objective Loss 0.798040    Top1 72.127232    Top5 97.921875    LR 0.300000    Time 0.022622    
2018-10-27 22:47:12,749 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            188 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   56.48148 | 0.50584 | -0.00104 |    0.24755 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            818 |    0.00000 |    0.00000 |  0.00000 |  4.29688 |  0.00000 |   64.49653 | 0.17898 | -0.01543 |    0.08061 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            724 |    0.00000 |    0.00000 |  0.00000 |  4.29688 |  0.00000 |   68.57639 | 0.17852 | -0.00518 |    0.07869 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            854 |    0.00000 |    0.00000 |  0.00000 |  1.17188 |  0.00000 |   62.93403 | 0.16788 | -0.00740 |    0.07967 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            721 |    0.00000 |    0.00000 |  0.00000 |  4.29688 |  0.00000 |   68.70660 | 0.14699 | -0.00838 |    0.06621 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |           1156 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   49.82639 | 0.15269 | -0.01103 |    0.08633 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            885 |    0.00000 |    0.00000 |  0.00000 |  1.56250 |  0.00000 |   61.58854 | 0.14168 | -0.00405 |    0.07031 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2637 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   42.77344 | 0.15804 | -0.00817 |    0.09371 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           4721 |    0.00000 |    0.00000 |  0.00000 |  0.29297 |  0.00000 |   48.77387 | 0.12839 | -0.00473 |    0.07405 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            206 |    0.00000 |    0.00000 |  0.00000 | 59.76562 |  0.00000 |   59.76562 | 0.29753 |  0.01137 |    0.15990 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           3935 |    0.00000 |    0.00000 |  0.00000 |  0.19531 |  0.00000 |   57.30252 | 0.11782 | -0.00516 |    0.06209 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           3401 |    0.00000 |    0.00000 |  0.00000 |  1.85547 |  0.00000 |   63.09679 | 0.10638 | -0.00463 |    0.05337 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           4338 |    0.00000 |    0.00000 |  0.00000 |  0.58594 |  0.00000 |   52.92969 | 0.11389 | -0.00474 |    0.06287 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           3688 |    0.00000 |    0.00000 |  0.00000 |  1.17188 |  0.00000 |   59.98264 | 0.09926 | -0.00966 |    0.05195 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           9031 |    0.00000 |    0.00000 |  0.00000 |  0.09766 |  0.00000 |   51.00369 | 0.10389 | -0.00238 |    0.05861 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          20232 |    0.00000 |    0.00000 |  0.00000 |  0.07324 |  0.00000 |   45.11719 | 0.09045 | -0.00708 |    0.05351 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            752 |    0.00000 |    0.00000 |  0.00000 | 63.28125 |  0.00000 |   63.28125 | 0.18116 | -0.00758 |    0.09355 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          18400 |    0.00000 |    0.00000 |  0.00000 |  0.21973 |  0.00000 |   50.08681 | 0.08229 | -0.00346 |    0.04652 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          15623 |    0.00000 |    0.00000 |  0.00000 |  0.80566 |  0.00000 |   57.61990 | 0.07160 | -0.00371 |    0.03761 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          16765 |    0.00000 |    0.00000 |  0.00000 |  0.73242 |  0.00000 |   54.52203 | 0.06358 | -0.00035 |    0.03481 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |          10231 |    0.00000 |    0.00000 |  0.00000 |  5.39551 |  0.00000 |   72.24664 | 0.05601 | -0.00218 |    0.02489 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            298 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   53.43750 | 0.43210 | -0.08108 |    0.24834 |
| 22 | Total sparsity:                     | -              |        270896 |         119604 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   55.84874 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:47:12,749 - Total sparsity: 55.85

2018-10-27 22:47:12,749 - --- validate (epoch=3)-----------
2018-10-27 22:47:12,750 - 10000 samples (128 per mini-batch)
2018-10-27 22:47:13,479 - Epoch: [3][   50/   78]    Loss 0.776585    Top1 73.203125    Top5 98.250000    
2018-10-27 22:47:13,867 - ==> Top1: 72.900    Top5: 98.180    Loss: 0.784

2018-10-27 22:47:13,868 - ==> Best Top1: 72.900   On Epoch: 3

2018-10-27 22:47:13,868 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:47:13,893 - 

2018-10-27 22:47:13,895 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:47:15,076 - Epoch: [4][   50/  391]    Overall Loss 0.722605    Objective Loss 0.722605    Top1 74.500000    Top5 98.312500    LR 0.300000    Time 0.023588    
2018-10-27 22:47:16,230 - Epoch: [4][  100/  391]    Overall Loss 0.722212    Objective Loss 0.722212    Top1 74.476562    Top5 98.312500    LR 0.300000    Time 0.023322    
2018-10-27 22:47:17,384 - Epoch: [4][  150/  391]    Overall Loss 0.713849    Objective Loss 0.713849    Top1 74.776042    Top5 98.291667    LR 0.300000    Time 0.023229    
2018-10-27 22:47:18,533 - Epoch: [4][  200/  391]    Overall Loss 0.713453    Objective Loss 0.713453    Top1 74.832031    Top5 98.285156    LR 0.300000    Time 0.023159    
2018-10-27 22:47:19,681 - Epoch: [4][  250/  391]    Overall Loss 0.708027    Objective Loss 0.708027    Top1 75.087500    Top5 98.321875    LR 0.300000    Time 0.023117    
2018-10-27 22:47:20,831 - Epoch: [4][  300/  391]    Overall Loss 0.708913    Objective Loss 0.708913    Top1 75.062500    Top5 98.315104    LR 0.300000    Time 0.023091    
2018-10-27 22:47:21,980 - Epoch: [4][  350/  391]    Overall Loss 0.705118    Objective Loss 0.705118    Top1 75.205357    Top5 98.330357    LR 0.300000    Time 0.023063    
2018-10-27 22:47:23,003 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            171 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   60.41667 | 0.51120 | -0.00559 |    0.24459 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            677 |    0.00000 |    0.00000 |  0.00000 |  7.42188 |  0.00000 |   70.61632 | 0.17601 | -0.01210 |    0.07449 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            582 |    0.00000 |    0.00000 |  0.00000 |  8.98438 |  0.00000 |   74.73958 | 0.17968 | -0.00464 |    0.07326 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            698 |    0.00000 |    0.00000 |  0.00000 |  3.90625 |  0.00000 |   69.70486 | 0.16988 | -0.00602 |    0.07488 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            567 |    0.00000 |    0.00000 |  0.00000 | 10.93750 |  0.00000 |   75.39062 | 0.14645 | -0.00693 |    0.06057 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            960 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   58.33333 | 0.15266 | -0.00964 |    0.08024 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            708 |    0.00000 |    0.00000 |  0.00000 |  3.51562 |  0.00000 |   69.27083 | 0.13928 | -0.00334 |    0.06440 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2279 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   50.54253 | 0.16497 | -0.00750 |    0.09322 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           3993 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   56.67318 | 0.13168 | -0.00471 |    0.07147 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            170 |    0.00000 |    0.00000 |  0.00000 | 66.79688 |  0.00000 |   66.79688 | 0.29354 |  0.00956 |    0.14876 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           3241 |    0.00000 |    0.00000 |  0.00000 |  1.66016 |  0.00000 |   64.83290 | 0.11869 | -0.00454 |    0.05873 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           2749 |    0.00000 |    0.00000 |  0.00000 |  5.76172 |  0.00000 |   70.17144 | 0.10613 | -0.00430 |    0.04936 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           3605 |    0.00000 |    0.00000 |  0.00000 |  1.26953 |  0.00000 |   60.88325 | 0.11672 | -0.00463 |    0.06037 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2942 |    0.00000 |    0.00000 |  0.00000 |  3.71094 |  0.00000 |   68.07726 | 0.09980 | -0.00912 |    0.04837 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           7717 |    0.00000 |    0.00000 |  0.00000 |  0.68359 |  0.00000 |   58.13260 | 0.10893 | -0.00281 |    0.05838 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          17255 |    0.00000 |    0.00000 |  0.00000 |  0.31738 |  0.00000 |   53.19282 | 0.09555 | -0.00710 |    0.05358 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            615 |    0.00000 |    0.00000 |  0.00000 | 69.97070 |  0.00000 |   69.97070 | 0.17727 | -0.00696 |    0.08562 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          15309 |    0.00000 |    0.00000 |  0.00000 |  1.09863 |  0.00000 |   58.47168 | 0.08694 | -0.00396 |    0.04619 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          12424 |    0.00000 |    0.00000 |  0.00000 |  2.97852 |  0.00000 |   66.29774 | 0.07390 | -0.00378 |    0.03579 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          13333 |    0.00000 |    0.00000 |  0.00000 |  2.44141 |  0.00000 |   63.83192 | 0.06475 | -0.00028 |    0.03269 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           7918 |    0.00000 |    0.00000 |  0.00000 | 12.01172 |  0.00000 |   78.52105 | 0.05519 | -0.00167 |    0.02210 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            276 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   56.87500 | 0.44076 | -0.08259 |    0.25218 |
| 22 | Total sparsity:                     | -              |        270896 |          98189 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   63.75399 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:47:23,004 - Total sparsity: 63.75

2018-10-27 22:47:23,004 - --- validate (epoch=4)-----------
2018-10-27 22:47:23,004 - 10000 samples (128 per mini-batch)
2018-10-27 22:47:23,722 - Epoch: [4][   50/   78]    Loss 0.922544    Top1 70.281250    Top5 97.156250    
2018-10-27 22:47:24,109 - ==> Top1: 69.950    Top5: 97.000    Loss: 0.930

2018-10-27 22:47:24,110 - ==> Best Top1: 72.900   On Epoch: 3

2018-10-27 22:47:24,110 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:47:24,126 - 

2018-10-27 22:47:24,126 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:47:25,282 - Epoch: [5][   50/  391]    Overall Loss 0.681007    Objective Loss 0.681007    Top1 76.796875    Top5 98.562500    LR 0.300000    Time 0.023095    
2018-10-27 22:47:26,407 - Epoch: [5][  100/  391]    Overall Loss 0.669567    Objective Loss 0.669567    Top1 76.984375    Top5 98.632812    LR 0.300000    Time 0.022783    
2018-10-27 22:47:27,533 - Epoch: [5][  150/  391]    Overall Loss 0.663693    Objective Loss 0.663693    Top1 77.145833    Top5 98.661458    LR 0.300000    Time 0.022689    
2018-10-27 22:47:28,661 - Epoch: [5][  200/  391]    Overall Loss 0.664461    Objective Loss 0.664461    Top1 77.121094    Top5 98.628906    LR 0.300000    Time 0.022648    
2018-10-27 22:47:29,786 - Epoch: [5][  250/  391]    Overall Loss 0.666264    Objective Loss 0.666264    Top1 77.003125    Top5 98.646875    LR 0.300000    Time 0.022615    
2018-10-27 22:47:30,911 - Epoch: [5][  300/  391]    Overall Loss 0.669369    Objective Loss 0.669369    Top1 76.869792    Top5 98.570312    LR 0.300000    Time 0.022592    
2018-10-27 22:47:32,038 - Epoch: [5][  350/  391]    Overall Loss 0.671338    Objective Loss 0.671338    Top1 76.756696    Top5 98.535714    LR 0.300000    Time 0.022579    
2018-10-27 22:47:33,042 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            171 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   60.41667 | 0.51363 | -0.00128 |    0.24258 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            677 |    0.00000 |    0.00000 |  0.00000 |  7.42188 |  0.00000 |   70.61632 | 0.17373 | -0.01219 |    0.07174 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            582 |    0.00000 |    0.00000 |  0.00000 |  8.98438 |  0.00000 |   74.73958 | 0.17959 | -0.00166 |    0.07116 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            698 |    0.00000 |    0.00000 |  0.00000 |  3.90625 |  0.00000 |   69.70486 | 0.17014 | -0.00565 |    0.07268 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            567 |    0.00000 |    0.00000 |  0.00000 | 10.93750 |  0.00000 |   75.39062 | 0.14656 | -0.00594 |    0.05853 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            960 |    0.00000 |    0.00000 |  0.00000 |  0.39062 |  0.00000 |   58.33333 | 0.15286 | -0.01058 |    0.07843 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            708 |    0.00000 |    0.00000 |  0.00000 |  3.51562 |  0.00000 |   69.27083 | 0.13841 | -0.00297 |    0.06216 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2279 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   50.54253 | 0.16946 | -0.00706 |    0.09385 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           3993 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   56.67318 | 0.13455 | -0.00572 |    0.07161 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            170 |    0.00000 |    0.00000 |  0.00000 | 66.79688 |  0.00000 |   66.79688 | 0.28978 |  0.00945 |    0.14379 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           3241 |    0.00000 |    0.00000 |  0.00000 |  1.66016 |  0.00000 |   64.83290 | 0.12002 | -0.00463 |    0.05797 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           2749 |    0.00000 |    0.00000 |  0.00000 |  5.76172 |  0.00000 |   70.17144 | 0.10688 | -0.00385 |    0.04875 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           3605 |    0.00000 |    0.00000 |  0.00000 |  1.26953 |  0.00000 |   60.88325 | 0.11945 | -0.00507 |    0.06054 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2942 |    0.00000 |    0.00000 |  0.00000 |  3.71094 |  0.00000 |   68.07726 | 0.10081 | -0.00868 |    0.04746 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           7717 |    0.00000 |    0.00000 |  0.00000 |  0.68359 |  0.00000 |   58.13260 | 0.11260 | -0.00340 |    0.05943 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          17255 |    0.00000 |    0.00000 |  0.00000 |  0.31738 |  0.00000 |   53.19282 | 0.09990 | -0.00762 |    0.05529 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            615 |    0.00000 |    0.00000 |  0.00000 | 69.97070 |  0.00000 |   69.97070 | 0.17429 | -0.00672 |    0.08227 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          15309 |    0.00000 |    0.00000 |  0.00000 |  1.09863 |  0.00000 |   58.47168 | 0.09116 | -0.00411 |    0.04742 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          12424 |    0.00000 |    0.00000 |  0.00000 |  2.97852 |  0.00000 |   66.29774 | 0.07643 | -0.00442 |    0.03606 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          13333 |    0.00000 |    0.00000 |  0.00000 |  2.44141 |  0.00000 |   63.83192 | 0.06656 | -0.00043 |    0.03269 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           7918 |    0.00000 |    0.00000 |  0.00000 | 12.01172 |  0.00000 |   78.52105 | 0.05539 | -0.00142 |    0.02136 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            276 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   56.87500 | 0.44849 | -0.08671 |    0.25592 |
| 22 | Total sparsity:                     | -              |        270896 |          98189 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   63.75399 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:47:33,042 - Total sparsity: 63.75

2018-10-27 22:47:33,042 - --- validate (epoch=5)-----------
2018-10-27 22:47:33,042 - 10000 samples (128 per mini-batch)
2018-10-27 22:47:33,769 - Epoch: [5][   50/   78]    Loss 0.953135    Top1 68.953125    Top5 96.484375    
2018-10-27 22:47:34,161 - ==> Top1: 68.630    Top5: 96.640    Loss: 0.952

2018-10-27 22:47:34,162 - ==> Best Top1: 72.900   On Epoch: 3

2018-10-27 22:47:34,162 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:47:34,173 - 

2018-10-27 22:47:34,175 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:47:35,383 - Epoch: [6][   50/  391]    Overall Loss 0.609070    Objective Loss 0.609070    Top1 78.828125    Top5 98.734375    LR 0.300000    Time 0.024127    
2018-10-27 22:47:36,536 - Epoch: [6][  100/  391]    Overall Loss 0.624346    Objective Loss 0.624346    Top1 78.242188    Top5 98.710938    LR 0.300000    Time 0.023582    
2018-10-27 22:47:37,690 - Epoch: [6][  150/  391]    Overall Loss 0.631348    Objective Loss 0.631348    Top1 77.796875    Top5 98.744792    LR 0.300000    Time 0.023402    
2018-10-27 22:47:38,843 - Epoch: [6][  200/  391]    Overall Loss 0.626485    Objective Loss 0.626485    Top1 78.027344    Top5 98.761719    LR 0.300000    Time 0.023311    
2018-10-27 22:47:39,994 - Epoch: [6][  250/  391]    Overall Loss 0.629098    Objective Loss 0.629098    Top1 78.050000    Top5 98.753125    LR 0.300000    Time 0.023245    
2018-10-27 22:47:41,144 - Epoch: [6][  300/  391]    Overall Loss 0.630817    Objective Loss 0.630817    Top1 77.966146    Top5 98.731771    LR 0.300000    Time 0.023202    
2018-10-27 22:47:42,298 - Epoch: [6][  350/  391]    Overall Loss 0.629703    Objective Loss 0.629703    Top1 78.040179    Top5 98.752232    LR 0.300000    Time 0.023179    
2018-10-27 22:47:43,322 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            163 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   62.26852 | 0.51621 | -0.00640 |    0.24236 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            585 |    0.00000 |    0.00000 |  0.00000 | 10.93750 |  0.00000 |   74.60938 | 0.17195 | -0.01065 |    0.06617 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            495 |    0.00000 |    0.00000 |  0.00000 | 14.84375 |  0.00000 |   78.51562 | 0.17705 | -0.00273 |    0.06709 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            574 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   75.08681 | 0.16922 | -0.00589 |    0.06839 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            467 |    0.00000 |    0.00000 |  0.00000 | 19.92188 |  0.00000 |   79.73090 | 0.14552 | -0.00596 |    0.05472 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            828 |    0.00000 |    0.00000 |  0.00000 |  1.95312 |  0.00000 |   64.06250 | 0.15379 | -0.01038 |    0.07484 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            581 |    0.00000 |    0.00000 |  0.00000 |  8.98438 |  0.00000 |   74.78299 | 0.13781 | -0.00372 |    0.05768 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2030 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   55.94618 | 0.17129 | -0.00762 |    0.09173 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           3474 |    0.00000 |    0.00000 |  0.00000 |  2.63672 |  0.00000 |   62.30469 | 0.13542 | -0.00576 |    0.06910 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            148 |    0.00000 |    0.00000 |  0.00000 | 71.09375 |  0.00000 |   71.09375 | 0.28253 |  0.00560 |    0.13530 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2779 |    0.00000 |    0.00000 |  0.00000 |  4.00391 |  0.00000 |   69.84592 | 0.12062 | -0.00392 |    0.05539 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           2320 |    0.00000 |    0.00000 |  0.00000 |  8.98438 |  0.00000 |   74.82639 | 0.10686 | -0.00423 |    0.04602 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           3108 |    0.00000 |    0.00000 |  0.00000 |  3.02734 |  0.00000 |   66.27604 | 0.12070 | -0.00438 |    0.05841 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2466 |    0.00000 |    0.00000 |  0.00000 |  7.71484 |  0.00000 |   73.24219 | 0.10071 | -0.00871 |    0.04470 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           6763 |    0.00000 |    0.00000 |  0.00000 |  2.19727 |  0.00000 |   63.30838 | 0.11496 | -0.00334 |    0.05822 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          15272 |    0.00000 |    0.00000 |  0.00000 |  0.83008 |  0.00000 |   58.57205 | 0.10292 | -0.00729 |    0.05466 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            521 |    0.00000 |    0.00000 |  0.00000 | 74.56055 |  0.00000 |   74.56055 | 0.17073 | -0.00586 |    0.07576 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          13339 |    0.00000 |    0.00000 |  0.00000 |  2.46582 |  0.00000 |   63.81565 | 0.09363 | -0.00469 |    0.04653 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          10479 |    0.00000 |    0.00000 |  0.00000 |  6.17676 |  0.00000 |   71.57389 | 0.07789 | -0.00423 |    0.03465 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          11181 |    0.00000 |    0.00000 |  0.00000 |  5.49316 |  0.00000 |   69.66960 | 0.06792 | -0.00043 |    0.03145 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           6306 |    0.00000 |    0.00000 |  0.00000 | 20.45898 |  0.00000 |   82.89388 | 0.05485 | -0.00067 |    0.01940 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            270 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   57.81250 | 0.45198 | -0.08765 |    0.25484 |
| 22 | Total sparsity:                     | -              |        270896 |          84149 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   68.93679 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:47:43,322 - Total sparsity: 68.94

2018-10-27 22:47:43,323 - --- validate (epoch=6)-----------
2018-10-27 22:47:43,323 - 10000 samples (128 per mini-batch)
2018-10-27 22:47:44,047 - Epoch: [6][   50/   78]    Loss 0.706929    Top1 76.265625    Top5 98.656250    
2018-10-27 22:47:44,435 - ==> Top1: 75.900    Top5: 98.720    Loss: 0.707

2018-10-27 22:47:44,436 - ==> Best Top1: 75.900   On Epoch: 6

2018-10-27 22:47:44,437 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:47:44,456 - 

2018-10-27 22:47:44,457 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:47:45,614 - Epoch: [7][   50/  391]    Overall Loss 0.582371    Objective Loss 0.582371    Top1 79.921875    Top5 99.000000    LR 0.300000    Time 0.023120    
2018-10-27 22:47:46,740 - Epoch: [7][  100/  391]    Overall Loss 0.582522    Objective Loss 0.582522    Top1 79.804688    Top5 98.914062    LR 0.300000    Time 0.022804    
2018-10-27 22:47:47,867 - Epoch: [7][  150/  391]    Overall Loss 0.599220    Objective Loss 0.599220    Top1 79.192708    Top5 98.854167    LR 0.300000    Time 0.022709    
2018-10-27 22:47:48,994 - Epoch: [7][  200/  391]    Overall Loss 0.599613    Objective Loss 0.599613    Top1 79.210938    Top5 98.832031    LR 0.300000    Time 0.022657    
2018-10-27 22:47:50,120 - Epoch: [7][  250/  391]    Overall Loss 0.605632    Objective Loss 0.605632    Top1 78.909375    Top5 98.815625    LR 0.300000    Time 0.022627    
2018-10-27 22:47:51,247 - Epoch: [7][  300/  391]    Overall Loss 0.604761    Objective Loss 0.604761    Top1 78.955729    Top5 98.789062    LR 0.300000    Time 0.022606    
2018-10-27 22:47:52,373 - Epoch: [7][  350/  391]    Overall Loss 0.605294    Objective Loss 0.605294    Top1 79.055804    Top5 98.810268    LR 0.300000    Time 0.022590    
2018-10-27 22:47:53,377 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            163 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   62.26852 | 0.51338 | -0.00246 |    0.23842 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            585 |    0.00000 |    0.00000 |  0.00000 | 10.93750 |  0.00000 |   74.60938 | 0.17022 | -0.00845 |    0.06364 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            495 |    0.00000 |    0.00000 |  0.00000 | 14.84375 |  0.00000 |   78.51562 | 0.17564 | -0.00294 |    0.06508 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            574 |    0.00000 |    0.00000 |  0.00000 | 10.15625 |  0.00000 |   75.08681 | 0.16942 | -0.00709 |    0.06704 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            467 |    0.00000 |    0.00000 |  0.00000 | 19.92188 |  0.00000 |   79.73090 | 0.14447 | -0.00686 |    0.05355 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            828 |    0.00000 |    0.00000 |  0.00000 |  1.95312 |  0.00000 |   64.06250 | 0.15735 | -0.00946 |    0.07506 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            581 |    0.00000 |    0.00000 |  0.00000 |  8.98438 |  0.00000 |   74.78299 | 0.13859 | -0.00385 |    0.05653 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           2030 |    0.00000 |    0.00000 |  0.00000 |  0.78125 |  0.00000 |   55.94618 | 0.17261 | -0.00772 |    0.09179 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           3474 |    0.00000 |    0.00000 |  0.00000 |  2.63672 |  0.00000 |   62.30469 | 0.13642 | -0.00638 |    0.06859 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            148 |    0.00000 |    0.00000 |  0.00000 | 71.09375 |  0.00000 |   71.09375 | 0.27800 |  0.00475 |    0.13306 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2779 |    0.00000 |    0.00000 |  0.00000 |  4.00391 |  0.00000 |   69.84592 | 0.12144 | -0.00395 |    0.05495 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           2320 |    0.00000 |    0.00000 |  0.00000 |  8.98438 |  0.00000 |   74.82639 | 0.10697 | -0.00455 |    0.04533 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           3108 |    0.00000 |    0.00000 |  0.00000 |  3.02734 |  0.00000 |   66.27604 | 0.12194 | -0.00530 |    0.05819 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2466 |    0.00000 |    0.00000 |  0.00000 |  7.71484 |  0.00000 |   73.24219 | 0.10071 | -0.00798 |    0.04383 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           6763 |    0.00000 |    0.00000 |  0.00000 |  2.19727 |  0.00000 |   63.30838 | 0.11709 | -0.00329 |    0.05872 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          15272 |    0.00000 |    0.00000 |  0.00000 |  0.83008 |  0.00000 |   58.57205 | 0.10556 | -0.00746 |    0.05536 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            521 |    0.00000 |    0.00000 |  0.00000 | 74.56055 |  0.00000 |   74.56055 | 0.16655 | -0.00522 |    0.07279 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          13339 |    0.00000 |    0.00000 |  0.00000 |  2.46582 |  0.00000 |   63.81565 | 0.09604 | -0.00477 |    0.04700 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |          10479 |    0.00000 |    0.00000 |  0.00000 |  6.17676 |  0.00000 |   71.57389 | 0.07952 | -0.00448 |    0.03479 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |          11181 |    0.00000 |    0.00000 |  0.00000 |  5.49316 |  0.00000 |   69.66960 | 0.06951 | -0.00092 |    0.03145 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           6306 |    0.00000 |    0.00000 |  0.00000 | 20.45898 |  0.00000 |   82.89388 | 0.05500 | -0.00046 |    0.01891 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            270 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   57.81250 | 0.45111 | -0.08544 |    0.25578 |
| 22 | Total sparsity:                     | -              |        270896 |          84149 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   68.93679 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:47:53,377 - Total sparsity: 68.94

2018-10-27 22:47:53,377 - --- validate (epoch=7)-----------
2018-10-27 22:47:53,377 - 10000 samples (128 per mini-batch)
2018-10-27 22:47:54,089 - Epoch: [7][   50/   78]    Loss 0.829739    Top1 72.296875    Top5 98.031250    
2018-10-27 22:47:54,470 - ==> Top1: 72.320    Top5: 98.090    Loss: 0.829

2018-10-27 22:47:54,471 - ==> Best Top1: 75.900   On Epoch: 6

2018-10-27 22:47:54,471 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:47:54,481 - 

2018-10-27 22:47:54,482 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:47:55,690 - Epoch: [8][   50/  391]    Overall Loss 0.566490    Objective Loss 0.566490    Top1 80.359375    Top5 99.031250    LR 0.300000    Time 0.024112    
2018-10-27 22:47:56,841 - Epoch: [8][  100/  391]    Overall Loss 0.580523    Objective Loss 0.580523    Top1 79.984375    Top5 98.921875    LR 0.300000    Time 0.023553    
2018-10-27 22:47:57,992 - Epoch: [8][  150/  391]    Overall Loss 0.579524    Objective Loss 0.579524    Top1 80.109375    Top5 98.911458    LR 0.300000    Time 0.023370    
2018-10-27 22:47:59,143 - Epoch: [8][  200/  391]    Overall Loss 0.584060    Objective Loss 0.584060    Top1 79.832031    Top5 98.890625    LR 0.300000    Time 0.023274    
2018-10-27 22:48:00,299 - Epoch: [8][  250/  391]    Overall Loss 0.580762    Objective Loss 0.580762    Top1 79.896875    Top5 98.881250    LR 0.300000    Time 0.023238    
2018-10-27 22:48:01,454 - Epoch: [8][  300/  391]    Overall Loss 0.580143    Objective Loss 0.580143    Top1 79.914062    Top5 98.861979    LR 0.300000    Time 0.023212    
2018-10-27 22:48:02,609 - Epoch: [8][  350/  391]    Overall Loss 0.580147    Objective Loss 0.580147    Top1 79.883929    Top5 98.841518    LR 0.300000    Time 0.023194    
2018-10-27 22:48:03,635 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            158 |    0.00000 |    0.00000 |  0.00000 |  2.08333 |  0.00000 |   63.42593 | 0.51244 | -0.00094 |    0.23721 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            498 |    0.00000 |    0.00000 |  0.00000 | 15.62500 |  0.00000 |   78.38542 | 0.16776 | -0.00868 |    0.05927 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            435 |    0.00000 |    0.00000 |  0.00000 | 18.75000 |  0.00000 |   81.11979 | 0.17342 | -0.00185 |    0.06278 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            494 |    0.00000 |    0.00000 |  0.00000 | 15.23438 |  0.00000 |   78.55903 | 0.16843 | -0.00523 |    0.06363 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            403 |    0.00000 |    0.00000 |  0.00000 | 25.00000 |  0.00000 |   82.50868 | 0.14252 | -0.00703 |    0.05048 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            710 |    0.00000 |    0.00000 |  0.00000 |  3.12500 |  0.00000 |   69.18403 | 0.15782 | -0.01091 |    0.07126 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            492 |    0.00000 |    0.00000 |  0.00000 | 16.01562 |  0.00000 |   78.64583 | 0.13807 | -0.00430 |    0.05388 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1843 |    0.00000 |    0.00000 |  0.00000 |  2.14844 |  0.00000 |   60.00434 | 0.17361 | -0.00829 |    0.09012 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           3095 |    0.00000 |    0.00000 |  0.00000 |  3.32031 |  0.00000 |   66.41710 | 0.13729 | -0.00535 |    0.06635 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            136 |    0.00000 |    0.00000 |  0.00000 | 73.43750 |  0.00000 |   73.43750 | 0.27018 |  0.00238 |    0.12348 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2451 |    0.00000 |    0.00000 |  0.00000 |  7.03125 |  0.00000 |   73.40495 | 0.12129 | -0.00431 |    0.05263 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           2036 |    0.00000 |    0.00000 |  0.00000 | 12.79297 |  0.00000 |   77.90799 | 0.10659 | -0.00463 |    0.04308 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2743 |    0.00000 |    0.00000 |  0.00000 |  5.27344 |  0.00000 |   70.23655 | 0.12236 | -0.00522 |    0.05611 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2156 |    0.00000 |    0.00000 |  0.00000 | 11.32812 |  0.00000 |   76.60590 | 0.10059 | -0.00730 |    0.04183 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           6116 |    0.00000 |    0.00000 |  0.00000 |  4.10156 |  0.00000 |   66.81858 | 0.11812 | -0.00348 |    0.05719 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          13799 |    0.00000 |    0.00000 |  0.00000 |  1.63574 |  0.00000 |   62.56782 | 0.10727 | -0.00765 |    0.05443 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            454 |    0.00000 |    0.00000 |  0.00000 | 77.83203 |  0.00000 |   77.83203 | 0.16223 | -0.00579 |    0.06717 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          11892 |    0.00000 |    0.00000 |  0.00000 |  3.93066 |  0.00000 |   67.74089 | 0.09765 | -0.00479 |    0.04600 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           9145 |    0.00000 |    0.00000 |  0.00000 |  9.20410 |  0.00000 |   75.19260 | 0.08042 | -0.00424 |    0.03361 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           9676 |    0.00000 |    0.00000 |  0.00000 |  9.08203 |  0.00000 |   73.75217 | 0.07045 | -0.00079 |    0.03035 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           5204 |    0.00000 |    0.00000 |  0.00000 | 29.58984 |  0.00000 |   85.88325 | 0.05452 | -0.00022 |    0.01745 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            264 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   58.75000 | 0.46061 | -0.08642 |    0.26000 |
| 22 | Total sparsity:                     | -              |        270896 |          74200 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   72.60941 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:48:03,635 - Total sparsity: 72.61

2018-10-27 22:48:03,635 - --- validate (epoch=8)-----------
2018-10-27 22:48:03,635 - 10000 samples (128 per mini-batch)
2018-10-27 22:48:04,359 - Epoch: [8][   50/   78]    Loss 0.820676    Top1 73.609375    Top5 98.640625    
2018-10-27 22:48:04,751 - ==> Top1: 73.350    Top5: 98.620    Loss: 0.829

2018-10-27 22:48:04,752 - ==> Best Top1: 75.900   On Epoch: 6

2018-10-27 22:48:04,752 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:48:04,766 - 

2018-10-27 22:48:04,766 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:48:05,951 - Epoch: [9][   50/  391]    Overall Loss 0.565900    Objective Loss 0.565900    Top1 80.125000    Top5 99.093750    LR 0.300000    Time 0.023665    
2018-10-27 22:48:07,079 - Epoch: [9][  100/  391]    Overall Loss 0.568107    Objective Loss 0.568107    Top1 80.265625    Top5 99.023438    LR 0.300000    Time 0.023100    
2018-10-27 22:48:08,209 - Epoch: [9][  150/  391]    Overall Loss 0.568980    Objective Loss 0.568980    Top1 80.328125    Top5 99.062500    LR 0.300000    Time 0.022922    
2018-10-27 22:48:09,338 - Epoch: [9][  200/  391]    Overall Loss 0.573872    Objective Loss 0.573872    Top1 80.187500    Top5 99.011719    LR 0.300000    Time 0.022834    
2018-10-27 22:48:10,468 - Epoch: [9][  250/  391]    Overall Loss 0.569919    Objective Loss 0.569919    Top1 80.328125    Top5 99.031250    LR 0.300000    Time 0.022780    
2018-10-27 22:48:11,598 - Epoch: [9][  300/  391]    Overall Loss 0.571233    Objective Loss 0.571233    Top1 80.307292    Top5 99.018229    LR 0.300000    Time 0.022747    
2018-10-27 22:48:12,730 - Epoch: [9][  350/  391]    Overall Loss 0.572515    Objective Loss 0.572515    Top1 80.310268    Top5 99.006696    LR 0.300000    Time 0.022729    
2018-10-27 22:48:13,733 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            158 |    0.00000 |    0.00000 |  0.00000 |  2.08333 |  0.00000 |   63.42593 | 0.51076 | -0.00362 |    0.23513 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            498 |    0.00000 |    0.00000 |  0.00000 | 15.62500 |  0.00000 |   78.38542 | 0.16595 | -0.00793 |    0.05802 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            435 |    0.00000 |    0.00000 |  0.00000 | 18.75000 |  0.00000 |   81.11979 | 0.17255 | -0.00089 |    0.06100 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            494 |    0.00000 |    0.00000 |  0.00000 | 15.23438 |  0.00000 |   78.55903 | 0.16798 | -0.00617 |    0.06245 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            403 |    0.00000 |    0.00000 |  0.00000 | 25.00000 |  0.00000 |   82.50868 | 0.14188 | -0.00539 |    0.04921 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            710 |    0.00000 |    0.00000 |  0.00000 |  3.12500 |  0.00000 |   69.18403 | 0.15889 | -0.01132 |    0.07053 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            492 |    0.00000 |    0.00000 |  0.00000 | 16.01562 |  0.00000 |   78.64583 | 0.13842 | -0.00404 |    0.05250 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1843 |    0.00000 |    0.00000 |  0.00000 |  2.14844 |  0.00000 |   60.00434 | 0.17508 | -0.00775 |    0.08959 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           3095 |    0.00000 |    0.00000 |  0.00000 |  3.32031 |  0.00000 |   66.41710 | 0.13828 | -0.00449 |    0.06614 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            136 |    0.00000 |    0.00000 |  0.00000 | 73.43750 |  0.00000 |   73.43750 | 0.26576 |  0.00132 |    0.12225 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2451 |    0.00000 |    0.00000 |  0.00000 |  7.03125 |  0.00000 |   73.40495 | 0.12157 | -0.00449 |    0.05213 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           2036 |    0.00000 |    0.00000 |  0.00000 | 12.79297 |  0.00000 |   77.90799 | 0.10642 | -0.00389 |    0.04240 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2743 |    0.00000 |    0.00000 |  0.00000 |  5.27344 |  0.00000 |   70.23655 | 0.12346 | -0.00407 |    0.05569 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           2156 |    0.00000 |    0.00000 |  0.00000 | 11.32812 |  0.00000 |   76.60590 | 0.10138 | -0.00712 |    0.04149 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           6116 |    0.00000 |    0.00000 |  0.00000 |  4.10156 |  0.00000 |   66.81858 | 0.11910 | -0.00345 |    0.05706 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          13799 |    0.00000 |    0.00000 |  0.00000 |  1.63574 |  0.00000 |   62.56782 | 0.10866 | -0.00738 |    0.05475 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            454 |    0.00000 |    0.00000 |  0.00000 | 77.83203 |  0.00000 |   77.83203 | 0.15998 | -0.00460 |    0.06591 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          11892 |    0.00000 |    0.00000 |  0.00000 |  3.93066 |  0.00000 |   67.74089 | 0.09872 | -0.00456 |    0.04600 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           9145 |    0.00000 |    0.00000 |  0.00000 |  9.20410 |  0.00000 |   75.19260 | 0.08114 | -0.00424 |    0.03335 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           9676 |    0.00000 |    0.00000 |  0.00000 |  9.08203 |  0.00000 |   73.75217 | 0.07157 | -0.00068 |    0.03036 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           5204 |    0.00000 |    0.00000 |  0.00000 | 29.58984 |  0.00000 |   85.88325 | 0.05478 |  0.00015 |    0.01715 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            264 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   58.75000 | 0.45503 | -0.08561 |    0.25495 |
| 22 | Total sparsity:                     | -              |        270896 |          74200 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   72.60941 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:48:13,733 - Total sparsity: 72.61

2018-10-27 22:48:13,733 - --- validate (epoch=9)-----------
2018-10-27 22:48:13,734 - 10000 samples (128 per mini-batch)
2018-10-27 22:48:14,447 - Epoch: [9][   50/   78]    Loss 0.750902    Top1 75.484375    Top5 98.203125    
2018-10-27 22:48:14,835 - ==> Top1: 75.110    Top5: 98.130    Loss: 0.756

2018-10-27 22:48:14,836 - ==> Best Top1: 75.900   On Epoch: 6

2018-10-27 22:48:14,836 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:48:14,853 - 

2018-10-27 22:48:14,855 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:48:16,039 - Epoch: [10][   50/  391]    Overall Loss 0.543319    Objective Loss 0.543319    Top1 81.359375    Top5 98.859375    LR 0.300000    Time 0.023643    
2018-10-27 22:48:17,194 - Epoch: [10][  100/  391]    Overall Loss 0.539388    Objective Loss 0.539388    Top1 81.375000    Top5 98.898438    LR 0.300000    Time 0.023358    
2018-10-27 22:48:18,349 - Epoch: [10][  150/  391]    Overall Loss 0.547981    Objective Loss 0.547981    Top1 80.973958    Top5 98.968750    LR 0.300000    Time 0.023265    
2018-10-27 22:48:19,507 - Epoch: [10][  200/  391]    Overall Loss 0.549549    Objective Loss 0.549549    Top1 80.769531    Top5 99.015625    LR 0.300000    Time 0.023231    
2018-10-27 22:48:20,663 - Epoch: [10][  250/  391]    Overall Loss 0.550663    Objective Loss 0.550663    Top1 80.771875    Top5 99.021875    LR 0.300000    Time 0.023205    
2018-10-27 22:48:21,820 - Epoch: [10][  300/  391]    Overall Loss 0.549628    Objective Loss 0.549628    Top1 80.799479    Top5 99.023438    LR 0.300000    Time 0.023190    
2018-10-27 22:48:22,977 - Epoch: [10][  350/  391]    Overall Loss 0.551030    Objective Loss 0.551030    Top1 80.825893    Top5 98.975446    LR 0.300000    Time 0.023180    
2018-10-27 22:48:24,003 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            153 |    0.00000 |    0.00000 |  0.00000 |  2.08333 |  0.00000 |   64.58333 | 0.51400 | -0.00219 |    0.23388 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            419 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   81.81424 | 0.16392 | -0.00760 |    0.05406 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            383 |    0.00000 |    0.00000 |  0.00000 | 21.87500 |  0.00000 |   83.37674 | 0.17053 | -0.00091 |    0.05831 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            444 |    0.00000 |    0.00000 |  0.00000 | 19.14062 |  0.00000 |   80.72917 | 0.16671 | -0.00698 |    0.06045 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            346 |    0.00000 |    0.00000 |  0.00000 | 28.51562 |  0.00000 |   84.98264 | 0.14117 | -0.00437 |    0.04716 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            632 |    0.00000 |    0.00000 |  0.00000 |  5.46875 |  0.00000 |   72.56944 | 0.15900 | -0.00998 |    0.06756 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            420 |    0.00000 |    0.00000 |  0.00000 | 23.43750 |  0.00000 |   81.77083 | 0.13682 | -0.00520 |    0.04997 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1711 |    0.00000 |    0.00000 |  0.00000 |  3.32031 |  0.00000 |   62.86892 | 0.17574 | -0.00828 |    0.08784 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           2824 |    0.00000 |    0.00000 |  0.00000 |  4.58984 |  0.00000 |   69.35764 | 0.13851 | -0.00508 |    0.06416 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            128 |    0.00000 |    0.00000 |  0.00000 | 75.00000 |  0.00000 |   75.00000 | 0.26022 | -0.00005 |    0.11611 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2183 |    0.00000 |    0.00000 |  0.00000 | 10.83984 |  0.00000 |   76.31293 | 0.12082 | -0.00371 |    0.04999 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1789 |    0.00000 |    0.00000 |  0.00000 | 18.26172 |  0.00000 |   80.58811 | 0.10532 | -0.00403 |    0.04017 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2446 |    0.00000 |    0.00000 |  0.00000 |  8.59375 |  0.00000 |   73.45920 | 0.12347 | -0.00461 |    0.05387 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           1908 |    0.00000 |    0.00000 |  0.00000 | 15.42969 |  0.00000 |   79.29688 | 0.10131 | -0.00651 |    0.03979 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           5584 |    0.00000 |    0.00000 |  0.00000 |  5.71289 |  0.00000 |   69.70486 | 0.11952 | -0.00321 |    0.05545 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          12620 |    0.00000 |    0.00000 |  0.00000 |  2.80762 |  0.00000 |   65.76606 | 0.10934 | -0.00739 |    0.05347 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            397 |    0.00000 |    0.00000 |  0.00000 | 80.61523 |  0.00000 |   80.61523 | 0.15705 | -0.00502 |    0.06140 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          10761 |    0.00000 |    0.00000 |  0.00000 |  5.49316 |  0.00000 |   70.80892 | 0.09956 | -0.00438 |    0.04501 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           8114 |    0.00000 |    0.00000 |  0.00000 | 13.03711 |  0.00000 |   77.98937 | 0.08148 | -0.00442 |    0.03223 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           8603 |    0.00000 |    0.00000 |  0.00000 | 12.37793 |  0.00000 |   76.66287 | 0.07225 | -0.00031 |    0.02949 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           4426 |    0.00000 |    0.00000 |  0.00000 | 37.35352 |  0.00000 |   87.99371 | 0.05453 |  0.00041 |    0.01614 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            257 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   59.84375 | 0.45716 | -0.08464 |    0.25502 |
| 22 | Total sparsity:                     | -              |        270896 |          66548 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   75.43411 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:48:24,003 - Total sparsity: 75.43

2018-10-27 22:48:24,003 - --- validate (epoch=10)-----------
2018-10-27 22:48:24,003 - 10000 samples (128 per mini-batch)
2018-10-27 22:48:24,723 - Epoch: [10][   50/   78]    Loss 1.218312    Top1 64.250000    Top5 97.046875    
2018-10-27 22:48:25,113 - ==> Top1: 63.530    Top5: 96.780    Loss: 1.242

2018-10-27 22:48:25,114 - ==> Best Top1: 75.900   On Epoch: 6

2018-10-27 22:48:25,114 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:48:25,124 - 

2018-10-27 22:48:25,125 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:48:26,312 - Epoch: [11][   50/  391]    Overall Loss 0.527856    Objective Loss 0.527856    Top1 81.796875    Top5 99.234375    LR 0.300000    Time 0.023705    
2018-10-27 22:48:27,443 - Epoch: [11][  100/  391]    Overall Loss 0.538878    Objective Loss 0.538878    Top1 81.125000    Top5 99.148438    LR 0.300000    Time 0.023148    
2018-10-27 22:48:28,575 - Epoch: [11][  150/  391]    Overall Loss 0.540474    Objective Loss 0.540474    Top1 81.093750    Top5 99.151042    LR 0.300000    Time 0.022970    
2018-10-27 22:48:29,709 - Epoch: [11][  200/  391]    Overall Loss 0.542815    Objective Loss 0.542815    Top1 80.976562    Top5 99.152344    LR 0.300000    Time 0.022873    
2018-10-27 22:48:30,840 - Epoch: [11][  250/  391]    Overall Loss 0.543641    Objective Loss 0.543641    Top1 80.915625    Top5 99.140625    LR 0.300000    Time 0.022818    
2018-10-27 22:48:31,971 - Epoch: [11][  300/  391]    Overall Loss 0.542422    Objective Loss 0.542422    Top1 81.005208    Top5 99.127604    LR 0.300000    Time 0.022783    
2018-10-27 22:48:33,104 - Epoch: [11][  350/  391]    Overall Loss 0.542159    Objective Loss 0.542159    Top1 81.046875    Top5 99.098214    LR 0.300000    Time 0.022761    
2018-10-27 22:48:34,117 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            153 |    0.00000 |    0.00000 |  0.00000 |  2.08333 |  0.00000 |   64.58333 | 0.51501 | -0.00636 |    0.23314 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            419 |    0.00000 |    0.00000 |  0.00000 | 22.26562 |  0.00000 |   81.81424 | 0.16429 | -0.00775 |    0.05368 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            383 |    0.00000 |    0.00000 |  0.00000 | 21.87500 |  0.00000 |   83.37674 | 0.17105 | -0.00056 |    0.05812 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            444 |    0.00000 |    0.00000 |  0.00000 | 19.14062 |  0.00000 |   80.72917 | 0.16574 | -0.00606 |    0.05908 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            346 |    0.00000 |    0.00000 |  0.00000 | 28.51562 |  0.00000 |   84.98264 | 0.13934 | -0.00436 |    0.04552 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            632 |    0.00000 |    0.00000 |  0.00000 |  5.46875 |  0.00000 |   72.56944 | 0.16223 | -0.00884 |    0.06790 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            420 |    0.00000 |    0.00000 |  0.00000 | 23.43750 |  0.00000 |   81.77083 | 0.13738 | -0.00403 |    0.04972 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1711 |    0.00000 |    0.00000 |  0.00000 |  3.32031 |  0.00000 |   62.86892 | 0.17649 | -0.00854 |    0.08760 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           2824 |    0.00000 |    0.00000 |  0.00000 |  4.58984 |  0.00000 |   69.35764 | 0.13854 | -0.00568 |    0.06340 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            128 |    0.00000 |    0.00000 |  0.00000 | 75.00000 |  0.00000 |   75.00000 | 0.25675 |  0.00144 |    0.11340 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           2183 |    0.00000 |    0.00000 |  0.00000 | 10.83984 |  0.00000 |   76.31293 | 0.12077 | -0.00375 |    0.04946 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1789 |    0.00000 |    0.00000 |  0.00000 | 18.26172 |  0.00000 |   80.58811 | 0.10498 | -0.00355 |    0.03969 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2446 |    0.00000 |    0.00000 |  0.00000 |  8.59375 |  0.00000 |   73.45920 | 0.12426 | -0.00499 |    0.05352 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           1908 |    0.00000 |    0.00000 |  0.00000 | 15.42969 |  0.00000 |   79.29688 | 0.10181 | -0.00625 |    0.03951 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           5584 |    0.00000 |    0.00000 |  0.00000 |  5.71289 |  0.00000 |   69.70486 | 0.12010 | -0.00319 |    0.05528 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          12620 |    0.00000 |    0.00000 |  0.00000 |  2.80762 |  0.00000 |   65.76606 | 0.11038 | -0.00733 |    0.05358 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            397 |    0.00000 |    0.00000 |  0.00000 | 80.61523 |  0.00000 |   80.61523 | 0.15490 | -0.00564 |    0.06041 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |          10761 |    0.00000 |    0.00000 |  0.00000 |  5.49316 |  0.00000 |   70.80892 | 0.10072 | -0.00432 |    0.04500 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           8114 |    0.00000 |    0.00000 |  0.00000 | 13.03711 |  0.00000 |   77.98937 | 0.08235 | -0.00460 |    0.03209 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           8603 |    0.00000 |    0.00000 |  0.00000 | 12.37793 |  0.00000 |   76.66287 | 0.07336 | -0.00045 |    0.02951 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           4426 |    0.00000 |    0.00000 |  0.00000 | 37.35352 |  0.00000 |   87.99371 | 0.05452 |  0.00065 |    0.01586 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            257 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   59.84375 | 0.45628 | -0.08437 |    0.25407 |
| 22 | Total sparsity:                     | -              |        270896 |          66548 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   75.43411 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:48:34,117 - Total sparsity: 75.43

2018-10-27 22:48:34,117 - --- validate (epoch=11)-----------
2018-10-27 22:48:34,117 - 10000 samples (128 per mini-batch)
2018-10-27 22:48:34,841 - Epoch: [11][   50/   78]    Loss 0.723280    Top1 77.109375    Top5 98.406250    
2018-10-27 22:48:35,235 - ==> Top1: 77.410    Top5: 98.500    Loss: 0.710

2018-10-27 22:48:35,235 - ==> Best Top1: 77.410   On Epoch: 11

2018-10-27 22:48:35,236 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:48:35,252 - 

2018-10-27 22:48:35,254 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:48:36,441 - Epoch: [12][   50/  391]    Overall Loss 0.518701    Objective Loss 0.518701    Top1 81.703125    Top5 98.968750    LR 0.300000    Time 0.023715    
2018-10-27 22:48:37,599 - Epoch: [12][  100/  391]    Overall Loss 0.516419    Objective Loss 0.516419    Top1 81.742188    Top5 99.140625    LR 0.300000    Time 0.023419    
2018-10-27 22:48:38,757 - Epoch: [12][  150/  391]    Overall Loss 0.529153    Objective Loss 0.529153    Top1 81.234375    Top5 99.078125    LR 0.300000    Time 0.023328    
2018-10-27 22:48:39,914 - Epoch: [12][  200/  391]    Overall Loss 0.537612    Objective Loss 0.537612    Top1 81.207031    Top5 99.035156    LR 0.300000    Time 0.023275    
2018-10-27 22:48:41,071 - Epoch: [12][  250/  391]    Overall Loss 0.538618    Objective Loss 0.538618    Top1 81.146875    Top5 99.065625    LR 0.300000    Time 0.023242    
2018-10-27 22:48:42,227 - Epoch: [12][  300/  391]    Overall Loss 0.539616    Objective Loss 0.539616    Top1 81.138021    Top5 99.106771    LR 0.300000    Time 0.023218    
2018-10-27 22:48:43,383 - Epoch: [12][  350/  391]    Overall Loss 0.539710    Objective Loss 0.539710    Top1 81.100446    Top5 99.091518    LR 0.300000    Time 0.023199    
2018-10-27 22:48:44,409 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            150 |    0.00000 |    0.00000 |  0.00000 |  2.08333 |  0.00000 |   65.27778 | 0.51499 | -0.00554 |    0.22902 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            361 |    0.00000 |    0.00000 |  0.00000 | 30.07812 |  0.00000 |   84.33160 | 0.16442 | -0.00793 |    0.05155 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            349 |    0.00000 |    0.00000 |  0.00000 | 27.73438 |  0.00000 |   84.85243 | 0.16975 | -0.00020 |    0.05649 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            403 |    0.00000 |    0.00000 |  0.00000 | 21.87500 |  0.00000 |   82.50868 | 0.16308 | -0.00726 |    0.05623 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            306 |    0.00000 |    0.00000 |  0.00000 | 34.76562 |  0.00000 |   86.71875 | 0.13726 | -0.00281 |    0.04340 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            568 |    0.00000 |    0.00000 |  0.00000 |  8.59375 |  0.00000 |   75.34722 | 0.16329 | -0.00887 |    0.06603 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            388 |    0.00000 |    0.00000 |  0.00000 | 27.73438 |  0.00000 |   83.15972 | 0.13833 | -0.00411 |    0.04775 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1602 |    0.00000 |    0.00000 |  0.00000 |  4.10156 |  0.00000 |   65.23438 | 0.17647 | -0.00778 |    0.08556 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           2561 |    0.00000 |    0.00000 |  0.00000 |  6.44531 |  0.00000 |   72.21137 | 0.13806 | -0.00578 |    0.06094 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            115 |    0.00000 |    0.00000 |  0.00000 | 77.53906 |  0.00000 |   77.53906 | 0.25326 | -0.00080 |    0.10779 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1990 |    0.00000 |    0.00000 |  0.00000 | 13.47656 |  0.00000 |   78.40712 | 0.12018 | -0.00349 |    0.04785 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1622 |    0.00000 |    0.00000 |  0.00000 | 22.07031 |  0.00000 |   82.40017 | 0.10481 | -0.00303 |    0.03810 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2230 |    0.00000 |    0.00000 |  0.00000 | 11.42578 |  0.00000 |   75.80295 | 0.12495 | -0.00438 |    0.05224 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           1704 |    0.00000 |    0.00000 |  0.00000 | 20.31250 |  0.00000 |   81.51042 | 0.10196 | -0.00632 |    0.03802 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           5156 |    0.00000 |    0.00000 |  0.00000 |  7.81250 |  0.00000 |   72.02691 | 0.12017 | -0.00292 |    0.05417 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          11651 |    0.00000 |    0.00000 |  0.00000 |  4.07715 |  0.00000 |   68.39464 | 0.11086 | -0.00710 |    0.05255 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            359 |    0.00000 |    0.00000 |  0.00000 | 82.47070 |  0.00000 |   82.47070 | 0.15267 | -0.00442 |    0.05751 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9881 |    0.00000 |    0.00000 |  0.00000 |  7.78809 |  0.00000 |   73.19607 | 0.10163 | -0.00461 |    0.04427 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           7293 |    0.00000 |    0.00000 |  0.00000 | 16.89453 |  0.00000 |   80.21647 | 0.08275 | -0.00455 |    0.03114 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           7763 |    0.00000 |    0.00000 |  0.00000 | 16.25977 |  0.00000 |   78.94151 | 0.07399 | -0.00032 |    0.02872 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3862 |    0.00000 |    0.00000 |  0.00000 | 43.62793 |  0.00000 |   89.52365 | 0.05435 |  0.00075 |    0.01506 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            256 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   60.00000 | 0.45873 | -0.08357 |    0.25288 |
| 22 | Total sparsity:                     | -              |        270896 |          60570 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   77.64087 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:48:44,409 - Total sparsity: 77.64

2018-10-27 22:48:44,409 - --- validate (epoch=12)-----------
2018-10-27 22:48:44,409 - 10000 samples (128 per mini-batch)
2018-10-27 22:48:45,131 - Epoch: [12][   50/   78]    Loss 0.869976    Top1 72.843750    Top5 98.109375    
2018-10-27 22:48:45,523 - ==> Top1: 72.970    Top5: 98.140    Loss: 0.868

2018-10-27 22:48:45,524 - ==> Best Top1: 77.410   On Epoch: 11

2018-10-27 22:48:45,524 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:48:45,534 - 

2018-10-27 22:48:45,534 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:48:46,724 - Epoch: [13][   50/  391]    Overall Loss 0.511595    Objective Loss 0.511595    Top1 82.375000    Top5 98.921875    LR 0.300000    Time 0.023759    
2018-10-27 22:48:47,855 - Epoch: [13][  100/  391]    Overall Loss 0.516108    Objective Loss 0.516108    Top1 82.304688    Top5 98.953125    LR 0.300000    Time 0.023179    
2018-10-27 22:48:48,986 - Epoch: [13][  150/  391]    Overall Loss 0.521458    Objective Loss 0.521458    Top1 82.041667    Top5 99.015625    LR 0.300000    Time 0.022986    
2018-10-27 22:48:50,115 - Epoch: [13][  200/  391]    Overall Loss 0.524863    Objective Loss 0.524863    Top1 81.980469    Top5 99.000000    LR 0.300000    Time 0.022880    
2018-10-27 22:48:51,246 - Epoch: [13][  250/  391]    Overall Loss 0.523496    Objective Loss 0.523496    Top1 82.046875    Top5 99.037500    LR 0.300000    Time 0.022820    
2018-10-27 22:48:52,377 - Epoch: [13][  300/  391]    Overall Loss 0.521404    Objective Loss 0.521404    Top1 82.046875    Top5 99.059896    LR 0.300000    Time 0.022782    
2018-10-27 22:48:53,507 - Epoch: [13][  350/  391]    Overall Loss 0.527282    Objective Loss 0.527282    Top1 81.937500    Top5 99.049107    LR 0.300000    Time 0.022755    
2018-10-27 22:48:54,512 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            150 |    0.00000 |    0.00000 |  0.00000 |  2.08333 |  0.00000 |   65.27778 | 0.52307 | -0.00484 |    0.23254 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            361 |    0.00000 |    0.00000 |  0.00000 | 30.07812 |  0.00000 |   84.33160 | 0.16558 | -0.00762 |    0.05135 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            349 |    0.00000 |    0.00000 |  0.00000 | 27.73438 |  0.00000 |   84.85243 | 0.16978 | -0.00004 |    0.05568 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            403 |    0.00000 |    0.00000 |  0.00000 | 21.87500 |  0.00000 |   82.50868 | 0.16376 | -0.00706 |    0.05559 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            306 |    0.00000 |    0.00000 |  0.00000 | 34.76562 |  0.00000 |   86.71875 | 0.13732 | -0.00289 |    0.04248 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            568 |    0.00000 |    0.00000 |  0.00000 |  8.59375 |  0.00000 |   75.34722 | 0.16367 | -0.00980 |    0.06555 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            388 |    0.00000 |    0.00000 |  0.00000 | 27.73438 |  0.00000 |   83.15972 | 0.13809 | -0.00411 |    0.04699 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1602 |    0.00000 |    0.00000 |  0.00000 |  4.10156 |  0.00000 |   65.23438 | 0.17725 | -0.00964 |    0.08500 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           2561 |    0.00000 |    0.00000 |  0.00000 |  6.44531 |  0.00000 |   72.21137 | 0.13856 | -0.00591 |    0.06056 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            115 |    0.00000 |    0.00000 |  0.00000 | 77.53906 |  0.00000 |   77.53906 | 0.24960 | -0.00108 |    0.10636 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1990 |    0.00000 |    0.00000 |  0.00000 | 13.47656 |  0.00000 |   78.40712 | 0.12032 | -0.00346 |    0.04734 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1622 |    0.00000 |    0.00000 |  0.00000 | 22.07031 |  0.00000 |   82.40017 | 0.10462 | -0.00296 |    0.03755 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2230 |    0.00000 |    0.00000 |  0.00000 | 11.42578 |  0.00000 |   75.80295 | 0.12535 | -0.00403 |    0.05181 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           1704 |    0.00000 |    0.00000 |  0.00000 | 20.31250 |  0.00000 |   81.51042 | 0.10193 | -0.00627 |    0.03752 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           5156 |    0.00000 |    0.00000 |  0.00000 |  7.81250 |  0.00000 |   72.02691 | 0.12072 | -0.00342 |    0.05403 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          11651 |    0.00000 |    0.00000 |  0.00000 |  4.07715 |  0.00000 |   68.39464 | 0.11153 | -0.00727 |    0.05240 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            359 |    0.00000 |    0.00000 |  0.00000 | 82.47070 |  0.00000 |   82.47070 | 0.15123 | -0.00377 |    0.05618 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9881 |    0.00000 |    0.00000 |  0.00000 |  7.78809 |  0.00000 |   73.19607 | 0.10263 | -0.00456 |    0.04423 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           7293 |    0.00000 |    0.00000 |  0.00000 | 16.89453 |  0.00000 |   80.21647 | 0.08311 | -0.00450 |    0.03093 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           7763 |    0.00000 |    0.00000 |  0.00000 | 16.25977 |  0.00000 |   78.94151 | 0.07467 | -0.00037 |    0.02864 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3862 |    0.00000 |    0.00000 |  0.00000 | 43.62793 |  0.00000 |   89.52365 | 0.05442 |  0.00079 |    0.01492 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            256 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   60.00000 | 0.45814 | -0.08597 |    0.25254 |
| 22 | Total sparsity:                     | -              |        270896 |          60570 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   77.64087 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:48:54,513 - Total sparsity: 77.64

2018-10-27 22:48:54,513 - --- validate (epoch=13)-----------
2018-10-27 22:48:54,513 - 10000 samples (128 per mini-batch)
2018-10-27 22:48:55,225 - Epoch: [13][   50/   78]    Loss 0.674648    Top1 78.000000    Top5 98.875000    
2018-10-27 22:48:55,611 - ==> Top1: 77.870    Top5: 98.920    Loss: 0.691

2018-10-27 22:48:55,612 - ==> Best Top1: 77.870   On Epoch: 13

2018-10-27 22:48:55,612 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:48:55,633 - 

2018-10-27 22:48:55,634 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:48:56,823 - Epoch: [14][   50/  391]    Overall Loss 0.512530    Objective Loss 0.512530    Top1 82.421875    Top5 99.250000    LR 0.300000    Time 0.023737    
2018-10-27 22:48:57,981 - Epoch: [14][  100/  391]    Overall Loss 0.524805    Objective Loss 0.524805    Top1 81.804688    Top5 99.242188    LR 0.300000    Time 0.023434    
2018-10-27 22:48:59,138 - Epoch: [14][  150/  391]    Overall Loss 0.528753    Objective Loss 0.528753    Top1 81.708333    Top5 99.203125    LR 0.300000    Time 0.023329    
2018-10-27 22:49:00,297 - Epoch: [14][  200/  391]    Overall Loss 0.519197    Objective Loss 0.519197    Top1 82.046875    Top5 99.234375    LR 0.300000    Time 0.023287    
2018-10-27 22:49:01,455 - Epoch: [14][  250/  391]    Overall Loss 0.522778    Objective Loss 0.522778    Top1 81.856250    Top5 99.206250    LR 0.300000    Time 0.023257    
2018-10-27 22:49:02,613 - Epoch: [14][  300/  391]    Overall Loss 0.522834    Objective Loss 0.522834    Top1 81.908854    Top5 99.171875    LR 0.300000    Time 0.023235    
2018-10-27 22:49:03,771 - Epoch: [14][  350/  391]    Overall Loss 0.521049    Objective Loss 0.521049    Top1 82.071429    Top5 99.147321    LR 0.300000    Time 0.023221    
2018-10-27 22:49:04,799 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            140 |    0.00000 |    0.00000 |  0.00000 |  2.08333 |  0.00000 |   67.59259 | 0.51417 | -0.00972 |    0.22557 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            316 |    0.00000 |    0.00000 |  0.00000 | 37.50000 |  0.00000 |   86.28472 | 0.16502 | -0.00671 |    0.04912 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            318 |    0.00000 |    0.00000 |  0.00000 | 31.64062 |  0.00000 |   86.19792 | 0.16821 | -0.00053 |    0.05380 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            365 |    0.00000 |    0.00000 |  0.00000 | 25.39062 |  0.00000 |   84.15799 | 0.16369 | -0.00861 |    0.05433 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 39.45312 |  0.00000 |   87.89062 | 0.13617 | -0.00331 |    0.04026 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            510 |    0.00000 |    0.00000 |  0.00000 | 11.71875 |  0.00000 |   77.86458 | 0.16245 | -0.00862 |    0.06304 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            355 |    0.00000 |    0.00000 |  0.00000 | 31.25000 |  0.00000 |   84.59201 | 0.13808 | -0.00488 |    0.04546 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1507 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   67.29601 | 0.17751 | -0.00834 |    0.08357 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           2336 |    0.00000 |    0.00000 |  0.00000 |  8.98438 |  0.00000 |   74.65278 | 0.13829 | -0.00604 |    0.05863 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            107 |    0.00000 |    0.00000 |  0.00000 | 79.10156 |  0.00000 |   79.10156 | 0.24406 | -0.00139 |    0.10209 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1828 |    0.00000 |    0.00000 |  0.00000 | 16.40625 |  0.00000 |   80.16493 | 0.11989 | -0.00319 |    0.04593 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1474 |    0.00000 |    0.00000 |  0.00000 | 26.56250 |  0.00000 |   84.00608 | 0.10379 | -0.00349 |    0.03625 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2055 |    0.00000 |    0.00000 |  0.00000 | 14.35547 |  0.00000 |   77.70182 | 0.12508 | -0.00512 |    0.05038 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           1540 |    0.00000 |    0.00000 |  0.00000 | 24.31641 |  0.00000 |   83.28993 | 0.10109 | -0.00589 |    0.03607 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           4854 |    0.00000 |    0.00000 |  0.00000 |  9.27734 |  0.00000 |   73.66536 | 0.12041 | -0.00378 |    0.05280 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          10870 |    0.00000 |    0.00000 |  0.00000 |  5.49316 |  0.00000 |   70.51324 | 0.11130 | -0.00690 |    0.05095 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            327 |    0.00000 |    0.00000 |  0.00000 | 84.03320 |  0.00000 |   84.03320 | 0.14728 | -0.00373 |    0.05319 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9135 |    0.00000 |    0.00000 |  0.00000 |  9.93652 |  0.00000 |   75.21973 | 0.10255 | -0.00457 |    0.04323 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           6632 |    0.00000 |    0.00000 |  0.00000 | 20.36133 |  0.00000 |   82.00955 | 0.08307 | -0.00423 |    0.03006 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           7066 |    0.00000 |    0.00000 |  0.00000 | 19.79980 |  0.00000 |   80.83225 | 0.07466 | -0.00055 |    0.02783 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 48.87695 |  0.00000 |   90.68197 | 0.05405 |  0.00081 |    0.01426 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            252 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   60.62500 | 0.46266 | -0.08455 |    0.25349 |
| 22 | Total sparsity:                     | -              |        270896 |          55701 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   79.43823 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:49:04,799 - Total sparsity: 79.44

2018-10-27 22:49:04,799 - --- validate (epoch=14)-----------
2018-10-27 22:49:04,800 - 10000 samples (128 per mini-batch)
2018-10-27 22:49:05,508 - Epoch: [14][   50/   78]    Loss 0.612387    Top1 79.421875    Top5 98.687500    
2018-10-27 22:49:05,897 - ==> Top1: 79.240    Top5: 98.730    Loss: 0.615

2018-10-27 22:49:05,898 - ==> Best Top1: 79.240   On Epoch: 14

2018-10-27 22:49:05,898 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:49:05,914 - 

2018-10-27 22:49:05,914 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:49:07,075 - Epoch: [15][   50/  391]    Overall Loss 0.502357    Objective Loss 0.502357    Top1 82.609375    Top5 99.234375    LR 0.300000    Time 0.023192    
2018-10-27 22:49:08,207 - Epoch: [15][  100/  391]    Overall Loss 0.507610    Objective Loss 0.507610    Top1 82.343750    Top5 99.195312    LR 0.300000    Time 0.022897    
2018-10-27 22:49:09,338 - Epoch: [15][  150/  391]    Overall Loss 0.511861    Objective Loss 0.511861    Top1 82.343750    Top5 99.171875    LR 0.300000    Time 0.022801    
2018-10-27 22:49:10,471 - Epoch: [15][  200/  391]    Overall Loss 0.505777    Objective Loss 0.505777    Top1 82.460938    Top5 99.191406    LR 0.300000    Time 0.022759    
2018-10-27 22:49:11,605 - Epoch: [15][  250/  391]    Overall Loss 0.508247    Objective Loss 0.508247    Top1 82.409375    Top5 99.196875    LR 0.300000    Time 0.022738    
2018-10-27 22:49:12,739 - Epoch: [15][  300/  391]    Overall Loss 0.513636    Objective Loss 0.513636    Top1 82.161458    Top5 99.179688    LR 0.300000    Time 0.022723    
2018-10-27 22:49:13,873 - Epoch: [15][  350/  391]    Overall Loss 0.514649    Objective Loss 0.514649    Top1 82.227679    Top5 99.169643    LR 0.300000    Time 0.022713    
2018-10-27 22:49:14,882 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            140 |    0.00000 |    0.00000 |  0.00000 |  2.08333 |  0.00000 |   67.59259 | 0.51230 | -0.00136 |    0.22376 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            316 |    0.00000 |    0.00000 |  0.00000 | 37.50000 |  0.00000 |   86.28472 | 0.16500 | -0.00810 |    0.04879 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            318 |    0.00000 |    0.00000 |  0.00000 | 31.64062 |  0.00000 |   86.19792 | 0.16717 |  0.00049 |    0.05347 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            365 |    0.00000 |    0.00000 |  0.00000 | 25.39062 |  0.00000 |   84.15799 | 0.16317 | -0.00746 |    0.05270 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            279 |    0.00000 |    0.00000 |  0.00000 | 39.45312 |  0.00000 |   87.89062 | 0.13505 | -0.00311 |    0.03913 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            510 |    0.00000 |    0.00000 |  0.00000 | 11.71875 |  0.00000 |   77.86458 | 0.16186 | -0.00921 |    0.06275 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            355 |    0.00000 |    0.00000 |  0.00000 | 31.25000 |  0.00000 |   84.59201 | 0.13661 | -0.00402 |    0.04520 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1507 |    0.00000 |    0.00000 |  0.00000 |  6.05469 |  0.00000 |   67.29601 | 0.17747 | -0.00893 |    0.08334 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           2336 |    0.00000 |    0.00000 |  0.00000 |  8.98438 |  0.00000 |   74.65278 | 0.13823 | -0.00630 |    0.05802 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            107 |    0.00000 |    0.00000 |  0.00000 | 79.10156 |  0.00000 |   79.10156 | 0.24295 |  0.00006 |    0.10203 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1828 |    0.00000 |    0.00000 |  0.00000 | 16.40625 |  0.00000 |   80.16493 | 0.12022 | -0.00317 |    0.04561 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1474 |    0.00000 |    0.00000 |  0.00000 | 26.56250 |  0.00000 |   84.00608 | 0.10385 | -0.00349 |    0.03581 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           2055 |    0.00000 |    0.00000 |  0.00000 | 14.35547 |  0.00000 |   77.70182 | 0.12509 | -0.00431 |    0.04989 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           1540 |    0.00000 |    0.00000 |  0.00000 | 24.31641 |  0.00000 |   83.28993 | 0.10060 | -0.00632 |    0.03562 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           4854 |    0.00000 |    0.00000 |  0.00000 |  9.27734 |  0.00000 |   73.66536 | 0.12081 | -0.00339 |    0.05271 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          10870 |    0.00000 |    0.00000 |  0.00000 |  5.49316 |  0.00000 |   70.51324 | 0.11188 | -0.00701 |    0.05080 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            327 |    0.00000 |    0.00000 |  0.00000 | 84.03320 |  0.00000 |   84.03320 | 0.14522 | -0.00380 |    0.05194 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           9135 |    0.00000 |    0.00000 |  0.00000 |  9.93652 |  0.00000 |   75.21973 | 0.10304 | -0.00466 |    0.04302 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           6632 |    0.00000 |    0.00000 |  0.00000 | 20.36133 |  0.00000 |   82.00955 | 0.08352 | -0.00426 |    0.02993 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           7066 |    0.00000 |    0.00000 |  0.00000 | 19.79980 |  0.00000 |   80.83225 | 0.07511 | -0.00071 |    0.02764 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3435 |    0.00000 |    0.00000 |  0.00000 | 48.87695 |  0.00000 |   90.68197 | 0.05395 |  0.00095 |    0.01409 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            252 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   60.62500 | 0.45442 | -0.08339 |    0.24890 |
| 22 | Total sparsity:                     | -              |        270896 |          55701 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   79.43823 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:49:14,882 - Total sparsity: 79.44

2018-10-27 22:49:14,883 - --- validate (epoch=15)-----------
2018-10-27 22:49:14,883 - 10000 samples (128 per mini-batch)
2018-10-27 22:49:15,605 - Epoch: [15][   50/   78]    Loss 0.584590    Top1 80.078125    Top5 99.093750    
2018-10-27 22:49:15,995 - ==> Top1: 79.700    Top5: 98.990    Loss: 0.598

2018-10-27 22:49:15,996 - ==> Best Top1: 79.700   On Epoch: 15

2018-10-27 22:49:15,996 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:49:16,013 - 

2018-10-27 22:49:16,014 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:49:17,203 - Epoch: [16][   50/  391]    Overall Loss 0.499822    Objective Loss 0.499822    Top1 83.203125    Top5 99.218750    LR 0.300000    Time 0.023735    
2018-10-27 22:49:18,361 - Epoch: [16][  100/  391]    Overall Loss 0.518087    Objective Loss 0.518087    Top1 82.421875    Top5 99.148438    LR 0.300000    Time 0.023436    
2018-10-27 22:49:19,520 - Epoch: [16][  150/  391]    Overall Loss 0.504743    Objective Loss 0.504743    Top1 82.791667    Top5 99.229167    LR 0.300000    Time 0.023342    
2018-10-27 22:49:20,678 - Epoch: [16][  200/  391]    Overall Loss 0.508221    Objective Loss 0.508221    Top1 82.656250    Top5 99.191406    LR 0.300000    Time 0.023290    
2018-10-27 22:49:21,837 - Epoch: [16][  250/  391]    Overall Loss 0.506716    Objective Loss 0.506716    Top1 82.687500    Top5 99.175000    LR 0.300000    Time 0.023261    
2018-10-27 22:49:22,995 - Epoch: [16][  300/  391]    Overall Loss 0.507858    Objective Loss 0.507858    Top1 82.533854    Top5 99.195312    LR 0.300000    Time 0.023241    
2018-10-27 22:49:24,154 - Epoch: [16][  350/  391]    Overall Loss 0.508919    Objective Loss 0.508919    Top1 82.569196    Top5 99.174107    LR 0.300000    Time 0.023228    
2018-10-27 22:49:25,184 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            128 |    0.00000 |    0.00000 |  0.00000 |  4.16667 |  0.00000 |   70.37037 | 0.50735 |  0.00203 |    0.21632 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            284 |    0.00000 |    0.00000 |  0.00000 | 41.01562 |  0.00000 |   87.67361 | 0.16444 | -0.00744 |    0.04696 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            299 |    0.00000 |    0.00000 |  0.00000 | 35.15625 |  0.00000 |   87.02257 | 0.16637 |  0.00021 |    0.05222 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            325 |    0.00000 |    0.00000 |  0.00000 | 29.68750 |  0.00000 |   85.89410 | 0.16191 | -0.00839 |    0.05107 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            244 |    0.00000 |    0.00000 |  0.00000 | 44.14062 |  0.00000 |   89.40972 | 0.13300 | -0.00292 |    0.03702 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            468 |    0.00000 |    0.00000 |  0.00000 | 17.57812 |  0.00000 |   79.68750 | 0.16138 | -0.00832 |    0.06105 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            325 |    0.00000 |    0.00000 |  0.00000 | 34.37500 |  0.00000 |   85.89410 | 0.13488 | -0.00347 |    0.04337 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1413 |    0.00000 |    0.00000 |  0.00000 |  7.42188 |  0.00000 |   69.33594 | 0.17690 | -0.00841 |    0.08091 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           2173 |    0.00000 |    0.00000 |  0.00000 | 11.03516 |  0.00000 |   76.42144 | 0.13774 | -0.00548 |    0.05643 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            104 |    0.00000 |    0.00000 |  0.00000 | 79.68750 |  0.00000 |   79.68750 | 0.23918 |  0.00011 |    0.09839 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1700 |    0.00000 |    0.00000 |  0.00000 | 19.33594 |  0.00000 |   81.55382 | 0.11964 | -0.00352 |    0.04441 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1344 |    0.00000 |    0.00000 |  0.00000 | 30.76172 |  0.00000 |   85.41667 | 0.10299 | -0.00333 |    0.03450 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1913 |    0.00000 |    0.00000 |  0.00000 | 17.18750 |  0.00000 |   79.24262 | 0.12514 | -0.00303 |    0.04846 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           1409 |    0.00000 |    0.00000 |  0.00000 | 27.92969 |  0.00000 |   84.71137 | 0.10018 | -0.00638 |    0.03420 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           4544 |    0.00000 |    0.00000 |  0.00000 | 11.57227 |  0.00000 |   75.34722 | 0.12069 | -0.00329 |    0.05142 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          10156 |    0.00000 |    0.00000 |  0.00000 |  6.78711 |  0.00000 |   72.45009 | 0.11187 | -0.00672 |    0.04981 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            304 |    0.00000 |    0.00000 |  0.00000 | 85.15625 |  0.00000 |   85.15625 | 0.14279 | -0.00352 |    0.04973 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           8500 |    0.00000 |    0.00000 |  0.00000 | 12.15820 |  0.00000 |   76.94227 | 0.10291 | -0.00432 |    0.04199 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           6134 |    0.00000 |    0.00000 |  0.00000 | 23.33984 |  0.00000 |   83.36046 | 0.08325 | -0.00417 |    0.02901 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           6525 |    0.00000 |    0.00000 |  0.00000 | 23.14453 |  0.00000 |   82.29980 | 0.07500 | -0.00055 |    0.02687 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3131 |    0.00000 |    0.00000 |  0.00000 | 52.63672 |  0.00000 |   91.50662 | 0.05367 |  0.00100 |    0.01350 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            250 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   60.93750 | 0.46203 | -0.08321 |    0.25158 |
| 22 | Total sparsity:                     | -              |        270896 |          51673 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   80.92515 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:49:25,184 - Total sparsity: 80.93

2018-10-27 22:49:25,184 - --- validate (epoch=16)-----------
2018-10-27 22:49:25,184 - 10000 samples (128 per mini-batch)
2018-10-27 22:49:25,909 - Epoch: [16][   50/   78]    Loss 0.855932    Top1 74.359375    Top5 98.687500    
2018-10-27 22:49:26,305 - ==> Top1: 74.260    Top5: 98.630    Loss: 0.865

2018-10-27 22:49:26,306 - ==> Best Top1: 79.700   On Epoch: 15

2018-10-27 22:49:26,306 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:49:26,315 - 

2018-10-27 22:49:26,316 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:49:27,507 - Epoch: [17][   50/  391]    Overall Loss 0.522585    Objective Loss 0.522585    Top1 81.875000    Top5 99.015625    LR 0.300000    Time 0.023790    
2018-10-27 22:49:28,638 - Epoch: [17][  100/  391]    Overall Loss 0.516061    Objective Loss 0.516061    Top1 81.968750    Top5 99.148438    LR 0.300000    Time 0.023192    
2018-10-27 22:49:29,772 - Epoch: [17][  150/  391]    Overall Loss 0.510873    Objective Loss 0.510873    Top1 82.223958    Top5 99.166667    LR 0.300000    Time 0.023011    
2018-10-27 22:49:30,906 - Epoch: [17][  200/  391]    Overall Loss 0.510823    Objective Loss 0.510823    Top1 82.230469    Top5 99.156250    LR 0.300000    Time 0.022924    
2018-10-27 22:49:32,042 - Epoch: [17][  250/  391]    Overall Loss 0.508115    Objective Loss 0.508115    Top1 82.328125    Top5 99.187500    LR 0.300000    Time 0.022877    
2018-10-27 22:49:33,176 - Epoch: [17][  300/  391]    Overall Loss 0.507062    Objective Loss 0.507062    Top1 82.335938    Top5 99.216146    LR 0.300000    Time 0.022839    
2018-10-27 22:49:34,310 - Epoch: [17][  350/  391]    Overall Loss 0.508079    Objective Loss 0.508079    Top1 82.348214    Top5 99.220982    LR 0.300000    Time 0.022814    
2018-10-27 22:49:35,316 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            128 |    0.00000 |    0.00000 |  0.00000 |  4.16667 |  0.00000 |   70.37037 | 0.50851 | -0.00869 |    0.21527 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            284 |    0.00000 |    0.00000 |  0.00000 | 41.01562 |  0.00000 |   87.67361 | 0.16445 | -0.00707 |    0.04712 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            299 |    0.00000 |    0.00000 |  0.00000 | 35.15625 |  0.00000 |   87.02257 | 0.16528 | -0.00033 |    0.05162 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            325 |    0.00000 |    0.00000 |  0.00000 | 29.68750 |  0.00000 |   85.89410 | 0.16359 | -0.00747 |    0.05159 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            244 |    0.00000 |    0.00000 |  0.00000 | 44.14062 |  0.00000 |   89.40972 | 0.13306 | -0.00216 |    0.03655 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            468 |    0.00000 |    0.00000 |  0.00000 | 17.57812 |  0.00000 |   79.68750 | 0.16118 | -0.00825 |    0.06069 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            325 |    0.00000 |    0.00000 |  0.00000 | 34.37500 |  0.00000 |   85.89410 | 0.13449 | -0.00307 |    0.04304 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1413 |    0.00000 |    0.00000 |  0.00000 |  7.42188 |  0.00000 |   69.33594 | 0.17748 | -0.00707 |    0.08000 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           2173 |    0.00000 |    0.00000 |  0.00000 | 11.03516 |  0.00000 |   76.42144 | 0.13791 | -0.00561 |    0.05614 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |            104 |    0.00000 |    0.00000 |  0.00000 | 79.68750 |  0.00000 |   79.68750 | 0.23747 | -0.00284 |    0.09717 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1700 |    0.00000 |    0.00000 |  0.00000 | 19.33594 |  0.00000 |   81.55382 | 0.11933 | -0.00317 |    0.04394 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1344 |    0.00000 |    0.00000 |  0.00000 | 30.76172 |  0.00000 |   85.41667 | 0.10273 | -0.00340 |    0.03406 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1913 |    0.00000 |    0.00000 |  0.00000 | 17.18750 |  0.00000 |   79.24262 | 0.12540 | -0.00361 |    0.04826 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           1409 |    0.00000 |    0.00000 |  0.00000 | 27.92969 |  0.00000 |   84.71137 | 0.10045 | -0.00579 |    0.03410 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           4544 |    0.00000 |    0.00000 |  0.00000 | 11.57227 |  0.00000 |   75.34722 | 0.12102 | -0.00325 |    0.05130 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |          10156 |    0.00000 |    0.00000 |  0.00000 |  6.78711 |  0.00000 |   72.45009 | 0.11255 | -0.00663 |    0.04973 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            304 |    0.00000 |    0.00000 |  0.00000 | 85.15625 |  0.00000 |   85.15625 | 0.14154 | -0.00442 |    0.04907 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           8500 |    0.00000 |    0.00000 |  0.00000 | 12.15820 |  0.00000 |   76.94227 | 0.10336 | -0.00403 |    0.04190 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           6134 |    0.00000 |    0.00000 |  0.00000 | 23.33984 |  0.00000 |   83.36046 | 0.08374 | -0.00401 |    0.02893 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           6525 |    0.00000 |    0.00000 |  0.00000 | 23.14453 |  0.00000 |   82.29980 | 0.07563 | -0.00031 |    0.02686 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           3131 |    0.00000 |    0.00000 |  0.00000 | 52.63672 |  0.00000 |   91.50662 | 0.05392 |  0.00117 |    0.01339 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            250 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   60.93750 | 0.45930 | -0.08655 |    0.24965 |
| 22 | Total sparsity:                     | -              |        270896 |          51673 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   80.92515 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:49:35,317 - Total sparsity: 80.93

2018-10-27 22:49:35,317 - --- validate (epoch=17)-----------
2018-10-27 22:49:35,317 - 10000 samples (128 per mini-batch)
2018-10-27 22:49:36,041 - Epoch: [17][   50/   78]    Loss 0.676061    Top1 78.187500    Top5 98.875000    
2018-10-27 22:49:36,433 - ==> Top1: 77.540    Top5: 98.940    Loss: 0.695

2018-10-27 22:49:36,434 - ==> Best Top1: 79.700   On Epoch: 15

2018-10-27 22:49:36,434 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:49:36,446 - 

2018-10-27 22:49:36,447 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:49:37,660 - Epoch: [18][   50/  391]    Overall Loss 0.494117    Objective Loss 0.494117    Top1 83.343750    Top5 99.250000    LR 0.300000    Time 0.024224    
2018-10-27 22:49:38,818 - Epoch: [18][  100/  391]    Overall Loss 0.490278    Objective Loss 0.490278    Top1 83.250000    Top5 99.320312    LR 0.300000    Time 0.023676    
2018-10-27 22:49:39,977 - Epoch: [18][  150/  391]    Overall Loss 0.500102    Objective Loss 0.500102    Top1 82.828125    Top5 99.291667    LR 0.300000    Time 0.023500    
2018-10-27 22:49:41,135 - Epoch: [18][  200/  391]    Overall Loss 0.499841    Objective Loss 0.499841    Top1 82.679688    Top5 99.265625    LR 0.300000    Time 0.023408    
2018-10-27 22:49:42,291 - Epoch: [18][  250/  391]    Overall Loss 0.496841    Objective Loss 0.496841    Top1 82.787500    Top5 99.237500    LR 0.300000    Time 0.023346    
2018-10-27 22:49:43,449 - Epoch: [18][  300/  391]    Overall Loss 0.496986    Objective Loss 0.496986    Top1 82.778646    Top5 99.226562    LR 0.300000    Time 0.023310    
2018-10-27 22:49:44,603 - Epoch: [18][  350/  391]    Overall Loss 0.496077    Objective Loss 0.496077    Top1 82.819196    Top5 99.212054    LR 0.300000    Time 0.023276    
2018-10-27 22:49:45,632 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            121 |    0.00000 |    0.00000 |  0.00000 |  8.33333 |  0.00000 |   71.99074 | 0.50361 | -0.00305 |    0.21074 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            256 |    0.00000 |    0.00000 | 18.75000 | 46.48438 |  0.00000 |   88.88889 | 0.16448 | -0.00736 |    0.04578 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            282 |    0.00000 |    0.00000 |  0.00000 | 37.50000 |  0.00000 |   87.76042 | 0.16490 | -0.00038 |    0.05037 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            307 |    0.00000 |    0.00000 |  0.00000 | 31.64062 |  0.00000 |   86.67535 | 0.16267 | -0.00775 |    0.04999 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            220 |    0.00000 |    0.00000 |  0.00000 | 48.04688 |  0.00000 |   90.45139 | 0.13152 | -0.00156 |    0.03504 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            444 |    0.00000 |    0.00000 |  0.00000 | 20.70312 |  0.00000 |   80.72917 | 0.16195 | -0.00876 |    0.05919 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            303 |    0.00000 |    0.00000 |  0.00000 | 37.89062 |  0.00000 |   86.84896 | 0.13360 | -0.00341 |    0.04182 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1317 |    0.00000 |    0.00000 |  0.00000 |  9.37500 |  0.00000 |   71.41927 | 0.17659 | -0.00781 |    0.07831 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           2014 |    0.00000 |    0.00000 |  0.00000 | 13.37891 |  0.00000 |   78.14670 | 0.13730 | -0.00559 |    0.05457 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             98 |    0.00000 |    0.00000 |  0.00000 | 80.85938 |  0.00000 |   80.85938 | 0.23452 | -0.00259 |    0.09440 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1594 |    0.00000 |    0.00000 |  0.00000 | 22.46094 |  0.00000 |   82.70399 | 0.11833 | -0.00292 |    0.04286 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1242 |    0.00000 |    0.00000 |  0.00000 | 33.88672 |  0.00000 |   86.52344 | 0.10185 | -0.00333 |    0.03277 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1783 |    0.00000 |    0.00000 |  0.00000 | 20.01953 |  0.00000 |   80.65321 | 0.12490 | -0.00418 |    0.04713 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           1296 |    0.00000 |    0.00000 |  0.00000 | 31.44531 |  0.00000 |   85.93750 | 0.09934 | -0.00600 |    0.03296 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           4268 |    0.00000 |    0.00000 |  0.00000 | 13.67188 |  0.00000 |   76.84462 | 0.12059 | -0.00261 |    0.05006 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           9549 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   74.09668 | 0.11222 | -0.00652 |    0.04859 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            277 |    0.00000 |    0.00000 |  0.00000 | 86.47461 |  0.00000 |   86.47461 | 0.13828 | -0.00435 |    0.04617 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           7950 |    0.00000 |    0.00000 |  0.00000 | 14.57520 |  0.00000 |   78.43424 | 0.10299 | -0.00407 |    0.04083 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           5682 |    0.00000 |    0.00000 |  0.00000 | 25.87891 |  0.00000 |   84.58659 | 0.08354 | -0.00418 |    0.02819 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           6047 |    0.00000 |    0.00000 |  0.00000 | 26.58691 |  0.00000 |   83.59646 | 0.07547 | -0.00050 |    0.02616 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           2861 |    0.00000 |    0.00000 |  0.00000 | 55.83496 |  0.00000 |   92.23904 | 0.05357 |  0.00140 |    0.01290 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            248 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   61.25000 | 0.46223 | -0.08614 |    0.24993 |
| 22 | Total sparsity:                     | -              |        270896 |          48159 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   82.22233 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:49:45,632 - Total sparsity: 82.22

2018-10-27 22:49:45,632 - --- validate (epoch=18)-----------
2018-10-27 22:49:45,632 - 10000 samples (128 per mini-batch)
2018-10-27 22:49:46,355 - Epoch: [18][   50/   78]    Loss 0.875122    Top1 72.718750    Top5 98.375000    
2018-10-27 22:49:46,749 - ==> Top1: 72.640    Top5: 98.330    Loss: 0.885

2018-10-27 22:49:46,749 - ==> Best Top1: 79.700   On Epoch: 15

2018-10-27 22:49:46,750 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:49:46,766 - 

2018-10-27 22:49:46,766 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:49:47,929 - Epoch: [19][   50/  391]    Overall Loss 0.470608    Objective Loss 0.470608    Top1 83.312500    Top5 99.421875    LR 0.300000    Time 0.023224    
2018-10-27 22:49:49,061 - Epoch: [19][  100/  391]    Overall Loss 0.488508    Objective Loss 0.488508    Top1 82.875000    Top5 99.351562    LR 0.300000    Time 0.022926    
2018-10-27 22:49:50,194 - Epoch: [19][  150/  391]    Overall Loss 0.497349    Objective Loss 0.497349    Top1 82.604167    Top5 99.239583    LR 0.300000    Time 0.022828    
2018-10-27 22:49:51,330 - Epoch: [19][  200/  391]    Overall Loss 0.494775    Objective Loss 0.494775    Top1 82.804688    Top5 99.261719    LR 0.300000    Time 0.022791    
2018-10-27 22:49:52,464 - Epoch: [19][  250/  391]    Overall Loss 0.498069    Objective Loss 0.498069    Top1 82.643750    Top5 99.259375    LR 0.300000    Time 0.022766    
2018-10-27 22:49:53,599 - Epoch: [19][  300/  391]    Overall Loss 0.498444    Objective Loss 0.498444    Top1 82.703125    Top5 99.242188    LR 0.300000    Time 0.022750    
2018-10-27 22:49:54,733 - Epoch: [19][  350/  391]    Overall Loss 0.499687    Objective Loss 0.499687    Top1 82.665179    Top5 99.225446    LR 0.300000    Time 0.022737    
2018-10-27 22:49:55,741 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            121 |    0.00000 |    0.00000 |  0.00000 |  8.33333 |  0.00000 |   71.99074 | 0.50587 | -0.00331 |    0.21146 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            256 |    0.00000 |    0.00000 | 18.75000 | 46.48438 |  0.00000 |   88.88889 | 0.16460 | -0.00588 |    0.04440 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            282 |    0.00000 |    0.00000 |  0.00000 | 37.50000 |  0.00000 |   87.76042 | 0.16432 |  0.00080 |    0.05012 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            307 |    0.00000 |    0.00000 |  0.00000 | 31.64062 |  0.00000 |   86.67535 | 0.16258 | -0.00776 |    0.04946 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            220 |    0.00000 |    0.00000 |  0.00000 | 48.04688 |  0.00000 |   90.45139 | 0.13118 | -0.00169 |    0.03457 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            444 |    0.00000 |    0.00000 |  0.00000 | 20.70312 |  0.00000 |   80.72917 | 0.16304 | -0.00943 |    0.05912 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            303 |    0.00000 |    0.00000 |  0.00000 | 37.89062 |  0.00000 |   86.84896 | 0.13304 | -0.00276 |    0.04087 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1317 |    0.00000 |    0.00000 |  0.00000 |  9.37500 |  0.00000 |   71.41927 | 0.17705 | -0.00687 |    0.07751 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           2014 |    0.00000 |    0.00000 |  0.00000 | 13.37891 |  0.00000 |   78.14670 | 0.13737 | -0.00553 |    0.05442 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             98 |    0.00000 |    0.00000 |  0.00000 | 80.85938 |  0.00000 |   80.85938 | 0.23085 | -0.00440 |    0.09148 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1594 |    0.00000 |    0.00000 |  0.00000 | 22.46094 |  0.00000 |   82.70399 | 0.11868 | -0.00313 |    0.04249 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1242 |    0.00000 |    0.00000 |  0.00000 | 33.88672 |  0.00000 |   86.52344 | 0.10144 | -0.00316 |    0.03219 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1783 |    0.00000 |    0.00000 |  0.00000 | 20.01953 |  0.00000 |   80.65321 | 0.12505 | -0.00406 |    0.04706 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           1296 |    0.00000 |    0.00000 |  0.00000 | 31.44531 |  0.00000 |   85.93750 | 0.09891 | -0.00547 |    0.03231 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           4268 |    0.00000 |    0.00000 |  0.00000 | 13.67188 |  0.00000 |   76.84462 | 0.12037 | -0.00281 |    0.04974 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           9549 |    0.00000 |    0.00000 |  0.00000 |  8.10547 |  0.00000 |   74.09668 | 0.11236 | -0.00641 |    0.04826 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            277 |    0.00000 |    0.00000 |  0.00000 | 86.47461 |  0.00000 |   86.47461 | 0.13713 | -0.00343 |    0.04538 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           7950 |    0.00000 |    0.00000 |  0.00000 | 14.57520 |  0.00000 |   78.43424 | 0.10301 | -0.00390 |    0.04035 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           5682 |    0.00000 |    0.00000 |  0.00000 | 25.87891 |  0.00000 |   84.58659 | 0.08378 | -0.00415 |    0.02794 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           6047 |    0.00000 |    0.00000 |  0.00000 | 26.58691 |  0.00000 |   83.59646 | 0.07551 | -0.00047 |    0.02598 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           2861 |    0.00000 |    0.00000 |  0.00000 | 55.83496 |  0.00000 |   92.23904 | 0.05345 |  0.00147 |    0.01277 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            248 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   61.25000 | 0.45807 | -0.08524 |    0.24783 |
| 22 | Total sparsity:                     | -              |        270896 |          48159 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   82.22233 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:49:55,741 - Total sparsity: 82.22

2018-10-27 22:49:55,742 - --- validate (epoch=19)-----------
2018-10-27 22:49:55,742 - 10000 samples (128 per mini-batch)
2018-10-27 22:49:56,467 - Epoch: [19][   50/   78]    Loss 0.655019    Top1 78.906250    Top5 98.843750    
2018-10-27 22:49:56,859 - ==> Top1: 78.480    Top5: 98.850    Loss: 0.669

2018-10-27 22:49:56,859 - ==> Best Top1: 79.700   On Epoch: 15

2018-10-27 22:49:56,860 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:49:56,871 - 

2018-10-27 22:49:56,873 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:49:58,087 - Epoch: [20][   50/  391]    Overall Loss 0.488562    Objective Loss 0.488562    Top1 82.781250    Top5 99.265625    LR 0.300000    Time 0.024247    
2018-10-27 22:49:59,246 - Epoch: [20][  100/  391]    Overall Loss 0.486960    Objective Loss 0.486960    Top1 83.007812    Top5 99.273438    LR 0.300000    Time 0.023696    
2018-10-27 22:50:00,406 - Epoch: [20][  150/  391]    Overall Loss 0.490693    Objective Loss 0.490693    Top1 82.989583    Top5 99.260417    LR 0.300000    Time 0.023522    
2018-10-27 22:50:01,565 - Epoch: [20][  200/  391]    Overall Loss 0.491227    Objective Loss 0.491227    Top1 83.140625    Top5 99.242188    LR 0.300000    Time 0.023430    
2018-10-27 22:50:02,723 - Epoch: [20][  250/  391]    Overall Loss 0.489541    Objective Loss 0.489541    Top1 83.131250    Top5 99.250000    LR 0.300000    Time 0.023370    
2018-10-27 22:50:03,879 - Epoch: [20][  300/  391]    Overall Loss 0.489791    Objective Loss 0.489791    Top1 83.130208    Top5 99.223958    LR 0.300000    Time 0.023325    
2018-10-27 22:50:05,037 - Epoch: [20][  350/  391]    Overall Loss 0.490631    Objective Loss 0.490631    Top1 83.073661    Top5 99.214286    LR 0.300000    Time 0.023298    
2018-10-27 22:50:06,067 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            114 |    0.00000 |    0.00000 |  0.00000 | 14.58333 |  6.25000 |   73.61111 | 0.50813 |  0.00055 |    0.21113 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            232 |    0.00000 |    0.00000 | 18.75000 | 48.04688 |  0.00000 |   89.93056 | 0.16363 | -0.00632 |    0.04364 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            266 |    0.00000 |    0.00000 |  0.00000 | 41.01562 |  0.00000 |   88.45486 | 0.16224 |  0.00018 |    0.04882 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            286 |    0.00000 |    0.00000 |  0.00000 | 34.76562 |  0.00000 |   87.58681 | 0.16195 | -0.00671 |    0.04801 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            206 |    0.00000 |    0.00000 |  0.00000 | 50.78125 |  0.00000 |   91.05903 | 0.13044 | -0.00236 |    0.03388 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            409 |    0.00000 |    0.00000 |  0.00000 | 24.60938 |  0.00000 |   82.24826 | 0.16196 | -0.00784 |    0.05728 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            283 |    0.00000 |    0.00000 |  0.00000 | 41.01562 |  0.00000 |   87.71701 | 0.13136 | -0.00302 |    0.03921 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1257 |    0.00000 |    0.00000 |  0.00000 | 11.32812 |  0.00000 |   72.72135 | 0.17577 | -0.00904 |    0.07584 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1895 |    0.00000 |    0.00000 |  0.00000 | 15.52734 |  0.00000 |   79.43793 | 0.13680 | -0.00415 |    0.05271 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             93 |    0.00000 |    0.00000 |  0.00000 | 81.83594 |  3.12500 |   81.83594 | 0.22904 | -0.00407 |    0.08999 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1488 |    0.00000 |    0.00000 |  0.00000 | 24.90234 |  0.00000 |   83.85417 | 0.11756 | -0.00308 |    0.04118 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1153 |    0.00000 |    0.00000 |  0.00000 | 36.52344 |  0.00000 |   87.48915 | 0.10017 | -0.00271 |    0.03102 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1680 |    0.00000 |    0.00000 |  0.00000 | 22.85156 |  0.00000 |   81.77083 | 0.12492 | -0.00398 |    0.04613 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           1207 |    0.00000 |    0.00000 |  0.00000 | 34.27734 |  0.00000 |   86.90321 | 0.09822 | -0.00572 |    0.03139 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           4037 |    0.00000 |    0.00000 |  0.00000 | 15.23438 |  0.00000 |   78.09787 | 0.11992 | -0.00174 |    0.04864 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           9002 |    0.00000 |    0.00000 |  0.00000 |  9.59473 |  0.00000 |   75.58051 | 0.11192 | -0.00668 |    0.04727 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            257 |    0.00000 |    0.00000 |  3.12500 | 87.45117 |  0.00000 |   87.45117 | 0.13517 | -0.00273 |    0.04337 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           7482 |    0.00000 |    0.00000 |  0.00000 | 16.33301 |  0.00000 |   79.70378 | 0.10268 | -0.00381 |    0.03955 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           5290 |    0.00000 |    0.00000 |  0.00000 | 28.93066 |  0.00000 |   85.64996 | 0.08345 | -0.00399 |    0.02728 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5664 |    0.00000 |    0.00000 |  0.00000 | 29.29688 |  0.00000 |   84.63542 | 0.07549 | -0.00044 |    0.02541 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           2631 |    0.00000 |    0.00000 |  0.00000 | 58.95996 |  0.00000 |   92.86296 | 0.05307 |  0.00136 |    0.01236 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            246 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   61.56250 | 0.46204 | -0.08598 |    0.24961 |
| 22 | Total sparsity:                     | -              |        270896 |          45178 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   83.32275 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:50:06,067 - Total sparsity: 83.32

2018-10-27 22:50:06,067 - --- validate (epoch=20)-----------
2018-10-27 22:50:06,067 - 10000 samples (128 per mini-batch)
2018-10-27 22:50:06,787 - Epoch: [20][   50/   78]    Loss 0.694675    Top1 77.796875    Top5 98.312500    
2018-10-27 22:50:07,178 - ==> Top1: 77.940    Top5: 98.480    Loss: 0.689

2018-10-27 22:50:07,179 - ==> Best Top1: 79.700   On Epoch: 15

2018-10-27 22:50:07,179 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:50:07,190 - 

2018-10-27 22:50:07,190 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:50:08,380 - Epoch: [21][   50/  391]    Overall Loss 0.496369    Objective Loss 0.496369    Top1 82.687500    Top5 99.296875    LR 0.300000    Time 0.023759    
2018-10-27 22:50:09,515 - Epoch: [21][  100/  391]    Overall Loss 0.499273    Objective Loss 0.499273    Top1 82.796875    Top5 99.265625    LR 0.300000    Time 0.023214    
2018-10-27 22:50:10,646 - Epoch: [21][  150/  391]    Overall Loss 0.505859    Objective Loss 0.505859    Top1 82.703125    Top5 99.234375    LR 0.300000    Time 0.023012    
2018-10-27 22:50:11,779 - Epoch: [21][  200/  391]    Overall Loss 0.505529    Objective Loss 0.505529    Top1 82.613281    Top5 99.230469    LR 0.300000    Time 0.022917    
2018-10-27 22:50:12,913 - Epoch: [21][  250/  391]    Overall Loss 0.500573    Objective Loss 0.500573    Top1 82.740625    Top5 99.234375    LR 0.300000    Time 0.022849    
2018-10-27 22:50:14,047 - Epoch: [21][  300/  391]    Overall Loss 0.502662    Objective Loss 0.502662    Top1 82.653646    Top5 99.223958    LR 0.300000    Time 0.022815    
2018-10-27 22:50:15,180 - Epoch: [21][  350/  391]    Overall Loss 0.502576    Objective Loss 0.502576    Top1 82.647321    Top5 99.212054    LR 0.300000    Time 0.022790    
2018-10-27 22:50:16,188 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            114 |    0.00000 |    0.00000 |  0.00000 | 14.58333 |  6.25000 |   73.61111 | 0.51125 | -0.00256 |    0.20946 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            232 |    0.00000 |    0.00000 | 18.75000 | 48.04688 |  0.00000 |   89.93056 | 0.16360 | -0.00586 |    0.04319 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            266 |    0.00000 |    0.00000 |  0.00000 | 41.01562 |  0.00000 |   88.45486 | 0.16191 |  0.00194 |    0.04805 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            286 |    0.00000 |    0.00000 |  0.00000 | 34.76562 |  0.00000 |   87.58681 | 0.16241 | -0.00552 |    0.04778 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            206 |    0.00000 |    0.00000 |  0.00000 | 50.78125 |  0.00000 |   91.05903 | 0.13097 | -0.00274 |    0.03311 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            409 |    0.00000 |    0.00000 |  0.00000 | 24.60938 |  0.00000 |   82.24826 | 0.16297 | -0.00893 |    0.05751 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            283 |    0.00000 |    0.00000 |  0.00000 | 41.01562 |  0.00000 |   87.71701 | 0.13161 | -0.00317 |    0.03884 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1257 |    0.00000 |    0.00000 |  0.00000 | 11.32812 |  0.00000 |   72.72135 | 0.17578 | -0.00824 |    0.07594 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1895 |    0.00000 |    0.00000 |  0.00000 | 15.52734 |  0.00000 |   79.43793 | 0.13701 | -0.00476 |    0.05260 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             93 |    0.00000 |    0.00000 |  0.00000 | 81.83594 |  3.12500 |   81.83594 | 0.22965 | -0.00246 |    0.09045 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1488 |    0.00000 |    0.00000 |  0.00000 | 24.90234 |  0.00000 |   83.85417 | 0.11777 | -0.00293 |    0.04078 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1153 |    0.00000 |    0.00000 |  0.00000 | 36.52344 |  0.00000 |   87.48915 | 0.09985 | -0.00284 |    0.03071 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1680 |    0.00000 |    0.00000 |  0.00000 | 22.85156 |  0.00000 |   81.77083 | 0.12570 | -0.00431 |    0.04619 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           1207 |    0.00000 |    0.00000 |  0.00000 | 34.27734 |  0.00000 |   86.90321 | 0.09841 | -0.00579 |    0.03131 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           4037 |    0.00000 |    0.00000 |  0.00000 | 15.23438 |  0.00000 |   78.09787 | 0.12039 | -0.00223 |    0.04851 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           9002 |    0.00000 |    0.00000 |  0.00000 |  9.59473 |  0.00000 |   75.58051 | 0.11224 | -0.00657 |    0.04702 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            257 |    0.00000 |    0.00000 |  3.12500 | 87.45117 |  0.00000 |   87.45117 | 0.13382 | -0.00298 |    0.04276 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           7482 |    0.00000 |    0.00000 |  0.00000 | 16.33301 |  0.00000 |   79.70378 | 0.10296 | -0.00384 |    0.03934 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           5290 |    0.00000 |    0.00000 |  0.00000 | 28.93066 |  0.00000 |   85.64996 | 0.08348 | -0.00393 |    0.02709 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5664 |    0.00000 |    0.00000 |  0.00000 | 29.29688 |  0.00000 |   84.63542 | 0.07598 | -0.00018 |    0.02539 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           2631 |    0.00000 |    0.00000 |  0.00000 | 58.95996 |  0.00000 |   92.86296 | 0.05316 |  0.00137 |    0.01226 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            246 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   61.56250 | 0.45806 | -0.08457 |    0.24631 |
| 22 | Total sparsity:                     | -              |        270896 |          45178 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   83.32275 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:50:16,189 - Total sparsity: 83.32

2018-10-27 22:50:16,189 - --- validate (epoch=21)-----------
2018-10-27 22:50:16,189 - 10000 samples (128 per mini-batch)
2018-10-27 22:50:16,909 - Epoch: [21][   50/   78]    Loss 0.890193    Top1 72.593750    Top5 97.171875    
2018-10-27 22:50:17,301 - ==> Top1: 72.730    Top5: 97.180    Loss: 0.885

2018-10-27 22:50:17,302 - ==> Best Top1: 79.700   On Epoch: 15

2018-10-27 22:50:17,302 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:50:17,312 - 

2018-10-27 22:50:17,314 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:50:18,526 - Epoch: [22][   50/  391]    Overall Loss 0.489399    Objective Loss 0.489399    Top1 82.593750    Top5 99.312500    LR 0.300000    Time 0.024221    
2018-10-27 22:50:19,684 - Epoch: [22][  100/  391]    Overall Loss 0.489521    Objective Loss 0.489521    Top1 83.085938    Top5 99.296875    LR 0.300000    Time 0.023670    
2018-10-27 22:50:20,842 - Epoch: [22][  150/  391]    Overall Loss 0.492779    Objective Loss 0.492779    Top1 83.093750    Top5 99.239583    LR 0.300000    Time 0.023491    
2018-10-27 22:50:21,999 - Epoch: [22][  200/  391]    Overall Loss 0.491104    Objective Loss 0.491104    Top1 83.019531    Top5 99.269531    LR 0.300000    Time 0.023399    
2018-10-27 22:50:23,155 - Epoch: [22][  250/  391]    Overall Loss 0.489555    Objective Loss 0.489555    Top1 83.153125    Top5 99.237500    LR 0.300000    Time 0.023338    
2018-10-27 22:50:24,312 - Epoch: [22][  300/  391]    Overall Loss 0.492370    Objective Loss 0.492370    Top1 83.015625    Top5 99.242188    LR 0.300000    Time 0.023301    
2018-10-27 22:50:25,469 - Epoch: [22][  350/  391]    Overall Loss 0.492803    Objective Loss 0.492803    Top1 83.020089    Top5 99.212054    LR 0.300000    Time 0.023275    
2018-10-27 22:50:26,500 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            105 |    0.00000 |    0.00000 |  0.00000 | 18.75000 | 12.50000 |   75.69444 | 0.50727 | -0.01148 |    0.20620 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            223 |    0.00000 |    0.00000 | 18.75000 | 50.00000 |  0.00000 |   90.32118 | 0.16357 | -0.00673 |    0.04273 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            252 |    0.00000 |    0.00000 |  0.00000 | 43.35938 |  0.00000 |   89.06250 | 0.16114 |  0.00219 |    0.04742 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            265 |    0.00000 |    0.00000 |  0.00000 | 39.45312 |  0.00000 |   88.49826 | 0.16030 | -0.00586 |    0.04578 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            188 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.84028 | 0.12990 | -0.00237 |    0.03194 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            386 |    0.00000 |    0.00000 |  0.00000 | 28.51562 |  0.00000 |   83.24653 | 0.16286 | -0.00776 |    0.05568 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            260 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.71528 | 0.13078 | -0.00222 |    0.03762 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1199 |    0.00000 |    0.00000 |  0.00000 | 13.28125 |  0.00000 |   73.98003 | 0.17487 | -0.00689 |    0.07428 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1796 |    0.00000 |    0.00000 |  0.00000 | 17.18750 |  0.00000 |   80.51215 | 0.13639 | -0.00411 |    0.05151 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             91 |    0.00000 |    0.00000 |  0.00000 | 82.22656 |  6.25000 |   82.22656 | 0.22819 | -0.00277 |    0.08883 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1400 |    0.00000 |    0.00000 |  0.00000 | 27.73438 |  0.00000 |   84.80903 | 0.11700 | -0.00357 |    0.03985 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1077 |    0.00000 |    0.00000 |  0.00000 | 39.16016 |  0.00000 |   88.31380 | 0.09941 | -0.00217 |    0.02984 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1602 |    0.00000 |    0.00000 |  0.00000 | 24.80469 |  0.00000 |   82.61719 | 0.12542 | -0.00393 |    0.04524 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           1128 |    0.00000 |    0.00000 |  0.00000 | 37.10938 |  0.00000 |   87.76042 | 0.09825 | -0.00587 |    0.03055 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3824 |    0.00000 |    0.00000 |  0.00000 | 17.33398 |  0.00000 |   79.25347 | 0.11999 | -0.00235 |    0.04750 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           8520 |    0.00000 |    0.00000 |  0.00000 | 11.42578 |  0.00000 |   76.88802 | 0.11191 | -0.00619 |    0.04618 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            242 |    0.00000 |    0.00000 |  3.12500 | 88.18359 |  0.00000 |   88.18359 | 0.13172 | -0.00170 |    0.04104 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           7064 |    0.00000 |    0.00000 |  0.00000 | 18.45703 |  0.00000 |   80.83767 | 0.10261 | -0.00415 |    0.03844 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           4974 |    0.00000 |    0.00000 |  0.00000 | 31.34766 |  0.00000 |   86.50716 | 0.08308 | -0.00385 |    0.02629 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5321 |    0.00000 |    0.00000 |  0.00000 | 31.76270 |  0.00000 |   85.56586 | 0.07580 | -0.00029 |    0.02486 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           2468 |    0.00000 |    0.00000 |  0.00000 | 61.27930 |  0.00000 |   93.30512 | 0.05286 |  0.00139 |    0.01195 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            244 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   61.87500 | 0.45943 | -0.08298 |    0.24555 |
| 22 | Total sparsity:                     | -              |        270896 |          42629 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   84.26370 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:50:26,500 - Total sparsity: 84.26

2018-10-27 22:50:26,500 - --- validate (epoch=22)-----------
2018-10-27 22:50:26,501 - 10000 samples (128 per mini-batch)
2018-10-27 22:50:27,227 - Epoch: [22][   50/   78]    Loss 0.629738    Top1 78.937500    Top5 98.359375    
2018-10-27 22:50:27,621 - ==> Top1: 79.220    Top5: 98.390    Loss: 0.623

2018-10-27 22:50:27,622 - ==> Best Top1: 79.700   On Epoch: 15

2018-10-27 22:50:27,622 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:50:27,638 - 

2018-10-27 22:50:27,638 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:50:28,801 - Epoch: [23][   50/  391]    Overall Loss 0.471510    Objective Loss 0.471510    Top1 83.453125    Top5 99.234375    LR 0.300000    Time 0.023224    
2018-10-27 22:50:29,933 - Epoch: [23][  100/  391]    Overall Loss 0.475893    Objective Loss 0.475893    Top1 83.484375    Top5 99.171875    LR 0.300000    Time 0.022924    
2018-10-27 22:50:31,065 - Epoch: [23][  150/  391]    Overall Loss 0.481209    Objective Loss 0.481209    Top1 83.223958    Top5 99.265625    LR 0.300000    Time 0.022820    
2018-10-27 22:50:32,198 - Epoch: [23][  200/  391]    Overall Loss 0.484749    Objective Loss 0.484749    Top1 83.113281    Top5 99.292969    LR 0.300000    Time 0.022770    
2018-10-27 22:50:33,331 - Epoch: [23][  250/  391]    Overall Loss 0.487137    Objective Loss 0.487137    Top1 83.056250    Top5 99.253125    LR 0.300000    Time 0.022743    
2018-10-27 22:50:34,464 - Epoch: [23][  300/  391]    Overall Loss 0.484893    Objective Loss 0.484893    Top1 83.044271    Top5 99.286458    LR 0.300000    Time 0.022726    
2018-10-27 22:50:35,596 - Epoch: [23][  350/  391]    Overall Loss 0.485769    Objective Loss 0.485769    Top1 83.071429    Top5 99.292411    LR 0.300000    Time 0.022700    
2018-10-27 22:50:36,602 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            105 |    0.00000 |    0.00000 |  0.00000 | 18.75000 | 12.50000 |   75.69444 | 0.50513 | -0.00760 |    0.20462 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            223 |    0.00000 |    0.00000 | 18.75000 | 50.00000 |  0.00000 |   90.32118 | 0.16332 | -0.00657 |    0.04232 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            252 |    0.00000 |    0.00000 |  0.00000 | 43.35938 |  0.00000 |   89.06250 | 0.16014 |  0.00128 |    0.04715 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            265 |    0.00000 |    0.00000 |  0.00000 | 39.45312 |  0.00000 |   88.49826 | 0.15955 | -0.00750 |    0.04470 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            188 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.84028 | 0.12965 | -0.00226 |    0.03214 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            386 |    0.00000 |    0.00000 |  0.00000 | 28.51562 |  0.00000 |   83.24653 | 0.16150 | -0.00839 |    0.05450 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            260 |    0.00000 |    0.00000 |  0.00000 | 43.75000 |  0.00000 |   88.71528 | 0.12953 | -0.00176 |    0.03659 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1199 |    0.00000 |    0.00000 |  0.00000 | 13.28125 |  0.00000 |   73.98003 | 0.17487 | -0.00557 |    0.07383 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1796 |    0.00000 |    0.00000 |  0.00000 | 17.18750 |  0.00000 |   80.51215 | 0.13602 | -0.00453 |    0.05098 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             91 |    0.00000 |    0.00000 |  0.00000 | 82.22656 |  6.25000 |   82.22656 | 0.22570 | -0.00409 |    0.08791 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1400 |    0.00000 |    0.00000 |  0.00000 | 27.73438 |  0.00000 |   84.80903 | 0.11637 | -0.00295 |    0.03914 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1077 |    0.00000 |    0.00000 |  0.00000 | 39.16016 |  0.00000 |   88.31380 | 0.09866 | -0.00242 |    0.02932 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1602 |    0.00000 |    0.00000 |  0.00000 | 24.80469 |  0.00000 |   82.61719 | 0.12515 | -0.00415 |    0.04481 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           1128 |    0.00000 |    0.00000 |  0.00000 | 37.10938 |  0.00000 |   87.76042 | 0.09808 | -0.00589 |    0.03027 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3824 |    0.00000 |    0.00000 |  0.00000 | 17.33398 |  0.00000 |   79.25347 | 0.12017 | -0.00244 |    0.04735 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           8520 |    0.00000 |    0.00000 |  0.00000 | 11.42578 |  0.00000 |   76.88802 | 0.11196 | -0.00624 |    0.04585 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            242 |    0.00000 |    0.00000 |  3.12500 | 88.18359 |  0.00000 |   88.18359 | 0.12983 | -0.00224 |    0.04058 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           7064 |    0.00000 |    0.00000 |  0.00000 | 18.45703 |  0.00000 |   80.83767 | 0.10258 | -0.00458 |    0.03817 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           4974 |    0.00000 |    0.00000 |  0.00000 | 31.34766 |  0.00000 |   86.50716 | 0.08282 | -0.00353 |    0.02608 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           5321 |    0.00000 |    0.00000 |  0.00000 | 31.76270 |  0.00000 |   85.56586 | 0.07597 | -0.00019 |    0.02465 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           2468 |    0.00000 |    0.00000 |  0.00000 | 61.27930 |  0.00000 |   93.30512 | 0.05265 |  0.00164 |    0.01179 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            244 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   61.87500 | 0.46094 | -0.08356 |    0.24690 |
| 22 | Total sparsity:                     | -              |        270896 |          42629 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   84.26370 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:50:36,602 - Total sparsity: 84.26

2018-10-27 22:50:36,603 - --- validate (epoch=23)-----------
2018-10-27 22:50:36,603 - 10000 samples (128 per mini-batch)
2018-10-27 22:50:37,323 - Epoch: [23][   50/   78]    Loss 0.753067    Top1 76.593750    Top5 98.343750    
2018-10-27 22:50:37,702 - ==> Top1: 76.660    Top5: 98.510    Loss: 0.753

2018-10-27 22:50:37,703 - ==> Best Top1: 79.700   On Epoch: 15

2018-10-27 22:50:37,703 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:50:37,720 - 

2018-10-27 22:50:37,722 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:50:38,909 - Epoch: [24][   50/  391]    Overall Loss 0.503963    Objective Loss 0.503963    Top1 82.484375    Top5 99.234375    LR 0.300000    Time 0.023713    
2018-10-27 22:50:40,066 - Epoch: [24][  100/  391]    Overall Loss 0.495915    Objective Loss 0.495915    Top1 82.835938    Top5 99.265625    LR 0.300000    Time 0.023407    
2018-10-27 22:50:41,226 - Epoch: [24][  150/  391]    Overall Loss 0.488567    Objective Loss 0.488567    Top1 83.109375    Top5 99.276042    LR 0.300000    Time 0.023334    
2018-10-27 22:50:42,387 - Epoch: [24][  200/  391]    Overall Loss 0.484166    Objective Loss 0.484166    Top1 83.363281    Top5 99.250000    LR 0.300000    Time 0.023295    
2018-10-27 22:50:43,545 - Epoch: [24][  250/  391]    Overall Loss 0.484973    Objective Loss 0.484973    Top1 83.321875    Top5 99.250000    LR 0.300000    Time 0.023266    
2018-10-27 22:50:44,704 - Epoch: [24][  300/  391]    Overall Loss 0.486111    Objective Loss 0.486111    Top1 83.244792    Top5 99.252604    LR 0.300000    Time 0.023246    
2018-10-27 22:50:45,867 - Epoch: [24][  350/  391]    Overall Loss 0.484720    Objective Loss 0.484720    Top1 83.234375    Top5 99.258929    LR 0.300000    Time 0.023244    
2018-10-27 22:50:46,895 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            102 |    0.00000 |    0.00000 |  0.00000 | 20.83333 | 18.75000 |   76.38889 | 0.50232 | -0.00903 |    0.20271 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            212 |    0.00000 |    0.00000 | 18.75000 | 52.34375 |  0.00000 |   90.79861 | 0.16366 | -0.00666 |    0.04124 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            245 |    0.00000 |    0.00000 |  0.00000 | 44.92188 |  0.00000 |   89.36632 | 0.15978 |  0.00106 |    0.04607 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            242 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   89.49653 | 0.15889 | -0.00607 |    0.04314 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            175 |    0.00000 |    0.00000 |  0.00000 | 54.29688 |  0.00000 |   92.40451 | 0.12907 | -0.00308 |    0.03098 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            360 |    0.00000 |    0.00000 |  0.00000 | 31.25000 |  0.00000 |   84.37500 | 0.16079 | -0.00847 |    0.05283 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            244 |    0.00000 |    0.00000 |  0.00000 | 47.65625 |  0.00000 |   89.40972 | 0.12898 | -0.00210 |    0.03574 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1146 |    0.00000 |    0.00000 |  0.00000 | 15.62500 |  0.00000 |   75.13021 | 0.17459 | -0.00651 |    0.07276 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1702 |    0.00000 |    0.00000 |  0.00000 | 19.53125 |  0.00000 |   81.53212 | 0.13571 | -0.00435 |    0.04985 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             90 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  6.25000 |   82.42188 | 0.22508 | -0.00415 |    0.08700 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1304 |    0.00000 |    0.00000 |  0.00000 | 30.95703 |  0.00000 |   85.85069 | 0.11604 | -0.00279 |    0.03836 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1014 |    0.00000 |    0.00000 |  0.00000 | 41.30859 |  0.00000 |   88.99740 | 0.09837 | -0.00221 |    0.02857 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1516 |    0.00000 |    0.00000 |  0.00000 | 27.14844 |  0.00000 |   83.55035 | 0.12490 | -0.00393 |    0.04380 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           1059 |    0.00000 |    0.00000 |  0.00000 | 39.55078 |  0.00000 |   88.50911 | 0.09824 | -0.00590 |    0.02945 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3641 |    0.00000 |    0.00000 |  0.00000 | 19.62891 |  0.00000 |   80.24631 | 0.11999 | -0.00293 |    0.04643 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           8128 |    0.00000 |    0.00000 |  0.00000 | 12.62207 |  0.00000 |   77.95139 | 0.11185 | -0.00621 |    0.04517 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            229 |    0.00000 |    0.00000 |  3.12500 | 88.81836 |  0.00000 |   88.81836 | 0.12813 | -0.00161 |    0.03905 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           6676 |    0.00000 |    0.00000 |  0.00000 | 20.23926 |  0.00000 |   81.89019 | 0.10247 | -0.00422 |    0.03742 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           4661 |    0.00000 |    0.00000 |  0.00000 | 34.39941 |  0.00000 |   87.35623 | 0.08274 | -0.00382 |    0.02562 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           4998 |    0.00000 |    0.00000 |  0.00000 | 34.37500 |  0.00000 |   86.44206 | 0.07616 | -0.00029 |    0.02426 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           2323 |    0.00000 |    0.00000 |  0.00000 | 63.30566 |  0.00000 |   93.69846 | 0.05252 |  0.00163 |    0.01153 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            242 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   62.18750 | 0.46200 | -0.08262 |    0.24631 |
| 22 | Total sparsity:                     | -              |        270896 |          40309 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   85.12012 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:50:46,896 - Total sparsity: 85.12

2018-10-27 22:50:46,896 - --- validate (epoch=24)-----------
2018-10-27 22:50:46,896 - 10000 samples (128 per mini-batch)
2018-10-27 22:50:47,617 - Epoch: [24][   50/   78]    Loss 0.800052    Top1 75.703125    Top5 98.140625    
2018-10-27 22:50:48,009 - ==> Top1: 75.370    Top5: 98.170    Loss: 0.814

2018-10-27 22:50:48,009 - ==> Best Top1: 79.700   On Epoch: 15

2018-10-27 22:50:48,010 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:50:48,020 - 

2018-10-27 22:50:48,021 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:50:49,213 - Epoch: [25][   50/  391]    Overall Loss 0.447930    Objective Loss 0.447930    Top1 84.406250    Top5 99.265625    LR 0.300000    Time 0.023816    
2018-10-27 22:50:50,347 - Epoch: [25][  100/  391]    Overall Loss 0.462296    Objective Loss 0.462296    Top1 84.023438    Top5 99.281250    LR 0.300000    Time 0.023230    
2018-10-27 22:50:51,481 - Epoch: [25][  150/  391]    Overall Loss 0.478484    Objective Loss 0.478484    Top1 83.421875    Top5 99.276042    LR 0.300000    Time 0.023041    
2018-10-27 22:50:52,616 - Epoch: [25][  200/  391]    Overall Loss 0.481060    Objective Loss 0.481060    Top1 83.332031    Top5 99.289062    LR 0.300000    Time 0.022929    
2018-10-27 22:50:53,749 - Epoch: [25][  250/  391]    Overall Loss 0.479860    Objective Loss 0.479860    Top1 83.350000    Top5 99.287500    LR 0.300000    Time 0.022870    
2018-10-27 22:50:54,883 - Epoch: [25][  300/  391]    Overall Loss 0.480548    Objective Loss 0.480548    Top1 83.317708    Top5 99.286458    LR 0.300000    Time 0.022836    
2018-10-27 22:50:56,018 - Epoch: [25][  350/  391]    Overall Loss 0.482983    Objective Loss 0.482983    Top1 83.272321    Top5 99.265625    LR 0.300000    Time 0.022812    
2018-10-27 22:50:57,025 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            102 |    0.00000 |    0.00000 |  0.00000 | 20.83333 | 18.75000 |   76.38889 | 0.50951 | -0.00771 |    0.20542 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            212 |    0.00000 |    0.00000 | 18.75000 | 52.34375 |  0.00000 |   90.79861 | 0.16474 | -0.00536 |    0.04149 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            245 |    0.00000 |    0.00000 |  0.00000 | 44.92188 |  0.00000 |   89.36632 | 0.16042 |  0.00091 |    0.04594 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            242 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   89.49653 | 0.15764 | -0.00623 |    0.04341 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            175 |    0.00000 |    0.00000 |  0.00000 | 54.29688 |  0.00000 |   92.40451 | 0.12905 | -0.00194 |    0.03150 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            360 |    0.00000 |    0.00000 |  0.00000 | 31.25000 |  0.00000 |   84.37500 | 0.16022 | -0.00785 |    0.05249 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            244 |    0.00000 |    0.00000 |  0.00000 | 47.65625 |  0.00000 |   89.40972 | 0.12805 | -0.00250 |    0.03481 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1146 |    0.00000 |    0.00000 |  0.00000 | 15.62500 |  0.00000 |   75.13021 | 0.17399 | -0.00735 |    0.07220 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1702 |    0.00000 |    0.00000 |  0.00000 | 19.53125 |  0.00000 |   81.53212 | 0.13573 | -0.00393 |    0.04933 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             90 |    0.00000 |    0.00000 |  0.00000 | 82.42188 |  6.25000 |   82.42188 | 0.22440 | -0.00280 |    0.08590 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1304 |    0.00000 |    0.00000 |  0.00000 | 30.95703 |  0.00000 |   85.85069 | 0.11568 | -0.00327 |    0.03790 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |           1014 |    0.00000 |    0.00000 |  0.00000 | 41.30859 |  0.00000 |   88.99740 | 0.09803 | -0.00189 |    0.02826 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1516 |    0.00000 |    0.00000 |  0.00000 | 27.14844 |  0.00000 |   83.55035 | 0.12487 | -0.00385 |    0.04323 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |           1059 |    0.00000 |    0.00000 |  0.00000 | 39.55078 |  0.00000 |   88.50911 | 0.09848 | -0.00642 |    0.02943 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3641 |    0.00000 |    0.00000 |  0.00000 | 19.62891 |  0.00000 |   80.24631 | 0.12002 | -0.00241 |    0.04632 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           8128 |    0.00000 |    0.00000 |  0.00000 | 12.62207 |  0.00000 |   77.95139 | 0.11185 | -0.00597 |    0.04483 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            229 |    0.00000 |    0.00000 |  3.12500 | 88.81836 |  0.00000 |   88.81836 | 0.12826 | -0.00130 |    0.03887 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           6676 |    0.00000 |    0.00000 |  0.00000 | 20.23926 |  0.00000 |   81.89019 | 0.10234 | -0.00408 |    0.03730 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           4661 |    0.00000 |    0.00000 |  0.00000 | 34.39941 |  0.00000 |   87.35623 | 0.08279 | -0.00362 |    0.02543 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           4998 |    0.00000 |    0.00000 |  0.00000 | 34.37500 |  0.00000 |   86.44206 | 0.07638 | -0.00037 |    0.02416 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           2323 |    0.00000 |    0.00000 |  0.00000 | 63.30566 |  0.00000 |   93.69846 | 0.05247 |  0.00162 |    0.01145 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            242 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   62.18750 | 0.45787 | -0.08293 |    0.24296 |
| 22 | Total sparsity:                     | -              |        270896 |          40309 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   85.12012 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:50:57,026 - Total sparsity: 85.12

2018-10-27 22:50:57,026 - --- validate (epoch=25)-----------
2018-10-27 22:50:57,026 - 10000 samples (128 per mini-batch)
2018-10-27 22:50:57,749 - Epoch: [25][   50/   78]    Loss 0.589311    Top1 80.218750    Top5 98.843750    
2018-10-27 22:50:58,143 - ==> Top1: 80.060    Top5: 98.990    Loss: 0.588

2018-10-27 22:50:58,143 - ==> Best Top1: 80.060   On Epoch: 25

2018-10-27 22:50:58,143 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:50:58,160 - 

2018-10-27 22:50:58,162 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:50:59,351 - Epoch: [26][   50/  391]    Overall Loss 0.483335    Objective Loss 0.483335    Top1 82.984375    Top5 99.281250    LR 0.300000    Time 0.023758    
2018-10-27 22:51:00,509 - Epoch: [26][  100/  391]    Overall Loss 0.477509    Objective Loss 0.477509    Top1 83.226562    Top5 99.304688    LR 0.300000    Time 0.023447    
2018-10-27 22:51:01,675 - Epoch: [26][  150/  391]    Overall Loss 0.483087    Objective Loss 0.483087    Top1 83.072917    Top5 99.213542    LR 0.300000    Time 0.023395    
2018-10-27 22:51:02,850 - Epoch: [26][  200/  391]    Overall Loss 0.478160    Objective Loss 0.478160    Top1 83.304688    Top5 99.222656    LR 0.300000    Time 0.023416    
2018-10-27 22:51:04,026 - Epoch: [26][  250/  391]    Overall Loss 0.481480    Objective Loss 0.481480    Top1 83.200000    Top5 99.246875    LR 0.300000    Time 0.023428    
2018-10-27 22:51:05,200 - Epoch: [26][  300/  391]    Overall Loss 0.483014    Objective Loss 0.483014    Top1 83.174479    Top5 99.257812    LR 0.300000    Time 0.023434    
2018-10-27 22:51:06,375 - Epoch: [26][  350/  391]    Overall Loss 0.482655    Objective Loss 0.482655    Top1 83.229911    Top5 99.267857    LR 0.300000    Time 0.023439    
2018-10-27 22:51:07,421 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            102 |    0.00000 |    0.00000 |  0.00000 | 20.83333 | 18.75000 |   76.38889 | 0.51382 | -0.00513 |    0.20825 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            202 |    0.00000 |    0.00000 | 18.75000 | 53.51562 |  0.00000 |   91.23264 | 0.16419 | -0.00483 |    0.04062 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            233 |    0.00000 |    0.00000 |  0.00000 | 45.70312 |  0.00000 |   89.88715 | 0.16052 |  0.00109 |    0.04527 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            231 |    0.00000 |    0.00000 |  0.00000 | 45.31250 |  0.00000 |   89.97396 | 0.15703 | -0.00564 |    0.04224 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            170 |    0.00000 |    0.00000 |  0.00000 | 55.46875 |  0.00000 |   92.62153 | 0.12839 | -0.00216 |    0.03078 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            338 |    0.00000 |    0.00000 |  0.00000 | 33.59375 |  0.00000 |   85.32986 | 0.15924 | -0.00886 |    0.05153 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            226 |    0.00000 |    0.00000 |  0.00000 | 50.78125 |  0.00000 |   90.19097 | 0.12749 | -0.00208 |    0.03418 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1085 |    0.00000 |    0.00000 |  0.00000 | 18.16406 |  0.00000 |   76.45399 | 0.17331 | -0.00677 |    0.07093 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1617 |    0.00000 |    0.00000 |  0.00000 | 21.58203 |  0.00000 |   82.45443 | 0.13565 | -0.00446 |    0.04903 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             85 |    0.00000 |    0.00000 |  0.00000 | 83.39844 |  9.37500 |   83.39844 | 0.22231 | -0.00382 |    0.08267 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1241 |    0.00000 |    0.00000 |  0.00000 | 32.91016 |  0.00000 |   86.53429 | 0.11496 | -0.00310 |    0.03713 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            945 |    0.00000 |    0.00000 |  0.00000 | 43.84766 |  0.00000 |   89.74609 | 0.09708 | -0.00225 |    0.02743 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1430 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   84.48351 | 0.12433 | -0.00383 |    0.04237 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            989 |    0.00000 |    0.00000 |  0.00000 | 42.67578 |  0.00000 |   89.26866 | 0.09773 | -0.00661 |    0.02867 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3481 |    0.00000 |    0.00000 |  0.00000 | 21.33789 |  0.00000 |   81.11437 | 0.11956 | -0.00261 |    0.04548 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           7761 |    0.00000 |    0.00000 |  0.00000 | 13.79395 |  0.00000 |   78.94694 | 0.11129 | -0.00589 |    0.04397 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            215 |    0.00000 |    0.00000 |  3.12500 | 89.50195 |  1.56250 |   89.50195 | 0.12715 | -0.00133 |    0.03766 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           6322 |    0.00000 |    0.00000 |  0.00000 | 22.02148 |  0.00000 |   82.85048 | 0.10190 | -0.00415 |    0.03656 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           4410 |    0.00000 |    0.00000 |  0.00000 | 36.98730 |  0.00000 |   88.03711 | 0.08226 | -0.00335 |    0.02476 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           4737 |    0.00000 |    0.00000 |  0.00000 | 36.42578 |  0.00000 |   87.15007 | 0.07596 | -0.00022 |    0.02368 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           2217 |    0.00000 |    0.00000 |  0.00000 | 64.94141 |  0.00000 |   93.98600 | 0.05215 |  0.00161 |    0.01114 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            240 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   62.50000 | 0.46097 | -0.08261 |    0.24427 |
| 22 | Total sparsity:                     | -              |        270896 |          38277 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   85.87022 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:51:07,421 - Total sparsity: 85.87

2018-10-27 22:51:07,421 - --- validate (epoch=26)-----------
2018-10-27 22:51:07,421 - 10000 samples (128 per mini-batch)
2018-10-27 22:51:08,135 - Epoch: [26][   50/   78]    Loss 0.633535    Top1 78.953125    Top5 98.859375    
2018-10-27 22:51:08,522 - ==> Top1: 79.040    Top5: 98.900    Loss: 0.630

2018-10-27 22:51:08,523 - ==> Best Top1: 80.060   On Epoch: 25

2018-10-27 22:51:08,523 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:51:08,533 - 

2018-10-27 22:51:08,534 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:51:09,744 - Epoch: [27][   50/  391]    Overall Loss 0.491333    Objective Loss 0.491333    Top1 82.859375    Top5 99.140625    LR 0.300000    Time 0.024159    
2018-10-27 22:51:10,892 - Epoch: [27][  100/  391]    Overall Loss 0.504134    Objective Loss 0.504134    Top1 82.531250    Top5 99.171875    LR 0.300000    Time 0.023553    
2018-10-27 22:51:12,043 - Epoch: [27][  150/  391]    Overall Loss 0.496322    Objective Loss 0.496322    Top1 82.708333    Top5 99.213542    LR 0.300000    Time 0.023362    
2018-10-27 22:51:13,190 - Epoch: [27][  200/  391]    Overall Loss 0.492338    Objective Loss 0.492338    Top1 82.855469    Top5 99.253906    LR 0.300000    Time 0.023253    
2018-10-27 22:51:14,339 - Epoch: [27][  250/  391]    Overall Loss 0.488634    Objective Loss 0.488634    Top1 82.987500    Top5 99.240625    LR 0.300000    Time 0.023192    
2018-10-27 22:51:15,488 - Epoch: [27][  300/  391]    Overall Loss 0.484581    Objective Loss 0.484581    Top1 83.059896    Top5 99.260417    LR 0.300000    Time 0.023152    
2018-10-27 22:51:16,636 - Epoch: [27][  350/  391]    Overall Loss 0.484320    Objective Loss 0.484320    Top1 82.988839    Top5 99.241071    LR 0.300000    Time 0.023119    
2018-10-27 22:51:17,657 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            102 |    0.00000 |    0.00000 |  0.00000 | 20.83333 | 18.75000 |   76.38889 | 0.51442 | -0.00907 |    0.20730 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            202 |    0.00000 |    0.00000 | 18.75000 | 53.51562 |  0.00000 |   91.23264 | 0.16338 | -0.00420 |    0.04010 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            233 |    0.00000 |    0.00000 |  0.00000 | 45.70312 |  0.00000 |   89.88715 | 0.16035 |  0.00057 |    0.04489 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            231 |    0.00000 |    0.00000 |  0.00000 | 45.31250 |  0.00000 |   89.97396 | 0.15791 | -0.00681 |    0.04229 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            170 |    0.00000 |    0.00000 |  0.00000 | 55.46875 |  0.00000 |   92.62153 | 0.12932 | -0.00130 |    0.03024 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            338 |    0.00000 |    0.00000 |  0.00000 | 33.59375 |  0.00000 |   85.32986 | 0.15939 | -0.00832 |    0.05167 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            226 |    0.00000 |    0.00000 |  0.00000 | 50.78125 |  0.00000 |   90.19097 | 0.12834 | -0.00232 |    0.03379 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1085 |    0.00000 |    0.00000 |  0.00000 | 18.16406 |  0.00000 |   76.45399 | 0.17334 | -0.00679 |    0.07059 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1617 |    0.00000 |    0.00000 |  0.00000 | 21.58203 |  0.00000 |   82.45443 | 0.13575 | -0.00470 |    0.04856 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             85 |    0.00000 |    0.00000 |  0.00000 | 83.39844 |  9.37500 |   83.39844 | 0.22216 | -0.00471 |    0.08162 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1241 |    0.00000 |    0.00000 |  0.00000 | 32.91016 |  0.00000 |   86.53429 | 0.11489 | -0.00350 |    0.03680 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            945 |    0.00000 |    0.00000 |  0.00000 | 43.84766 |  0.00000 |   89.74609 | 0.09671 | -0.00224 |    0.02710 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1430 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   84.48351 | 0.12453 | -0.00339 |    0.04210 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            989 |    0.00000 |    0.00000 |  0.00000 | 42.67578 |  0.00000 |   89.26866 | 0.09779 | -0.00645 |    0.02844 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3481 |    0.00000 |    0.00000 |  0.00000 | 21.33789 |  0.00000 |   81.11437 | 0.11933 | -0.00249 |    0.04528 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           7761 |    0.00000 |    0.00000 |  0.00000 | 13.79395 |  0.00000 |   78.94694 | 0.11117 | -0.00586 |    0.04361 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            215 |    0.00000 |    0.00000 |  3.12500 | 89.50195 |  1.56250 |   89.50195 | 0.12728 | -0.00089 |    0.03742 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           6322 |    0.00000 |    0.00000 |  0.00000 | 22.02148 |  0.00000 |   82.85048 | 0.10203 | -0.00402 |    0.03640 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           4410 |    0.00000 |    0.00000 |  0.00000 | 36.98730 |  0.00000 |   88.03711 | 0.08259 | -0.00366 |    0.02469 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           4737 |    0.00000 |    0.00000 |  0.00000 | 36.42578 |  0.00000 |   87.15007 | 0.07623 | -0.00014 |    0.02361 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           2217 |    0.00000 |    0.00000 |  0.00000 | 64.94141 |  0.00000 |   93.98600 | 0.05226 |  0.00158 |    0.01113 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            240 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   62.50000 | 0.46554 | -0.08137 |    0.24562 |
| 22 | Total sparsity:                     | -              |        270896 |          38277 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   85.87022 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:51:17,658 - Total sparsity: 85.87

2018-10-27 22:51:17,658 - --- validate (epoch=27)-----------
2018-10-27 22:51:17,658 - 10000 samples (128 per mini-batch)
2018-10-27 22:51:18,383 - Epoch: [27][   50/   78]    Loss 0.771405    Top1 77.093750    Top5 98.390625    
2018-10-27 22:51:18,772 - ==> Top1: 76.570    Top5: 98.410    Loss: 0.775

2018-10-27 22:51:18,773 - ==> Best Top1: 80.060   On Epoch: 25

2018-10-27 22:51:18,773 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:51:18,783 - 

2018-10-27 22:51:18,785 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:51:20,017 - Epoch: [28][   50/  391]    Overall Loss 0.486338    Objective Loss 0.486338    Top1 83.484375    Top5 99.328125    LR 0.300000    Time 0.024604    
2018-10-27 22:51:21,191 - Epoch: [28][  100/  391]    Overall Loss 0.480116    Objective Loss 0.480116    Top1 83.445312    Top5 99.312500    LR 0.300000    Time 0.024034    
2018-10-27 22:51:22,366 - Epoch: [28][  150/  391]    Overall Loss 0.475032    Objective Loss 0.475032    Top1 83.546875    Top5 99.302083    LR 0.300000    Time 0.023844    
2018-10-27 22:51:23,541 - Epoch: [28][  200/  391]    Overall Loss 0.475211    Objective Loss 0.475211    Top1 83.597656    Top5 99.308594    LR 0.300000    Time 0.023753    
2018-10-27 22:51:24,716 - Epoch: [28][  250/  391]    Overall Loss 0.481246    Objective Loss 0.481246    Top1 83.409375    Top5 99.306250    LR 0.300000    Time 0.023696    
2018-10-27 22:51:25,887 - Epoch: [28][  300/  391]    Overall Loss 0.477975    Objective Loss 0.477975    Top1 83.382812    Top5 99.320312    LR 0.300000    Time 0.023634    
2018-10-27 22:51:27,061 - Epoch: [28][  350/  391]    Overall Loss 0.476459    Objective Loss 0.476459    Top1 83.453125    Top5 99.348214    LR 0.300000    Time 0.023609    
2018-10-27 22:51:28,104 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            101 |    0.00000 |    0.00000 |  0.00000 | 20.83333 | 18.75000 |   76.62037 | 0.51331 | -0.00357 |    0.20485 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 | 18.75000 | 55.85938 |  0.00000 |   91.79688 | 0.16405 | -0.00341 |    0.03995 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            220 |    0.00000 |    0.00000 |  0.00000 | 47.26562 |  0.00000 |   90.45139 | 0.15955 |  0.00111 |    0.04379 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            225 |    0.00000 |    0.00000 |  0.00000 | 46.48438 |  0.00000 |   90.23438 | 0.15835 | -0.00655 |    0.04161 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            160 |    0.00000 |    0.00000 |  0.00000 | 57.03125 |  0.00000 |   93.05556 | 0.12940 | -0.00219 |    0.02956 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            325 |    0.00000 |    0.00000 |  0.00000 | 35.93750 |  0.00000 |   85.89410 | 0.16039 | -0.00818 |    0.05115 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            207 |    0.00000 |    0.00000 |  0.00000 | 55.07812 |  0.00000 |   91.01562 | 0.12764 | -0.00224 |    0.03304 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1034 |    0.00000 |    0.00000 |  0.00000 | 21.48438 |  0.00000 |   77.56076 | 0.17286 | -0.00669 |    0.06954 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1535 |    0.00000 |    0.00000 |  0.00000 | 23.24219 |  0.00000 |   83.34418 | 0.13591 | -0.00433 |    0.04785 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             79 |    0.00000 |    0.00000 |  0.00000 | 84.57031 | 12.50000 |   84.57031 | 0.22038 | -0.00418 |    0.07933 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1190 |    0.00000 |    0.00000 |  0.00000 | 35.15625 |  0.00000 |   87.08767 | 0.11473 | -0.00375 |    0.03609 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            886 |    0.00000 |    0.00000 |  0.00000 | 46.09375 |  0.00000 |   90.38628 | 0.09638 | -0.00241 |    0.02656 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1370 |    0.00000 |    0.00000 |  0.00000 | 30.76172 |  0.00000 |   85.13455 | 0.12440 | -0.00360 |    0.04157 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            944 |    0.00000 |    0.00000 |  0.00000 | 44.72656 |  0.00000 |   89.75694 | 0.09741 | -0.00587 |    0.02788 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3354 |    0.00000 |    0.00000 |  0.00000 | 23.09570 |  0.00000 |   81.80339 | 0.11917 | -0.00209 |    0.04427 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           7409 |    0.00000 |    0.00000 |  0.00000 | 15.52734 |  0.00000 |   79.90180 | 0.11079 | -0.00581 |    0.04270 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            205 |    0.00000 |    0.00000 |  3.12500 | 89.99023 |  1.56250 |   89.99023 | 0.12566 | -0.00127 |    0.03625 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           6033 |    0.00000 |    0.00000 |  0.00000 | 23.73047 |  0.00000 |   83.63444 | 0.10150 | -0.00383 |    0.03565 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           4183 |    0.00000 |    0.00000 |  0.00000 | 38.76953 |  0.00000 |   88.65289 | 0.08252 | -0.00372 |    0.02436 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           4502 |    0.00000 |    0.00000 |  0.00000 | 38.37891 |  0.00000 |   87.78754 | 0.07607 |  0.00001 |    0.02312 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           2108 |    0.00000 |    0.00000 |  0.00000 | 66.43066 |  0.00000 |   94.28168 | 0.05214 |  0.00172 |    0.01091 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            239 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   62.65625 | 0.45858 | -0.07994 |    0.24136 |
| 22 | Total sparsity:                     | -              |        270896 |          36498 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   86.52693 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:51:28,104 - Total sparsity: 86.53

2018-10-27 22:51:28,104 - --- validate (epoch=28)-----------
2018-10-27 22:51:28,104 - 10000 samples (128 per mini-batch)
2018-10-27 22:51:28,829 - Epoch: [28][   50/   78]    Loss 0.548320    Top1 81.484375    Top5 99.015625    
2018-10-27 22:51:29,221 - ==> Top1: 81.480    Top5: 99.060    Loss: 0.549

2018-10-27 22:51:29,222 - ==> Best Top1: 81.480   On Epoch: 28

2018-10-27 22:51:29,222 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:51:29,241 - 

2018-10-27 22:51:29,242 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:51:30,419 - Epoch: [29][   50/  391]    Overall Loss 0.488194    Objective Loss 0.488194    Top1 83.078125    Top5 99.250000    LR 0.300000    Time 0.023518    
2018-10-27 22:51:31,568 - Epoch: [29][  100/  391]    Overall Loss 0.483388    Objective Loss 0.483388    Top1 83.171875    Top5 99.281250    LR 0.300000    Time 0.023230    
2018-10-27 22:51:32,713 - Epoch: [29][  150/  391]    Overall Loss 0.469992    Objective Loss 0.469992    Top1 83.765625    Top5 99.291667    LR 0.300000    Time 0.023114    
2018-10-27 22:51:33,860 - Epoch: [29][  200/  391]    Overall Loss 0.471552    Objective Loss 0.471552    Top1 83.582031    Top5 99.269531    LR 0.300000    Time 0.023062    
2018-10-27 22:51:35,010 - Epoch: [29][  250/  391]    Overall Loss 0.475839    Objective Loss 0.475839    Top1 83.437500    Top5 99.293750    LR 0.300000    Time 0.023043    
2018-10-27 22:51:36,160 - Epoch: [29][  300/  391]    Overall Loss 0.477569    Objective Loss 0.477569    Top1 83.398438    Top5 99.255208    LR 0.300000    Time 0.023032    
2018-10-27 22:51:37,309 - Epoch: [29][  350/  391]    Overall Loss 0.477880    Objective Loss 0.477880    Top1 83.392857    Top5 99.254464    LR 0.300000    Time 0.023022    
2018-10-27 22:51:38,332 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |            101 |    0.00000 |    0.00000 |  0.00000 | 20.83333 | 18.75000 |   76.62037 | 0.51645 | -0.00358 |    0.20783 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 | 18.75000 | 55.85938 |  0.00000 |   91.79688 | 0.16447 | -0.00345 |    0.03962 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            220 |    0.00000 |    0.00000 |  0.00000 | 47.26562 |  0.00000 |   90.45139 | 0.15955 |  0.00008 |    0.04358 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            225 |    0.00000 |    0.00000 |  0.00000 | 46.48438 |  0.00000 |   90.23438 | 0.15797 | -0.00625 |    0.04190 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            160 |    0.00000 |    0.00000 |  0.00000 | 57.03125 |  0.00000 |   93.05556 | 0.12905 | -0.00174 |    0.02981 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            325 |    0.00000 |    0.00000 |  0.00000 | 35.93750 |  0.00000 |   85.89410 | 0.16027 | -0.00853 |    0.05055 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            207 |    0.00000 |    0.00000 |  0.00000 | 55.07812 |  0.00000 |   91.01562 | 0.12632 | -0.00183 |    0.03283 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |           1034 |    0.00000 |    0.00000 |  0.00000 | 21.48438 |  0.00000 |   77.56076 | 0.17295 | -0.00630 |    0.06956 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1535 |    0.00000 |    0.00000 |  0.00000 | 23.24219 |  0.00000 |   83.34418 | 0.13620 | -0.00418 |    0.04754 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             79 |    0.00000 |    0.00000 |  0.00000 | 84.57031 | 12.50000 |   84.57031 | 0.21959 | -0.00407 |    0.07922 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1190 |    0.00000 |    0.00000 |  0.00000 | 35.15625 |  0.00000 |   87.08767 | 0.11457 | -0.00352 |    0.03603 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            886 |    0.00000 |    0.00000 |  0.00000 | 46.09375 |  0.00000 |   90.38628 | 0.09609 | -0.00171 |    0.02623 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1370 |    0.00000 |    0.00000 |  0.00000 | 30.76172 |  0.00000 |   85.13455 | 0.12462 | -0.00311 |    0.04141 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            944 |    0.00000 |    0.00000 |  0.00000 | 44.72656 |  0.00000 |   89.75694 | 0.09706 | -0.00556 |    0.02760 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3354 |    0.00000 |    0.00000 |  0.00000 | 23.09570 |  0.00000 |   81.80339 | 0.11913 | -0.00208 |    0.04403 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           7409 |    0.00000 |    0.00000 |  0.00000 | 15.52734 |  0.00000 |   79.90180 | 0.11045 | -0.00571 |    0.04246 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            205 |    0.00000 |    0.00000 |  3.12500 | 89.99023 |  1.56250 |   89.99023 | 0.12411 | -0.00138 |    0.03564 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           6033 |    0.00000 |    0.00000 |  0.00000 | 23.73047 |  0.00000 |   83.63444 | 0.10113 | -0.00407 |    0.03537 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           4183 |    0.00000 |    0.00000 |  0.00000 | 38.76953 |  0.00000 |   88.65289 | 0.08221 | -0.00365 |    0.02406 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           4502 |    0.00000 |    0.00000 |  0.00000 | 38.37891 |  0.00000 |   87.78754 | 0.07576 | -0.00012 |    0.02286 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           2108 |    0.00000 |    0.00000 |  0.00000 | 66.43066 |  0.00000 |   94.28168 | 0.05185 |  0.00175 |    0.01074 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            239 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   62.65625 | 0.45849 | -0.08132 |    0.24104 |
| 22 | Total sparsity:                     | -              |        270896 |          36498 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   86.52693 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:51:38,333 - Total sparsity: 86.53

2018-10-27 22:51:38,333 - --- validate (epoch=29)-----------
2018-10-27 22:51:38,333 - 10000 samples (128 per mini-batch)
2018-10-27 22:51:39,060 - Epoch: [29][   50/   78]    Loss 0.747136    Top1 76.750000    Top5 98.328125    
2018-10-27 22:51:39,453 - ==> Top1: 76.300    Top5: 98.250    Loss: 0.761

2018-10-27 22:51:39,454 - ==> Best Top1: 81.480   On Epoch: 28

2018-10-27 22:51:39,454 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:51:39,476 - 

2018-10-27 22:51:39,477 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:51:40,681 - Epoch: [30][   50/  391]    Overall Loss 0.452385    Objective Loss 0.452385    Top1 84.125000    Top5 99.453125    LR 0.300000    Time 0.024035    
2018-10-27 22:51:41,855 - Epoch: [30][  100/  391]    Overall Loss 0.463126    Objective Loss 0.463126    Top1 83.890625    Top5 99.320312    LR 0.300000    Time 0.023745    
2018-10-27 22:51:43,027 - Epoch: [30][  150/  391]    Overall Loss 0.464195    Objective Loss 0.464195    Top1 83.973958    Top5 99.333333    LR 0.300000    Time 0.023636    
2018-10-27 22:51:44,199 - Epoch: [30][  200/  391]    Overall Loss 0.462226    Objective Loss 0.462226    Top1 84.070312    Top5 99.328125    LR 0.300000    Time 0.023580    
2018-10-27 22:51:45,371 - Epoch: [30][  250/  391]    Overall Loss 0.469322    Objective Loss 0.469322    Top1 83.862500    Top5 99.303125    LR 0.300000    Time 0.023546    
2018-10-27 22:51:46,544 - Epoch: [30][  300/  391]    Overall Loss 0.469299    Objective Loss 0.469299    Top1 83.908854    Top5 99.307292    LR 0.300000    Time 0.023528    
2018-10-27 22:51:47,718 - Epoch: [30][  350/  391]    Overall Loss 0.471695    Objective Loss 0.471695    Top1 83.799107    Top5 99.276786    LR 0.300000    Time 0.023518    
2018-10-27 22:51:48,762 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             99 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   77.08333 | 0.51307 | -0.00503 |    0.20366 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            182 |    0.00000 |    0.00000 | 18.75000 | 56.64062 |  0.00000 |   92.10069 | 0.16256 | -0.00360 |    0.03916 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            212 |    0.00000 |    0.00000 |  0.00000 | 48.82812 |  0.00000 |   90.79861 | 0.15751 | -0.00032 |    0.04273 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            214 |    0.00000 |    0.00000 |  0.00000 | 48.82812 |  0.00000 |   90.71181 | 0.15589 | -0.00707 |    0.04061 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            152 |    0.00000 |    0.00000 |  0.00000 | 58.20312 |  0.00000 |   93.40278 | 0.12752 | -0.00290 |    0.02883 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            312 |    0.00000 |    0.00000 |  0.00000 | 37.50000 |  0.00000 |   86.45833 | 0.15904 | -0.00771 |    0.04938 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            194 |    0.00000 |    0.00000 |  0.00000 | 57.81250 |  0.00000 |   91.57986 | 0.12488 | -0.00187 |    0.03134 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            998 |    0.00000 |    0.00000 |  0.00000 | 23.04688 |  0.00000 |   78.34201 | 0.17278 | -0.00613 |    0.06855 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1465 |    0.00000 |    0.00000 |  0.00000 | 25.09766 |  0.00000 |   84.10373 | 0.13597 | -0.00397 |    0.04669 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             77 |    0.00000 |    0.00000 |  0.00000 | 84.96094 | 15.62500 |   84.96094 | 0.21669 | -0.00249 |    0.07804 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1142 |    0.00000 |    0.00000 |  0.00000 | 36.62109 |  0.00000 |   87.60851 | 0.11412 | -0.00288 |    0.03528 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            839 |    0.00000 |    0.00000 |  0.00000 | 48.14453 |  0.00000 |   90.89627 | 0.09557 | -0.00211 |    0.02576 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1313 |    0.00000 |    0.00000 |  0.00000 | 33.30078 |  0.00000 |   85.75304 | 0.12457 | -0.00352 |    0.04067 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            905 |    0.00000 |    0.00000 |  0.00000 | 46.09375 |  0.00000 |   90.18012 | 0.09671 | -0.00546 |    0.02714 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3216 |    0.00000 |    0.00000 |  0.00000 | 24.75586 |  0.00000 |   82.55208 | 0.11861 | -0.00187 |    0.04328 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           7107 |    0.00000 |    0.00000 |  0.00000 | 17.01660 |  0.00000 |   80.72103 | 0.11014 | -0.00544 |    0.04153 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            189 |    0.00000 |    0.00000 |  3.12500 | 90.77148 |  6.25000 |   90.77148 | 0.12212 | -0.00070 |    0.03404 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5783 |    0.00000 |    0.00000 |  0.00000 | 25.17090 |  0.00000 |   84.31261 | 0.10090 | -0.00382 |    0.03462 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           4001 |    0.00000 |    0.00000 |  0.00000 | 40.45410 |  0.00000 |   89.14659 | 0.08219 | -0.00374 |    0.02371 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           4278 |    0.00000 |    0.00000 |  0.00000 | 40.40527 |  0.00000 |   88.39518 | 0.07574 | -0.00015 |    0.02251 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1990 |    0.00000 |    0.00000 |  0.00000 | 68.16406 |  0.00000 |   94.60178 | 0.05166 |  0.00182 |    0.01050 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            236 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   63.12500 | 0.46162 | -0.08033 |    0.24247 |
| 22 | Total sparsity:                     | -              |        270896 |          34904 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   87.11535 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:51:48,762 - Total sparsity: 87.12

2018-10-27 22:51:48,763 - --- validate (epoch=30)-----------
2018-10-27 22:51:48,763 - 10000 samples (128 per mini-batch)
2018-10-27 22:51:49,492 - Epoch: [30][   50/   78]    Loss 0.693849    Top1 78.734375    Top5 98.515625    
2018-10-27 22:51:49,891 - ==> Top1: 78.090    Top5: 98.680    Loss: 0.702

2018-10-27 22:51:49,892 - ==> Best Top1: 81.480   On Epoch: 28

2018-10-27 22:51:49,892 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:51:49,902 - 

2018-10-27 22:51:49,903 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:51:51,108 - Epoch: [31][   50/  391]    Overall Loss 0.457523    Objective Loss 0.457523    Top1 84.062500    Top5 99.406250    LR 0.300000    Time 0.024064    
2018-10-27 22:51:52,258 - Epoch: [31][  100/  391]    Overall Loss 0.469798    Objective Loss 0.469798    Top1 83.789062    Top5 99.375000    LR 0.300000    Time 0.023517    
2018-10-27 22:51:53,407 - Epoch: [31][  150/  391]    Overall Loss 0.474740    Objective Loss 0.474740    Top1 83.604167    Top5 99.348958    LR 0.300000    Time 0.023332    
2018-10-27 22:51:54,557 - Epoch: [31][  200/  391]    Overall Loss 0.471365    Objective Loss 0.471365    Top1 83.722656    Top5 99.363281    LR 0.300000    Time 0.023242    
2018-10-27 22:51:55,705 - Epoch: [31][  250/  391]    Overall Loss 0.469722    Objective Loss 0.469722    Top1 83.815625    Top5 99.346875    LR 0.300000    Time 0.023180    
2018-10-27 22:51:56,854 - Epoch: [31][  300/  391]    Overall Loss 0.470416    Objective Loss 0.470416    Top1 83.679688    Top5 99.346354    LR 0.300000    Time 0.023142    
2018-10-27 22:51:58,004 - Epoch: [31][  350/  391]    Overall Loss 0.469416    Objective Loss 0.469416    Top1 83.747768    Top5 99.337054    LR 0.300000    Time 0.023118    
2018-10-27 22:51:59,028 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             99 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   77.08333 | 0.50783 | -0.00726 |    0.20154 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            182 |    0.00000 |    0.00000 | 18.75000 | 56.64062 |  0.00000 |   92.10069 | 0.16133 | -0.00413 |    0.03873 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            212 |    0.00000 |    0.00000 |  0.00000 | 48.82812 |  0.00000 |   90.79861 | 0.15579 |  0.00145 |    0.04232 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            214 |    0.00000 |    0.00000 |  0.00000 | 48.82812 |  0.00000 |   90.71181 | 0.15461 | -0.00705 |    0.04016 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            152 |    0.00000 |    0.00000 |  0.00000 | 58.20312 |  0.00000 |   93.40278 | 0.12649 | -0.00297 |    0.02821 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            312 |    0.00000 |    0.00000 |  0.00000 | 37.50000 |  0.00000 |   86.45833 | 0.15807 | -0.00886 |    0.04925 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            194 |    0.00000 |    0.00000 |  0.00000 | 57.81250 |  0.00000 |   91.57986 | 0.12434 | -0.00188 |    0.03115 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            998 |    0.00000 |    0.00000 |  0.00000 | 23.04688 |  0.00000 |   78.34201 | 0.17298 | -0.00474 |    0.06812 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1465 |    0.00000 |    0.00000 |  0.00000 | 25.09766 |  0.00000 |   84.10373 | 0.13573 | -0.00381 |    0.04665 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             77 |    0.00000 |    0.00000 |  0.00000 | 84.96094 | 15.62500 |   84.96094 | 0.21490 | -0.00563 |    0.07682 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1142 |    0.00000 |    0.00000 |  0.00000 | 36.62109 |  0.00000 |   87.60851 | 0.11358 | -0.00211 |    0.03505 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            839 |    0.00000 |    0.00000 |  0.00000 | 48.14453 |  0.00000 |   90.89627 | 0.09525 | -0.00207 |    0.02553 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1313 |    0.00000 |    0.00000 |  0.00000 | 33.30078 |  0.00000 |   85.75304 | 0.12457 | -0.00375 |    0.04057 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            905 |    0.00000 |    0.00000 |  0.00000 | 46.09375 |  0.00000 |   90.18012 | 0.09670 | -0.00588 |    0.02721 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3216 |    0.00000 |    0.00000 |  0.00000 | 24.75586 |  0.00000 |   82.55208 | 0.11849 | -0.00204 |    0.04321 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           7107 |    0.00000 |    0.00000 |  0.00000 | 17.01660 |  0.00000 |   80.72103 | 0.11012 | -0.00556 |    0.04146 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            189 |    0.00000 |    0.00000 |  3.12500 | 90.77148 |  6.25000 |   90.77148 | 0.12070 | -0.00152 |    0.03376 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5783 |    0.00000 |    0.00000 |  0.00000 | 25.17090 |  0.00000 |   84.31261 | 0.10083 | -0.00381 |    0.03453 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           4001 |    0.00000 |    0.00000 |  0.00000 | 40.45410 |  0.00000 |   89.14659 | 0.08212 | -0.00362 |    0.02355 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           4278 |    0.00000 |    0.00000 |  0.00000 | 40.40527 |  0.00000 |   88.39518 | 0.07559 | -0.00023 |    0.02237 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1990 |    0.00000 |    0.00000 |  0.00000 | 68.16406 |  0.00000 |   94.60178 | 0.05147 |  0.00171 |    0.01047 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            236 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   63.12500 | 0.46228 | -0.08178 |    0.24194 |
| 22 | Total sparsity:                     | -              |        270896 |          34904 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   87.11535 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:51:59,028 - Total sparsity: 87.12

2018-10-27 22:51:59,028 - --- validate (epoch=31)-----------
2018-10-27 22:51:59,029 - 10000 samples (128 per mini-batch)
2018-10-27 22:51:59,764 - Epoch: [31][   50/   78]    Loss 0.945150    Top1 72.015625    Top5 98.171875    
2018-10-27 22:52:00,160 - ==> Top1: 71.540    Top5: 98.150    Loss: 0.962

2018-10-27 22:52:00,161 - ==> Best Top1: 81.480   On Epoch: 28

2018-10-27 22:52:00,161 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:52:00,172 - 

2018-10-27 22:52:00,174 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:52:01,407 - Epoch: [32][   50/  391]    Overall Loss 0.472532    Objective Loss 0.472532    Top1 83.781250    Top5 99.281250    LR 0.300000    Time 0.024618    
2018-10-27 22:52:02,583 - Epoch: [32][  100/  391]    Overall Loss 0.472097    Objective Loss 0.472097    Top1 83.617188    Top5 99.296875    LR 0.300000    Time 0.024055    
2018-10-27 22:52:03,756 - Epoch: [32][  150/  391]    Overall Loss 0.476079    Objective Loss 0.476079    Top1 83.375000    Top5 99.239583    LR 0.300000    Time 0.023847    
2018-10-27 22:52:04,929 - Epoch: [32][  200/  391]    Overall Loss 0.475067    Objective Loss 0.475067    Top1 83.550781    Top5 99.222656    LR 0.300000    Time 0.023744    
2018-10-27 22:52:06,105 - Epoch: [32][  250/  391]    Overall Loss 0.473007    Objective Loss 0.473007    Top1 83.665625    Top5 99.253125    LR 0.300000    Time 0.023696    
2018-10-27 22:52:07,278 - Epoch: [32][  300/  391]    Overall Loss 0.471790    Objective Loss 0.471790    Top1 83.739583    Top5 99.239583    LR 0.300000    Time 0.023652    
2018-10-27 22:52:08,453 - Epoch: [32][  350/  391]    Overall Loss 0.472873    Objective Loss 0.472873    Top1 83.622768    Top5 99.254464    LR 0.300000    Time 0.023624    
2018-10-27 22:52:09,499 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             98 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   77.31481 | 0.50980 | -0.00722 |    0.20345 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            174 |    0.00000 |    0.00000 | 18.75000 | 58.20312 |  0.00000 |   92.44792 | 0.15918 | -0.00410 |    0.03785 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            200 |    0.00000 |    0.00000 |  0.00000 | 50.78125 |  0.00000 |   91.31944 | 0.15373 |  0.00014 |    0.04053 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            207 |    0.00000 |    0.00000 |  0.00000 | 50.00000 |  0.00000 |   91.01562 | 0.15377 | -0.00633 |    0.04014 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            145 |    0.00000 |    0.00000 |  0.00000 | 59.76562 |  0.00000 |   93.70660 | 0.12566 | -0.00243 |    0.02780 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            298 |    0.00000 |    0.00000 |  0.00000 | 39.45312 |  0.00000 |   87.06597 | 0.15829 | -0.00783 |    0.04841 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            183 |    0.00000 |    0.00000 |  0.00000 | 59.37500 |  0.00000 |   92.05729 | 0.12362 | -0.00100 |    0.03048 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            951 |    0.00000 |    0.00000 |  0.00000 | 26.17188 |  0.00000 |   79.36198 | 0.17255 | -0.00519 |    0.06690 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1409 |    0.00000 |    0.00000 |  0.00000 | 26.36719 |  0.00000 |   84.71137 | 0.13528 | -0.00373 |    0.04597 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             74 |    0.00000 |    0.00000 |  0.00000 | 85.54688 | 18.75000 |   85.54688 | 0.21169 | -0.00655 |    0.07490 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1100 |    0.00000 |    0.00000 |  0.00000 | 37.69531 |  0.00000 |   88.06424 | 0.11308 | -0.00177 |    0.03455 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            803 |    0.00000 |    0.00000 |  0.00000 | 49.70703 |  0.00000 |   91.28689 | 0.09475 | -0.00279 |    0.02502 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1257 |    0.00000 |    0.00000 |  0.00000 | 35.25391 |  0.00000 |   86.36068 | 0.12405 | -0.00394 |    0.03988 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            880 |    0.00000 |    0.00000 |  0.00000 | 46.97266 |  0.00000 |   90.45139 | 0.09628 | -0.00590 |    0.02665 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3105 |    0.00000 |    0.00000 |  0.00000 | 25.87891 |  0.00000 |   83.15430 | 0.11811 | -0.00179 |    0.04261 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6792 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.57552 | 0.10971 | -0.00579 |    0.04075 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            181 |    0.00000 |    0.00000 |  3.12500 | 91.16211 |  7.81250 |   91.16211 | 0.11857 | -0.00146 |    0.03245 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5543 |    0.00000 |    0.00000 |  0.00000 | 26.78223 |  0.00000 |   84.96365 | 0.10041 | -0.00398 |    0.03394 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3833 |    0.00000 |    0.00000 |  0.00000 | 41.99219 |  0.00000 |   89.60232 | 0.08172 | -0.00361 |    0.02314 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           4075 |    0.00000 |    0.00000 |  0.00000 | 42.35840 |  0.00000 |   88.94586 | 0.07539 | -0.00016 |    0.02190 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1892 |    0.00000 |    0.00000 |  0.00000 | 69.60449 |  0.00000 |   94.86762 | 0.05126 |  0.00176 |    0.01028 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            233 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   63.59375 | 0.46337 | -0.08004 |    0.24232 |
| 22 | Total sparsity:                     | -              |        270896 |          33433 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   87.65836 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:52:09,499 - Total sparsity: 87.66

2018-10-27 22:52:09,499 - --- validate (epoch=32)-----------
2018-10-27 22:52:09,499 - 10000 samples (128 per mini-batch)
2018-10-27 22:52:10,219 - Epoch: [32][   50/   78]    Loss 0.571602    Top1 81.234375    Top5 99.000000    
2018-10-27 22:52:10,612 - ==> Top1: 80.990    Top5: 99.040    Loss: 0.579

2018-10-27 22:52:10,613 - ==> Best Top1: 81.480   On Epoch: 28

2018-10-27 22:52:10,613 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:52:10,623 - 

2018-10-27 22:52:10,623 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:52:11,828 - Epoch: [33][   50/  391]    Overall Loss 0.448313    Objective Loss 0.448313    Top1 84.390625    Top5 99.453125    LR 0.300000    Time 0.024069    
2018-10-27 22:52:12,977 - Epoch: [33][  100/  391]    Overall Loss 0.463065    Objective Loss 0.463065    Top1 83.765625    Top5 99.351562    LR 0.300000    Time 0.023508    
2018-10-27 22:52:14,126 - Epoch: [33][  150/  391]    Overall Loss 0.465705    Objective Loss 0.465705    Top1 83.817708    Top5 99.343750    LR 0.300000    Time 0.023324    
2018-10-27 22:52:15,275 - Epoch: [33][  200/  391]    Overall Loss 0.471327    Objective Loss 0.471327    Top1 83.660156    Top5 99.316406    LR 0.300000    Time 0.023228    
2018-10-27 22:52:16,422 - Epoch: [33][  250/  391]    Overall Loss 0.470216    Objective Loss 0.470216    Top1 83.596875    Top5 99.343750    LR 0.300000    Time 0.023164    
2018-10-27 22:52:17,597 - Epoch: [33][  300/  391]    Overall Loss 0.473164    Objective Loss 0.473164    Top1 83.505208    Top5 99.322917    LR 0.300000    Time 0.023218    
2018-10-27 22:52:18,835 - Epoch: [33][  350/  391]    Overall Loss 0.474418    Objective Loss 0.474418    Top1 83.497768    Top5 99.316964    LR 0.300000    Time 0.023434    
2018-10-27 22:52:19,928 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             98 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   77.31481 | 0.51187 | -0.00668 |    0.20107 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            174 |    0.00000 |    0.00000 | 18.75000 | 58.20312 |  0.00000 |   92.44792 | 0.16128 | -0.00321 |    0.03789 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            200 |    0.00000 |    0.00000 |  0.00000 | 50.78125 |  0.00000 |   91.31944 | 0.15365 |  0.00235 |    0.04037 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            207 |    0.00000 |    0.00000 |  0.00000 | 50.00000 |  0.00000 |   91.01562 | 0.15429 | -0.00618 |    0.03921 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            145 |    0.00000 |    0.00000 |  0.00000 | 59.76562 |  0.00000 |   93.70660 | 0.12642 | -0.00247 |    0.02747 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            298 |    0.00000 |    0.00000 |  0.00000 | 39.45312 |  0.00000 |   87.06597 | 0.15931 | -0.00740 |    0.04880 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            183 |    0.00000 |    0.00000 |  0.00000 | 59.37500 |  0.00000 |   92.05729 | 0.12388 | -0.00144 |    0.03060 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            951 |    0.00000 |    0.00000 |  0.00000 | 26.17188 |  0.00000 |   79.36198 | 0.17315 | -0.00449 |    0.06680 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1409 |    0.00000 |    0.00000 |  0.00000 | 26.36719 |  0.00000 |   84.71137 | 0.13578 | -0.00399 |    0.04578 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             74 |    0.00000 |    0.00000 |  0.00000 | 85.54688 | 18.75000 |   85.54688 | 0.21163 | -0.00611 |    0.07465 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1100 |    0.00000 |    0.00000 |  0.00000 | 37.69531 |  0.00000 |   88.06424 | 0.11309 | -0.00259 |    0.03435 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            803 |    0.00000 |    0.00000 |  0.00000 | 49.70703 |  0.00000 |   91.28689 | 0.09477 | -0.00225 |    0.02475 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1257 |    0.00000 |    0.00000 |  0.00000 | 35.25391 |  0.00000 |   86.36068 | 0.12409 | -0.00340 |    0.03976 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            880 |    0.00000 |    0.00000 |  0.00000 | 46.97266 |  0.00000 |   90.45139 | 0.09642 | -0.00615 |    0.02667 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           3105 |    0.00000 |    0.00000 |  0.00000 | 25.87891 |  0.00000 |   83.15430 | 0.11808 | -0.00200 |    0.04235 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6792 |    0.00000 |    0.00000 |  0.00000 | 18.55469 |  0.00000 |   81.57552 | 0.10971 | -0.00554 |    0.04057 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            181 |    0.00000 |    0.00000 |  3.12500 | 91.16211 |  7.81250 |   91.16211 | 0.11821 | -0.00101 |    0.03235 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5543 |    0.00000 |    0.00000 |  0.00000 | 26.78223 |  0.00000 |   84.96365 | 0.10024 | -0.00377 |    0.03373 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3833 |    0.00000 |    0.00000 |  0.00000 | 41.99219 |  0.00000 |   89.60232 | 0.08142 | -0.00350 |    0.02293 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           4075 |    0.00000 |    0.00000 |  0.00000 | 42.35840 |  0.00000 |   88.94586 | 0.07519 | -0.00027 |    0.02166 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1892 |    0.00000 |    0.00000 |  0.00000 | 69.60449 |  0.00000 |   94.86762 | 0.05112 |  0.00177 |    0.01015 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            233 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   63.59375 | 0.46326 | -0.08190 |    0.24228 |
| 22 | Total sparsity:                     | -              |        270896 |          33433 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   87.65836 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:52:19,928 - Total sparsity: 87.66

2018-10-27 22:52:19,928 - --- validate (epoch=33)-----------
2018-10-27 22:52:19,928 - 10000 samples (128 per mini-batch)
2018-10-27 22:52:20,663 - Epoch: [33][   50/   78]    Loss 0.571699    Top1 81.546875    Top5 98.968750    
2018-10-27 22:52:21,062 - ==> Top1: 81.340    Top5: 99.070    Loss: 0.582

2018-10-27 22:52:21,062 - ==> Best Top1: 81.480   On Epoch: 28

2018-10-27 22:52:21,063 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:52:21,081 - 

2018-10-27 22:52:21,082 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:52:22,386 - Epoch: [34][   50/  391]    Overall Loss 0.452842    Objective Loss 0.452842    Top1 84.406250    Top5 99.375000    LR 0.300000    Time 0.026049    
2018-10-27 22:52:23,660 - Epoch: [34][  100/  391]    Overall Loss 0.468083    Objective Loss 0.468083    Top1 83.914062    Top5 99.265625    LR 0.300000    Time 0.025748    
2018-10-27 22:52:24,936 - Epoch: [34][  150/  391]    Overall Loss 0.463793    Objective Loss 0.463793    Top1 83.973958    Top5 99.291667    LR 0.300000    Time 0.025666    
2018-10-27 22:52:26,204 - Epoch: [34][  200/  391]    Overall Loss 0.465927    Objective Loss 0.465927    Top1 83.816406    Top5 99.281250    LR 0.300000    Time 0.025584    
2018-10-27 22:52:27,470 - Epoch: [34][  250/  391]    Overall Loss 0.468624    Objective Loss 0.468624    Top1 83.712500    Top5 99.296875    LR 0.300000    Time 0.025526    
2018-10-27 22:52:28,708 - Epoch: [34][  300/  391]    Overall Loss 0.468046    Objective Loss 0.468046    Top1 83.752604    Top5 99.286458    LR 0.300000    Time 0.025380    
2018-10-27 22:52:29,912 - Epoch: [34][  350/  391]    Overall Loss 0.469535    Objective Loss 0.469535    Top1 83.727679    Top5 99.279018    LR 0.300000    Time 0.025192    
2018-10-27 22:52:30,968 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             97 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   77.54630 | 0.51171 | -0.00857 |    0.19952 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            170 |    0.00000 |    0.00000 | 18.75000 | 58.98438 |  0.00000 |   92.62153 | 0.16062 | -0.00506 |    0.03749 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            194 |    0.00000 |    0.00000 |  0.00000 | 51.56250 |  0.00000 |   91.57986 | 0.15378 |  0.00095 |    0.03992 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            196 |    0.00000 |    0.00000 |  0.00000 | 51.17188 |  0.00000 |   91.49306 | 0.15371 | -0.00639 |    0.03842 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            137 |    0.00000 |    0.00000 |  0.00000 | 61.71875 |  0.00000 |   94.05382 | 0.12534 | -0.00253 |    0.02714 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            290 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.41319 | 0.15959 | -0.00778 |    0.04813 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            177 |    0.00000 |    0.00000 |  0.00000 | 60.15625 |  0.00000 |   92.31771 | 0.12319 | -0.00079 |    0.02948 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            908 |    0.00000 |    0.00000 |  0.00000 | 28.12500 |  0.00000 |   80.29514 | 0.17221 | -0.00494 |    0.06548 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1349 |    0.00000 |    0.00000 |  0.00000 | 28.80859 |  0.00000 |   85.36241 | 0.13547 | -0.00436 |    0.04483 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             73 |    0.00000 |    0.00000 |  0.00000 | 85.74219 | 18.75000 |   85.74219 | 0.21105 | -0.00676 |    0.07266 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1059 |    0.00000 |    0.00000 |  0.00000 | 39.06250 |  0.00000 |   88.50911 | 0.11293 | -0.00245 |    0.03376 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            763 |    0.00000 |    0.00000 |  0.00000 | 51.56250 |  0.00000 |   91.72092 | 0.09447 | -0.00210 |    0.02436 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1203 |    0.00000 |    0.00000 |  0.00000 | 36.32812 |  0.00000 |   86.94661 | 0.12402 | -0.00349 |    0.03901 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            839 |    0.00000 |    0.00000 |  0.00000 | 48.53516 |  0.00000 |   90.89627 | 0.09611 | -0.00619 |    0.02624 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2999 |    0.00000 |    0.00000 |  0.00000 | 27.68555 |  0.00000 |   83.72938 | 0.11788 | -0.00180 |    0.04174 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6521 |    0.00000 |    0.00000 |  0.00000 | 20.01953 |  0.00000 |   82.31066 | 0.10959 | -0.00500 |    0.03995 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            174 |    0.00000 |    0.00000 |  3.12500 | 91.50391 |  9.37500 |   91.50391 | 0.11684 | -0.00161 |    0.03162 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5277 |    0.00000 |    0.00000 |  0.00000 | 28.90625 |  0.00000 |   85.68522 | 0.09988 | -0.00377 |    0.03308 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3690 |    0.00000 |    0.00000 |  0.00000 | 43.62793 |  0.00000 |   89.99023 | 0.08125 | -0.00337 |    0.02253 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3893 |    0.00000 |    0.00000 |  0.00000 | 44.18945 |  0.00000 |   89.43956 | 0.07506 | -0.00016 |    0.02136 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1821 |    0.00000 |    0.00000 |  0.00000 | 70.67871 |  0.00000 |   95.06022 | 0.05101 |  0.00168 |    0.01001 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            231 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   63.90625 | 0.46494 | -0.08006 |    0.24261 |
| 22 | Total sparsity:                     | -              |        270896 |          32061 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.16483 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:52:30,968 - Total sparsity: 88.16

2018-10-27 22:52:30,968 - --- validate (epoch=34)-----------
2018-10-27 22:52:30,968 - 10000 samples (128 per mini-batch)
2018-10-27 22:52:31,714 - Epoch: [34][   50/   78]    Loss 0.671823    Top1 78.437500    Top5 98.703125    
2018-10-27 22:52:32,108 - ==> Top1: 78.350    Top5: 98.850    Loss: 0.666

2018-10-27 22:52:32,109 - ==> Best Top1: 81.480   On Epoch: 28

2018-10-27 22:52:32,109 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:52:32,120 - 

2018-10-27 22:52:32,120 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:52:33,342 - Epoch: [35][   50/  391]    Overall Loss 0.444117    Objective Loss 0.444117    Top1 84.312500    Top5 99.453125    LR 0.300000    Time 0.024399    
2018-10-27 22:52:34,506 - Epoch: [35][  100/  391]    Overall Loss 0.456153    Objective Loss 0.456153    Top1 83.984375    Top5 99.375000    LR 0.300000    Time 0.023823    
2018-10-27 22:52:35,670 - Epoch: [35][  150/  391]    Overall Loss 0.454274    Objective Loss 0.454274    Top1 84.036458    Top5 99.343750    LR 0.300000    Time 0.023633    
2018-10-27 22:52:36,828 - Epoch: [35][  200/  391]    Overall Loss 0.459572    Objective Loss 0.459572    Top1 83.855469    Top5 99.324219    LR 0.300000    Time 0.023511    
2018-10-27 22:52:37,992 - Epoch: [35][  250/  391]    Overall Loss 0.460739    Objective Loss 0.460739    Top1 83.887500    Top5 99.293750    LR 0.300000    Time 0.023457    
2018-10-27 22:52:39,154 - Epoch: [35][  300/  391]    Overall Loss 0.466964    Objective Loss 0.466964    Top1 83.757812    Top5 99.273438    LR 0.300000    Time 0.023418    
2018-10-27 22:52:40,317 - Epoch: [35][  350/  391]    Overall Loss 0.466583    Objective Loss 0.466583    Top1 83.705357    Top5 99.276786    LR 0.300000    Time 0.023390    
2018-10-27 22:52:41,351 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             97 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   77.54630 | 0.51842 | -0.00661 |    0.20070 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            170 |    0.00000 |    0.00000 | 18.75000 | 58.98438 |  0.00000 |   92.62153 | 0.16002 | -0.00407 |    0.03711 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            194 |    0.00000 |    0.00000 |  0.00000 | 51.56250 |  0.00000 |   91.57986 | 0.15352 |  0.00171 |    0.03945 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            196 |    0.00000 |    0.00000 |  0.00000 | 51.17188 |  0.00000 |   91.49306 | 0.15395 | -0.00648 |    0.03855 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            137 |    0.00000 |    0.00000 |  0.00000 | 61.71875 |  0.00000 |   94.05382 | 0.12514 | -0.00313 |    0.02669 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            290 |    0.00000 |    0.00000 |  0.00000 | 40.23438 |  0.00000 |   87.41319 | 0.15872 | -0.00822 |    0.04745 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            177 |    0.00000 |    0.00000 |  0.00000 | 60.15625 |  0.00000 |   92.31771 | 0.12281 | -0.00189 |    0.02956 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            908 |    0.00000 |    0.00000 |  0.00000 | 28.12500 |  0.00000 |   80.29514 | 0.17164 | -0.00428 |    0.06478 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1349 |    0.00000 |    0.00000 |  0.00000 | 28.80859 |  0.00000 |   85.36241 | 0.13510 | -0.00394 |    0.04454 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             73 |    0.00000 |    0.00000 |  0.00000 | 85.74219 | 18.75000 |   85.74219 | 0.21198 | -0.00737 |    0.07350 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1059 |    0.00000 |    0.00000 |  0.00000 | 39.06250 |  0.00000 |   88.50911 | 0.11244 | -0.00256 |    0.03344 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            763 |    0.00000 |    0.00000 |  0.00000 | 51.56250 |  0.00000 |   91.72092 | 0.09407 | -0.00266 |    0.02402 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1203 |    0.00000 |    0.00000 |  0.00000 | 36.32812 |  0.00000 |   86.94661 | 0.12448 | -0.00385 |    0.03912 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            839 |    0.00000 |    0.00000 |  0.00000 | 48.53516 |  0.00000 |   90.89627 | 0.09575 | -0.00582 |    0.02598 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2999 |    0.00000 |    0.00000 |  0.00000 | 27.68555 |  0.00000 |   83.72938 | 0.11765 | -0.00153 |    0.04142 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6521 |    0.00000 |    0.00000 |  0.00000 | 20.01953 |  0.00000 |   82.31066 | 0.10937 | -0.00523 |    0.03970 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            174 |    0.00000 |    0.00000 |  3.12500 | 91.50391 |  9.37500 |   91.50391 | 0.11634 | -0.00106 |    0.03126 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5277 |    0.00000 |    0.00000 |  0.00000 | 28.90625 |  0.00000 |   85.68522 | 0.09966 | -0.00345 |    0.03277 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3690 |    0.00000 |    0.00000 |  0.00000 | 43.62793 |  0.00000 |   89.99023 | 0.08135 | -0.00353 |    0.02250 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3893 |    0.00000 |    0.00000 |  0.00000 | 44.18945 |  0.00000 |   89.43956 | 0.07529 | -0.00015 |    0.02132 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1821 |    0.00000 |    0.00000 |  0.00000 | 70.67871 |  0.00000 |   95.06022 | 0.05094 |  0.00184 |    0.00988 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            231 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   63.90625 | 0.46233 | -0.08202 |    0.24069 |
| 22 | Total sparsity:                     | -              |        270896 |          32061 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.16483 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:52:41,351 - Total sparsity: 88.16

2018-10-27 22:52:41,351 - --- validate (epoch=35)-----------
2018-10-27 22:52:41,352 - 10000 samples (128 per mini-batch)
2018-10-27 22:52:42,091 - Epoch: [35][   50/   78]    Loss 0.588465    Top1 80.109375    Top5 98.703125    
2018-10-27 22:52:42,491 - ==> Top1: 80.270    Top5: 98.880    Loss: 0.589

2018-10-27 22:52:42,492 - ==> Best Top1: 81.480   On Epoch: 28

2018-10-27 22:52:42,492 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:52:42,509 - 

2018-10-27 22:52:42,511 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:52:43,741 - Epoch: [36][   50/  391]    Overall Loss 0.457588    Objective Loss 0.457588    Top1 83.937500    Top5 99.468750    LR 0.300000    Time 0.024572    
2018-10-27 22:52:44,938 - Epoch: [36][  100/  391]    Overall Loss 0.462433    Objective Loss 0.462433    Top1 83.765625    Top5 99.429688    LR 0.300000    Time 0.024245    
2018-10-27 22:52:46,139 - Epoch: [36][  150/  391]    Overall Loss 0.460895    Objective Loss 0.460895    Top1 83.885417    Top5 99.380208    LR 0.300000    Time 0.024157    
2018-10-27 22:52:47,338 - Epoch: [36][  200/  391]    Overall Loss 0.460789    Objective Loss 0.460789    Top1 83.843750    Top5 99.406250    LR 0.300000    Time 0.024107    
2018-10-27 22:52:48,539 - Epoch: [36][  250/  391]    Overall Loss 0.459237    Objective Loss 0.459237    Top1 84.006250    Top5 99.396875    LR 0.300000    Time 0.024085    
2018-10-27 22:52:49,734 - Epoch: [36][  300/  391]    Overall Loss 0.463136    Objective Loss 0.463136    Top1 83.856771    Top5 99.393229    LR 0.300000    Time 0.024051    
2018-10-27 22:52:50,928 - Epoch: [36][  350/  391]    Overall Loss 0.465539    Objective Loss 0.465539    Top1 83.747768    Top5 99.392857    LR 0.300000    Time 0.024021    
2018-10-27 22:52:51,986 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51675 | -0.00801 |    0.19948 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15918 | -0.00510 |    0.03688 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15300 |  0.00186 |    0.03985 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15323 | -0.00483 |    0.03798 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12391 | -0.00353 |    0.02591 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15827 | -0.00760 |    0.04633 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12123 | -0.00085 |    0.02880 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17038 | -0.00443 |    0.06425 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13464 | -0.00386 |    0.04415 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20897 | -0.00585 |    0.07099 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11216 | -0.00253 |    0.03281 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09373 | -0.00230 |    0.02380 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12371 | -0.00367 |    0.03818 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09489 | -0.00596 |    0.02533 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11717 | -0.00195 |    0.04076 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10895 | -0.00490 |    0.03910 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11595 | -0.00094 |    0.03075 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09940 | -0.00324 |    0.03234 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08118 | -0.00345 |    0.02214 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07511 | -0.00020 |    0.02100 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05078 |  0.00176 |    0.00969 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46216 | -0.08164 |    0.24008 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:52:51,986 - Total sparsity: 88.62

2018-10-27 22:52:51,986 - --- validate (epoch=36)-----------
2018-10-27 22:52:51,986 - 10000 samples (128 per mini-batch)
2018-10-27 22:52:52,727 - Epoch: [36][   50/   78]    Loss 0.586186    Top1 80.468750    Top5 98.828125    
2018-10-27 22:52:53,131 - ==> Top1: 80.620    Top5: 98.900    Loss: 0.578

2018-10-27 22:52:53,132 - ==> Best Top1: 81.480   On Epoch: 28

2018-10-27 22:52:53,132 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:52:53,148 - 

2018-10-27 22:52:53,148 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:52:54,336 - Epoch: [37][   50/  391]    Overall Loss 0.478347    Objective Loss 0.478347    Top1 83.218750    Top5 99.390625    LR 0.300000    Time 0.023730    
2018-10-27 22:52:55,498 - Epoch: [37][  100/  391]    Overall Loss 0.472851    Objective Loss 0.472851    Top1 83.539062    Top5 99.335938    LR 0.300000    Time 0.023474    
2018-10-27 22:52:56,660 - Epoch: [37][  150/  391]    Overall Loss 0.474250    Objective Loss 0.474250    Top1 83.598958    Top5 99.291667    LR 0.300000    Time 0.023386    
2018-10-27 22:52:57,824 - Epoch: [37][  200/  391]    Overall Loss 0.475735    Objective Loss 0.475735    Top1 83.417969    Top5 99.332031    LR 0.300000    Time 0.023353    
2018-10-27 22:52:58,986 - Epoch: [37][  250/  391]    Overall Loss 0.477402    Objective Loss 0.477402    Top1 83.321875    Top5 99.281250    LR 0.300000    Time 0.023323    
2018-10-27 22:53:00,152 - Epoch: [37][  300/  391]    Overall Loss 0.478237    Objective Loss 0.478237    Top1 83.393229    Top5 99.263021    LR 0.300000    Time 0.023319    
2018-10-27 22:53:01,319 - Epoch: [37][  350/  391]    Overall Loss 0.473284    Objective Loss 0.473284    Top1 83.642857    Top5 99.261161    LR 0.300000    Time 0.023318    
2018-10-27 22:53:02,357 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51468 | -0.00674 |    0.19978 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15767 | -0.00416 |    0.03643 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15165 |  0.00230 |    0.03906 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15361 | -0.00573 |    0.03784 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12376 | -0.00331 |    0.02613 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15794 | -0.00727 |    0.04646 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12087 | -0.00109 |    0.02858 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.16990 | -0.00584 |    0.06407 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13444 | -0.00397 |    0.04382 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20611 | -0.00710 |    0.07020 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11153 | -0.00260 |    0.03247 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09322 | -0.00224 |    0.02353 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12330 | -0.00345 |    0.03804 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09453 | -0.00602 |    0.02506 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11689 | -0.00203 |    0.04041 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10870 | -0.00490 |    0.03894 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11524 | -0.00092 |    0.03052 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09914 | -0.00302 |    0.03205 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08090 | -0.00321 |    0.02200 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07500 | -0.00009 |    0.02091 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05059 |  0.00197 |    0.00959 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.47368 | -0.07980 |    0.24513 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:53:02,357 - Total sparsity: 88.62

2018-10-27 22:53:02,357 - --- validate (epoch=37)-----------
2018-10-27 22:53:02,357 - 10000 samples (128 per mini-batch)
2018-10-27 22:53:03,103 - Epoch: [37][   50/   78]    Loss 0.578407    Top1 81.750000    Top5 98.812500    
2018-10-27 22:53:03,506 - ==> Top1: 81.260    Top5: 98.930    Loss: 0.587

2018-10-27 22:53:03,506 - ==> Best Top1: 81.480   On Epoch: 28

2018-10-27 22:53:03,507 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:53:03,515 - 

2018-10-27 22:53:03,516 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:53:04,737 - Epoch: [38][   50/  391]    Overall Loss 0.489128    Objective Loss 0.489128    Top1 82.765625    Top5 99.375000    LR 0.300000    Time 0.024385    
2018-10-27 22:53:05,902 - Epoch: [38][  100/  391]    Overall Loss 0.480199    Objective Loss 0.480199    Top1 83.367188    Top5 99.351562    LR 0.300000    Time 0.023835    
2018-10-27 22:53:07,066 - Epoch: [38][  150/  391]    Overall Loss 0.477421    Objective Loss 0.477421    Top1 83.406250    Top5 99.348958    LR 0.300000    Time 0.023641    
2018-10-27 22:53:08,230 - Epoch: [38][  200/  391]    Overall Loss 0.474232    Objective Loss 0.474232    Top1 83.609375    Top5 99.367188    LR 0.300000    Time 0.023540    
2018-10-27 22:53:09,398 - Epoch: [38][  250/  391]    Overall Loss 0.470439    Objective Loss 0.470439    Top1 83.684375    Top5 99.409375    LR 0.300000    Time 0.023487    
2018-10-27 22:53:10,560 - Epoch: [38][  300/  391]    Overall Loss 0.469187    Objective Loss 0.469187    Top1 83.783854    Top5 99.382812    LR 0.300000    Time 0.023439    
2018-10-27 22:53:11,721 - Epoch: [38][  350/  391]    Overall Loss 0.472862    Objective Loss 0.472862    Top1 83.633929    Top5 99.350446    LR 0.300000    Time 0.023406    
2018-10-27 22:53:12,752 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51296 | -0.00494 |    0.19750 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15843 | -0.00407 |    0.03544 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15105 |  0.00160 |    0.03889 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15448 | -0.00569 |    0.03819 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12329 | -0.00335 |    0.02584 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15899 | -0.00685 |    0.04665 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12104 | -0.00138 |    0.02877 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17056 | -0.00668 |    0.06400 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13494 | -0.00388 |    0.04382 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20569 | -0.00767 |    0.06827 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11236 | -0.00232 |    0.03276 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09352 | -0.00236 |    0.02354 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12281 | -0.00392 |    0.03780 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09441 | -0.00594 |    0.02502 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11706 | -0.00205 |    0.04037 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10869 | -0.00461 |    0.03866 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11412 | -0.00086 |    0.03012 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09910 | -0.00349 |    0.03200 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08096 | -0.00334 |    0.02200 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07510 | -0.00024 |    0.02089 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05066 |  0.00201 |    0.00960 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46558 | -0.08061 |    0.24140 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:53:12,753 - Total sparsity: 88.62

2018-10-27 22:53:12,753 - --- validate (epoch=38)-----------
2018-10-27 22:53:12,753 - 10000 samples (128 per mini-batch)
2018-10-27 22:53:13,478 - Epoch: [38][   50/   78]    Loss 0.892016    Top1 74.859375    Top5 97.484375    
2018-10-27 22:53:13,875 - ==> Top1: 75.120    Top5: 97.720    Loss: 0.874

2018-10-27 22:53:13,876 - ==> Best Top1: 81.480   On Epoch: 28

2018-10-27 22:53:13,876 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:53:13,885 - 

2018-10-27 22:53:13,886 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:53:15,110 - Epoch: [39][   50/  391]    Overall Loss 0.463975    Objective Loss 0.463975    Top1 84.156250    Top5 99.500000    LR 0.300000    Time 0.024446    
2018-10-27 22:53:16,269 - Epoch: [39][  100/  391]    Overall Loss 0.465008    Objective Loss 0.465008    Top1 83.960938    Top5 99.382812    LR 0.300000    Time 0.023802    
2018-10-27 22:53:17,433 - Epoch: [39][  150/  391]    Overall Loss 0.466213    Objective Loss 0.466213    Top1 83.854167    Top5 99.338542    LR 0.300000    Time 0.023616    
2018-10-27 22:53:18,598 - Epoch: [39][  200/  391]    Overall Loss 0.466174    Objective Loss 0.466174    Top1 83.898438    Top5 99.296875    LR 0.300000    Time 0.023533    
2018-10-27 22:53:19,764 - Epoch: [39][  250/  391]    Overall Loss 0.462679    Objective Loss 0.462679    Top1 84.028125    Top5 99.290625    LR 0.300000    Time 0.023485    
2018-10-27 22:53:20,929 - Epoch: [39][  300/  391]    Overall Loss 0.467869    Objective Loss 0.467869    Top1 83.791667    Top5 99.302083    LR 0.300000    Time 0.023436    
2018-10-27 22:53:22,096 - Epoch: [39][  350/  391]    Overall Loss 0.468787    Objective Loss 0.468787    Top1 83.754464    Top5 99.294643    LR 0.300000    Time 0.023417    
2018-10-27 22:53:23,132 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51267 | -0.00265 |    0.19708 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15853 | -0.00478 |    0.03608 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15072 |  0.00235 |    0.03862 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15331 | -0.00508 |    0.03785 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12298 | -0.00340 |    0.02548 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15891 | -0.00680 |    0.04699 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12189 | -0.00148 |    0.02889 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17045 | -0.00560 |    0.06377 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13543 | -0.00396 |    0.04390 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20433 | -0.00978 |    0.06873 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11238 | -0.00226 |    0.03263 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09387 | -0.00227 |    0.02361 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12303 | -0.00418 |    0.03769 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09461 | -0.00574 |    0.02505 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11727 | -0.00174 |    0.04037 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10883 | -0.00515 |    0.03876 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11375 | -0.00113 |    0.02977 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09930 | -0.00350 |    0.03204 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08096 | -0.00345 |    0.02196 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07523 |  0.00001 |    0.02085 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05072 |  0.00192 |    0.00959 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46617 | -0.08258 |    0.24246 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:53:23,132 - Total sparsity: 88.62

2018-10-27 22:53:23,133 - --- validate (epoch=39)-----------
2018-10-27 22:53:23,133 - 10000 samples (128 per mini-batch)
2018-10-27 22:53:23,872 - Epoch: [39][   50/   78]    Loss 0.676944    Top1 78.125000    Top5 98.328125    
2018-10-27 22:53:24,277 - ==> Top1: 78.290    Top5: 98.410    Loss: 0.672

2018-10-27 22:53:24,278 - ==> Best Top1: 81.480   On Epoch: 28

2018-10-27 22:53:24,278 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:53:24,288 - 

2018-10-27 22:53:24,288 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:53:25,506 - Epoch: [40][   50/  391]    Overall Loss 0.446747    Objective Loss 0.446747    Top1 84.187500    Top5 99.343750    LR 0.300000    Time 0.024327    
2018-10-27 22:53:26,675 - Epoch: [40][  100/  391]    Overall Loss 0.460718    Objective Loss 0.460718    Top1 83.921875    Top5 99.351562    LR 0.300000    Time 0.023835    
2018-10-27 22:53:27,841 - Epoch: [40][  150/  391]    Overall Loss 0.464958    Objective Loss 0.464958    Top1 83.947917    Top5 99.260417    LR 0.300000    Time 0.023658    
2018-10-27 22:53:29,008 - Epoch: [40][  200/  391]    Overall Loss 0.468876    Objective Loss 0.468876    Top1 83.835938    Top5 99.269531    LR 0.300000    Time 0.023572    
2018-10-27 22:53:30,173 - Epoch: [40][  250/  391]    Overall Loss 0.467786    Objective Loss 0.467786    Top1 83.915625    Top5 99.253125    LR 0.300000    Time 0.023509    
2018-10-27 22:53:31,334 - Epoch: [40][  300/  391]    Overall Loss 0.470297    Objective Loss 0.470297    Top1 83.723958    Top5 99.250000    LR 0.300000    Time 0.023457    
2018-10-27 22:53:32,502 - Epoch: [40][  350/  391]    Overall Loss 0.472946    Objective Loss 0.472946    Top1 83.636161    Top5 99.252232    LR 0.300000    Time 0.023439    
2018-10-27 22:53:33,538 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51633 | -0.00340 |    0.19975 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15823 | -0.00428 |    0.03613 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15000 |  0.00184 |    0.03806 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15350 | -0.00595 |    0.03809 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12282 | -0.00244 |    0.02551 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15960 | -0.00744 |    0.04677 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12220 | -0.00156 |    0.02909 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17083 | -0.00617 |    0.06345 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13575 | -0.00352 |    0.04367 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20209 | -0.01075 |    0.06780 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11265 | -0.00257 |    0.03279 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09378 | -0.00244 |    0.02362 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12270 | -0.00378 |    0.03757 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09399 | -0.00532 |    0.02479 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11690 | -0.00173 |    0.04039 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10868 | -0.00525 |    0.03866 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11368 | -0.00115 |    0.02980 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09911 | -0.00349 |    0.03187 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08081 | -0.00350 |    0.02190 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07519 |  0.00004 |    0.02077 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05084 |  0.00195 |    0.00959 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46475 | -0.08240 |    0.24077 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:53:33,538 - Total sparsity: 88.62

2018-10-27 22:53:33,538 - --- validate (epoch=40)-----------
2018-10-27 22:53:33,538 - 10000 samples (128 per mini-batch)
2018-10-27 22:53:34,263 - Epoch: [40][   50/   78]    Loss 0.613471    Top1 79.703125    Top5 99.046875    
2018-10-27 22:53:34,657 - ==> Top1: 79.760    Top5: 99.080    Loss: 0.609

2018-10-27 22:53:34,658 - ==> Best Top1: 81.480   On Epoch: 28

2018-10-27 22:53:34,658 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:53:34,668 - 

2018-10-27 22:53:34,669 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:53:35,887 - Epoch: [41][   50/  391]    Overall Loss 0.473705    Objective Loss 0.473705    Top1 83.359375    Top5 99.437500    LR 0.300000    Time 0.024338    
2018-10-27 22:53:37,049 - Epoch: [41][  100/  391]    Overall Loss 0.468759    Objective Loss 0.468759    Top1 83.703125    Top5 99.382812    LR 0.300000    Time 0.023769    
2018-10-27 22:53:38,212 - Epoch: [41][  150/  391]    Overall Loss 0.470789    Objective Loss 0.470789    Top1 83.682292    Top5 99.406250    LR 0.300000    Time 0.023590    
2018-10-27 22:53:39,379 - Epoch: [41][  200/  391]    Overall Loss 0.469724    Objective Loss 0.469724    Top1 83.675781    Top5 99.382812    LR 0.300000    Time 0.023518    
2018-10-27 22:53:40,543 - Epoch: [41][  250/  391]    Overall Loss 0.466759    Objective Loss 0.466759    Top1 83.715625    Top5 99.371875    LR 0.300000    Time 0.023468    
2018-10-27 22:53:41,706 - Epoch: [41][  300/  391]    Overall Loss 0.470395    Objective Loss 0.470395    Top1 83.578125    Top5 99.372396    LR 0.300000    Time 0.023429    
2018-10-27 22:53:42,869 - Epoch: [41][  350/  391]    Overall Loss 0.470767    Objective Loss 0.470767    Top1 83.580357    Top5 99.348214    LR 0.300000    Time 0.023399    
2018-10-27 22:53:43,905 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51168 | -0.00955 |    0.19772 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15759 | -0.00442 |    0.03541 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.14968 |  0.00303 |    0.03810 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15264 | -0.00707 |    0.03758 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12246 | -0.00242 |    0.02523 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15889 | -0.00876 |    0.04581 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12152 | -0.00096 |    0.02891 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17076 | -0.00549 |    0.06315 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13568 | -0.00380 |    0.04361 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20169 | -0.00946 |    0.06651 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11287 | -0.00234 |    0.03254 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09368 | -0.00187 |    0.02347 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12229 | -0.00350 |    0.03749 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09362 | -0.00548 |    0.02471 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11697 | -0.00168 |    0.04030 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10866 | -0.00528 |    0.03856 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11359 | -0.00081 |    0.03012 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09913 | -0.00341 |    0.03184 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08075 | -0.00355 |    0.02185 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07513 | -0.00004 |    0.02073 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05076 |  0.00187 |    0.00958 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46361 | -0.08089 |    0.24100 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:53:43,906 - Total sparsity: 88.62

2018-10-27 22:53:43,906 - --- validate (epoch=41)-----------
2018-10-27 22:53:43,906 - 10000 samples (128 per mini-batch)
2018-10-27 22:53:44,629 - Epoch: [41][   50/   78]    Loss 0.556389    Top1 81.312500    Top5 99.031250    
2018-10-27 22:53:45,019 - ==> Top1: 80.840    Top5: 99.010    Loss: 0.560

2018-10-27 22:53:45,020 - ==> Best Top1: 81.480   On Epoch: 28

2018-10-27 22:53:45,020 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:53:45,031 - 

2018-10-27 22:53:45,031 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:53:46,255 - Epoch: [42][   50/  391]    Overall Loss 0.445054    Objective Loss 0.445054    Top1 84.921875    Top5 99.281250    LR 0.300000    Time 0.024433    
2018-10-27 22:53:47,420 - Epoch: [42][  100/  391]    Overall Loss 0.473037    Objective Loss 0.473037    Top1 83.773438    Top5 99.273438    LR 0.300000    Time 0.023855    
2018-10-27 22:53:48,587 - Epoch: [42][  150/  391]    Overall Loss 0.473236    Objective Loss 0.473236    Top1 83.796875    Top5 99.286458    LR 0.300000    Time 0.023674    
2018-10-27 22:53:49,750 - Epoch: [42][  200/  391]    Overall Loss 0.478544    Objective Loss 0.478544    Top1 83.484375    Top5 99.242188    LR 0.300000    Time 0.023547    
2018-10-27 22:53:50,919 - Epoch: [42][  250/  391]    Overall Loss 0.476785    Objective Loss 0.476785    Top1 83.506250    Top5 99.284375    LR 0.300000    Time 0.023508    
2018-10-27 22:53:52,083 - Epoch: [42][  300/  391]    Overall Loss 0.469394    Objective Loss 0.469394    Top1 83.710938    Top5 99.322917    LR 0.300000    Time 0.023464    
2018-10-27 22:53:53,242 - Epoch: [42][  350/  391]    Overall Loss 0.473229    Objective Loss 0.473229    Top1 83.678571    Top5 99.312500    LR 0.300000    Time 0.023420    
2018-10-27 22:53:54,275 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51115 | -0.01439 |    0.19713 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15748 | -0.00463 |    0.03582 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.14952 |  0.00344 |    0.03788 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15301 | -0.00600 |    0.03739 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12220 | -0.00218 |    0.02499 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15885 | -0.00826 |    0.04581 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12133 | -0.00123 |    0.02837 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17047 | -0.00509 |    0.06300 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13526 | -0.00378 |    0.04373 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20301 | -0.00735 |    0.06681 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11292 | -0.00244 |    0.03240 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09386 | -0.00223 |    0.02340 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12209 | -0.00402 |    0.03732 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09352 | -0.00496 |    0.02471 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11692 | -0.00208 |    0.04035 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10872 | -0.00555 |    0.03853 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11324 | -0.00116 |    0.02955 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09921 | -0.00359 |    0.03176 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08085 | -0.00344 |    0.02192 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07515 | -0.00005 |    0.02069 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05070 |  0.00190 |    0.00955 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46468 | -0.08379 |    0.24148 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:53:54,276 - Total sparsity: 88.62

2018-10-27 22:53:54,276 - --- validate (epoch=42)-----------
2018-10-27 22:53:54,276 - 10000 samples (128 per mini-batch)
2018-10-27 22:53:55,008 - Epoch: [42][   50/   78]    Loss 0.623955    Top1 79.421875    Top5 98.828125    
2018-10-27 22:53:55,405 - ==> Top1: 79.360    Top5: 98.920    Loss: 0.624

2018-10-27 22:53:55,406 - ==> Best Top1: 81.480   On Epoch: 28

2018-10-27 22:53:55,406 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:53:55,417 - 

2018-10-27 22:53:55,417 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:53:56,639 - Epoch: [43][   50/  391]    Overall Loss 0.453206    Objective Loss 0.453206    Top1 84.250000    Top5 99.296875    LR 0.300000    Time 0.024405    
2018-10-27 22:53:57,806 - Epoch: [43][  100/  391]    Overall Loss 0.459928    Objective Loss 0.459928    Top1 84.093750    Top5 99.382812    LR 0.300000    Time 0.023854    
2018-10-27 22:53:58,971 - Epoch: [43][  150/  391]    Overall Loss 0.465705    Objective Loss 0.465705    Top1 83.817708    Top5 99.338542    LR 0.300000    Time 0.023661    
2018-10-27 22:54:00,141 - Epoch: [43][  200/  391]    Overall Loss 0.468675    Objective Loss 0.468675    Top1 83.878906    Top5 99.320312    LR 0.300000    Time 0.023588    
2018-10-27 22:54:01,304 - Epoch: [43][  250/  391]    Overall Loss 0.468778    Objective Loss 0.468778    Top1 83.743750    Top5 99.325000    LR 0.300000    Time 0.023517    
2018-10-27 22:54:02,436 - Epoch: [43][  300/  391]    Overall Loss 0.469889    Objective Loss 0.469889    Top1 83.744792    Top5 99.296875    LR 0.300000    Time 0.023369    
2018-10-27 22:54:03,569 - Epoch: [43][  350/  391]    Overall Loss 0.472915    Objective Loss 0.472915    Top1 83.575893    Top5 99.301339    LR 0.300000    Time 0.023265    
2018-10-27 22:54:04,580 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51093 | -0.00620 |    0.19747 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15882 | -0.00552 |    0.03609 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15038 |  0.00277 |    0.03829 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15271 | -0.00477 |    0.03706 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12167 | -0.00270 |    0.02490 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15899 | -0.00833 |    0.04589 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12130 | -0.00107 |    0.02868 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.16983 | -0.00497 |    0.06201 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13517 | -0.00357 |    0.04352 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20393 | -0.00630 |    0.06711 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11236 | -0.00216 |    0.03223 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09385 | -0.00237 |    0.02331 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12204 | -0.00320 |    0.03713 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09359 | -0.00543 |    0.02466 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11704 | -0.00190 |    0.04033 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10898 | -0.00538 |    0.03862 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11374 | -0.00075 |    0.02938 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09945 | -0.00341 |    0.03177 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08108 | -0.00354 |    0.02187 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07528 |  0.00017 |    0.02068 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05068 |  0.00196 |    0.00950 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46469 | -0.08138 |    0.24079 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:54:04,581 - Total sparsity: 88.62

2018-10-27 22:54:04,581 - --- validate (epoch=43)-----------
2018-10-27 22:54:04,581 - 10000 samples (128 per mini-batch)
2018-10-27 22:54:05,303 - Epoch: [43][   50/   78]    Loss 0.765136    Top1 76.500000    Top5 98.718750    
2018-10-27 22:54:05,697 - ==> Top1: 76.290    Top5: 98.700    Loss: 0.776

2018-10-27 22:54:05,698 - ==> Best Top1: 81.480   On Epoch: 28

2018-10-27 22:54:05,698 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:54:05,708 - 

2018-10-27 22:54:05,708 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:54:06,898 - Epoch: [44][   50/  391]    Overall Loss 0.445905    Objective Loss 0.445905    Top1 84.937500    Top5 99.453125    LR 0.300000    Time 0.023769    
2018-10-27 22:54:08,033 - Epoch: [44][  100/  391]    Overall Loss 0.461215    Objective Loss 0.461215    Top1 83.984375    Top5 99.359375    LR 0.300000    Time 0.023213    
2018-10-27 22:54:09,166 - Epoch: [44][  150/  391]    Overall Loss 0.464399    Objective Loss 0.464399    Top1 84.062500    Top5 99.338542    LR 0.300000    Time 0.023020    
2018-10-27 22:54:10,298 - Epoch: [44][  200/  391]    Overall Loss 0.462689    Objective Loss 0.462689    Top1 84.101562    Top5 99.382812    LR 0.300000    Time 0.022919    
2018-10-27 22:54:11,431 - Epoch: [44][  250/  391]    Overall Loss 0.464952    Objective Loss 0.464952    Top1 84.059375    Top5 99.353125    LR 0.300000    Time 0.022864    
2018-10-27 22:54:12,567 - Epoch: [44][  300/  391]    Overall Loss 0.469824    Objective Loss 0.469824    Top1 83.932292    Top5 99.317708    LR 0.300000    Time 0.022835    
2018-10-27 22:54:13,700 - Epoch: [44][  350/  391]    Overall Loss 0.468493    Objective Loss 0.468493    Top1 83.946429    Top5 99.321429    LR 0.300000    Time 0.022807    
2018-10-27 22:54:14,708 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51522 | -0.00342 |    0.19923 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15936 | -0.00579 |    0.03572 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15076 |  0.00245 |    0.03869 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15273 | -0.00588 |    0.03709 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12223 | -0.00225 |    0.02481 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15897 | -0.00704 |    0.04605 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12074 | -0.00131 |    0.02869 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.16951 | -0.00452 |    0.06210 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13524 | -0.00347 |    0.04357 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20419 | -0.00598 |    0.06670 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11261 | -0.00253 |    0.03211 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09423 | -0.00213 |    0.02327 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12269 | -0.00320 |    0.03714 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09394 | -0.00507 |    0.02465 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11736 | -0.00241 |    0.04037 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10946 | -0.00534 |    0.03871 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11385 | -0.00218 |    0.02926 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09966 | -0.00298 |    0.03180 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08135 | -0.00332 |    0.02193 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07534 |  0.00023 |    0.02069 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05064 |  0.00208 |    0.00949 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46398 | -0.08131 |    0.24030 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:54:14,708 - Total sparsity: 88.62

2018-10-27 22:54:14,708 - --- validate (epoch=44)-----------
2018-10-27 22:54:14,708 - 10000 samples (128 per mini-batch)
2018-10-27 22:54:15,426 - Epoch: [44][   50/   78]    Loss 0.766439    Top1 76.359375    Top5 98.390625    
2018-10-27 22:54:15,813 - ==> Top1: 76.220    Top5: 98.490    Loss: 0.766

2018-10-27 22:54:15,814 - ==> Best Top1: 81.480   On Epoch: 28

2018-10-27 22:54:15,814 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:54:15,825 - 

2018-10-27 22:54:15,825 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:54:17,018 - Epoch: [45][   50/  391]    Overall Loss 0.492642    Objective Loss 0.492642    Top1 82.890625    Top5 99.250000    LR 0.300000    Time 0.023814    
2018-10-27 22:54:18,151 - Epoch: [45][  100/  391]    Overall Loss 0.477427    Objective Loss 0.477427    Top1 83.625000    Top5 99.296875    LR 0.300000    Time 0.023224    
2018-10-27 22:54:19,285 - Epoch: [45][  150/  391]    Overall Loss 0.469548    Objective Loss 0.469548    Top1 83.895833    Top5 99.322917    LR 0.300000    Time 0.023033    
2018-10-27 22:54:20,419 - Epoch: [45][  200/  391]    Overall Loss 0.472725    Objective Loss 0.472725    Top1 83.636719    Top5 99.339844    LR 0.300000    Time 0.022941    
2018-10-27 22:54:21,551 - Epoch: [45][  250/  391]    Overall Loss 0.471616    Objective Loss 0.471616    Top1 83.584375    Top5 99.306250    LR 0.300000    Time 0.022873    
2018-10-27 22:54:22,687 - Epoch: [45][  300/  391]    Overall Loss 0.474643    Objective Loss 0.474643    Top1 83.513021    Top5 99.268229    LR 0.300000    Time 0.022845    
2018-10-27 22:54:23,822 - Epoch: [45][  350/  391]    Overall Loss 0.472834    Objective Loss 0.472834    Top1 83.591518    Top5 99.267857    LR 0.300000    Time 0.022820    
2018-10-27 22:54:24,831 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51142 | -0.01042 |    0.19541 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15855 | -0.00476 |    0.03570 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15113 |  0.00183 |    0.03877 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15260 | -0.00486 |    0.03691 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12381 | -0.00248 |    0.02558 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15862 | -0.00790 |    0.04607 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12058 | -0.00179 |    0.02842 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17006 | -0.00452 |    0.06242 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13546 | -0.00294 |    0.04350 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20282 | -0.00818 |    0.06845 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11178 | -0.00255 |    0.03225 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09392 | -0.00197 |    0.02333 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12277 | -0.00320 |    0.03733 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09429 | -0.00525 |    0.02475 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11725 | -0.00200 |    0.04021 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10942 | -0.00535 |    0.03868 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11411 | -0.00098 |    0.02954 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09955 | -0.00343 |    0.03170 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08133 | -0.00338 |    0.02187 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07533 |  0.00003 |    0.02068 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05046 |  0.00181 |    0.00942 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46070 | -0.08215 |    0.23752 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:54:24,832 - Total sparsity: 88.62

2018-10-27 22:54:24,832 - --- validate (epoch=45)-----------
2018-10-27 22:54:24,832 - 10000 samples (128 per mini-batch)
2018-10-27 22:54:25,555 - Epoch: [45][   50/   78]    Loss 0.740056    Top1 76.187500    Top5 98.578125    
2018-10-27 22:54:25,949 - ==> Top1: 75.670    Top5: 98.550    Loss: 0.751

2018-10-27 22:54:25,950 - ==> Best Top1: 81.480   On Epoch: 28

2018-10-27 22:54:25,950 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:54:25,961 - 

2018-10-27 22:54:25,962 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:54:27,151 - Epoch: [46][   50/  391]    Overall Loss 0.455554    Objective Loss 0.455554    Top1 83.906250    Top5 99.375000    LR 0.300000    Time 0.023762    
2018-10-27 22:54:28,287 - Epoch: [46][  100/  391]    Overall Loss 0.451094    Objective Loss 0.451094    Top1 84.210938    Top5 99.359375    LR 0.300000    Time 0.023219    
2018-10-27 22:54:29,422 - Epoch: [46][  150/  391]    Overall Loss 0.460414    Objective Loss 0.460414    Top1 83.776042    Top5 99.348958    LR 0.300000    Time 0.023040    
2018-10-27 22:54:30,556 - Epoch: [46][  200/  391]    Overall Loss 0.466590    Objective Loss 0.466590    Top1 83.726562    Top5 99.316406    LR 0.300000    Time 0.022941    
2018-10-27 22:54:31,689 - Epoch: [46][  250/  391]    Overall Loss 0.470883    Objective Loss 0.470883    Top1 83.575000    Top5 99.300000    LR 0.300000    Time 0.022881    
2018-10-27 22:54:32,829 - Epoch: [46][  300/  391]    Overall Loss 0.469115    Objective Loss 0.469115    Top1 83.630208    Top5 99.309896    LR 0.300000    Time 0.022864    
2018-10-27 22:54:33,968 - Epoch: [46][  350/  391]    Overall Loss 0.468058    Objective Loss 0.468058    Top1 83.720982    Top5 99.301339    LR 0.300000    Time 0.022847    
2018-10-27 22:54:34,985 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51130 | -0.00456 |    0.19602 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15843 | -0.00594 |    0.03566 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15153 |  0.00243 |    0.03870 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15399 | -0.00493 |    0.03715 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12408 | -0.00214 |    0.02546 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15921 | -0.00766 |    0.04617 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12046 | -0.00180 |    0.02839 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17075 | -0.00478 |    0.06260 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13533 | -0.00364 |    0.04312 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20381 | -0.01085 |    0.06731 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11178 | -0.00276 |    0.03198 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09403 | -0.00185 |    0.02330 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12282 | -0.00393 |    0.03740 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09424 | -0.00498 |    0.02474 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11759 | -0.00208 |    0.04029 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10959 | -0.00536 |    0.03873 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11377 | -0.00126 |    0.02930 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09973 | -0.00339 |    0.03178 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08142 | -0.00327 |    0.02187 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07546 | -0.00007 |    0.02066 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05041 |  0.00175 |    0.00944 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46463 | -0.08089 |    0.23895 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:54:34,985 - Total sparsity: 88.62

2018-10-27 22:54:34,985 - --- validate (epoch=46)-----------
2018-10-27 22:54:34,985 - 10000 samples (128 per mini-batch)
2018-10-27 22:54:35,705 - Epoch: [46][   50/   78]    Loss 0.646878    Top1 79.328125    Top5 98.953125    
2018-10-27 22:54:36,098 - ==> Top1: 79.390    Top5: 98.900    Loss: 0.649

2018-10-27 22:54:36,099 - ==> Best Top1: 81.480   On Epoch: 28

2018-10-27 22:54:36,099 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:54:36,110 - 

2018-10-27 22:54:36,110 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:54:37,308 - Epoch: [47][   50/  391]    Overall Loss 0.482770    Objective Loss 0.482770    Top1 83.843750    Top5 99.093750    LR 0.300000    Time 0.023929    
2018-10-27 22:54:38,449 - Epoch: [47][  100/  391]    Overall Loss 0.481385    Objective Loss 0.481385    Top1 83.828125    Top5 99.171875    LR 0.300000    Time 0.023366    
2018-10-27 22:54:39,590 - Epoch: [47][  150/  391]    Overall Loss 0.475206    Objective Loss 0.475206    Top1 83.843750    Top5 99.203125    LR 0.300000    Time 0.023174    
2018-10-27 22:54:40,732 - Epoch: [47][  200/  391]    Overall Loss 0.474921    Objective Loss 0.474921    Top1 83.859375    Top5 99.222656    LR 0.300000    Time 0.023065    
2018-10-27 22:54:41,874 - Epoch: [47][  250/  391]    Overall Loss 0.473560    Objective Loss 0.473560    Top1 83.890625    Top5 99.271875    LR 0.300000    Time 0.023014    
2018-10-27 22:54:43,016 - Epoch: [47][  300/  391]    Overall Loss 0.474023    Objective Loss 0.474023    Top1 83.877604    Top5 99.268229    LR 0.300000    Time 0.022982    
2018-10-27 22:54:44,158 - Epoch: [47][  350/  391]    Overall Loss 0.472255    Objective Loss 0.472255    Top1 83.948661    Top5 99.265625    LR 0.300000    Time 0.022959    
2018-10-27 22:54:45,171 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.50952 | -0.01145 |    0.19496 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15802 | -0.00445 |    0.03572 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15146 |  0.00284 |    0.03861 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15292 | -0.00523 |    0.03691 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12463 | -0.00179 |    0.02601 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16002 | -0.00906 |    0.04573 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12066 | -0.00163 |    0.02850 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17102 | -0.00395 |    0.06286 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13557 | -0.00330 |    0.04337 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20301 | -0.00967 |    0.06641 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11226 | -0.00280 |    0.03225 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09401 | -0.00209 |    0.02331 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12292 | -0.00347 |    0.03743 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09421 | -0.00487 |    0.02463 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11741 | -0.00221 |    0.04049 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10963 | -0.00524 |    0.03865 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11336 | -0.00180 |    0.02930 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09953 | -0.00335 |    0.03166 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08117 | -0.00326 |    0.02179 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07543 | -0.00010 |    0.02066 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05037 |  0.00192 |    0.00942 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46452 | -0.07987 |    0.23833 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:54:45,171 - Total sparsity: 88.62

2018-10-27 22:54:45,171 - --- validate (epoch=47)-----------
2018-10-27 22:54:45,171 - 10000 samples (128 per mini-batch)
2018-10-27 22:54:45,897 - Epoch: [47][   50/   78]    Loss 0.685231    Top1 78.046875    Top5 98.312500    
2018-10-27 22:54:46,290 - ==> Top1: 77.480    Top5: 98.300    Loss: 0.706

2018-10-27 22:54:46,291 - ==> Best Top1: 81.480   On Epoch: 28

2018-10-27 22:54:46,291 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:54:46,301 - 

2018-10-27 22:54:46,301 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:54:47,497 - Epoch: [48][   50/  391]    Overall Loss 0.472296    Objective Loss 0.472296    Top1 83.531250    Top5 99.312500    LR 0.300000    Time 0.023874    
2018-10-27 22:54:48,636 - Epoch: [48][  100/  391]    Overall Loss 0.463520    Objective Loss 0.463520    Top1 83.851562    Top5 99.320312    LR 0.300000    Time 0.023320    
2018-10-27 22:54:49,778 - Epoch: [48][  150/  391]    Overall Loss 0.464444    Objective Loss 0.464444    Top1 83.802083    Top5 99.343750    LR 0.300000    Time 0.023149    
2018-10-27 22:54:50,919 - Epoch: [48][  200/  391]    Overall Loss 0.465118    Objective Loss 0.465118    Top1 83.781250    Top5 99.324219    LR 0.300000    Time 0.023060    
2018-10-27 22:54:52,059 - Epoch: [48][  250/  391]    Overall Loss 0.464458    Objective Loss 0.464458    Top1 83.753125    Top5 99.318750    LR 0.300000    Time 0.022991    
2018-10-27 22:54:53,201 - Epoch: [48][  300/  391]    Overall Loss 0.468339    Objective Loss 0.468339    Top1 83.598958    Top5 99.320312    LR 0.300000    Time 0.022961    
2018-10-27 22:54:54,343 - Epoch: [48][  350/  391]    Overall Loss 0.469710    Objective Loss 0.469710    Top1 83.573661    Top5 99.323661    LR 0.300000    Time 0.022939    
2018-10-27 22:54:55,359 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.50880 | -0.00571 |    0.19622 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15812 | -0.00449 |    0.03548 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15112 |  0.00300 |    0.03843 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15290 | -0.00383 |    0.03717 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12518 | -0.00314 |    0.02577 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16011 | -0.00901 |    0.04619 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12059 | -0.00154 |    0.02822 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17122 | -0.00331 |    0.06308 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13552 | -0.00312 |    0.04342 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20273 | -0.00987 |    0.06640 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11198 | -0.00293 |    0.03221 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09381 | -0.00178 |    0.02307 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12267 | -0.00320 |    0.03724 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09384 | -0.00507 |    0.02463 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11722 | -0.00184 |    0.04020 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10958 | -0.00550 |    0.03858 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11316 | -0.00122 |    0.02916 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09969 | -0.00337 |    0.03168 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08127 | -0.00325 |    0.02179 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07553 |  0.00002 |    0.02072 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05051 |  0.00189 |    0.00942 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46513 | -0.08202 |    0.23884 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:54:55,359 - Total sparsity: 88.62

2018-10-27 22:54:55,360 - --- validate (epoch=48)-----------
2018-10-27 22:54:55,360 - 10000 samples (128 per mini-batch)
2018-10-27 22:54:56,081 - Epoch: [48][   50/   78]    Loss 0.554802    Top1 81.718750    Top5 99.015625    
2018-10-27 22:54:56,473 - ==> Top1: 81.590    Top5: 98.970    Loss: 0.560

2018-10-27 22:54:56,473 - ==> Best Top1: 81.590   On Epoch: 48

2018-10-27 22:54:56,473 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:54:56,488 - 

2018-10-27 22:54:56,488 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:54:57,660 - Epoch: [49][   50/  391]    Overall Loss 0.478465    Objective Loss 0.478465    Top1 83.562500    Top5 99.203125    LR 0.300000    Time 0.023402    
2018-10-27 22:54:58,802 - Epoch: [49][  100/  391]    Overall Loss 0.468487    Objective Loss 0.468487    Top1 84.046875    Top5 99.203125    LR 0.300000    Time 0.023110    
2018-10-27 22:54:59,945 - Epoch: [49][  150/  391]    Overall Loss 0.468804    Objective Loss 0.468804    Top1 83.885417    Top5 99.244792    LR 0.300000    Time 0.023016    
2018-10-27 22:55:01,089 - Epoch: [49][  200/  391]    Overall Loss 0.468011    Objective Loss 0.468011    Top1 83.820312    Top5 99.238281    LR 0.300000    Time 0.022977    
2018-10-27 22:55:02,231 - Epoch: [49][  250/  391]    Overall Loss 0.468705    Objective Loss 0.468705    Top1 83.771875    Top5 99.253125    LR 0.300000    Time 0.022928    
2018-10-27 22:55:03,374 - Epoch: [49][  300/  391]    Overall Loss 0.469447    Objective Loss 0.469447    Top1 83.796875    Top5 99.276042    LR 0.300000    Time 0.022914    
2018-10-27 22:55:04,518 - Epoch: [49][  350/  391]    Overall Loss 0.468859    Objective Loss 0.468859    Top1 83.810268    Top5 99.305804    LR 0.300000    Time 0.022904    
2018-10-27 22:55:05,532 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.50829 | -0.00642 |    0.19592 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15842 | -0.00479 |    0.03574 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15171 |  0.00212 |    0.03816 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15336 | -0.00506 |    0.03735 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12494 | -0.00310 |    0.02571 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15945 | -0.00806 |    0.04636 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12080 | -0.00100 |    0.02861 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17116 | -0.00378 |    0.06271 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13527 | -0.00341 |    0.04343 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20420 | -0.01015 |    0.06624 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11208 | -0.00258 |    0.03233 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09386 | -0.00140 |    0.02329 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12330 | -0.00335 |    0.03742 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09461 | -0.00591 |    0.02486 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11717 | -0.00206 |    0.04012 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10976 | -0.00521 |    0.03877 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11402 | -0.00155 |    0.02930 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09954 | -0.00323 |    0.03168 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08122 | -0.00311 |    0.02177 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07576 | -0.00001 |    0.02078 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05045 |  0.00178 |    0.00940 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46166 | -0.08088 |    0.23768 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:55:05,533 - Total sparsity: 88.62

2018-10-27 22:55:05,533 - --- validate (epoch=49)-----------
2018-10-27 22:55:05,533 - 10000 samples (128 per mini-batch)
2018-10-27 22:55:06,255 - Epoch: [49][   50/   78]    Loss 0.602479    Top1 80.125000    Top5 99.046875    
2018-10-27 22:55:06,644 - ==> Top1: 80.170    Top5: 99.030    Loss: 0.600

2018-10-27 22:55:06,645 - ==> Best Top1: 81.590   On Epoch: 48

2018-10-27 22:55:06,645 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:55:06,655 - 

2018-10-27 22:55:06,656 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:55:07,854 - Epoch: [50][   50/  391]    Overall Loss 0.436273    Objective Loss 0.436273    Top1 84.671875    Top5 99.406250    LR 0.300000    Time 0.023932    
2018-10-27 22:55:08,994 - Epoch: [50][  100/  391]    Overall Loss 0.453516    Objective Loss 0.453516    Top1 84.156250    Top5 99.312500    LR 0.300000    Time 0.023354    
2018-10-27 22:55:10,134 - Epoch: [50][  150/  391]    Overall Loss 0.455374    Objective Loss 0.455374    Top1 84.067708    Top5 99.286458    LR 0.300000    Time 0.023161    
2018-10-27 22:55:11,276 - Epoch: [50][  200/  391]    Overall Loss 0.463674    Objective Loss 0.463674    Top1 83.843750    Top5 99.246094    LR 0.300000    Time 0.023074    
2018-10-27 22:55:12,419 - Epoch: [50][  250/  391]    Overall Loss 0.466468    Objective Loss 0.466468    Top1 83.831250    Top5 99.275000    LR 0.300000    Time 0.023024    
2018-10-27 22:55:13,561 - Epoch: [50][  300/  391]    Overall Loss 0.469123    Objective Loss 0.469123    Top1 83.778646    Top5 99.255208    LR 0.300000    Time 0.022991    
2018-10-27 22:55:14,702 - Epoch: [50][  350/  391]    Overall Loss 0.472798    Objective Loss 0.472798    Top1 83.640625    Top5 99.263393    LR 0.300000    Time 0.022961    
2018-10-27 22:55:15,715 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.50605 | -0.00380 |    0.19560 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15959 | -0.00591 |    0.03613 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15265 |  0.00271 |    0.03831 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15326 | -0.00478 |    0.03815 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12389 | -0.00320 |    0.02546 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15941 | -0.00887 |    0.04655 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12076 | -0.00192 |    0.02804 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17173 | -0.00532 |    0.06325 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13513 | -0.00376 |    0.04333 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20434 | -0.01120 |    0.06672 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11228 | -0.00199 |    0.03232 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09400 | -0.00130 |    0.02337 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12336 | -0.00322 |    0.03740 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09488 | -0.00531 |    0.02471 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11745 | -0.00213 |    0.04031 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.11000 | -0.00524 |    0.03864 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11465 | -0.00067 |    0.02964 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09955 | -0.00370 |    0.03168 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08115 | -0.00373 |    0.02171 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07585 | -0.00001 |    0.02075 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05060 |  0.00188 |    0.00944 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46322 | -0.08158 |    0.23798 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:55:15,715 - Total sparsity: 88.62

2018-10-27 22:55:15,715 - --- validate (epoch=50)-----------
2018-10-27 22:55:15,715 - 10000 samples (128 per mini-batch)
2018-10-27 22:55:16,434 - Epoch: [50][   50/   78]    Loss 0.600647    Top1 80.437500    Top5 98.640625    
2018-10-27 22:55:16,824 - ==> Top1: 80.420    Top5: 98.770    Loss: 0.590

2018-10-27 22:55:16,825 - ==> Best Top1: 81.590   On Epoch: 48

2018-10-27 22:55:16,825 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:55:16,839 - 

2018-10-27 22:55:16,839 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:55:18,036 - Epoch: [51][   50/  391]    Overall Loss 0.449574    Objective Loss 0.449574    Top1 84.406250    Top5 99.375000    LR 0.300000    Time 0.023894    
2018-10-27 22:55:19,175 - Epoch: [51][  100/  391]    Overall Loss 0.466574    Objective Loss 0.466574    Top1 84.109375    Top5 99.304688    LR 0.300000    Time 0.023331    
2018-10-27 22:55:20,315 - Epoch: [51][  150/  391]    Overall Loss 0.462788    Objective Loss 0.462788    Top1 84.140625    Top5 99.276042    LR 0.300000    Time 0.023141    
2018-10-27 22:55:21,454 - Epoch: [51][  200/  391]    Overall Loss 0.467321    Objective Loss 0.467321    Top1 83.902344    Top5 99.250000    LR 0.300000    Time 0.023047    
2018-10-27 22:55:22,595 - Epoch: [51][  250/  391]    Overall Loss 0.471899    Objective Loss 0.471899    Top1 83.796875    Top5 99.253125    LR 0.300000    Time 0.022994    
2018-10-27 22:55:23,738 - Epoch: [51][  300/  391]    Overall Loss 0.472312    Objective Loss 0.472312    Top1 83.789062    Top5 99.257812    LR 0.300000    Time 0.022968    
2018-10-27 22:55:24,879 - Epoch: [51][  350/  391]    Overall Loss 0.474761    Objective Loss 0.474761    Top1 83.622768    Top5 99.276786    LR 0.300000    Time 0.022943    
2018-10-27 22:55:25,897 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51013 | -0.00499 |    0.19432 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.16068 | -0.00510 |    0.03628 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15345 |  0.00269 |    0.03854 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15307 | -0.00491 |    0.03792 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12401 | -0.00271 |    0.02535 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15947 | -0.00622 |    0.04607 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12036 | -0.00187 |    0.02792 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17154 | -0.00336 |    0.06287 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13491 | -0.00347 |    0.04307 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20322 | -0.01228 |    0.06632 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11259 | -0.00262 |    0.03256 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09407 | -0.00166 |    0.02331 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12304 | -0.00303 |    0.03745 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09472 | -0.00524 |    0.02454 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11739 | -0.00193 |    0.04030 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10974 | -0.00501 |    0.03847 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11451 | -0.00047 |    0.02967 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09950 | -0.00367 |    0.03162 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08114 | -0.00348 |    0.02174 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07567 |  0.00005 |    0.02075 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05056 |  0.00185 |    0.00944 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46409 | -0.08130 |    0.23788 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:55:25,898 - Total sparsity: 88.62

2018-10-27 22:55:25,898 - --- validate (epoch=51)-----------
2018-10-27 22:55:25,898 - 10000 samples (128 per mini-batch)
2018-10-27 22:55:26,621 - Epoch: [51][   50/   78]    Loss 0.585790    Top1 80.468750    Top5 98.937500    
2018-10-27 22:55:27,012 - ==> Top1: 80.640    Top5: 99.040    Loss: 0.582

2018-10-27 22:55:27,013 - ==> Best Top1: 81.590   On Epoch: 48

2018-10-27 22:55:27,013 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:55:27,031 - 

2018-10-27 22:55:27,031 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:55:28,203 - Epoch: [52][   50/  391]    Overall Loss 0.462254    Objective Loss 0.462254    Top1 84.203125    Top5 99.406250    LR 0.300000    Time 0.023403    
2018-10-27 22:55:29,344 - Epoch: [52][  100/  391]    Overall Loss 0.468926    Objective Loss 0.468926    Top1 84.015625    Top5 99.359375    LR 0.300000    Time 0.023094    
2018-10-27 22:55:30,484 - Epoch: [52][  150/  391]    Overall Loss 0.468960    Objective Loss 0.468960    Top1 83.994792    Top5 99.354167    LR 0.300000    Time 0.022990    
2018-10-27 22:55:31,625 - Epoch: [52][  200/  391]    Overall Loss 0.467023    Objective Loss 0.467023    Top1 83.949219    Top5 99.347656    LR 0.300000    Time 0.022942    
2018-10-27 22:55:32,766 - Epoch: [52][  250/  391]    Overall Loss 0.467559    Objective Loss 0.467559    Top1 83.881250    Top5 99.353125    LR 0.300000    Time 0.022910    
2018-10-27 22:55:33,905 - Epoch: [52][  300/  391]    Overall Loss 0.467726    Objective Loss 0.467726    Top1 83.932292    Top5 99.356771    LR 0.300000    Time 0.022885    
2018-10-27 22:55:35,046 - Epoch: [52][  350/  391]    Overall Loss 0.470987    Objective Loss 0.470987    Top1 83.763393    Top5 99.368304    LR 0.300000    Time 0.022873    
2018-10-27 22:55:36,060 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.50919 | -0.00731 |    0.19539 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.16044 | -0.00530 |    0.03574 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15338 |  0.00156 |    0.03923 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15333 | -0.00512 |    0.03773 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12390 | -0.00249 |    0.02560 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16034 | -0.00847 |    0.04621 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12104 | -0.00158 |    0.02831 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17139 | -0.00359 |    0.06263 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13510 | -0.00301 |    0.04302 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20165 | -0.01191 |    0.06616 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11284 | -0.00278 |    0.03254 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09432 | -0.00181 |    0.02333 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12356 | -0.00325 |    0.03749 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09505 | -0.00571 |    0.02491 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11749 | -0.00202 |    0.04036 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10966 | -0.00485 |    0.03857 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11383 |  0.00009 |    0.02947 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09950 | -0.00336 |    0.03167 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08118 | -0.00343 |    0.02175 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07575 | -0.00002 |    0.02074 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05080 |  0.00192 |    0.00942 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46279 | -0.08303 |    0.23828 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:55:36,061 - Total sparsity: 88.62

2018-10-27 22:55:36,061 - --- validate (epoch=52)-----------
2018-10-27 22:55:36,061 - 10000 samples (128 per mini-batch)
2018-10-27 22:55:36,787 - Epoch: [52][   50/   78]    Loss 0.626785    Top1 78.765625    Top5 98.781250    
2018-10-27 22:55:37,181 - ==> Top1: 78.870    Top5: 98.790    Loss: 0.622

2018-10-27 22:55:37,182 - ==> Best Top1: 81.590   On Epoch: 48

2018-10-27 22:55:37,182 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:55:37,193 - 

2018-10-27 22:55:37,193 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:55:38,394 - Epoch: [53][   50/  391]    Overall Loss 0.459549    Objective Loss 0.459549    Top1 83.906250    Top5 99.296875    LR 0.300000    Time 0.023976    
2018-10-27 22:55:39,533 - Epoch: [53][  100/  391]    Overall Loss 0.478418    Objective Loss 0.478418    Top1 83.125000    Top5 99.273438    LR 0.300000    Time 0.023371    
2018-10-27 22:55:40,673 - Epoch: [53][  150/  391]    Overall Loss 0.470397    Objective Loss 0.470397    Top1 83.557292    Top5 99.307292    LR 0.300000    Time 0.023170    
2018-10-27 22:55:41,815 - Epoch: [53][  200/  391]    Overall Loss 0.468564    Objective Loss 0.468564    Top1 83.566406    Top5 99.332031    LR 0.300000    Time 0.023063    
2018-10-27 22:55:42,958 - Epoch: [53][  250/  391]    Overall Loss 0.469094    Objective Loss 0.469094    Top1 83.537500    Top5 99.321875    LR 0.300000    Time 0.023015    
2018-10-27 22:55:44,099 - Epoch: [53][  300/  391]    Overall Loss 0.469043    Objective Loss 0.469043    Top1 83.643229    Top5 99.302083    LR 0.300000    Time 0.022979    
2018-10-27 22:55:45,239 - Epoch: [53][  350/  391]    Overall Loss 0.470862    Objective Loss 0.470862    Top1 83.658482    Top5 99.283482    LR 0.300000    Time 0.022952    
2018-10-27 22:55:46,253 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51301 | -0.00843 |    0.19498 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15970 | -0.00483 |    0.03606 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15208 |  0.00156 |    0.03806 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15378 | -0.00406 |    0.03786 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12429 | -0.00321 |    0.02557 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15997 | -0.00827 |    0.04583 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12137 | -0.00105 |    0.02835 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17144 | -0.00421 |    0.06312 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13504 | -0.00360 |    0.04314 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20112 | -0.01065 |    0.06542 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11302 | -0.00209 |    0.03261 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09422 | -0.00176 |    0.02359 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12317 | -0.00291 |    0.03713 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09496 | -0.00571 |    0.02478 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11716 | -0.00220 |    0.04013 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10954 | -0.00500 |    0.03849 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11354 | -0.00064 |    0.02952 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09937 | -0.00364 |    0.03165 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08120 | -0.00330 |    0.02175 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07588 | -0.00003 |    0.02073 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05090 |  0.00202 |    0.00944 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46204 | -0.08202 |    0.23850 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:55:46,253 - Total sparsity: 88.62

2018-10-27 22:55:46,253 - --- validate (epoch=53)-----------
2018-10-27 22:55:46,253 - 10000 samples (128 per mini-batch)
2018-10-27 22:55:46,973 - Epoch: [53][   50/   78]    Loss 0.569769    Top1 81.359375    Top5 99.031250    
2018-10-27 22:55:47,363 - ==> Top1: 81.570    Top5: 99.000    Loss: 0.560

2018-10-27 22:55:47,364 - ==> Best Top1: 81.590   On Epoch: 48

2018-10-27 22:55:47,364 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:55:47,376 - 

2018-10-27 22:55:47,376 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:55:48,574 - Epoch: [54][   50/  391]    Overall Loss 0.453032    Objective Loss 0.453032    Top1 84.171875    Top5 99.359375    LR 0.300000    Time 0.023925    
2018-10-27 22:55:49,715 - Epoch: [54][  100/  391]    Overall Loss 0.464707    Objective Loss 0.464707    Top1 83.789062    Top5 99.343750    LR 0.300000    Time 0.023365    
2018-10-27 22:55:50,857 - Epoch: [54][  150/  391]    Overall Loss 0.469297    Objective Loss 0.469297    Top1 83.786458    Top5 99.328125    LR 0.300000    Time 0.023181    
2018-10-27 22:55:51,998 - Epoch: [54][  200/  391]    Overall Loss 0.465541    Objective Loss 0.465541    Top1 83.910156    Top5 99.355469    LR 0.300000    Time 0.023082    
2018-10-27 22:55:53,139 - Epoch: [54][  250/  391]    Overall Loss 0.465429    Objective Loss 0.465429    Top1 83.953125    Top5 99.334375    LR 0.300000    Time 0.023024    
2018-10-27 22:55:54,281 - Epoch: [54][  300/  391]    Overall Loss 0.465630    Objective Loss 0.465630    Top1 83.942708    Top5 99.328125    LR 0.300000    Time 0.022989    
2018-10-27 22:55:55,421 - Epoch: [54][  350/  391]    Overall Loss 0.469197    Objective Loss 0.469197    Top1 83.805804    Top5 99.330357    LR 0.300000    Time 0.022959    
2018-10-27 22:55:56,434 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51707 | -0.00232 |    0.19981 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15995 | -0.00488 |    0.03654 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15150 |  0.00146 |    0.03796 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15353 | -0.00429 |    0.03721 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12404 | -0.00282 |    0.02555 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16136 | -0.00754 |    0.04645 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12149 | -0.00073 |    0.02821 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17146 | -0.00548 |    0.06319 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13532 | -0.00374 |    0.04354 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20131 | -0.00966 |    0.06557 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11290 | -0.00230 |    0.03261 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09407 | -0.00163 |    0.02331 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12328 | -0.00305 |    0.03725 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09476 | -0.00538 |    0.02458 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11748 | -0.00213 |    0.04028 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10965 | -0.00523 |    0.03848 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11350 | -0.00003 |    0.02927 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09952 | -0.00372 |    0.03162 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08130 | -0.00322 |    0.02171 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07575 | -0.00000 |    0.02070 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05083 |  0.00193 |    0.00946 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.45608 | -0.08087 |    0.23526 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:55:56,434 - Total sparsity: 88.62

2018-10-27 22:55:56,434 - --- validate (epoch=54)-----------
2018-10-27 22:55:56,434 - 10000 samples (128 per mini-batch)
2018-10-27 22:55:57,151 - Epoch: [54][   50/   78]    Loss 0.879352    Top1 73.156250    Top5 98.406250    
2018-10-27 22:55:57,541 - ==> Top1: 72.910    Top5: 98.300    Loss: 0.889

2018-10-27 22:55:57,542 - ==> Best Top1: 81.590   On Epoch: 48

2018-10-27 22:55:57,542 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:55:57,551 - 

2018-10-27 22:55:57,552 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:55:58,748 - Epoch: [55][   50/  391]    Overall Loss 0.458130    Objective Loss 0.458130    Top1 84.125000    Top5 99.265625    LR 0.300000    Time 0.023896    
2018-10-27 22:55:59,890 - Epoch: [55][  100/  391]    Overall Loss 0.459821    Objective Loss 0.459821    Top1 83.968750    Top5 99.320312    LR 0.300000    Time 0.023354    
2018-10-27 22:56:01,035 - Epoch: [55][  150/  391]    Overall Loss 0.463310    Objective Loss 0.463310    Top1 83.807292    Top5 99.328125    LR 0.300000    Time 0.023194    
2018-10-27 22:56:02,179 - Epoch: [55][  200/  391]    Overall Loss 0.467220    Objective Loss 0.467220    Top1 83.691406    Top5 99.300781    LR 0.300000    Time 0.023107    
2018-10-27 22:56:03,322 - Epoch: [55][  250/  391]    Overall Loss 0.470828    Objective Loss 0.470828    Top1 83.634375    Top5 99.265625    LR 0.300000    Time 0.023052    
2018-10-27 22:56:04,465 - Epoch: [55][  300/  391]    Overall Loss 0.468621    Objective Loss 0.468621    Top1 83.658854    Top5 99.278646    LR 0.300000    Time 0.023018    
2018-10-27 22:56:05,608 - Epoch: [55][  350/  391]    Overall Loss 0.468862    Objective Loss 0.468862    Top1 83.660714    Top5 99.279018    LR 0.300000    Time 0.022990    
2018-10-27 22:56:06,621 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51558 | -0.00639 |    0.19847 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15912 | -0.00611 |    0.03584 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15173 |  0.00193 |    0.03790 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15189 | -0.00398 |    0.03686 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12290 | -0.00290 |    0.02525 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16058 | -0.00677 |    0.04652 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12142 | -0.00132 |    0.02823 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17237 | -0.00476 |    0.06361 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13542 | -0.00361 |    0.04336 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20155 | -0.00818 |    0.06590 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11223 | -0.00222 |    0.03222 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09403 | -0.00186 |    0.02336 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12325 | -0.00293 |    0.03713 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09450 | -0.00514 |    0.02441 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11709 | -0.00195 |    0.04016 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10937 | -0.00504 |    0.03842 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11308 | -0.00097 |    0.02922 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09918 | -0.00357 |    0.03144 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08079 | -0.00332 |    0.02157 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07563 | -0.00004 |    0.02066 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05062 |  0.00199 |    0.00938 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46215 | -0.08005 |    0.23739 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:56:06,621 - Total sparsity: 88.62

2018-10-27 22:56:06,621 - --- validate (epoch=55)-----------
2018-10-27 22:56:06,621 - 10000 samples (128 per mini-batch)
2018-10-27 22:56:07,343 - Epoch: [55][   50/   78]    Loss 0.670394    Top1 77.875000    Top5 98.859375    
2018-10-27 22:56:07,731 - ==> Top1: 77.830    Top5: 98.870    Loss: 0.674

2018-10-27 22:56:07,732 - ==> Best Top1: 81.590   On Epoch: 48

2018-10-27 22:56:07,732 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:56:07,753 - 

2018-10-27 22:56:07,753 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:56:08,924 - Epoch: [56][   50/  391]    Overall Loss 0.456743    Objective Loss 0.456743    Top1 84.343750    Top5 99.375000    LR 0.300000    Time 0.023379    
2018-10-27 22:56:10,066 - Epoch: [56][  100/  391]    Overall Loss 0.470209    Objective Loss 0.470209    Top1 83.835938    Top5 99.406250    LR 0.300000    Time 0.023097    
2018-10-27 22:56:11,208 - Epoch: [56][  150/  391]    Overall Loss 0.469200    Objective Loss 0.469200    Top1 84.098958    Top5 99.333333    LR 0.300000    Time 0.023001    
2018-10-27 22:56:12,351 - Epoch: [56][  200/  391]    Overall Loss 0.464149    Objective Loss 0.464149    Top1 84.226562    Top5 99.347656    LR 0.300000    Time 0.022959    
2018-10-27 22:56:13,493 - Epoch: [56][  250/  391]    Overall Loss 0.465112    Objective Loss 0.465112    Top1 84.187500    Top5 99.359375    LR 0.300000    Time 0.022931    
2018-10-27 22:56:14,635 - Epoch: [56][  300/  391]    Overall Loss 0.470753    Objective Loss 0.470753    Top1 83.994792    Top5 99.320312    LR 0.300000    Time 0.022913    
2018-10-27 22:56:15,777 - Epoch: [56][  350/  391]    Overall Loss 0.471092    Objective Loss 0.471092    Top1 83.991071    Top5 99.328125    LR 0.300000    Time 0.022887    
2018-10-27 22:56:16,791 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51041 | -0.00607 |    0.19617 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15862 | -0.00596 |    0.03602 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15155 |  0.00125 |    0.03826 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15348 | -0.00476 |    0.03717 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12349 | -0.00331 |    0.02502 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16037 | -0.00878 |    0.04524 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12090 | -0.00140 |    0.02834 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17245 | -0.00540 |    0.06301 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13538 | -0.00394 |    0.04332 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20108 | -0.00895 |    0.06552 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11183 | -0.00192 |    0.03206 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09396 | -0.00161 |    0.02337 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12286 | -0.00242 |    0.03714 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09456 | -0.00529 |    0.02461 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11726 | -0.00183 |    0.04020 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10955 | -0.00527 |    0.03838 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11245 | -0.00078 |    0.02920 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09936 | -0.00324 |    0.03152 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08088 | -0.00335 |    0.02168 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07564 | -0.00002 |    0.02064 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05066 |  0.00194 |    0.00940 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46256 | -0.08140 |    0.23834 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:56:16,791 - Total sparsity: 88.62

2018-10-27 22:56:16,791 - --- validate (epoch=56)-----------
2018-10-27 22:56:16,791 - 10000 samples (128 per mini-batch)
2018-10-27 22:56:17,513 - Epoch: [56][   50/   78]    Loss 0.833176    Top1 74.765625    Top5 97.828125    
2018-10-27 22:56:17,905 - ==> Top1: 75.020    Top5: 97.890    Loss: 0.826

2018-10-27 22:56:17,906 - ==> Best Top1: 81.590   On Epoch: 48

2018-10-27 22:56:17,906 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:56:17,920 - 

2018-10-27 22:56:17,920 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:56:19,118 - Epoch: [57][   50/  391]    Overall Loss 0.487118    Objective Loss 0.487118    Top1 83.093750    Top5 99.390625    LR 0.300000    Time 0.023918    
2018-10-27 22:56:20,259 - Epoch: [57][  100/  391]    Overall Loss 0.476619    Objective Loss 0.476619    Top1 83.304688    Top5 99.382812    LR 0.300000    Time 0.023360    
2018-10-27 22:56:21,400 - Epoch: [57][  150/  391]    Overall Loss 0.471393    Objective Loss 0.471393    Top1 83.666667    Top5 99.416667    LR 0.300000    Time 0.023173    
2018-10-27 22:56:22,540 - Epoch: [57][  200/  391]    Overall Loss 0.472174    Objective Loss 0.472174    Top1 83.605469    Top5 99.406250    LR 0.300000    Time 0.023072    
2018-10-27 22:56:23,682 - Epoch: [57][  250/  391]    Overall Loss 0.470858    Objective Loss 0.470858    Top1 83.740625    Top5 99.371875    LR 0.300000    Time 0.023017    
2018-10-27 22:56:24,821 - Epoch: [57][  300/  391]    Overall Loss 0.467189    Objective Loss 0.467189    Top1 83.885417    Top5 99.351562    LR 0.300000    Time 0.022974    
2018-10-27 22:56:25,960 - Epoch: [57][  350/  391]    Overall Loss 0.467362    Objective Loss 0.467362    Top1 83.857143    Top5 99.354911    LR 0.300000    Time 0.022944    
2018-10-27 22:56:26,974 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51197 | -0.00483 |    0.19606 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15903 | -0.00442 |    0.03599 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15070 |  0.00178 |    0.03816 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15334 | -0.00449 |    0.03723 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12414 | -0.00315 |    0.02520 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15977 | -0.00769 |    0.04578 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12010 | -0.00104 |    0.02815 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17171 | -0.00482 |    0.06266 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13530 | -0.00406 |    0.04348 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20077 | -0.00955 |    0.06605 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11239 | -0.00197 |    0.03216 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09416 | -0.00202 |    0.02344 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12287 | -0.00262 |    0.03715 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09452 | -0.00571 |    0.02453 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11707 | -0.00175 |    0.04026 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10941 | -0.00499 |    0.03839 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11346 |  0.00022 |    0.02926 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09933 | -0.00305 |    0.03148 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08087 | -0.00293 |    0.02165 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07563 | -0.00018 |    0.02070 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05065 |  0.00198 |    0.00938 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46648 | -0.08207 |    0.23808 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:56:26,974 - Total sparsity: 88.62

2018-10-27 22:56:26,974 - --- validate (epoch=57)-----------
2018-10-27 22:56:26,974 - 10000 samples (128 per mini-batch)
2018-10-27 22:56:27,693 - Epoch: [57][   50/   78]    Loss 0.620515    Top1 79.484375    Top5 98.734375    
2018-10-27 22:56:28,084 - ==> Top1: 79.580    Top5: 98.860    Loss: 0.618

2018-10-27 22:56:28,084 - ==> Best Top1: 81.590   On Epoch: 48

2018-10-27 22:56:28,085 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:56:28,096 - 

2018-10-27 22:56:28,096 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:56:29,292 - Epoch: [58][   50/  391]    Overall Loss 0.455839    Objective Loss 0.455839    Top1 83.984375    Top5 99.468750    LR 0.300000    Time 0.023883    
2018-10-27 22:56:30,435 - Epoch: [58][  100/  391]    Overall Loss 0.465925    Objective Loss 0.465925    Top1 83.804688    Top5 99.343750    LR 0.300000    Time 0.023361    
2018-10-27 22:56:31,578 - Epoch: [58][  150/  391]    Overall Loss 0.465527    Objective Loss 0.465527    Top1 84.000000    Top5 99.333333    LR 0.300000    Time 0.023182    
2018-10-27 22:56:32,718 - Epoch: [58][  200/  391]    Overall Loss 0.467677    Objective Loss 0.467677    Top1 83.906250    Top5 99.312500    LR 0.300000    Time 0.023062    
2018-10-27 22:56:33,860 - Epoch: [58][  250/  391]    Overall Loss 0.469410    Objective Loss 0.469410    Top1 83.781250    Top5 99.287500    LR 0.300000    Time 0.023013    
2018-10-27 22:56:35,001 - Epoch: [58][  300/  391]    Overall Loss 0.469346    Objective Loss 0.469346    Top1 83.812500    Top5 99.286458    LR 0.300000    Time 0.022976    
2018-10-27 22:56:36,143 - Epoch: [58][  350/  391]    Overall Loss 0.468839    Objective Loss 0.468839    Top1 83.854911    Top5 99.279018    LR 0.300000    Time 0.022954    
2018-10-27 22:56:37,156 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51157 | -0.01333 |    0.19770 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15996 | -0.00471 |    0.03556 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15147 |  0.00338 |    0.03789 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15213 | -0.00474 |    0.03728 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12353 | -0.00246 |    0.02521 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16195 | -0.00810 |    0.04698 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12149 | -0.00071 |    0.02846 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17161 | -0.00481 |    0.06236 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13575 | -0.00370 |    0.04363 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20171 | -0.01210 |    0.06603 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11276 | -0.00216 |    0.03225 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09429 | -0.00272 |    0.02334 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12354 | -0.00289 |    0.03733 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09494 | -0.00545 |    0.02480 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11714 | -0.00192 |    0.04030 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10957 | -0.00524 |    0.03854 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11404 | -0.00090 |    0.02926 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09924 | -0.00318 |    0.03156 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08116 | -0.00313 |    0.02169 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07578 | -0.00007 |    0.02067 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05082 |  0.00196 |    0.00939 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46772 | -0.08154 |    0.24002 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:56:37,156 - Total sparsity: 88.62

2018-10-27 22:56:37,157 - --- validate (epoch=58)-----------
2018-10-27 22:56:37,157 - 10000 samples (128 per mini-batch)
2018-10-27 22:56:37,881 - Epoch: [58][   50/   78]    Loss 0.725180    Top1 78.156250    Top5 98.078125    
2018-10-27 22:56:38,277 - ==> Top1: 77.890    Top5: 98.130    Loss: 0.734

2018-10-27 22:56:38,278 - ==> Best Top1: 81.590   On Epoch: 48

2018-10-27 22:56:38,278 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:56:38,288 - 

2018-10-27 22:56:38,289 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:56:39,485 - Epoch: [59][   50/  391]    Overall Loss 0.471404    Objective Loss 0.471404    Top1 83.906250    Top5 99.218750    LR 0.300000    Time 0.023889    
2018-10-27 22:56:40,624 - Epoch: [59][  100/  391]    Overall Loss 0.461787    Objective Loss 0.461787    Top1 83.671875    Top5 99.335938    LR 0.300000    Time 0.023315    
2018-10-27 22:56:41,762 - Epoch: [59][  150/  391]    Overall Loss 0.463489    Objective Loss 0.463489    Top1 83.697917    Top5 99.296875    LR 0.300000    Time 0.023123    
2018-10-27 22:56:42,903 - Epoch: [59][  200/  391]    Overall Loss 0.465943    Objective Loss 0.465943    Top1 83.656250    Top5 99.281250    LR 0.300000    Time 0.023043    
2018-10-27 22:56:44,043 - Epoch: [59][  250/  391]    Overall Loss 0.468373    Objective Loss 0.468373    Top1 83.528125    Top5 99.275000    LR 0.300000    Time 0.022987    
2018-10-27 22:56:45,183 - Epoch: [59][  300/  391]    Overall Loss 0.466757    Objective Loss 0.466757    Top1 83.627604    Top5 99.307292    LR 0.300000    Time 0.022952    
2018-10-27 22:56:46,324 - Epoch: [59][  350/  391]    Overall Loss 0.468774    Objective Loss 0.468774    Top1 83.575893    Top5 99.296875    LR 0.300000    Time 0.022929    
2018-10-27 22:56:47,337 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51052 | -0.00533 |    0.19921 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.16040 | -0.00578 |    0.03568 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15128 |  0.00237 |    0.03837 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15320 | -0.00400 |    0.03654 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12457 | -0.00290 |    0.02544 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16246 | -0.00671 |    0.04675 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12200 | -0.00025 |    0.02877 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17236 | -0.00516 |    0.06303 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13615 | -0.00339 |    0.04381 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20105 | -0.01269 |    0.06513 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11276 | -0.00219 |    0.03221 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09432 | -0.00241 |    0.02330 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12315 | -0.00246 |    0.03727 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09481 | -0.00577 |    0.02478 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11721 | -0.00221 |    0.04017 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10957 | -0.00531 |    0.03851 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11350 | -0.00035 |    0.02942 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09919 | -0.00311 |    0.03149 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08102 | -0.00337 |    0.02166 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07566 | -0.00009 |    0.02071 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05076 |  0.00188 |    0.00937 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46723 | -0.08200 |    0.24009 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:56:47,337 - Total sparsity: 88.62

2018-10-27 22:56:47,337 - --- validate (epoch=59)-----------
2018-10-27 22:56:47,337 - 10000 samples (128 per mini-batch)
2018-10-27 22:56:48,062 - Epoch: [59][   50/   78]    Loss 0.685572    Top1 78.781250    Top5 98.390625    
2018-10-27 22:56:48,455 - ==> Top1: 78.200    Top5: 98.540    Loss: 0.682

2018-10-27 22:56:48,456 - ==> Best Top1: 81.590   On Epoch: 48

2018-10-27 22:56:48,456 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:56:48,466 - 

2018-10-27 22:56:48,466 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:56:49,666 - Epoch: [60][   50/  391]    Overall Loss 0.458868    Objective Loss 0.458868    Top1 83.890625    Top5 99.359375    LR 0.300000    Time 0.023972    
2018-10-27 22:56:50,807 - Epoch: [60][  100/  391]    Overall Loss 0.456403    Objective Loss 0.456403    Top1 83.765625    Top5 99.445312    LR 0.300000    Time 0.023381    
2018-10-27 22:56:51,949 - Epoch: [60][  150/  391]    Overall Loss 0.454248    Objective Loss 0.454248    Top1 83.958333    Top5 99.411458    LR 0.300000    Time 0.023193    
2018-10-27 22:56:53,092 - Epoch: [60][  200/  391]    Overall Loss 0.463484    Objective Loss 0.463484    Top1 83.625000    Top5 99.421875    LR 0.300000    Time 0.023105    
2018-10-27 22:56:54,234 - Epoch: [60][  250/  391]    Overall Loss 0.466164    Objective Loss 0.466164    Top1 83.515625    Top5 99.425000    LR 0.300000    Time 0.023046    
2018-10-27 22:56:55,377 - Epoch: [60][  300/  391]    Overall Loss 0.467583    Objective Loss 0.467583    Top1 83.557292    Top5 99.388021    LR 0.300000    Time 0.023011    
2018-10-27 22:56:56,522 - Epoch: [60][  350/  391]    Overall Loss 0.469433    Objective Loss 0.469433    Top1 83.560268    Top5 99.368304    LR 0.300000    Time 0.022990    
2018-10-27 22:56:57,541 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51113 | -0.00741 |    0.19855 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15978 | -0.00549 |    0.03582 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15089 |  0.00284 |    0.03832 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15381 | -0.00411 |    0.03730 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12467 | -0.00227 |    0.02568 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16177 | -0.00706 |    0.04658 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12172 |  0.00008 |    0.02835 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17279 | -0.00521 |    0.06335 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13648 | -0.00386 |    0.04371 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20244 | -0.01395 |    0.06468 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11320 | -0.00229 |    0.03241 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09482 | -0.00195 |    0.02339 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12344 | -0.00273 |    0.03730 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09460 | -0.00503 |    0.02464 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11798 | -0.00184 |    0.04058 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10995 | -0.00551 |    0.03868 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11328 | -0.00057 |    0.02960 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09948 | -0.00301 |    0.03161 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08118 | -0.00343 |    0.02171 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07581 |  0.00011 |    0.02073 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05082 |  0.00192 |    0.00938 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46726 | -0.08076 |    0.23934 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:56:57,541 - Total sparsity: 88.62

2018-10-27 22:56:57,541 - --- validate (epoch=60)-----------
2018-10-27 22:56:57,541 - 10000 samples (128 per mini-batch)
2018-10-27 22:56:58,263 - Epoch: [60][   50/   78]    Loss 0.930582    Top1 71.890625    Top5 98.328125    
2018-10-27 22:56:58,665 - ==> Top1: 72.120    Top5: 98.360    Loss: 0.926

2018-10-27 22:56:58,666 - ==> Best Top1: 81.590   On Epoch: 48

2018-10-27 22:56:58,666 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:56:58,683 - 

2018-10-27 22:56:58,683 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:56:59,855 - Epoch: [61][   50/  391]    Overall Loss 0.473488    Objective Loss 0.473488    Top1 83.531250    Top5 99.359375    LR 0.300000    Time 0.023391    
2018-10-27 22:57:00,998 - Epoch: [61][  100/  391]    Overall Loss 0.471962    Objective Loss 0.471962    Top1 83.453125    Top5 99.312500    LR 0.300000    Time 0.023118    
2018-10-27 22:57:02,141 - Epoch: [61][  150/  391]    Overall Loss 0.476179    Objective Loss 0.476179    Top1 83.338542    Top5 99.260417    LR 0.300000    Time 0.023020    
2018-10-27 22:57:03,284 - Epoch: [61][  200/  391]    Overall Loss 0.470276    Objective Loss 0.470276    Top1 83.554688    Top5 99.281250    LR 0.300000    Time 0.022974    
2018-10-27 22:57:04,428 - Epoch: [61][  250/  391]    Overall Loss 0.472681    Objective Loss 0.472681    Top1 83.525000    Top5 99.231250    LR 0.300000    Time 0.022951    
2018-10-27 22:57:05,571 - Epoch: [61][  300/  391]    Overall Loss 0.472331    Objective Loss 0.472331    Top1 83.510417    Top5 99.247396    LR 0.300000    Time 0.022930    
2018-10-27 22:57:06,712 - Epoch: [61][  350/  391]    Overall Loss 0.470822    Objective Loss 0.470822    Top1 83.607143    Top5 99.238839    LR 0.300000    Time 0.022900    
2018-10-27 22:57:07,733 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51177 | -0.00592 |    0.19922 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15911 | -0.00515 |    0.03578 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15009 |  0.00183 |    0.03822 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15267 | -0.00370 |    0.03774 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12357 | -0.00318 |    0.02542 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16101 | -0.00726 |    0.04597 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12125 | -0.00041 |    0.02859 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17313 | -0.00450 |    0.06315 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13656 | -0.00412 |    0.04339 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20299 | -0.01045 |    0.06624 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11287 | -0.00193 |    0.03226 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09392 | -0.00165 |    0.02315 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12317 | -0.00302 |    0.03712 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09467 | -0.00487 |    0.02477 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11789 | -0.00136 |    0.04045 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.11009 | -0.00555 |    0.03871 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11291 | -0.00095 |    0.02940 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09980 | -0.00333 |    0.03168 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08118 | -0.00304 |    0.02160 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07567 |  0.00011 |    0.02076 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05065 |  0.00195 |    0.00934 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46820 | -0.08097 |    0.23920 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:57:07,733 - Total sparsity: 88.62

2018-10-27 22:57:07,733 - --- validate (epoch=61)-----------
2018-10-27 22:57:07,733 - 10000 samples (128 per mini-batch)
2018-10-27 22:57:08,454 - Epoch: [61][   50/   78]    Loss 0.674708    Top1 79.062500    Top5 98.578125    
2018-10-27 22:57:08,848 - ==> Top1: 79.020    Top5: 98.660    Loss: 0.672

2018-10-27 22:57:08,849 - ==> Best Top1: 81.590   On Epoch: 48

2018-10-27 22:57:08,849 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:57:08,870 - 

2018-10-27 22:57:08,870 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:57:10,041 - Epoch: [62][   50/  391]    Overall Loss 0.463179    Objective Loss 0.463179    Top1 83.625000    Top5 99.312500    LR 0.300000    Time 0.023385    
2018-10-27 22:57:11,182 - Epoch: [62][  100/  391]    Overall Loss 0.456479    Objective Loss 0.456479    Top1 84.203125    Top5 99.281250    LR 0.300000    Time 0.023091    
2018-10-27 22:57:12,324 - Epoch: [62][  150/  391]    Overall Loss 0.460848    Objective Loss 0.460848    Top1 84.098958    Top5 99.250000    LR 0.300000    Time 0.022999    
2018-10-27 22:57:13,466 - Epoch: [62][  200/  391]    Overall Loss 0.467638    Objective Loss 0.467638    Top1 83.914062    Top5 99.250000    LR 0.300000    Time 0.022948    
2018-10-27 22:57:14,606 - Epoch: [62][  250/  391]    Overall Loss 0.467002    Objective Loss 0.467002    Top1 83.971875    Top5 99.256250    LR 0.300000    Time 0.022916    
2018-10-27 22:57:15,750 - Epoch: [62][  300/  391]    Overall Loss 0.468588    Objective Loss 0.468588    Top1 83.807292    Top5 99.294271    LR 0.300000    Time 0.022904    
2018-10-27 22:57:16,893 - Epoch: [62][  350/  391]    Overall Loss 0.468183    Objective Loss 0.468183    Top1 83.796875    Top5 99.312500    LR 0.300000    Time 0.022893    
2018-10-27 22:57:17,911 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51799 | -0.00728 |    0.20055 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15825 | -0.00539 |    0.03560 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.14929 |  0.00246 |    0.03750 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15237 | -0.00305 |    0.03693 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12233 | -0.00249 |    0.02482 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16100 | -0.00665 |    0.04581 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12077 | -0.00081 |    0.02829 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17170 | -0.00446 |    0.06259 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13605 | -0.00444 |    0.04361 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20411 | -0.01182 |    0.06627 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11224 | -0.00180 |    0.03200 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09339 | -0.00169 |    0.02305 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12281 | -0.00324 |    0.03708 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09449 | -0.00535 |    0.02461 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11722 | -0.00155 |    0.04014 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10970 | -0.00506 |    0.03850 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11342 | -0.00066 |    0.02930 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09988 | -0.00317 |    0.03167 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08112 | -0.00314 |    0.02160 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07562 |  0.00016 |    0.02072 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05065 |  0.00197 |    0.00933 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46695 | -0.08418 |    0.23795 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:57:17,911 - Total sparsity: 88.62

2018-10-27 22:57:17,911 - --- validate (epoch=62)-----------
2018-10-27 22:57:17,911 - 10000 samples (128 per mini-batch)
2018-10-27 22:57:18,634 - Epoch: [62][   50/   78]    Loss 0.542494    Top1 82.140625    Top5 98.937500    
2018-10-27 22:57:19,027 - ==> Top1: 82.020    Top5: 99.080    Loss: 0.541

2018-10-27 22:57:19,028 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 22:57:19,028 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:57:19,043 - 

2018-10-27 22:57:19,044 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:57:20,220 - Epoch: [63][   50/  391]    Overall Loss 0.470152    Objective Loss 0.470152    Top1 84.187500    Top5 99.203125    LR 0.300000    Time 0.023496    
2018-10-27 22:57:21,363 - Epoch: [63][  100/  391]    Overall Loss 0.473337    Objective Loss 0.473337    Top1 84.203125    Top5 99.210938    LR 0.300000    Time 0.023165    
2018-10-27 22:57:22,504 - Epoch: [63][  150/  391]    Overall Loss 0.473318    Objective Loss 0.473318    Top1 84.036458    Top5 99.250000    LR 0.300000    Time 0.023038    
2018-10-27 22:57:23,648 - Epoch: [63][  200/  391]    Overall Loss 0.468578    Objective Loss 0.468578    Top1 84.089844    Top5 99.261719    LR 0.300000    Time 0.022975    
2018-10-27 22:57:24,792 - Epoch: [63][  250/  391]    Overall Loss 0.472583    Objective Loss 0.472583    Top1 83.921875    Top5 99.237500    LR 0.300000    Time 0.022951    
2018-10-27 22:57:25,936 - Epoch: [63][  300/  391]    Overall Loss 0.473816    Objective Loss 0.473816    Top1 83.799479    Top5 99.252604    LR 0.300000    Time 0.022935    
2018-10-27 22:57:27,078 - Epoch: [63][  350/  391]    Overall Loss 0.471392    Objective Loss 0.471392    Top1 83.781250    Top5 99.276786    LR 0.300000    Time 0.022918    
2018-10-27 22:57:28,097 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.52360 | -0.00936 |    0.20181 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15995 | -0.00520 |    0.03567 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15100 |  0.00278 |    0.03820 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15322 | -0.00424 |    0.03713 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12201 | -0.00282 |    0.02471 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16062 | -0.00642 |    0.04635 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12048 | -0.00136 |    0.02811 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17170 | -0.00507 |    0.06210 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13632 | -0.00402 |    0.04382 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20350 | -0.00962 |    0.06482 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11256 | -0.00232 |    0.03212 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09359 | -0.00197 |    0.02295 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12291 | -0.00326 |    0.03710 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09419 | -0.00530 |    0.02456 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11719 | -0.00143 |    0.04008 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10969 | -0.00534 |    0.03854 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11328 | -0.00105 |    0.02924 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09987 | -0.00319 |    0.03176 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08127 | -0.00320 |    0.02163 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07595 | -0.00006 |    0.02072 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05086 |  0.00187 |    0.00939 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46676 | -0.08361 |    0.23673 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:57:28,097 - Total sparsity: 88.62

2018-10-27 22:57:28,098 - --- validate (epoch=63)-----------
2018-10-27 22:57:28,098 - 10000 samples (128 per mini-batch)
2018-10-27 22:57:28,814 - Epoch: [63][   50/   78]    Loss 0.742275    Top1 75.656250    Top5 98.843750    
2018-10-27 22:57:29,205 - ==> Top1: 75.910    Top5: 98.810    Loss: 0.741

2018-10-27 22:57:29,206 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 22:57:29,206 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:57:29,218 - 

2018-10-27 22:57:29,218 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:57:30,416 - Epoch: [64][   50/  391]    Overall Loss 0.458726    Objective Loss 0.458726    Top1 84.218750    Top5 99.328125    LR 0.300000    Time 0.023935    
2018-10-27 22:57:31,557 - Epoch: [64][  100/  391]    Overall Loss 0.466865    Objective Loss 0.466865    Top1 84.015625    Top5 99.343750    LR 0.300000    Time 0.023360    
2018-10-27 22:57:32,696 - Epoch: [64][  150/  391]    Overall Loss 0.467711    Objective Loss 0.467711    Top1 84.140625    Top5 99.307292    LR 0.300000    Time 0.023161    
2018-10-27 22:57:33,839 - Epoch: [64][  200/  391]    Overall Loss 0.470649    Objective Loss 0.470649    Top1 83.917969    Top5 99.304688    LR 0.300000    Time 0.023078    
2018-10-27 22:57:34,978 - Epoch: [64][  250/  391]    Overall Loss 0.466450    Objective Loss 0.466450    Top1 84.050000    Top5 99.312500    LR 0.300000    Time 0.023015    
2018-10-27 22:57:36,118 - Epoch: [64][  300/  391]    Overall Loss 0.463034    Objective Loss 0.463034    Top1 84.182292    Top5 99.330729    LR 0.300000    Time 0.022974    
2018-10-27 22:57:37,258 - Epoch: [64][  350/  391]    Overall Loss 0.464745    Objective Loss 0.464745    Top1 84.022321    Top5 99.339286    LR 0.300000    Time 0.022944    
2018-10-27 22:57:38,273 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.52143 | -0.00350 |    0.20282 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15869 | -0.00419 |    0.03568 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15118 |  0.00159 |    0.03834 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15289 | -0.00539 |    0.03717 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12311 | -0.00361 |    0.02447 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15987 | -0.00729 |    0.04560 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11953 | -0.00134 |    0.02786 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17141 | -0.00427 |    0.06261 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13633 | -0.00362 |    0.04368 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20345 | -0.01070 |    0.06612 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11282 | -0.00194 |    0.03224 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09355 | -0.00186 |    0.02305 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12347 | -0.00322 |    0.03744 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09427 | -0.00536 |    0.02452 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11727 | -0.00208 |    0.04017 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10977 | -0.00519 |    0.03867 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11374 | -0.00066 |    0.02922 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09966 | -0.00308 |    0.03166 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08107 | -0.00316 |    0.02163 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07614 | -0.00017 |    0.02073 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05086 |  0.00179 |    0.00939 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46652 | -0.08403 |    0.23862 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:57:38,274 - Total sparsity: 88.62

2018-10-27 22:57:38,274 - --- validate (epoch=64)-----------
2018-10-27 22:57:38,274 - 10000 samples (128 per mini-batch)
2018-10-27 22:57:39,003 - Epoch: [64][   50/   78]    Loss 0.783207    Top1 74.531250    Top5 98.312500    
2018-10-27 22:57:39,397 - ==> Top1: 74.580    Top5: 98.380    Loss: 0.770

2018-10-27 22:57:39,398 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 22:57:39,398 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:57:39,408 - 

2018-10-27 22:57:39,408 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:57:40,606 - Epoch: [65][   50/  391]    Overall Loss 0.479580    Objective Loss 0.479580    Top1 83.453125    Top5 99.187500    LR 0.300000    Time 0.023920    
2018-10-27 22:57:41,747 - Epoch: [65][  100/  391]    Overall Loss 0.465957    Objective Loss 0.465957    Top1 84.023438    Top5 99.242188    LR 0.300000    Time 0.023359    
2018-10-27 22:57:42,887 - Epoch: [65][  150/  391]    Overall Loss 0.465210    Objective Loss 0.465210    Top1 84.192708    Top5 99.239583    LR 0.300000    Time 0.023165    
2018-10-27 22:57:44,026 - Epoch: [65][  200/  391]    Overall Loss 0.472785    Objective Loss 0.472785    Top1 83.894531    Top5 99.238281    LR 0.300000    Time 0.023061    
2018-10-27 22:57:45,166 - Epoch: [65][  250/  391]    Overall Loss 0.475805    Objective Loss 0.475805    Top1 83.765625    Top5 99.250000    LR 0.300000    Time 0.023003    
2018-10-27 22:57:46,306 - Epoch: [65][  300/  391]    Overall Loss 0.471564    Objective Loss 0.471564    Top1 83.890625    Top5 99.257812    LR 0.300000    Time 0.022964    
2018-10-27 22:57:47,446 - Epoch: [65][  350/  391]    Overall Loss 0.470881    Objective Loss 0.470881    Top1 83.861607    Top5 99.252232    LR 0.300000    Time 0.022936    
2018-10-27 22:57:48,458 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51924 | -0.00497 |    0.19853 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15839 | -0.00467 |    0.03510 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15128 |  0.00067 |    0.03794 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15289 | -0.00519 |    0.03683 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12314 | -0.00291 |    0.02481 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16026 | -0.00751 |    0.04641 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12019 | -0.00168 |    0.02784 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17147 | -0.00448 |    0.06237 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13603 | -0.00390 |    0.04348 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20319 | -0.01142 |    0.06572 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11282 | -0.00146 |    0.03214 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09361 | -0.00165 |    0.02305 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12332 | -0.00296 |    0.03731 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09405 | -0.00532 |    0.02457 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11729 | -0.00180 |    0.04030 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10968 | -0.00497 |    0.03858 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11380 | -0.00065 |    0.02969 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09952 | -0.00332 |    0.03158 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08091 | -0.00332 |    0.02157 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07607 | -0.00002 |    0.02079 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05068 |  0.00201 |    0.00936 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46431 | -0.08202 |    0.23714 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:57:48,458 - Total sparsity: 88.62

2018-10-27 22:57:48,458 - --- validate (epoch=65)-----------
2018-10-27 22:57:48,458 - 10000 samples (128 per mini-batch)
2018-10-27 22:57:49,183 - Epoch: [65][   50/   78]    Loss 0.623320    Top1 80.265625    Top5 98.625000    
2018-10-27 22:57:49,577 - ==> Top1: 79.890    Top5: 98.620    Loss: 0.621

2018-10-27 22:57:49,578 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 22:57:49,578 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:57:49,595 - 

2018-10-27 22:57:49,596 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:57:50,768 - Epoch: [66][   50/  391]    Overall Loss 0.460414    Objective Loss 0.460414    Top1 83.734375    Top5 99.296875    LR 0.300000    Time 0.023409    
2018-10-27 22:57:51,907 - Epoch: [66][  100/  391]    Overall Loss 0.463143    Objective Loss 0.463143    Top1 83.968750    Top5 99.273438    LR 0.300000    Time 0.023087    
2018-10-27 22:57:53,050 - Epoch: [66][  150/  391]    Overall Loss 0.463598    Objective Loss 0.463598    Top1 83.817708    Top5 99.291667    LR 0.300000    Time 0.023000    
2018-10-27 22:57:54,191 - Epoch: [66][  200/  391]    Overall Loss 0.461451    Objective Loss 0.461451    Top1 84.031250    Top5 99.312500    LR 0.300000    Time 0.022950    
2018-10-27 22:57:55,332 - Epoch: [66][  250/  391]    Overall Loss 0.461954    Objective Loss 0.461954    Top1 83.968750    Top5 99.290625    LR 0.300000    Time 0.022918    
2018-10-27 22:57:56,474 - Epoch: [66][  300/  391]    Overall Loss 0.465225    Objective Loss 0.465225    Top1 83.911458    Top5 99.283854    LR 0.300000    Time 0.022902    
2018-10-27 22:57:57,615 - Epoch: [66][  350/  391]    Overall Loss 0.465755    Objective Loss 0.465755    Top1 83.886161    Top5 99.292411    LR 0.300000    Time 0.022886    
2018-10-27 22:57:58,629 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51847 | -0.00233 |    0.19977 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15815 | -0.00514 |    0.03553 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15100 |  0.00127 |    0.03803 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15154 | -0.00454 |    0.03639 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12271 | -0.00341 |    0.02440 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16085 | -0.00724 |    0.04599 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11988 | -0.00084 |    0.02768 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17146 | -0.00412 |    0.06227 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13570 | -0.00387 |    0.04322 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20252 | -0.00949 |    0.06504 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11231 | -0.00139 |    0.03203 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09302 | -0.00216 |    0.02306 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12316 | -0.00324 |    0.03740 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09400 | -0.00564 |    0.02430 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11757 | -0.00194 |    0.04035 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10967 | -0.00526 |    0.03865 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11309 | -0.00094 |    0.02918 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09953 | -0.00347 |    0.03156 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08126 | -0.00341 |    0.02167 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07619 | -0.00007 |    0.02079 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05073 |  0.00196 |    0.00935 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46793 | -0.08242 |    0.23864 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:57:58,629 - Total sparsity: 88.62

2018-10-27 22:57:58,629 - --- validate (epoch=66)-----------
2018-10-27 22:57:58,629 - 10000 samples (128 per mini-batch)
2018-10-27 22:57:59,351 - Epoch: [66][   50/   78]    Loss 0.572779    Top1 80.812500    Top5 99.062500    
2018-10-27 22:57:59,745 - ==> Top1: 80.450    Top5: 99.170    Loss: 0.580

2018-10-27 22:57:59,746 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 22:57:59,746 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:57:59,762 - 

2018-10-27 22:57:59,762 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:58:00,938 - Epoch: [67][   50/  391]    Overall Loss 0.446289    Objective Loss 0.446289    Top1 84.515625    Top5 99.484375    LR 0.300000    Time 0.023490    
2018-10-27 22:58:02,080 - Epoch: [67][  100/  391]    Overall Loss 0.458887    Objective Loss 0.458887    Top1 84.125000    Top5 99.414062    LR 0.300000    Time 0.023143    
2018-10-27 22:58:03,221 - Epoch: [67][  150/  391]    Overall Loss 0.470865    Objective Loss 0.470865    Top1 83.583333    Top5 99.385417    LR 0.300000    Time 0.023026    
2018-10-27 22:58:04,362 - Epoch: [67][  200/  391]    Overall Loss 0.470131    Objective Loss 0.470131    Top1 83.609375    Top5 99.363281    LR 0.300000    Time 0.022968    
2018-10-27 22:58:05,504 - Epoch: [67][  250/  391]    Overall Loss 0.466329    Objective Loss 0.466329    Top1 83.678125    Top5 99.384375    LR 0.300000    Time 0.022938    
2018-10-27 22:58:06,645 - Epoch: [67][  300/  391]    Overall Loss 0.467565    Objective Loss 0.467565    Top1 83.713542    Top5 99.377604    LR 0.300000    Time 0.022915    
2018-10-27 22:58:07,784 - Epoch: [67][  350/  391]    Overall Loss 0.469512    Objective Loss 0.469512    Top1 83.687500    Top5 99.377232    LR 0.300000    Time 0.022891    
2018-10-27 22:58:08,796 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.52210 | -0.01135 |    0.20065 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15793 | -0.00509 |    0.03550 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15096 |  0.00161 |    0.03788 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15039 | -0.00451 |    0.03654 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12235 | -0.00360 |    0.02493 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16084 | -0.00846 |    0.04608 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12086 | -0.00189 |    0.02817 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17190 | -0.00469 |    0.06247 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13606 | -0.00410 |    0.04347 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20493 | -0.01056 |    0.06774 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11214 | -0.00236 |    0.03209 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09282 | -0.00203 |    0.02289 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12374 | -0.00360 |    0.03744 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09452 | -0.00518 |    0.02468 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11796 | -0.00168 |    0.04048 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10994 | -0.00533 |    0.03867 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11325 | -0.00171 |    0.02937 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09965 | -0.00321 |    0.03163 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08133 | -0.00307 |    0.02164 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07648 | -0.00011 |    0.02086 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05091 |  0.00203 |    0.00942 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46612 | -0.08264 |    0.23778 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:58:08,796 - Total sparsity: 88.62

2018-10-27 22:58:08,797 - --- validate (epoch=67)-----------
2018-10-27 22:58:08,797 - 10000 samples (128 per mini-batch)
2018-10-27 22:58:09,516 - Epoch: [67][   50/   78]    Loss 0.585572    Top1 80.859375    Top5 99.046875    
2018-10-27 22:58:09,909 - ==> Top1: 80.750    Top5: 99.120    Loss: 0.584

2018-10-27 22:58:09,910 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 22:58:09,910 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:58:09,921 - 

2018-10-27 22:58:09,921 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:58:11,118 - Epoch: [68][   50/  391]    Overall Loss 0.471275    Objective Loss 0.471275    Top1 83.531250    Top5 99.437500    LR 0.300000    Time 0.023905    
2018-10-27 22:58:12,258 - Epoch: [68][  100/  391]    Overall Loss 0.483543    Objective Loss 0.483543    Top1 83.187500    Top5 99.320312    LR 0.300000    Time 0.023340    
2018-10-27 22:58:13,399 - Epoch: [68][  150/  391]    Overall Loss 0.477329    Objective Loss 0.477329    Top1 83.619792    Top5 99.307292    LR 0.300000    Time 0.023155    
2018-10-27 22:58:14,539 - Epoch: [68][  200/  391]    Overall Loss 0.473332    Objective Loss 0.473332    Top1 83.781250    Top5 99.339844    LR 0.300000    Time 0.023059    
2018-10-27 22:58:15,678 - Epoch: [68][  250/  391]    Overall Loss 0.470841    Objective Loss 0.470841    Top1 83.853125    Top5 99.334375    LR 0.300000    Time 0.023000    
2018-10-27 22:58:16,821 - Epoch: [68][  300/  391]    Overall Loss 0.472620    Objective Loss 0.472620    Top1 83.846354    Top5 99.338542    LR 0.300000    Time 0.022969    
2018-10-27 22:58:17,962 - Epoch: [68][  350/  391]    Overall Loss 0.474990    Objective Loss 0.474990    Top1 83.758929    Top5 99.321429    LR 0.300000    Time 0.022945    
2018-10-27 22:58:18,978 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.52407 | -0.00629 |    0.20198 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15780 | -0.00475 |    0.03539 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15252 |  0.00102 |    0.03801 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15090 | -0.00540 |    0.03633 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12308 | -0.00324 |    0.02479 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16088 | -0.00808 |    0.04622 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12072 | -0.00114 |    0.02806 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17161 | -0.00422 |    0.06265 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13641 | -0.00371 |    0.04365 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20382 | -0.01118 |    0.06685 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11206 | -0.00203 |    0.03194 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09312 | -0.00221 |    0.02302 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12388 | -0.00333 |    0.03751 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09451 | -0.00501 |    0.02462 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11772 | -0.00251 |    0.04059 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10978 | -0.00518 |    0.03860 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11295 | -0.00081 |    0.02938 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09953 | -0.00318 |    0.03159 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08120 | -0.00328 |    0.02169 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07643 | -0.00018 |    0.02088 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05084 |  0.00210 |    0.00938 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.47050 | -0.08756 |    0.24270 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:58:18,978 - Total sparsity: 88.62

2018-10-27 22:58:18,978 - --- validate (epoch=68)-----------
2018-10-27 22:58:18,979 - 10000 samples (128 per mini-batch)
2018-10-27 22:58:19,703 - Epoch: [68][   50/   78]    Loss 0.562832    Top1 81.281250    Top5 99.000000    
2018-10-27 22:58:20,094 - ==> Top1: 81.210    Top5: 99.050    Loss: 0.563

2018-10-27 22:58:20,095 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 22:58:20,095 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:58:20,106 - 

2018-10-27 22:58:20,106 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:58:21,307 - Epoch: [69][   50/  391]    Overall Loss 0.427043    Objective Loss 0.427043    Top1 85.062500    Top5 99.359375    LR 0.300000    Time 0.023994    
2018-10-27 22:58:22,450 - Epoch: [69][  100/  391]    Overall Loss 0.441031    Objective Loss 0.441031    Top1 84.656250    Top5 99.328125    LR 0.300000    Time 0.023407    
2018-10-27 22:58:23,593 - Epoch: [69][  150/  391]    Overall Loss 0.450410    Objective Loss 0.450410    Top1 84.343750    Top5 99.338542    LR 0.300000    Time 0.023216    
2018-10-27 22:58:24,736 - Epoch: [69][  200/  391]    Overall Loss 0.455997    Objective Loss 0.455997    Top1 84.148438    Top5 99.332031    LR 0.300000    Time 0.023120    
2018-10-27 22:58:25,878 - Epoch: [69][  250/  391]    Overall Loss 0.460040    Objective Loss 0.460040    Top1 84.140625    Top5 99.325000    LR 0.300000    Time 0.023046    
2018-10-27 22:58:27,021 - Epoch: [69][  300/  391]    Overall Loss 0.464416    Objective Loss 0.464416    Top1 83.976562    Top5 99.320312    LR 0.300000    Time 0.023010    
2018-10-27 22:58:28,164 - Epoch: [69][  350/  391]    Overall Loss 0.467668    Objective Loss 0.467668    Top1 83.848214    Top5 99.292411    LR 0.300000    Time 0.022984    
2018-10-27 22:58:29,176 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.52321 | -0.00297 |    0.20232 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15816 | -0.00387 |    0.03537 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15284 |  0.00190 |    0.03843 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15166 | -0.00537 |    0.03616 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12391 | -0.00284 |    0.02525 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16064 | -0.00713 |    0.04528 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12046 | -0.00013 |    0.02790 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17240 | -0.00494 |    0.06288 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13713 | -0.00351 |    0.04398 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20323 | -0.01061 |    0.06657 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11217 | -0.00187 |    0.03176 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09314 | -0.00192 |    0.02280 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12405 | -0.00290 |    0.03743 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09479 | -0.00567 |    0.02463 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11789 | -0.00211 |    0.04078 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10981 | -0.00559 |    0.03856 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11327 |  0.00020 |    0.02957 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09970 | -0.00352 |    0.03168 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08121 | -0.00343 |    0.02172 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07656 | -0.00015 |    0.02092 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05084 |  0.00190 |    0.00942 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46737 | -0.08453 |    0.23968 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:58:29,176 - Total sparsity: 88.62

2018-10-27 22:58:29,176 - --- validate (epoch=69)-----------
2018-10-27 22:58:29,176 - 10000 samples (128 per mini-batch)
2018-10-27 22:58:29,913 - Epoch: [69][   50/   78]    Loss 0.726207    Top1 76.453125    Top5 98.640625    
2018-10-27 22:58:30,304 - ==> Top1: 76.140    Top5: 98.690    Loss: 0.729

2018-10-27 22:58:30,305 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 22:58:30,305 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:58:30,315 - 

2018-10-27 22:58:30,315 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:58:31,512 - Epoch: [70][   50/  391]    Overall Loss 0.441353    Objective Loss 0.441353    Top1 84.593750    Top5 99.390625    LR 0.300000    Time 0.023907    
2018-10-27 22:58:32,654 - Epoch: [70][  100/  391]    Overall Loss 0.447090    Objective Loss 0.447090    Top1 84.554688    Top5 99.429688    LR 0.300000    Time 0.023357    
2018-10-27 22:58:33,794 - Epoch: [70][  150/  391]    Overall Loss 0.458022    Objective Loss 0.458022    Top1 84.182292    Top5 99.375000    LR 0.300000    Time 0.023161    
2018-10-27 22:58:34,933 - Epoch: [70][  200/  391]    Overall Loss 0.462420    Objective Loss 0.462420    Top1 83.976562    Top5 99.281250    LR 0.300000    Time 0.023062    
2018-10-27 22:58:36,074 - Epoch: [70][  250/  391]    Overall Loss 0.465871    Objective Loss 0.465871    Top1 83.981250    Top5 99.253125    LR 0.300000    Time 0.022991    
2018-10-27 22:58:37,212 - Epoch: [70][  300/  391]    Overall Loss 0.465611    Objective Loss 0.465611    Top1 83.992188    Top5 99.255208    LR 0.300000    Time 0.022949    
2018-10-27 22:58:38,351 - Epoch: [70][  350/  391]    Overall Loss 0.468064    Objective Loss 0.468064    Top1 83.897321    Top5 99.272321    LR 0.300000    Time 0.022921    
2018-10-27 22:58:39,364 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51730 | -0.00216 |    0.19972 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15789 | -0.00544 |    0.03564 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15179 |  0.00300 |    0.03807 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15186 | -0.00556 |    0.03605 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12375 | -0.00312 |    0.02534 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16013 | -0.00817 |    0.04556 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12072 | -0.00133 |    0.02789 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17171 | -0.00495 |    0.06248 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13674 | -0.00353 |    0.04353 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20294 | -0.01112 |    0.06695 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11209 | -0.00205 |    0.03185 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09315 | -0.00189 |    0.02293 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12360 | -0.00282 |    0.03729 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09459 | -0.00533 |    0.02472 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11782 | -0.00245 |    0.04055 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10976 | -0.00548 |    0.03853 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11325 |  0.00071 |    0.02954 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09965 | -0.00326 |    0.03156 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08117 | -0.00334 |    0.02156 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07627 | -0.00043 |    0.02080 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05074 |  0.00192 |    0.00940 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46280 | -0.08337 |    0.23701 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:58:39,364 - Total sparsity: 88.62

2018-10-27 22:58:39,364 - --- validate (epoch=70)-----------
2018-10-27 22:58:39,364 - 10000 samples (128 per mini-batch)
2018-10-27 22:58:40,089 - Epoch: [70][   50/   78]    Loss 0.706889    Top1 78.031250    Top5 98.640625    
2018-10-27 22:58:40,483 - ==> Top1: 77.710    Top5: 98.640    Loss: 0.715

2018-10-27 22:58:40,484 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 22:58:40,484 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:58:40,494 - 

2018-10-27 22:58:40,494 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:58:41,692 - Epoch: [71][   50/  391]    Overall Loss 0.445781    Objective Loss 0.445781    Top1 84.515625    Top5 99.390625    LR 0.300000    Time 0.023932    
2018-10-27 22:58:42,833 - Epoch: [71][  100/  391]    Overall Loss 0.461298    Objective Loss 0.461298    Top1 84.031250    Top5 99.328125    LR 0.300000    Time 0.023359    
2018-10-27 22:58:43,972 - Epoch: [71][  150/  391]    Overall Loss 0.471926    Objective Loss 0.471926    Top1 83.630208    Top5 99.317708    LR 0.300000    Time 0.023157    
2018-10-27 22:58:45,112 - Epoch: [71][  200/  391]    Overall Loss 0.468289    Objective Loss 0.468289    Top1 83.796875    Top5 99.335938    LR 0.300000    Time 0.023061    
2018-10-27 22:58:46,252 - Epoch: [71][  250/  391]    Overall Loss 0.471758    Objective Loss 0.471758    Top1 83.568750    Top5 99.318750    LR 0.300000    Time 0.023005    
2018-10-27 22:58:47,393 - Epoch: [71][  300/  391]    Overall Loss 0.468315    Objective Loss 0.468315    Top1 83.661458    Top5 99.338542    LR 0.300000    Time 0.022968    
2018-10-27 22:58:48,534 - Epoch: [71][  350/  391]    Overall Loss 0.468098    Objective Loss 0.468098    Top1 83.792411    Top5 99.319196    LR 0.300000    Time 0.022943    
2018-10-27 22:58:49,549 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51214 | -0.00521 |    0.19734 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15900 | -0.00477 |    0.03619 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15103 |  0.00148 |    0.03808 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15151 | -0.00567 |    0.03606 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12385 | -0.00298 |    0.02513 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15921 | -0.00728 |    0.04544 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11974 | -0.00124 |    0.02747 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17095 | -0.00435 |    0.06239 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13618 | -0.00343 |    0.04327 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20377 | -0.01024 |    0.06652 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11200 | -0.00212 |    0.03188 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09271 | -0.00210 |    0.02273 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12360 | -0.00365 |    0.03729 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09465 | -0.00562 |    0.02470 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11760 | -0.00254 |    0.04033 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10947 | -0.00541 |    0.03842 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11248 |  0.00048 |    0.02907 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09941 | -0.00314 |    0.03151 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08112 | -0.00327 |    0.02156 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07609 | -0.00022 |    0.02073 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05066 |  0.00196 |    0.00934 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46488 | -0.07915 |    0.23742 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:58:49,549 - Total sparsity: 88.62

2018-10-27 22:58:49,549 - --- validate (epoch=71)-----------
2018-10-27 22:58:49,549 - 10000 samples (128 per mini-batch)
2018-10-27 22:58:50,273 - Epoch: [71][   50/   78]    Loss 0.623697    Top1 80.359375    Top5 98.500000    
2018-10-27 22:58:50,664 - ==> Top1: 80.370    Top5: 98.600    Loss: 0.614

2018-10-27 22:58:50,665 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 22:58:50,665 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:58:50,682 - 

2018-10-27 22:58:50,683 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:58:51,855 - Epoch: [72][   50/  391]    Overall Loss 0.459611    Objective Loss 0.459611    Top1 84.453125    Top5 99.296875    LR 0.300000    Time 0.023405    
2018-10-27 22:58:52,995 - Epoch: [72][  100/  391]    Overall Loss 0.461979    Objective Loss 0.461979    Top1 84.070312    Top5 99.250000    LR 0.300000    Time 0.023092    
2018-10-27 22:58:54,135 - Epoch: [72][  150/  391]    Overall Loss 0.468238    Objective Loss 0.468238    Top1 83.770833    Top5 99.255208    LR 0.300000    Time 0.022989    
2018-10-27 22:58:55,274 - Epoch: [72][  200/  391]    Overall Loss 0.471713    Objective Loss 0.471713    Top1 83.640625    Top5 99.226562    LR 0.300000    Time 0.022930    
2018-10-27 22:58:56,415 - Epoch: [72][  250/  391]    Overall Loss 0.467884    Objective Loss 0.467884    Top1 83.775000    Top5 99.243750    LR 0.300000    Time 0.022902    
2018-10-27 22:58:57,557 - Epoch: [72][  300/  391]    Overall Loss 0.466389    Objective Loss 0.466389    Top1 83.799479    Top5 99.265625    LR 0.300000    Time 0.022886    
2018-10-27 22:58:58,700 - Epoch: [72][  350/  391]    Overall Loss 0.468085    Objective Loss 0.468085    Top1 83.718750    Top5 99.272321    LR 0.300000    Time 0.022880    
2018-10-27 22:58:59,715 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51078 | -0.00850 |    0.19794 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15916 | -0.00445 |    0.03581 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15264 |  0.00252 |    0.03855 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15279 | -0.00657 |    0.03643 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12454 | -0.00289 |    0.02512 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16029 | -0.00740 |    0.04565 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11954 | -0.00065 |    0.02727 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17155 | -0.00495 |    0.06236 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13651 | -0.00375 |    0.04330 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20551 | -0.01020 |    0.06669 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11219 | -0.00210 |    0.03202 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09323 | -0.00226 |    0.02321 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12317 | -0.00312 |    0.03713 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09478 | -0.00518 |    0.02470 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11762 | -0.00203 |    0.04014 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10939 | -0.00538 |    0.03852 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11280 |  0.00014 |    0.02912 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09932 | -0.00317 |    0.03152 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08125 | -0.00337 |    0.02168 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07626 | -0.00019 |    0.02078 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05070 |  0.00201 |    0.00935 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46552 | -0.07968 |    0.23802 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:58:59,716 - Total sparsity: 88.62

2018-10-27 22:58:59,716 - --- validate (epoch=72)-----------
2018-10-27 22:58:59,716 - 10000 samples (128 per mini-batch)
2018-10-27 22:59:00,437 - Epoch: [72][   50/   78]    Loss 0.618836    Top1 80.406250    Top5 98.812500    
2018-10-27 22:59:00,827 - ==> Top1: 80.320    Top5: 98.870    Loss: 0.615

2018-10-27 22:59:00,828 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 22:59:00,828 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:59:00,848 - 

2018-10-27 22:59:00,848 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:59:02,041 - Epoch: [73][   50/  391]    Overall Loss 0.468210    Objective Loss 0.468210    Top1 83.390625    Top5 99.406250    LR 0.300000    Time 0.023808    
2018-10-27 22:59:03,216 - Epoch: [73][  100/  391]    Overall Loss 0.480692    Objective Loss 0.480692    Top1 82.968750    Top5 99.320312    LR 0.300000    Time 0.023642    
2018-10-27 22:59:04,390 - Epoch: [73][  150/  391]    Overall Loss 0.471183    Objective Loss 0.471183    Top1 83.364583    Top5 99.333333    LR 0.300000    Time 0.023578    
2018-10-27 22:59:05,562 - Epoch: [73][  200/  391]    Overall Loss 0.469974    Objective Loss 0.469974    Top1 83.433594    Top5 99.296875    LR 0.300000    Time 0.023536    
2018-10-27 22:59:06,732 - Epoch: [73][  250/  391]    Overall Loss 0.471993    Objective Loss 0.471993    Top1 83.478125    Top5 99.300000    LR 0.300000    Time 0.023506    
2018-10-27 22:59:07,900 - Epoch: [73][  300/  391]    Overall Loss 0.473353    Objective Loss 0.473353    Top1 83.460938    Top5 99.304688    LR 0.300000    Time 0.023477    
2018-10-27 22:59:09,068 - Epoch: [73][  350/  391]    Overall Loss 0.473195    Objective Loss 0.473195    Top1 83.506696    Top5 99.310268    LR 0.300000    Time 0.023456    
2018-10-27 22:59:10,105 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.50948 | -0.00585 |    0.19572 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15954 | -0.00487 |    0.03630 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15192 |  0.00227 |    0.03849 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15415 | -0.00538 |    0.03705 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12414 | -0.00324 |    0.02504 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16010 | -0.00770 |    0.04544 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11937 | -0.00103 |    0.02718 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17134 | -0.00527 |    0.06182 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13666 | -0.00411 |    0.04343 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20530 | -0.01157 |    0.06806 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11227 | -0.00214 |    0.03205 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09347 | -0.00243 |    0.02311 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12281 | -0.00334 |    0.03716 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09479 | -0.00528 |    0.02453 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11754 | -0.00145 |    0.04018 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10930 | -0.00525 |    0.03848 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11292 | -0.00049 |    0.02933 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09896 | -0.00314 |    0.03151 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08114 | -0.00313 |    0.02161 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07653 | -0.00030 |    0.02091 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05079 |  0.00191 |    0.00936 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.47030 | -0.08341 |    0.24031 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:59:10,105 - Total sparsity: 88.62

2018-10-27 22:59:10,106 - --- validate (epoch=73)-----------
2018-10-27 22:59:10,106 - 10000 samples (128 per mini-batch)
2018-10-27 22:59:10,834 - Epoch: [73][   50/   78]    Loss 0.542920    Top1 81.906250    Top5 98.890625    
2018-10-27 22:59:11,237 - ==> Top1: 81.570    Top5: 98.980    Loss: 0.543

2018-10-27 22:59:11,238 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 22:59:11,238 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:59:11,249 - 

2018-10-27 22:59:11,250 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:59:12,475 - Epoch: [74][   50/  391]    Overall Loss 0.446735    Objective Loss 0.446735    Top1 84.578125    Top5 99.234375    LR 0.300000    Time 0.024478    
2018-10-27 22:59:13,646 - Epoch: [74][  100/  391]    Overall Loss 0.462393    Objective Loss 0.462393    Top1 84.000000    Top5 99.289062    LR 0.300000    Time 0.023930    
2018-10-27 22:59:14,818 - Epoch: [74][  150/  391]    Overall Loss 0.460329    Objective Loss 0.460329    Top1 84.213542    Top5 99.317708    LR 0.300000    Time 0.023758    
2018-10-27 22:59:15,993 - Epoch: [74][  200/  391]    Overall Loss 0.458165    Objective Loss 0.458165    Top1 84.187500    Top5 99.343750    LR 0.300000    Time 0.023690    
2018-10-27 22:59:17,165 - Epoch: [74][  250/  391]    Overall Loss 0.463256    Objective Loss 0.463256    Top1 83.893750    Top5 99.362500    LR 0.300000    Time 0.023632    
2018-10-27 22:59:18,339 - Epoch: [74][  300/  391]    Overall Loss 0.468650    Objective Loss 0.468650    Top1 83.705729    Top5 99.320312    LR 0.300000    Time 0.023604    
2018-10-27 22:59:19,514 - Epoch: [74][  350/  391]    Overall Loss 0.473422    Objective Loss 0.473422    Top1 83.511161    Top5 99.316964    LR 0.300000    Time 0.023584    
2018-10-27 22:59:20,552 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.50958 | -0.00616 |    0.19527 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.16009 | -0.00474 |    0.03614 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15173 |  0.00329 |    0.03821 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15459 | -0.00529 |    0.03683 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12422 | -0.00247 |    0.02536 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16104 | -0.00675 |    0.04539 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11957 | -0.00137 |    0.02754 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17166 | -0.00534 |    0.06167 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13648 | -0.00353 |    0.04334 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20448 | -0.01189 |    0.06832 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11206 | -0.00165 |    0.03206 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09355 | -0.00212 |    0.02306 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12320 | -0.00303 |    0.03704 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09505 | -0.00551 |    0.02466 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11750 | -0.00144 |    0.04010 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10955 | -0.00550 |    0.03850 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11362 | -0.00039 |    0.02972 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09918 | -0.00320 |    0.03157 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08136 | -0.00319 |    0.02162 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07693 | -0.00026 |    0.02090 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05097 |  0.00190 |    0.00935 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46773 | -0.08365 |    0.23964 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:59:20,552 - Total sparsity: 88.62

2018-10-27 22:59:20,552 - --- validate (epoch=74)-----------
2018-10-27 22:59:20,552 - 10000 samples (128 per mini-batch)
2018-10-27 22:59:21,281 - Epoch: [74][   50/   78]    Loss 0.650663    Top1 79.250000    Top5 98.703125    
2018-10-27 22:59:21,679 - ==> Top1: 79.020    Top5: 98.820    Loss: 0.651

2018-10-27 22:59:21,680 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 22:59:21,680 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:59:21,695 - 

2018-10-27 22:59:21,695 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:59:22,925 - Epoch: [75][   50/  391]    Overall Loss 0.460201    Objective Loss 0.460201    Top1 84.234375    Top5 99.312500    LR 0.300000    Time 0.024556    
2018-10-27 22:59:24,098 - Epoch: [75][  100/  391]    Overall Loss 0.462064    Objective Loss 0.462064    Top1 84.070312    Top5 99.320312    LR 0.300000    Time 0.023994    
2018-10-27 22:59:25,269 - Epoch: [75][  150/  391]    Overall Loss 0.466650    Objective Loss 0.466650    Top1 83.822917    Top5 99.338542    LR 0.300000    Time 0.023794    
2018-10-27 22:59:26,442 - Epoch: [75][  200/  391]    Overall Loss 0.467256    Objective Loss 0.467256    Top1 83.878906    Top5 99.351562    LR 0.300000    Time 0.023705    
2018-10-27 22:59:27,614 - Epoch: [75][  250/  391]    Overall Loss 0.468638    Objective Loss 0.468638    Top1 83.834375    Top5 99.321875    LR 0.300000    Time 0.023646    
2018-10-27 22:59:28,790 - Epoch: [75][  300/  391]    Overall Loss 0.470700    Objective Loss 0.470700    Top1 83.718750    Top5 99.278646    LR 0.300000    Time 0.023620    
2018-10-27 22:59:29,964 - Epoch: [75][  350/  391]    Overall Loss 0.470780    Objective Loss 0.470780    Top1 83.725446    Top5 99.299107    LR 0.300000    Time 0.023595    
2018-10-27 22:59:31,012 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51147 | -0.00829 |    0.19541 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15980 | -0.00449 |    0.03548 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15151 |  0.00281 |    0.03825 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15324 | -0.00512 |    0.03667 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12322 | -0.00246 |    0.02510 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16094 | -0.00760 |    0.04577 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11967 | -0.00081 |    0.02764 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17187 | -0.00438 |    0.06244 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13608 | -0.00375 |    0.04348 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20367 | -0.01107 |    0.06702 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11193 | -0.00238 |    0.03184 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09354 | -0.00233 |    0.02310 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12361 | -0.00347 |    0.03710 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09522 | -0.00503 |    0.02467 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11720 | -0.00186 |    0.04019 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10941 | -0.00516 |    0.03853 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11351 | -0.00050 |    0.02983 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09914 | -0.00315 |    0.03156 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08147 | -0.00334 |    0.02168 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07683 | -0.00005 |    0.02088 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05097 |  0.00186 |    0.00937 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46302 | -0.08228 |    0.23671 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:59:31,012 - Total sparsity: 88.62

2018-10-27 22:59:31,012 - --- validate (epoch=75)-----------
2018-10-27 22:59:31,013 - 10000 samples (128 per mini-batch)
2018-10-27 22:59:31,742 - Epoch: [75][   50/   78]    Loss 0.565119    Top1 81.625000    Top5 99.078125    
2018-10-27 22:59:32,144 - ==> Top1: 81.660    Top5: 99.050    Loss: 0.559

2018-10-27 22:59:32,144 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 22:59:32,145 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:59:32,156 - 

2018-10-27 22:59:32,156 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:59:33,388 - Epoch: [76][   50/  391]    Overall Loss 0.462297    Objective Loss 0.462297    Top1 84.453125    Top5 99.406250    LR 0.300000    Time 0.024605    
2018-10-27 22:59:34,558 - Epoch: [76][  100/  391]    Overall Loss 0.466554    Objective Loss 0.466554    Top1 83.882812    Top5 99.406250    LR 0.300000    Time 0.023987    
2018-10-27 22:59:35,731 - Epoch: [76][  150/  391]    Overall Loss 0.460867    Objective Loss 0.460867    Top1 84.093750    Top5 99.390625    LR 0.300000    Time 0.023799    
2018-10-27 22:59:36,902 - Epoch: [76][  200/  391]    Overall Loss 0.466261    Objective Loss 0.466261    Top1 83.914062    Top5 99.375000    LR 0.300000    Time 0.023701    
2018-10-27 22:59:38,075 - Epoch: [76][  250/  391]    Overall Loss 0.465003    Objective Loss 0.465003    Top1 84.015625    Top5 99.365625    LR 0.300000    Time 0.023647    
2018-10-27 22:59:39,251 - Epoch: [76][  300/  391]    Overall Loss 0.469116    Objective Loss 0.469116    Top1 83.817708    Top5 99.330729    LR 0.300000    Time 0.023621    
2018-10-27 22:59:40,425 - Epoch: [76][  350/  391]    Overall Loss 0.467552    Objective Loss 0.467552    Top1 83.859375    Top5 99.332589    LR 0.300000    Time 0.023595    
2018-10-27 22:59:41,466 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.50505 | -0.00553 |    0.19417 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15954 | -0.00498 |    0.03607 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15102 |  0.00302 |    0.03805 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15282 | -0.00551 |    0.03678 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12305 | -0.00242 |    0.02495 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15970 | -0.00721 |    0.04583 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11969 | -0.00146 |    0.02724 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17123 | -0.00451 |    0.06251 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13570 | -0.00423 |    0.04338 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20301 | -0.01145 |    0.06714 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11224 | -0.00172 |    0.03191 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09355 | -0.00227 |    0.02309 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12349 | -0.00310 |    0.03699 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09489 | -0.00521 |    0.02483 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11718 | -0.00165 |    0.04030 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10928 | -0.00525 |    0.03847 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11242 | -0.00003 |    0.02925 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09909 | -0.00329 |    0.03146 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08121 | -0.00341 |    0.02166 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07656 |  0.00001 |    0.02077 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05072 |  0.00189 |    0.00929 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46133 | -0.08298 |    0.23596 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:59:41,466 - Total sparsity: 88.62

2018-10-27 22:59:41,466 - --- validate (epoch=76)-----------
2018-10-27 22:59:41,466 - 10000 samples (128 per mini-batch)
2018-10-27 22:59:42,192 - Epoch: [76][   50/   78]    Loss 0.643233    Top1 78.421875    Top5 98.828125    
2018-10-27 22:59:42,584 - ==> Top1: 78.410    Top5: 98.880    Loss: 0.633

2018-10-27 22:59:42,584 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 22:59:42,585 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:59:42,594 - 

2018-10-27 22:59:42,595 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:59:43,820 - Epoch: [77][   50/  391]    Overall Loss 0.456002    Objective Loss 0.456002    Top1 84.625000    Top5 99.359375    LR 0.300000    Time 0.024478    
2018-10-27 22:59:44,990 - Epoch: [77][  100/  391]    Overall Loss 0.464493    Objective Loss 0.464493    Top1 84.093750    Top5 99.257812    LR 0.300000    Time 0.023925    
2018-10-27 22:59:46,164 - Epoch: [77][  150/  391]    Overall Loss 0.460194    Objective Loss 0.460194    Top1 84.307292    Top5 99.307292    LR 0.300000    Time 0.023764    
2018-10-27 22:59:47,335 - Epoch: [77][  200/  391]    Overall Loss 0.467305    Objective Loss 0.467305    Top1 83.968750    Top5 99.304688    LR 0.300000    Time 0.023671    
2018-10-27 22:59:48,506 - Epoch: [77][  250/  391]    Overall Loss 0.477726    Objective Loss 0.477726    Top1 83.528125    Top5 99.275000    LR 0.300000    Time 0.023616    
2018-10-27 22:59:49,681 - Epoch: [77][  300/  391]    Overall Loss 0.476809    Objective Loss 0.476809    Top1 83.528646    Top5 99.270833    LR 0.300000    Time 0.023592    
2018-10-27 22:59:50,858 - Epoch: [77][  350/  391]    Overall Loss 0.476396    Objective Loss 0.476396    Top1 83.560268    Top5 99.258929    LR 0.300000    Time 0.023579    
2018-10-27 22:59:51,904 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.50833 | -0.00352 |    0.19696 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.16013 | -0.00458 |    0.03621 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15150 |  0.00235 |    0.03814 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15385 | -0.00499 |    0.03672 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12378 | -0.00254 |    0.02499 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15964 | -0.00839 |    0.04502 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11961 | -0.00171 |    0.02749 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17147 | -0.00449 |    0.06213 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13583 | -0.00363 |    0.04316 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20139 | -0.00900 |    0.06689 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11144 | -0.00214 |    0.03179 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09293 | -0.00183 |    0.02282 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12318 | -0.00353 |    0.03692 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09430 | -0.00509 |    0.02473 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11725 | -0.00195 |    0.04025 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10928 | -0.00508 |    0.03854 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11270 | -0.00043 |    0.02942 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09902 | -0.00330 |    0.03148 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08116 | -0.00337 |    0.02163 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07654 | -0.00009 |    0.02079 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05091 |  0.00187 |    0.00937 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46168 | -0.08347 |    0.23688 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 22:59:51,904 - Total sparsity: 88.62

2018-10-27 22:59:51,904 - --- validate (epoch=77)-----------
2018-10-27 22:59:51,904 - 10000 samples (128 per mini-batch)
2018-10-27 22:59:52,626 - Epoch: [77][   50/   78]    Loss 0.595550    Top1 79.593750    Top5 98.968750    
2018-10-27 22:59:53,010 - ==> Top1: 79.530    Top5: 99.120    Loss: 0.592

2018-10-27 22:59:53,011 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 22:59:53,011 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 22:59:53,028 - 

2018-10-27 22:59:53,029 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 22:59:54,231 - Epoch: [78][   50/  391]    Overall Loss 0.445486    Objective Loss 0.445486    Top1 84.406250    Top5 99.203125    LR 0.300000    Time 0.024019    
2018-10-27 22:59:55,403 - Epoch: [78][  100/  391]    Overall Loss 0.455204    Objective Loss 0.455204    Top1 84.273438    Top5 99.195312    LR 0.300000    Time 0.023707    
2018-10-27 22:59:56,572 - Epoch: [78][  150/  391]    Overall Loss 0.467661    Objective Loss 0.467661    Top1 83.786458    Top5 99.156250    LR 0.300000    Time 0.023592    
2018-10-27 22:59:57,742 - Epoch: [78][  200/  391]    Overall Loss 0.466915    Objective Loss 0.466915    Top1 83.886719    Top5 99.160156    LR 0.300000    Time 0.023537    
2018-10-27 22:59:58,914 - Epoch: [78][  250/  391]    Overall Loss 0.467913    Objective Loss 0.467913    Top1 83.900000    Top5 99.196875    LR 0.300000    Time 0.023511    
2018-10-27 23:00:00,088 - Epoch: [78][  300/  391]    Overall Loss 0.468149    Objective Loss 0.468149    Top1 83.919271    Top5 99.213542    LR 0.300000    Time 0.023503    
2018-10-27 23:00:01,262 - Epoch: [78][  350/  391]    Overall Loss 0.469032    Objective Loss 0.469032    Top1 83.828125    Top5 99.223214    LR 0.300000    Time 0.023496    
2018-10-27 23:00:02,303 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.50759 | -0.00255 |    0.19628 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15934 | -0.00492 |    0.03537 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15114 |  0.00225 |    0.03803 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15260 | -0.00533 |    0.03643 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12338 | -0.00309 |    0.02490 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15966 | -0.00806 |    0.04539 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11957 | -0.00113 |    0.02743 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17180 | -0.00599 |    0.06247 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13582 | -0.00366 |    0.04325 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20039 | -0.00937 |    0.06617 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11187 | -0.00226 |    0.03201 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09327 | -0.00162 |    0.02313 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12285 | -0.00346 |    0.03694 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09433 | -0.00513 |    0.02453 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11737 | -0.00208 |    0.04040 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10927 | -0.00506 |    0.03847 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11183 | -0.00018 |    0.02893 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09898 | -0.00345 |    0.03150 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08129 | -0.00331 |    0.02172 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07668 |  0.00012 |    0.02096 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05109 |  0.00185 |    0.00948 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46491 | -0.08295 |    0.23947 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:00:02,303 - Total sparsity: 88.62

2018-10-27 23:00:02,303 - --- validate (epoch=78)-----------
2018-10-27 23:00:02,304 - 10000 samples (128 per mini-batch)
2018-10-27 23:00:03,042 - Epoch: [78][   50/   78]    Loss 0.675783    Top1 76.765625    Top5 98.703125    
2018-10-27 23:00:03,443 - ==> Top1: 76.700    Top5: 98.770    Loss: 0.673

2018-10-27 23:00:03,444 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 23:00:03,444 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:00:03,460 - 

2018-10-27 23:00:03,460 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:00:04,666 - Epoch: [79][   50/  391]    Overall Loss 0.465143    Objective Loss 0.465143    Top1 84.375000    Top5 99.484375    LR 0.300000    Time 0.024085    
2018-10-27 23:00:05,837 - Epoch: [79][  100/  391]    Overall Loss 0.460907    Objective Loss 0.460907    Top1 84.187500    Top5 99.414062    LR 0.300000    Time 0.023737    
2018-10-27 23:00:07,008 - Epoch: [79][  150/  391]    Overall Loss 0.459384    Objective Loss 0.459384    Top1 84.114583    Top5 99.401042    LR 0.300000    Time 0.023623    
2018-10-27 23:00:08,182 - Epoch: [79][  200/  391]    Overall Loss 0.461151    Objective Loss 0.461151    Top1 84.089844    Top5 99.324219    LR 0.300000    Time 0.023581    
2018-10-27 23:00:09,352 - Epoch: [79][  250/  391]    Overall Loss 0.463531    Objective Loss 0.463531    Top1 83.968750    Top5 99.318750    LR 0.300000    Time 0.023537    
2018-10-27 23:00:10,524 - Epoch: [79][  300/  391]    Overall Loss 0.468562    Objective Loss 0.468562    Top1 83.864583    Top5 99.283854    LR 0.300000    Time 0.023517    
2018-10-27 23:00:11,697 - Epoch: [79][  350/  391]    Overall Loss 0.467382    Objective Loss 0.467382    Top1 83.883929    Top5 99.283482    LR 0.300000    Time 0.023506    
2018-10-27 23:00:12,737 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.50528 | -0.00709 |    0.19672 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15968 | -0.00430 |    0.03548 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15188 |  0.00380 |    0.03825 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15222 | -0.00434 |    0.03641 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12363 | -0.00334 |    0.02512 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15883 | -0.00694 |    0.04510 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11837 | -0.00186 |    0.02739 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17180 | -0.00454 |    0.06275 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13593 | -0.00380 |    0.04330 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20100 | -0.00753 |    0.06572 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11202 | -0.00213 |    0.03185 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09364 | -0.00176 |    0.02296 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12313 | -0.00304 |    0.03711 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09437 | -0.00558 |    0.02464 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11748 | -0.00190 |    0.04028 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10950 | -0.00491 |    0.03858 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11139 | -0.00033 |    0.02875 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09895 | -0.00345 |    0.03140 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08126 | -0.00315 |    0.02163 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07665 | -0.00022 |    0.02094 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05094 |  0.00194 |    0.00942 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46632 | -0.08190 |    0.23866 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:00:12,738 - Total sparsity: 88.62

2018-10-27 23:00:12,738 - --- validate (epoch=79)-----------
2018-10-27 23:00:12,738 - 10000 samples (128 per mini-batch)
2018-10-27 23:00:13,457 - Epoch: [79][   50/   78]    Loss 0.725467    Top1 77.609375    Top5 98.015625    
2018-10-27 23:00:13,851 - ==> Top1: 77.610    Top5: 98.110    Loss: 0.719

2018-10-27 23:00:13,852 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 23:00:13,852 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:00:13,870 - 

2018-10-27 23:00:13,870 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:00:15,073 - Epoch: [80][   50/  391]    Overall Loss 0.470221    Objective Loss 0.470221    Top1 83.796875    Top5 99.250000    LR 0.300000    Time 0.024016    
2018-10-27 23:00:16,245 - Epoch: [80][  100/  391]    Overall Loss 0.466641    Objective Loss 0.466641    Top1 83.742188    Top5 99.304688    LR 0.300000    Time 0.023715    
2018-10-27 23:00:17,416 - Epoch: [80][  150/  391]    Overall Loss 0.478424    Objective Loss 0.478424    Top1 83.328125    Top5 99.229167    LR 0.300000    Time 0.023610    
2018-10-27 23:00:18,586 - Epoch: [80][  200/  391]    Overall Loss 0.473597    Objective Loss 0.473597    Top1 83.664062    Top5 99.257812    LR 0.300000    Time 0.023551    
2018-10-27 23:00:19,758 - Epoch: [80][  250/  391]    Overall Loss 0.476248    Objective Loss 0.476248    Top1 83.537500    Top5 99.281250    LR 0.300000    Time 0.023523    
2018-10-27 23:00:20,927 - Epoch: [80][  300/  391]    Overall Loss 0.473580    Objective Loss 0.473580    Top1 83.661458    Top5 99.296875    LR 0.300000    Time 0.023496    
2018-10-27 23:00:22,098 - Epoch: [80][  350/  391]    Overall Loss 0.472534    Objective Loss 0.472534    Top1 83.703125    Top5 99.303571    LR 0.300000    Time 0.023481    
2018-10-27 23:00:23,137 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.50702 | -0.01411 |    0.19689 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15846 | -0.00488 |    0.03501 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15158 |  0.00246 |    0.03832 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15297 | -0.00474 |    0.03675 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12258 | -0.00296 |    0.02465 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15888 | -0.00704 |    0.04537 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11947 | -0.00147 |    0.02811 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17190 | -0.00492 |    0.06248 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13611 | -0.00378 |    0.04335 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.19897 | -0.01160 |    0.06626 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11244 | -0.00199 |    0.03207 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09392 | -0.00213 |    0.02324 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12331 | -0.00380 |    0.03713 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09439 | -0.00533 |    0.02454 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11721 | -0.00169 |    0.04021 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10928 | -0.00472 |    0.03851 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11149 | -0.00004 |    0.02912 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09887 | -0.00313 |    0.03140 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08129 | -0.00325 |    0.02162 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07652 | -0.00018 |    0.02087 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05097 |  0.00187 |    0.00938 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46301 | -0.08211 |    0.23785 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:00:23,137 - Total sparsity: 88.62

2018-10-27 23:00:23,138 - --- validate (epoch=80)-----------
2018-10-27 23:00:23,138 - 10000 samples (128 per mini-batch)
2018-10-27 23:00:23,866 - Epoch: [80][   50/   78]    Loss 0.655953    Top1 78.171875    Top5 98.703125    
2018-10-27 23:00:24,260 - ==> Top1: 78.300    Top5: 98.650    Loss: 0.656

2018-10-27 23:00:24,260 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 23:00:24,260 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:00:24,271 - 

2018-10-27 23:00:24,271 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:00:25,499 - Epoch: [81][   50/  391]    Overall Loss 0.452573    Objective Loss 0.452573    Top1 84.437500    Top5 99.328125    LR 0.300000    Time 0.024524    
2018-10-27 23:00:26,671 - Epoch: [81][  100/  391]    Overall Loss 0.463904    Objective Loss 0.463904    Top1 83.992188    Top5 99.359375    LR 0.300000    Time 0.023965    
2018-10-27 23:00:27,843 - Epoch: [81][  150/  391]    Overall Loss 0.462526    Objective Loss 0.462526    Top1 83.875000    Top5 99.348958    LR 0.300000    Time 0.023783    
2018-10-27 23:00:29,012 - Epoch: [81][  200/  391]    Overall Loss 0.457973    Objective Loss 0.457973    Top1 83.972656    Top5 99.347656    LR 0.300000    Time 0.023654    
2018-10-27 23:00:30,184 - Epoch: [81][  250/  391]    Overall Loss 0.464403    Objective Loss 0.464403    Top1 83.800000    Top5 99.318750    LR 0.300000    Time 0.023608    
2018-10-27 23:00:31,348 - Epoch: [81][  300/  391]    Overall Loss 0.469192    Objective Loss 0.469192    Top1 83.656250    Top5 99.307292    LR 0.300000    Time 0.023547    
2018-10-27 23:00:32,523 - Epoch: [81][  350/  391]    Overall Loss 0.471292    Objective Loss 0.471292    Top1 83.620536    Top5 99.303571    LR 0.300000    Time 0.023536    
2018-10-27 23:00:33,568 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51428 | -0.00414 |    0.19802 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15857 | -0.00475 |    0.03542 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15221 |  0.00302 |    0.03816 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15284 | -0.00571 |    0.03673 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12283 | -0.00238 |    0.02505 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15937 | -0.00658 |    0.04580 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12038 | -0.00114 |    0.02760 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17186 | -0.00584 |    0.06292 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13646 | -0.00438 |    0.04344 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.19879 | -0.00954 |    0.06566 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11291 | -0.00303 |    0.03208 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09367 | -0.00230 |    0.02329 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12307 | -0.00357 |    0.03697 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09426 | -0.00504 |    0.02448 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11722 | -0.00203 |    0.04027 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10918 | -0.00477 |    0.03835 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11147 | -0.00095 |    0.02899 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09892 | -0.00344 |    0.03146 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08151 | -0.00314 |    0.02170 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07657 | -0.00020 |    0.02087 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05090 |  0.00185 |    0.00939 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46779 | -0.08204 |    0.24076 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:00:33,568 - Total sparsity: 88.62

2018-10-27 23:00:33,568 - --- validate (epoch=81)-----------
2018-10-27 23:00:33,568 - 10000 samples (128 per mini-batch)
2018-10-27 23:00:34,309 - Epoch: [81][   50/   78]    Loss 0.749980    Top1 75.062500    Top5 98.734375    
2018-10-27 23:00:34,708 - ==> Top1: 75.350    Top5: 98.770    Loss: 0.741

2018-10-27 23:00:34,709 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 23:00:34,709 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:00:34,720 - 

2018-10-27 23:00:34,721 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:00:35,946 - Epoch: [82][   50/  391]    Overall Loss 0.435117    Objective Loss 0.435117    Top1 84.796875    Top5 99.609375    LR 0.300000    Time 0.024479    
2018-10-27 23:00:37,120 - Epoch: [82][  100/  391]    Overall Loss 0.457634    Objective Loss 0.457634    Top1 84.109375    Top5 99.414062    LR 0.300000    Time 0.023961    
2018-10-27 23:00:38,293 - Epoch: [82][  150/  391]    Overall Loss 0.463148    Objective Loss 0.463148    Top1 83.953125    Top5 99.375000    LR 0.300000    Time 0.023784    
2018-10-27 23:00:39,465 - Epoch: [82][  200/  391]    Overall Loss 0.469837    Objective Loss 0.469837    Top1 83.730469    Top5 99.363281    LR 0.300000    Time 0.023694    
2018-10-27 23:00:40,633 - Epoch: [82][  250/  391]    Overall Loss 0.467242    Objective Loss 0.467242    Top1 83.812500    Top5 99.400000    LR 0.300000    Time 0.023621    
2018-10-27 23:00:41,807 - Epoch: [82][  300/  391]    Overall Loss 0.471354    Objective Loss 0.471354    Top1 83.669271    Top5 99.361979    LR 0.300000    Time 0.023591    
2018-10-27 23:00:42,983 - Epoch: [82][  350/  391]    Overall Loss 0.473031    Objective Loss 0.473031    Top1 83.524554    Top5 99.328125    LR 0.300000    Time 0.023577    
2018-10-27 23:00:44,028 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51354 | -0.00467 |    0.19966 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15849 | -0.00545 |    0.03571 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15224 |  0.00241 |    0.03849 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15213 | -0.00611 |    0.03672 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12241 | -0.00322 |    0.02503 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15990 | -0.00734 |    0.04575 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12101 | -0.00193 |    0.02805 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17224 | -0.00545 |    0.06291 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13678 | -0.00347 |    0.04351 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20079 | -0.00917 |    0.06628 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11337 | -0.00244 |    0.03237 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09348 | -0.00238 |    0.02323 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12331 | -0.00325 |    0.03723 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09459 | -0.00525 |    0.02458 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11718 | -0.00197 |    0.04037 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10910 | -0.00509 |    0.03840 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11178 | -0.00090 |    0.02928 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09913 | -0.00356 |    0.03161 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08140 | -0.00308 |    0.02170 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07648 | -0.00029 |    0.02089 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05094 |  0.00196 |    0.00937 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46600 | -0.08020 |    0.23786 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:00:44,029 - Total sparsity: 88.62

2018-10-27 23:00:44,029 - --- validate (epoch=82)-----------
2018-10-27 23:00:44,029 - 10000 samples (128 per mini-batch)
2018-10-27 23:00:44,767 - Epoch: [82][   50/   78]    Loss 0.762594    Top1 76.015625    Top5 98.218750    
2018-10-27 23:00:45,161 - ==> Top1: 75.700    Top5: 98.190    Loss: 0.770

2018-10-27 23:00:45,162 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 23:00:45,162 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:00:45,171 - 

2018-10-27 23:00:45,171 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:00:46,400 - Epoch: [83][   50/  391]    Overall Loss 0.455330    Objective Loss 0.455330    Top1 83.953125    Top5 99.453125    LR 0.300000    Time 0.024541    
2018-10-27 23:00:47,569 - Epoch: [83][  100/  391]    Overall Loss 0.469850    Objective Loss 0.469850    Top1 83.585938    Top5 99.367188    LR 0.300000    Time 0.023953    
2018-10-27 23:00:48,742 - Epoch: [83][  150/  391]    Overall Loss 0.474712    Objective Loss 0.474712    Top1 83.369792    Top5 99.348958    LR 0.300000    Time 0.023774    
2018-10-27 23:00:49,915 - Epoch: [83][  200/  391]    Overall Loss 0.467825    Objective Loss 0.467825    Top1 83.691406    Top5 99.343750    LR 0.300000    Time 0.023689    
2018-10-27 23:00:51,087 - Epoch: [83][  250/  391]    Overall Loss 0.469271    Objective Loss 0.469271    Top1 83.746875    Top5 99.353125    LR 0.300000    Time 0.023635    
2018-10-27 23:00:52,262 - Epoch: [83][  300/  391]    Overall Loss 0.468785    Objective Loss 0.468785    Top1 83.789062    Top5 99.359375    LR 0.300000    Time 0.023608    
2018-10-27 23:00:53,434 - Epoch: [83][  350/  391]    Overall Loss 0.469372    Objective Loss 0.469372    Top1 83.736607    Top5 99.348214    LR 0.300000    Time 0.023579    
2018-10-27 23:00:54,478 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.50867 | -0.00950 |    0.19595 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15890 | -0.00419 |    0.03574 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15224 |  0.00249 |    0.03836 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15112 | -0.00638 |    0.03632 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12160 | -0.00257 |    0.02465 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15958 | -0.00876 |    0.04567 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12136 | -0.00181 |    0.02813 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17244 | -0.00528 |    0.06278 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13736 | -0.00401 |    0.04372 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20034 | -0.01049 |    0.06517 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11268 | -0.00251 |    0.03198 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09295 | -0.00225 |    0.02313 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12328 | -0.00297 |    0.03720 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09489 | -0.00515 |    0.02464 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11730 | -0.00225 |    0.04011 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10910 | -0.00467 |    0.03830 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11246 | -0.00026 |    0.02921 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09915 | -0.00358 |    0.03155 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08116 | -0.00309 |    0.02158 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07632 | -0.00022 |    0.02078 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05071 |  0.00193 |    0.00935 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46537 | -0.08119 |    0.23810 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:00:54,479 - Total sparsity: 88.62

2018-10-27 23:00:54,479 - --- validate (epoch=83)-----------
2018-10-27 23:00:54,479 - 10000 samples (128 per mini-batch)
2018-10-27 23:00:55,206 - Epoch: [83][   50/   78]    Loss 0.590613    Top1 80.734375    Top5 98.906250    
2018-10-27 23:00:55,599 - ==> Top1: 80.790    Top5: 98.920    Loss: 0.588

2018-10-27 23:00:55,600 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 23:00:55,600 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:00:55,618 - 

2018-10-27 23:00:55,618 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:00:56,819 - Epoch: [84][   50/  391]    Overall Loss 0.464018    Objective Loss 0.464018    Top1 83.718750    Top5 99.281250    LR 0.300000    Time 0.023965    
2018-10-27 23:00:57,989 - Epoch: [84][  100/  391]    Overall Loss 0.467701    Objective Loss 0.467701    Top1 83.757812    Top5 99.289062    LR 0.300000    Time 0.023668    
2018-10-27 23:00:59,157 - Epoch: [84][  150/  391]    Overall Loss 0.468757    Objective Loss 0.468757    Top1 83.723958    Top5 99.343750    LR 0.300000    Time 0.023563    
2018-10-27 23:01:00,333 - Epoch: [84][  200/  391]    Overall Loss 0.464807    Objective Loss 0.464807    Top1 83.773438    Top5 99.355469    LR 0.300000    Time 0.023544    
2018-10-27 23:01:01,514 - Epoch: [84][  250/  391]    Overall Loss 0.465729    Objective Loss 0.465729    Top1 83.784375    Top5 99.340625    LR 0.300000    Time 0.023553    
2018-10-27 23:01:02,687 - Epoch: [84][  300/  391]    Overall Loss 0.469808    Objective Loss 0.469808    Top1 83.674479    Top5 99.328125    LR 0.300000    Time 0.023532    
2018-10-27 23:01:03,862 - Epoch: [84][  350/  391]    Overall Loss 0.472041    Objective Loss 0.472041    Top1 83.613839    Top5 99.359375    LR 0.300000    Time 0.023516    
2018-10-27 23:01:04,903 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51084 | -0.00983 |    0.19713 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15886 | -0.00406 |    0.03536 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15230 |  0.00323 |    0.03831 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15193 | -0.00501 |    0.03679 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12285 | -0.00327 |    0.02486 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15921 | -0.00824 |    0.04559 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12058 | -0.00162 |    0.02797 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17233 | -0.00421 |    0.06262 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13723 | -0.00412 |    0.04359 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20005 | -0.01121 |    0.06607 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11257 | -0.00250 |    0.03202 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09288 | -0.00239 |    0.02304 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12344 | -0.00310 |    0.03707 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09506 | -0.00547 |    0.02460 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11756 | -0.00222 |    0.04020 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10959 | -0.00474 |    0.03844 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11274 | -0.00043 |    0.02934 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09965 | -0.00341 |    0.03163 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08148 | -0.00295 |    0.02168 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07661 | -0.00025 |    0.02091 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05101 |  0.00189 |    0.00943 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46432 | -0.07937 |    0.23700 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:01:04,903 - Total sparsity: 88.62

2018-10-27 23:01:04,903 - --- validate (epoch=84)-----------
2018-10-27 23:01:04,903 - 10000 samples (128 per mini-batch)
2018-10-27 23:01:05,634 - Epoch: [84][   50/   78]    Loss 0.597916    Top1 80.578125    Top5 98.906250    
2018-10-27 23:01:06,027 - ==> Top1: 80.620    Top5: 98.910    Loss: 0.594

2018-10-27 23:01:06,028 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 23:01:06,028 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:01:06,039 - 

2018-10-27 23:01:06,040 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:01:07,271 - Epoch: [85][   50/  391]    Overall Loss 0.456341    Objective Loss 0.456341    Top1 84.093750    Top5 99.265625    LR 0.300000    Time 0.024599    
2018-10-27 23:01:08,444 - Epoch: [85][  100/  391]    Overall Loss 0.472215    Objective Loss 0.472215    Top1 83.539062    Top5 99.203125    LR 0.300000    Time 0.024012    
2018-10-27 23:01:09,618 - Epoch: [85][  150/  391]    Overall Loss 0.472497    Objective Loss 0.472497    Top1 83.630208    Top5 99.244792    LR 0.300000    Time 0.023825    
2018-10-27 23:01:10,796 - Epoch: [85][  200/  391]    Overall Loss 0.464631    Objective Loss 0.464631    Top1 83.820312    Top5 99.324219    LR 0.300000    Time 0.023750    
2018-10-27 23:01:11,970 - Epoch: [85][  250/  391]    Overall Loss 0.465464    Objective Loss 0.465464    Top1 83.825000    Top5 99.309375    LR 0.300000    Time 0.023692    
2018-10-27 23:01:13,143 - Epoch: [85][  300/  391]    Overall Loss 0.466287    Objective Loss 0.466287    Top1 83.765625    Top5 99.296875    LR 0.300000    Time 0.023649    
2018-10-27 23:01:14,315 - Epoch: [85][  350/  391]    Overall Loss 0.466464    Objective Loss 0.466464    Top1 83.758929    Top5 99.305804    LR 0.300000    Time 0.023614    
2018-10-27 23:01:15,352 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51432 | -0.00477 |    0.19945 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15837 | -0.00523 |    0.03565 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15303 |  0.00338 |    0.03840 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15317 | -0.00472 |    0.03677 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12366 | -0.00348 |    0.02489 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15905 | -0.00783 |    0.04581 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12020 | -0.00105 |    0.02784 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17242 | -0.00532 |    0.06289 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13715 | -0.00382 |    0.04350 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20092 | -0.01032 |    0.06755 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11262 | -0.00248 |    0.03191 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09314 | -0.00224 |    0.02319 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12316 | -0.00335 |    0.03689 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09490 | -0.00511 |    0.02471 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11753 | -0.00220 |    0.04038 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10948 | -0.00494 |    0.03834 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11231 | -0.00096 |    0.02910 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09947 | -0.00325 |    0.03151 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08124 | -0.00325 |    0.02162 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07641 | -0.00019 |    0.02085 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05093 |  0.00198 |    0.00937 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46344 | -0.08023 |    0.23552 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:01:15,352 - Total sparsity: 88.62

2018-10-27 23:01:15,352 - --- validate (epoch=85)-----------
2018-10-27 23:01:15,352 - 10000 samples (128 per mini-batch)
2018-10-27 23:01:16,143 - Epoch: [85][   50/   78]    Loss 0.624061    Top1 78.984375    Top5 98.875000    
2018-10-27 23:01:16,553 - ==> Top1: 79.170    Top5: 98.950    Loss: 0.624

2018-10-27 23:01:16,554 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 23:01:16,554 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:01:16,562 - 

2018-10-27 23:01:16,563 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:01:17,791 - Epoch: [86][   50/  391]    Overall Loss 0.473109    Objective Loss 0.473109    Top1 84.015625    Top5 99.343750    LR 0.300000    Time 0.024536    
2018-10-27 23:01:18,963 - Epoch: [86][  100/  391]    Overall Loss 0.473419    Objective Loss 0.473419    Top1 83.726562    Top5 99.328125    LR 0.300000    Time 0.023965    
2018-10-27 23:01:20,137 - Epoch: [86][  150/  391]    Overall Loss 0.471801    Objective Loss 0.471801    Top1 83.765625    Top5 99.359375    LR 0.300000    Time 0.023795    
2018-10-27 23:01:21,310 - Epoch: [86][  200/  391]    Overall Loss 0.465039    Objective Loss 0.465039    Top1 83.968750    Top5 99.351562    LR 0.300000    Time 0.023708    
2018-10-27 23:01:22,488 - Epoch: [86][  250/  391]    Overall Loss 0.465435    Objective Loss 0.465435    Top1 83.978125    Top5 99.309375    LR 0.300000    Time 0.023670    
2018-10-27 23:01:23,663 - Epoch: [86][  300/  391]    Overall Loss 0.468536    Objective Loss 0.468536    Top1 83.778646    Top5 99.296875    LR 0.300000    Time 0.023639    
2018-10-27 23:01:24,836 - Epoch: [86][  350/  391]    Overall Loss 0.467362    Objective Loss 0.467362    Top1 83.794643    Top5 99.301339    LR 0.300000    Time 0.023610    
2018-10-27 23:01:25,880 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51505 | -0.00780 |    0.19908 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15926 | -0.00676 |    0.03572 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15404 |  0.00330 |    0.03833 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15326 | -0.00400 |    0.03672 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12406 | -0.00340 |    0.02461 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15921 | -0.00751 |    0.04599 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11975 | -0.00197 |    0.02782 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17181 | -0.00457 |    0.06242 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13676 | -0.00411 |    0.04325 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20271 | -0.01098 |    0.06786 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11196 | -0.00230 |    0.03189 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09300 | -0.00227 |    0.02296 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12314 | -0.00328 |    0.03693 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09464 | -0.00503 |    0.02469 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11715 | -0.00213 |    0.04024 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10935 | -0.00478 |    0.03838 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11275 | -0.00065 |    0.02950 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09957 | -0.00340 |    0.03149 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08131 | -0.00318 |    0.02154 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07636 | -0.00020 |    0.02076 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05096 |  0.00206 |    0.00936 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46158 | -0.07871 |    0.23455 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:01:25,881 - Total sparsity: 88.62

2018-10-27 23:01:25,881 - --- validate (epoch=86)-----------
2018-10-27 23:01:25,881 - 10000 samples (128 per mini-batch)
2018-10-27 23:01:26,601 - Epoch: [86][   50/   78]    Loss 1.080478    Top1 69.421875    Top5 97.312500    
2018-10-27 23:01:26,992 - ==> Top1: 69.590    Top5: 97.290    Loss: 1.074

2018-10-27 23:01:26,993 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 23:01:26,993 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:01:27,004 - 

2018-10-27 23:01:27,004 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:01:28,231 - Epoch: [87][   50/  391]    Overall Loss 0.458667    Objective Loss 0.458667    Top1 84.218750    Top5 99.218750    LR 0.300000    Time 0.024500    
2018-10-27 23:01:29,402 - Epoch: [87][  100/  391]    Overall Loss 0.463330    Objective Loss 0.463330    Top1 83.960938    Top5 99.210938    LR 0.300000    Time 0.023947    
2018-10-27 23:01:30,572 - Epoch: [87][  150/  391]    Overall Loss 0.461252    Objective Loss 0.461252    Top1 84.010417    Top5 99.234375    LR 0.300000    Time 0.023756    
2018-10-27 23:01:31,746 - Epoch: [87][  200/  391]    Overall Loss 0.465731    Objective Loss 0.465731    Top1 83.804688    Top5 99.250000    LR 0.300000    Time 0.023679    
2018-10-27 23:01:32,921 - Epoch: [87][  250/  391]    Overall Loss 0.464115    Objective Loss 0.464115    Top1 83.918750    Top5 99.243750    LR 0.300000    Time 0.023639    
2018-10-27 23:01:34,089 - Epoch: [87][  300/  391]    Overall Loss 0.466371    Objective Loss 0.466371    Top1 83.846354    Top5 99.265625    LR 0.300000    Time 0.023585    
2018-10-27 23:01:35,260 - Epoch: [87][  350/  391]    Overall Loss 0.469061    Objective Loss 0.469061    Top1 83.756696    Top5 99.270089    LR 0.300000    Time 0.023557    
2018-10-27 23:01:36,302 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51799 | -0.00588 |    0.19779 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15898 | -0.00432 |    0.03580 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15339 |  0.00306 |    0.03864 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15232 | -0.00526 |    0.03695 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12454 | -0.00278 |    0.02516 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15848 | -0.00766 |    0.04568 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11930 | -0.00187 |    0.02753 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17190 | -0.00519 |    0.06229 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13653 | -0.00369 |    0.04346 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20216 | -0.01067 |    0.06774 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11185 | -0.00225 |    0.03172 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09285 | -0.00199 |    0.02289 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12314 | -0.00305 |    0.03698 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09450 | -0.00545 |    0.02466 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11704 | -0.00201 |    0.04013 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10944 | -0.00504 |    0.03858 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11235 | -0.00081 |    0.02948 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09975 | -0.00323 |    0.03159 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08144 | -0.00347 |    0.02162 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07614 |  0.00003 |    0.02064 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05094 |  0.00193 |    0.00937 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46260 | -0.08110 |    0.23734 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:01:36,302 - Total sparsity: 88.62

2018-10-27 23:01:36,303 - --- validate (epoch=87)-----------
2018-10-27 23:01:36,303 - 10000 samples (128 per mini-batch)
2018-10-27 23:01:37,026 - Epoch: [87][   50/   78]    Loss 0.670075    Top1 78.937500    Top5 98.390625    
2018-10-27 23:01:37,417 - ==> Top1: 79.010    Top5: 98.420    Loss: 0.656

2018-10-27 23:01:37,418 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 23:01:37,418 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:01:37,428 - 

2018-10-27 23:01:37,429 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:01:38,660 - Epoch: [88][   50/  391]    Overall Loss 0.473022    Objective Loss 0.473022    Top1 83.375000    Top5 99.375000    LR 0.300000    Time 0.024579    
2018-10-27 23:01:39,832 - Epoch: [88][  100/  391]    Overall Loss 0.472735    Objective Loss 0.472735    Top1 83.742188    Top5 99.296875    LR 0.300000    Time 0.024000    
2018-10-27 23:01:41,009 - Epoch: [88][  150/  391]    Overall Loss 0.462440    Objective Loss 0.462440    Top1 84.109375    Top5 99.348958    LR 0.300000    Time 0.023838    
2018-10-27 23:01:42,188 - Epoch: [88][  200/  391]    Overall Loss 0.461726    Objective Loss 0.461726    Top1 84.164062    Top5 99.359375    LR 0.300000    Time 0.023766    
2018-10-27 23:01:43,365 - Epoch: [88][  250/  391]    Overall Loss 0.471119    Objective Loss 0.471119    Top1 83.881250    Top5 99.300000    LR 0.300000    Time 0.023717    
2018-10-27 23:01:44,537 - Epoch: [88][  300/  391]    Overall Loss 0.473774    Objective Loss 0.473774    Top1 83.744792    Top5 99.260417    LR 0.300000    Time 0.023665    
2018-10-27 23:01:45,712 - Epoch: [88][  350/  391]    Overall Loss 0.475689    Objective Loss 0.475689    Top1 83.616071    Top5 99.258929    LR 0.300000    Time 0.023637    
2018-10-27 23:01:46,759 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51821 | -0.00774 |    0.19841 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15845 | -0.00518 |    0.03590 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15351 |  0.00297 |    0.03854 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15104 | -0.00557 |    0.03649 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12431 | -0.00183 |    0.02472 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15895 | -0.00765 |    0.04520 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11982 | -0.00216 |    0.02774 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17172 | -0.00429 |    0.06225 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13679 | -0.00389 |    0.04359 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.19985 | -0.01183 |    0.06695 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11117 | -0.00203 |    0.03160 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09262 | -0.00227 |    0.02280 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12279 | -0.00267 |    0.03685 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09435 | -0.00538 |    0.02470 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11704 | -0.00220 |    0.04007 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10940 | -0.00482 |    0.03854 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11299 | -0.00137 |    0.02954 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09963 | -0.00318 |    0.03152 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08173 | -0.00355 |    0.02174 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07622 | -0.00015 |    0.02069 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05104 |  0.00204 |    0.00944 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46502 | -0.08074 |    0.23773 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:01:46,759 - Total sparsity: 88.62

2018-10-27 23:01:46,760 - --- validate (epoch=88)-----------
2018-10-27 23:01:46,760 - 10000 samples (128 per mini-batch)
2018-10-27 23:01:47,491 - Epoch: [88][   50/   78]    Loss 0.629470    Top1 79.515625    Top5 98.687500    
2018-10-27 23:01:47,912 - ==> Top1: 79.650    Top5: 98.840    Loss: 0.627

2018-10-27 23:01:47,913 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 23:01:47,913 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:01:47,923 - 

2018-10-27 23:01:47,923 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:01:49,152 - Epoch: [89][   50/  391]    Overall Loss 0.467619    Objective Loss 0.467619    Top1 83.468750    Top5 99.156250    LR 0.300000    Time 0.024532    
2018-10-27 23:01:50,324 - Epoch: [89][  100/  391]    Overall Loss 0.464610    Objective Loss 0.464610    Top1 83.804688    Top5 99.257812    LR 0.300000    Time 0.023977    
2018-10-27 23:01:51,495 - Epoch: [89][  150/  391]    Overall Loss 0.459785    Objective Loss 0.459785    Top1 84.208333    Top5 99.265625    LR 0.300000    Time 0.023781    
2018-10-27 23:01:52,666 - Epoch: [89][  200/  391]    Overall Loss 0.462782    Objective Loss 0.462782    Top1 84.031250    Top5 99.277344    LR 0.300000    Time 0.023682    
2018-10-27 23:01:53,833 - Epoch: [89][  250/  391]    Overall Loss 0.468373    Objective Loss 0.468373    Top1 83.768750    Top5 99.262500    LR 0.300000    Time 0.023608    
2018-10-27 23:01:55,006 - Epoch: [89][  300/  391]    Overall Loss 0.470666    Objective Loss 0.470666    Top1 83.630208    Top5 99.273438    LR 0.300000    Time 0.023580    
2018-10-27 23:01:56,182 - Epoch: [89][  350/  391]    Overall Loss 0.472819    Objective Loss 0.472819    Top1 83.616071    Top5 99.252232    LR 0.300000    Time 0.023567    
2018-10-27 23:01:57,229 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51730 | -0.00683 |    0.19637 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15770 | -0.00479 |    0.03541 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15239 |  0.00385 |    0.03821 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15189 | -0.00381 |    0.03684 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12518 | -0.00179 |    0.02519 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16060 | -0.00859 |    0.04572 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12021 | -0.00181 |    0.02777 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17213 | -0.00433 |    0.06226 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13715 | -0.00435 |    0.04369 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.19893 | -0.01121 |    0.06603 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11129 | -0.00256 |    0.03165 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09269 | -0.00206 |    0.02285 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12251 | -0.00321 |    0.03679 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09460 | -0.00524 |    0.02481 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11722 | -0.00225 |    0.04024 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10952 | -0.00514 |    0.03856 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11466 | -0.00225 |    0.02962 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09965 | -0.00327 |    0.03149 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08186 | -0.00345 |    0.02175 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07628 | -0.00014 |    0.02073 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05108 |  0.00197 |    0.00947 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46210 | -0.07987 |    0.23587 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:01:57,229 - Total sparsity: 88.62

2018-10-27 23:01:57,229 - --- validate (epoch=89)-----------
2018-10-27 23:01:57,229 - 10000 samples (128 per mini-batch)
2018-10-27 23:01:57,954 - Epoch: [89][   50/   78]    Loss 0.722125    Top1 76.437500    Top5 98.734375    
2018-10-27 23:01:58,347 - ==> Top1: 76.310    Top5: 98.860    Loss: 0.718

2018-10-27 23:01:58,347 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 23:01:58,348 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:01:58,364 - 

2018-10-27 23:01:58,364 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:01:59,566 - Epoch: [90][   50/  391]    Overall Loss 0.442469    Objective Loss 0.442469    Top1 84.468750    Top5 99.312500    LR 0.300000    Time 0.023997    
2018-10-27 23:02:00,739 - Epoch: [90][  100/  391]    Overall Loss 0.451821    Objective Loss 0.451821    Top1 84.117188    Top5 99.328125    LR 0.300000    Time 0.023718    
2018-10-27 23:02:01,894 - Epoch: [90][  150/  391]    Overall Loss 0.450488    Objective Loss 0.450488    Top1 84.156250    Top5 99.338542    LR 0.300000    Time 0.023498    
2018-10-27 23:02:03,033 - Epoch: [90][  200/  391]    Overall Loss 0.457351    Objective Loss 0.457351    Top1 84.039062    Top5 99.316406    LR 0.300000    Time 0.023314    
2018-10-27 23:02:04,175 - Epoch: [90][  250/  391]    Overall Loss 0.461854    Objective Loss 0.461854    Top1 84.015625    Top5 99.271875    LR 0.300000    Time 0.023213    
2018-10-27 23:02:05,314 - Epoch: [90][  300/  391]    Overall Loss 0.466180    Objective Loss 0.466180    Top1 83.815104    Top5 99.307292    LR 0.300000    Time 0.023138    
2018-10-27 23:02:06,456 - Epoch: [90][  350/  391]    Overall Loss 0.467673    Objective Loss 0.467673    Top1 83.729911    Top5 99.323661    LR 0.300000    Time 0.023091    
2018-10-27 23:02:07,471 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51536 | -0.00492 |    0.19696 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15851 | -0.00539 |    0.03533 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15210 |  0.00406 |    0.03869 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15202 | -0.00369 |    0.03658 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12502 | -0.00268 |    0.02528 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16051 | -0.00851 |    0.04655 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12085 | -0.00192 |    0.02818 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17241 | -0.00487 |    0.06244 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13734 | -0.00419 |    0.04358 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.19898 | -0.00877 |    0.06596 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11183 | -0.00252 |    0.03167 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09281 | -0.00175 |    0.02280 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12267 | -0.00310 |    0.03696 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09447 | -0.00558 |    0.02494 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11756 | -0.00197 |    0.04009 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10958 | -0.00525 |    0.03859 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11415 | -0.00114 |    0.02964 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09971 | -0.00321 |    0.03163 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08173 | -0.00334 |    0.02176 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07614 | -0.00017 |    0.02072 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05096 |  0.00201 |    0.00943 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46430 | -0.08078 |    0.23703 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:02:07,471 - Total sparsity: 88.62

2018-10-27 23:02:07,471 - --- validate (epoch=90)-----------
2018-10-27 23:02:07,472 - 10000 samples (128 per mini-batch)
2018-10-27 23:02:08,195 - Epoch: [90][   50/   78]    Loss 0.581074    Top1 81.078125    Top5 99.093750    
2018-10-27 23:02:08,588 - ==> Top1: 81.090    Top5: 99.040    Loss: 0.577

2018-10-27 23:02:08,589 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 23:02:08,589 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:02:08,607 - 

2018-10-27 23:02:08,607 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:02:09,778 - Epoch: [91][   50/  391]    Overall Loss 0.450174    Objective Loss 0.450174    Top1 84.359375    Top5 99.359375    LR 0.300000    Time 0.023383    
2018-10-27 23:02:10,919 - Epoch: [91][  100/  391]    Overall Loss 0.457052    Objective Loss 0.457052    Top1 84.195312    Top5 99.289062    LR 0.300000    Time 0.023087    
2018-10-27 23:02:12,061 - Epoch: [91][  150/  391]    Overall Loss 0.460992    Objective Loss 0.460992    Top1 84.046875    Top5 99.312500    LR 0.300000    Time 0.022993    
2018-10-27 23:02:13,202 - Epoch: [91][  200/  391]    Overall Loss 0.467918    Objective Loss 0.467918    Top1 83.898438    Top5 99.265625    LR 0.300000    Time 0.022945    
2018-10-27 23:02:14,344 - Epoch: [91][  250/  391]    Overall Loss 0.471110    Objective Loss 0.471110    Top1 83.853125    Top5 99.237500    LR 0.300000    Time 0.022917    
2018-10-27 23:02:15,486 - Epoch: [91][  300/  391]    Overall Loss 0.474032    Objective Loss 0.474032    Top1 83.703125    Top5 99.273438    LR 0.300000    Time 0.022902    
2018-10-27 23:02:16,628 - Epoch: [91][  350/  391]    Overall Loss 0.471373    Objective Loss 0.471373    Top1 83.743304    Top5 99.274554    LR 0.300000    Time 0.022888    
2018-10-27 23:02:17,640 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51698 | -0.00801 |    0.19942 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15859 | -0.00506 |    0.03599 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15102 |  0.00275 |    0.03825 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15185 | -0.00483 |    0.03697 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12474 | -0.00267 |    0.02536 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15998 | -0.00821 |    0.04588 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12040 | -0.00208 |    0.02805 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17311 | -0.00372 |    0.06251 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13738 | -0.00430 |    0.04361 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20197 | -0.00940 |    0.06584 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11170 | -0.00225 |    0.03181 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09294 | -0.00172 |    0.02268 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12254 | -0.00313 |    0.03681 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09416 | -0.00551 |    0.02474 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11739 | -0.00168 |    0.04021 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10948 | -0.00498 |    0.03850 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11440 | -0.00156 |    0.02978 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09950 | -0.00332 |    0.03161 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08176 | -0.00329 |    0.02175 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07598 | -0.00013 |    0.02076 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05086 |  0.00197 |    0.00943 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46106 | -0.08124 |    0.23647 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:02:17,641 - Total sparsity: 88.62

2018-10-27 23:02:17,641 - --- validate (epoch=91)-----------
2018-10-27 23:02:17,641 - 10000 samples (128 per mini-batch)
2018-10-27 23:02:18,361 - Epoch: [91][   50/   78]    Loss 0.808444    Top1 75.500000    Top5 97.828125    
2018-10-27 23:02:18,749 - ==> Top1: 75.350    Top5: 97.800    Loss: 0.815

2018-10-27 23:02:18,750 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 23:02:18,750 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:02:18,761 - 

2018-10-27 23:02:18,761 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:02:19,957 - Epoch: [92][   50/  391]    Overall Loss 0.463682    Objective Loss 0.463682    Top1 84.234375    Top5 99.515625    LR 0.300000    Time 0.023871    
2018-10-27 23:02:21,097 - Epoch: [92][  100/  391]    Overall Loss 0.457665    Objective Loss 0.457665    Top1 84.414062    Top5 99.429688    LR 0.300000    Time 0.023327    
2018-10-27 23:02:22,238 - Epoch: [92][  150/  391]    Overall Loss 0.466852    Objective Loss 0.466852    Top1 83.786458    Top5 99.338542    LR 0.300000    Time 0.023148    
2018-10-27 23:02:23,381 - Epoch: [92][  200/  391]    Overall Loss 0.465679    Objective Loss 0.465679    Top1 83.941406    Top5 99.339844    LR 0.300000    Time 0.023068    
2018-10-27 23:02:24,522 - Epoch: [92][  250/  391]    Overall Loss 0.469628    Objective Loss 0.469628    Top1 83.712500    Top5 99.268750    LR 0.300000    Time 0.023015    
2018-10-27 23:02:25,663 - Epoch: [92][  300/  391]    Overall Loss 0.466069    Objective Loss 0.466069    Top1 83.867188    Top5 99.273438    LR 0.300000    Time 0.022978    
2018-10-27 23:02:26,805 - Epoch: [92][  350/  391]    Overall Loss 0.467305    Objective Loss 0.467305    Top1 83.892857    Top5 99.254464    LR 0.300000    Time 0.022953    
2018-10-27 23:02:27,824 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51309 | -0.00765 |    0.19542 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15902 | -0.00516 |    0.03581 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15115 |  0.00349 |    0.03821 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15129 | -0.00489 |    0.03649 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12398 | -0.00340 |    0.02537 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16047 | -0.00823 |    0.04616 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12088 | -0.00254 |    0.02822 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17267 | -0.00365 |    0.06236 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13721 | -0.00438 |    0.04376 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20093 | -0.00768 |    0.06619 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11138 | -0.00261 |    0.03153 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09300 | -0.00182 |    0.02295 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12226 | -0.00276 |    0.03676 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09419 | -0.00525 |    0.02470 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11743 | -0.00200 |    0.04007 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10953 | -0.00514 |    0.03854 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11388 | -0.00144 |    0.02976 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09979 | -0.00324 |    0.03173 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08186 | -0.00333 |    0.02176 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07613 | -0.00011 |    0.02071 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05101 |  0.00191 |    0.00944 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.45790 | -0.07895 |    0.23415 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:02:27,824 - Total sparsity: 88.62

2018-10-27 23:02:27,824 - --- validate (epoch=92)-----------
2018-10-27 23:02:27,825 - 10000 samples (128 per mini-batch)
2018-10-27 23:02:28,554 - Epoch: [92][   50/   78]    Loss 0.722550    Top1 76.218750    Top5 98.453125    
2018-10-27 23:02:28,944 - ==> Top1: 75.870    Top5: 98.550    Loss: 0.731

2018-10-27 23:02:28,945 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 23:02:28,945 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:02:28,956 - 

2018-10-27 23:02:28,956 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:02:30,153 - Epoch: [93][   50/  391]    Overall Loss 0.460170    Objective Loss 0.460170    Top1 84.265625    Top5 99.265625    LR 0.300000    Time 0.023902    
2018-10-27 23:02:31,292 - Epoch: [93][  100/  391]    Overall Loss 0.457220    Objective Loss 0.457220    Top1 84.218750    Top5 99.328125    LR 0.300000    Time 0.023330    
2018-10-27 23:02:32,433 - Epoch: [93][  150/  391]    Overall Loss 0.466092    Objective Loss 0.466092    Top1 84.041667    Top5 99.302083    LR 0.300000    Time 0.023148    
2018-10-27 23:02:33,573 - Epoch: [93][  200/  391]    Overall Loss 0.466465    Objective Loss 0.466465    Top1 83.894531    Top5 99.343750    LR 0.300000    Time 0.023052    
2018-10-27 23:02:34,714 - Epoch: [93][  250/  391]    Overall Loss 0.469226    Objective Loss 0.469226    Top1 83.800000    Top5 99.346875    LR 0.300000    Time 0.023002    
2018-10-27 23:02:35,854 - Epoch: [93][  300/  391]    Overall Loss 0.470701    Objective Loss 0.470701    Top1 83.742188    Top5 99.315104    LR 0.300000    Time 0.022964    
2018-10-27 23:02:36,994 - Epoch: [93][  350/  391]    Overall Loss 0.472808    Objective Loss 0.472808    Top1 83.627232    Top5 99.312500    LR 0.300000    Time 0.022937    
2018-10-27 23:02:38,009 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51594 | -0.00374 |    0.20014 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15905 | -0.00640 |    0.03592 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15204 |  0.00300 |    0.03827 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15152 | -0.00431 |    0.03693 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12395 | -0.00257 |    0.02506 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16081 | -0.00744 |    0.04595 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12098 | -0.00219 |    0.02839 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17226 | -0.00424 |    0.06237 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13694 | -0.00366 |    0.04347 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20037 | -0.00954 |    0.06686 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11164 | -0.00209 |    0.03191 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09305 | -0.00184 |    0.02299 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12218 | -0.00315 |    0.03666 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09414 | -0.00528 |    0.02468 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11752 | -0.00243 |    0.04032 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10942 | -0.00543 |    0.03846 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11358 | -0.00074 |    0.02954 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09977 | -0.00332 |    0.03166 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08165 | -0.00329 |    0.02174 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07615 | -0.00016 |    0.02069 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05094 |  0.00190 |    0.00938 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46612 | -0.08142 |    0.23879 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:02:38,009 - Total sparsity: 88.62

2018-10-27 23:02:38,009 - --- validate (epoch=93)-----------
2018-10-27 23:02:38,009 - 10000 samples (128 per mini-batch)
2018-10-27 23:02:38,735 - Epoch: [93][   50/   78]    Loss 0.690849    Top1 77.390625    Top5 98.484375    
2018-10-27 23:02:39,121 - ==> Top1: 77.980    Top5: 98.610    Loss: 0.679

2018-10-27 23:02:39,122 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 23:02:39,122 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:02:39,132 - 

2018-10-27 23:02:39,132 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:02:40,328 - Epoch: [94][   50/  391]    Overall Loss 0.446445    Objective Loss 0.446445    Top1 84.281250    Top5 99.359375    LR 0.300000    Time 0.023876    
2018-10-27 23:02:41,467 - Epoch: [94][  100/  391]    Overall Loss 0.460916    Objective Loss 0.460916    Top1 84.031250    Top5 99.257812    LR 0.300000    Time 0.023314    
2018-10-27 23:02:42,606 - Epoch: [94][  150/  391]    Overall Loss 0.460692    Objective Loss 0.460692    Top1 84.046875    Top5 99.286458    LR 0.300000    Time 0.023130    
2018-10-27 23:02:43,750 - Epoch: [94][  200/  391]    Overall Loss 0.466515    Objective Loss 0.466515    Top1 83.843750    Top5 99.253906    LR 0.300000    Time 0.023057    
2018-10-27 23:02:44,889 - Epoch: [94][  250/  391]    Overall Loss 0.467944    Objective Loss 0.467944    Top1 83.815625    Top5 99.259375    LR 0.300000    Time 0.023000    
2018-10-27 23:02:46,030 - Epoch: [94][  300/  391]    Overall Loss 0.466169    Objective Loss 0.466169    Top1 83.815104    Top5 99.289062    LR 0.300000    Time 0.022963    
2018-10-27 23:02:47,171 - Epoch: [94][  350/  391]    Overall Loss 0.467542    Objective Loss 0.467542    Top1 83.792411    Top5 99.274554    LR 0.300000    Time 0.022941    
2018-10-27 23:02:48,185 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51497 | -0.00833 |    0.19588 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15883 | -0.00547 |    0.03536 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15092 |  0.00260 |    0.03810 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15178 | -0.00484 |    0.03653 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12396 | -0.00230 |    0.02485 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16056 | -0.00707 |    0.04613 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12116 | -0.00160 |    0.02808 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17208 | -0.00496 |    0.06318 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13727 | -0.00358 |    0.04376 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.19994 | -0.00603 |    0.06579 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11185 | -0.00258 |    0.03165 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09337 | -0.00191 |    0.02311 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12288 | -0.00308 |    0.03697 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09415 | -0.00508 |    0.02482 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11751 | -0.00239 |    0.04023 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10972 | -0.00532 |    0.03861 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11366 | -0.00025 |    0.02959 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09972 | -0.00350 |    0.03161 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08146 | -0.00317 |    0.02167 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07603 | -0.00011 |    0.02057 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05075 |  0.00197 |    0.00935 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46401 | -0.07902 |    0.23693 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:02:48,185 - Total sparsity: 88.62

2018-10-27 23:02:48,186 - --- validate (epoch=94)-----------
2018-10-27 23:02:48,186 - 10000 samples (128 per mini-batch)
2018-10-27 23:02:48,896 - Epoch: [94][   50/   78]    Loss 0.709454    Top1 77.031250    Top5 98.453125    
2018-10-27 23:02:49,276 - ==> Top1: 77.140    Top5: 98.540    Loss: 0.705

2018-10-27 23:02:49,277 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 23:02:49,277 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:02:49,293 - 

2018-10-27 23:02:49,293 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:02:50,467 - Epoch: [95][   50/  391]    Overall Loss 0.466366    Objective Loss 0.466366    Top1 83.656250    Top5 99.437500    LR 0.300000    Time 0.023429    
2018-10-27 23:02:51,608 - Epoch: [95][  100/  391]    Overall Loss 0.463012    Objective Loss 0.463012    Top1 83.867188    Top5 99.382812    LR 0.300000    Time 0.023114    
2018-10-27 23:02:52,749 - Epoch: [95][  150/  391]    Overall Loss 0.465752    Objective Loss 0.465752    Top1 83.786458    Top5 99.302083    LR 0.300000    Time 0.023012    
2018-10-27 23:02:53,891 - Epoch: [95][  200/  391]    Overall Loss 0.464858    Objective Loss 0.464858    Top1 83.785156    Top5 99.250000    LR 0.300000    Time 0.022963    
2018-10-27 23:02:55,033 - Epoch: [95][  250/  391]    Overall Loss 0.465074    Objective Loss 0.465074    Top1 83.834375    Top5 99.246875    LR 0.300000    Time 0.022930    
2018-10-27 23:02:56,173 - Epoch: [95][  300/  391]    Overall Loss 0.466186    Objective Loss 0.466186    Top1 83.755208    Top5 99.234375    LR 0.300000    Time 0.022905    
2018-10-27 23:02:57,314 - Epoch: [95][  350/  391]    Overall Loss 0.465191    Objective Loss 0.465191    Top1 83.805804    Top5 99.252232    LR 0.300000    Time 0.022890    
2018-10-27 23:02:58,329 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51500 | -0.01126 |    0.19727 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15846 | -0.00395 |    0.03553 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15055 |  0.00412 |    0.03816 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15296 | -0.00520 |    0.03696 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12346 | -0.00235 |    0.02524 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16033 | -0.00795 |    0.04599 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12066 | -0.00165 |    0.02780 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17177 | -0.00557 |    0.06264 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13695 | -0.00353 |    0.04384 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20078 | -0.00840 |    0.06575 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11240 | -0.00189 |    0.03197 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09368 | -0.00160 |    0.02309 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12283 | -0.00298 |    0.03690 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09420 | -0.00474 |    0.02472 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11744 | -0.00286 |    0.04027 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10976 | -0.00556 |    0.03855 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11368 | -0.00026 |    0.02946 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09989 | -0.00351 |    0.03170 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08152 | -0.00318 |    0.02173 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07602 | -0.00018 |    0.02060 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05059 |  0.00193 |    0.00933 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46448 | -0.07912 |    0.23666 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:02:58,329 - Total sparsity: 88.62

2018-10-27 23:02:58,330 - --- validate (epoch=95)-----------
2018-10-27 23:02:58,330 - 10000 samples (128 per mini-batch)
2018-10-27 23:02:59,055 - Epoch: [95][   50/   78]    Loss 0.602581    Top1 79.937500    Top5 98.937500    
2018-10-27 23:02:59,448 - ==> Top1: 80.240    Top5: 99.000    Loss: 0.589

2018-10-27 23:02:59,449 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 23:02:59,449 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:02:59,464 - 

2018-10-27 23:02:59,464 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:03:00,667 - Epoch: [96][   50/  391]    Overall Loss 0.469105    Objective Loss 0.469105    Top1 84.156250    Top5 99.328125    LR 0.300000    Time 0.024020    
2018-10-27 23:03:01,809 - Epoch: [96][  100/  391]    Overall Loss 0.465214    Objective Loss 0.465214    Top1 84.156250    Top5 99.304688    LR 0.300000    Time 0.023418    
2018-10-27 23:03:02,950 - Epoch: [96][  150/  391]    Overall Loss 0.461068    Objective Loss 0.461068    Top1 84.312500    Top5 99.348958    LR 0.300000    Time 0.023211    
2018-10-27 23:03:04,091 - Epoch: [96][  200/  391]    Overall Loss 0.464600    Objective Loss 0.464600    Top1 84.117188    Top5 99.339844    LR 0.300000    Time 0.023102    
2018-10-27 23:03:05,230 - Epoch: [96][  250/  391]    Overall Loss 0.471980    Objective Loss 0.471980    Top1 83.843750    Top5 99.293750    LR 0.300000    Time 0.023035    
2018-10-27 23:03:06,369 - Epoch: [96][  300/  391]    Overall Loss 0.471102    Objective Loss 0.471102    Top1 83.854167    Top5 99.281250    LR 0.300000    Time 0.022989    
2018-10-27 23:03:07,510 - Epoch: [96][  350/  391]    Overall Loss 0.471190    Objective Loss 0.471190    Top1 83.852679    Top5 99.270089    LR 0.300000    Time 0.022961    
2018-10-27 23:03:08,527 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51260 | -0.00695 |    0.19709 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.16024 | -0.00372 |    0.03595 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15147 |  0.00321 |    0.03836 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15356 | -0.00480 |    0.03743 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12336 | -0.00197 |    0.02514 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15947 | -0.00768 |    0.04562 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12057 | -0.00237 |    0.02750 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17217 | -0.00518 |    0.06272 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13675 | -0.00390 |    0.04378 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20200 | -0.00833 |    0.06634 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11264 | -0.00250 |    0.03205 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09409 | -0.00145 |    0.02327 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12328 | -0.00310 |    0.03723 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09475 | -0.00527 |    0.02475 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11771 | -0.00233 |    0.04034 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10982 | -0.00534 |    0.03870 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11376 | -0.00036 |    0.02972 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09985 | -0.00326 |    0.03171 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08147 | -0.00327 |    0.02174 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07614 | -0.00024 |    0.02059 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05061 |  0.00201 |    0.00929 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46057 | -0.07747 |    0.23559 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:03:08,527 - Total sparsity: 88.62

2018-10-27 23:03:08,528 - --- validate (epoch=96)-----------
2018-10-27 23:03:08,528 - 10000 samples (128 per mini-batch)
2018-10-27 23:03:09,245 - Epoch: [96][   50/   78]    Loss 0.557988    Top1 81.281250    Top5 98.937500    
2018-10-27 23:03:09,635 - ==> Top1: 81.190    Top5: 99.010    Loss: 0.557

2018-10-27 23:03:09,636 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 23:03:09,636 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:03:09,646 - 

2018-10-27 23:03:09,647 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:03:10,846 - Epoch: [97][   50/  391]    Overall Loss 0.469002    Objective Loss 0.469002    Top1 83.921875    Top5 99.171875    LR 0.300000    Time 0.023941    
2018-10-27 23:03:11,987 - Epoch: [97][  100/  391]    Overall Loss 0.474198    Objective Loss 0.474198    Top1 83.734375    Top5 99.273438    LR 0.300000    Time 0.023368    
2018-10-27 23:03:13,129 - Epoch: [97][  150/  391]    Overall Loss 0.473107    Objective Loss 0.473107    Top1 83.781250    Top5 99.244792    LR 0.300000    Time 0.023181    
2018-10-27 23:03:14,270 - Epoch: [97][  200/  391]    Overall Loss 0.473867    Objective Loss 0.473867    Top1 83.730469    Top5 99.261719    LR 0.300000    Time 0.023088    
2018-10-27 23:03:15,412 - Epoch: [97][  250/  391]    Overall Loss 0.473683    Objective Loss 0.473683    Top1 83.675000    Top5 99.315625    LR 0.300000    Time 0.023031    
2018-10-27 23:03:16,553 - Epoch: [97][  300/  391]    Overall Loss 0.471865    Objective Loss 0.471865    Top1 83.723958    Top5 99.296875    LR 0.300000    Time 0.022991    
2018-10-27 23:03:17,694 - Epoch: [97][  350/  391]    Overall Loss 0.470176    Objective Loss 0.470176    Top1 83.738839    Top5 99.316964    LR 0.300000    Time 0.022964    
2018-10-27 23:03:18,711 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51846 | -0.00392 |    0.19861 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15932 | -0.00394 |    0.03563 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15202 |  0.00269 |    0.03830 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15334 | -0.00468 |    0.03710 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12345 | -0.00245 |    0.02487 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16009 | -0.00896 |    0.04579 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12040 | -0.00194 |    0.02807 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17220 | -0.00461 |    0.06264 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13691 | -0.00371 |    0.04373 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20038 | -0.00835 |    0.06601 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11236 | -0.00240 |    0.03192 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09396 | -0.00159 |    0.02319 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12316 | -0.00341 |    0.03709 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09460 | -0.00491 |    0.02467 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11769 | -0.00247 |    0.04011 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10990 | -0.00528 |    0.03858 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11410 | -0.00080 |    0.02986 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.10008 | -0.00336 |    0.03176 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08143 | -0.00324 |    0.02163 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07612 | -0.00021 |    0.02069 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05062 |  0.00182 |    0.00932 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46378 | -0.08178 |    0.23839 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:03:18,711 - Total sparsity: 88.62

2018-10-27 23:03:18,711 - --- validate (epoch=97)-----------
2018-10-27 23:03:18,711 - 10000 samples (128 per mini-batch)
2018-10-27 23:03:19,439 - Epoch: [97][   50/   78]    Loss 0.724750    Top1 76.484375    Top5 98.593750    
2018-10-27 23:03:19,833 - ==> Top1: 75.710    Top5: 98.580    Loss: 0.734

2018-10-27 23:03:19,834 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 23:03:19,834 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:03:19,845 - 

2018-10-27 23:03:19,845 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:03:21,043 - Epoch: [98][   50/  391]    Overall Loss 0.458460    Objective Loss 0.458460    Top1 83.468750    Top5 99.296875    LR 0.300000    Time 0.023917    
2018-10-27 23:03:22,184 - Epoch: [98][  100/  391]    Overall Loss 0.462856    Objective Loss 0.462856    Top1 83.507812    Top5 99.218750    LR 0.300000    Time 0.023358    
2018-10-27 23:03:23,326 - Epoch: [98][  150/  391]    Overall Loss 0.472131    Objective Loss 0.472131    Top1 83.291667    Top5 99.234375    LR 0.300000    Time 0.023178    
2018-10-27 23:03:24,468 - Epoch: [98][  200/  391]    Overall Loss 0.468195    Objective Loss 0.468195    Top1 83.496094    Top5 99.257812    LR 0.300000    Time 0.023085    
2018-10-27 23:03:25,609 - Epoch: [98][  250/  391]    Overall Loss 0.467469    Objective Loss 0.467469    Top1 83.609375    Top5 99.262500    LR 0.300000    Time 0.023029    
2018-10-27 23:03:26,753 - Epoch: [98][  300/  391]    Overall Loss 0.467162    Objective Loss 0.467162    Top1 83.640625    Top5 99.281250    LR 0.300000    Time 0.023000    
2018-10-27 23:03:27,897 - Epoch: [98][  350/  391]    Overall Loss 0.468574    Objective Loss 0.468574    Top1 83.627232    Top5 99.279018    LR 0.300000    Time 0.022978    
2018-10-27 23:03:28,910 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51992 | -0.00685 |    0.19860 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15857 | -0.00539 |    0.03535 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15212 |  0.00142 |    0.03841 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15395 | -0.00403 |    0.03673 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12446 | -0.00228 |    0.02505 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16068 | -0.00715 |    0.04573 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12027 | -0.00086 |    0.02783 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17249 | -0.00433 |    0.06260 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13741 | -0.00334 |    0.04375 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.19998 | -0.00979 |    0.06581 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11239 | -0.00188 |    0.03191 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09349 | -0.00193 |    0.02310 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12321 | -0.00277 |    0.03719 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09444 | -0.00555 |    0.02473 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11772 | -0.00226 |    0.04026 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10992 | -0.00552 |    0.03874 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11429 | -0.00050 |    0.02990 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09995 | -0.00326 |    0.03167 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08138 | -0.00319 |    0.02161 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07604 | -0.00006 |    0.02069 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05065 |  0.00191 |    0.00931 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.46570 | -0.08138 |    0.23865 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:03:28,911 - Total sparsity: 88.62

2018-10-27 23:03:28,911 - --- validate (epoch=98)-----------
2018-10-27 23:03:28,911 - 10000 samples (128 per mini-batch)
2018-10-27 23:03:29,632 - Epoch: [98][   50/   78]    Loss 0.621350    Top1 79.953125    Top5 98.765625    
2018-10-27 23:03:30,023 - ==> Top1: 79.770    Top5: 98.780    Loss: 0.621

2018-10-27 23:03:30,024 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 23:03:30,024 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:03:30,033 - 

2018-10-27 23:03:30,034 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:03:31,233 - Epoch: [99][   50/  391]    Overall Loss 0.459928    Objective Loss 0.459928    Top1 84.078125    Top5 99.343750    LR 0.300000    Time 0.023954    
2018-10-27 23:03:32,376 - Epoch: [99][  100/  391]    Overall Loss 0.457960    Objective Loss 0.457960    Top1 84.171875    Top5 99.414062    LR 0.300000    Time 0.023394    
2018-10-27 23:03:33,519 - Epoch: [99][  150/  391]    Overall Loss 0.460417    Objective Loss 0.460417    Top1 84.078125    Top5 99.401042    LR 0.300000    Time 0.023208    
2018-10-27 23:03:34,662 - Epoch: [99][  200/  391]    Overall Loss 0.463648    Objective Loss 0.463648    Top1 84.046875    Top5 99.351562    LR 0.300000    Time 0.023114    
2018-10-27 23:03:35,805 - Epoch: [99][  250/  391]    Overall Loss 0.468097    Objective Loss 0.468097    Top1 83.846875    Top5 99.290625    LR 0.300000    Time 0.023058    
2018-10-27 23:03:36,948 - Epoch: [99][  300/  391]    Overall Loss 0.470667    Objective Loss 0.470667    Top1 83.783854    Top5 99.286458    LR 0.300000    Time 0.023022    
2018-10-27 23:03:38,090 - Epoch: [99][  350/  391]    Overall Loss 0.471557    Objective Loss 0.471557    Top1 83.685268    Top5 99.301339    LR 0.300000    Time 0.022993    
2018-10-27 23:03:39,104 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51890 | -0.00441 |    0.19706 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15847 | -0.00347 |    0.03499 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15201 |  0.00301 |    0.03816 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15419 | -0.00439 |    0.03702 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12504 | -0.00232 |    0.02503 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.16075 | -0.00835 |    0.04590 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.12023 | -0.00147 |    0.02789 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17201 | -0.00457 |    0.06199 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13683 | -0.00321 |    0.04377 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.20090 | -0.00989 |    0.06528 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11259 | -0.00215 |    0.03168 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09370 | -0.00208 |    0.02326 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12327 | -0.00328 |    0.03697 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09441 | -0.00553 |    0.02474 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11750 | -0.00221 |    0.04010 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10974 | -0.00513 |    0.03865 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11443 | -0.00053 |    0.02995 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09987 | -0.00349 |    0.03162 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08139 | -0.00302 |    0.02162 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07604 | -0.00009 |    0.02065 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05077 |  0.00187 |    0.00932 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.45950 | -0.08008 |    0.23529 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:03:39,105 - Total sparsity: 88.62

2018-10-27 23:03:39,105 - --- validate (epoch=99)-----------
2018-10-27 23:03:39,105 - 10000 samples (128 per mini-batch)
2018-10-27 23:03:39,823 - Epoch: [99][   50/   78]    Loss 0.658517    Top1 78.109375    Top5 98.828125    
2018-10-27 23:03:40,213 - ==> Top1: 78.140    Top5: 99.000    Loss: 0.666

2018-10-27 23:03:40,214 - ==> Best Top1: 82.020   On Epoch: 62

2018-10-27 23:03:40,214 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:03:40,230 - 

2018-10-27 23:03:40,230 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:03:41,402 - Epoch: [100][   50/  391]    Overall Loss 0.428403    Objective Loss 0.428403    Top1 85.125000    Top5 99.468750    LR 0.030000    Time 0.023404    
2018-10-27 23:03:42,543 - Epoch: [100][  100/  391]    Overall Loss 0.407081    Objective Loss 0.407081    Top1 85.781250    Top5 99.492188    LR 0.030000    Time 0.023099    
2018-10-27 23:03:43,684 - Epoch: [100][  150/  391]    Overall Loss 0.392405    Objective Loss 0.392405    Top1 86.369792    Top5 99.536458    LR 0.030000    Time 0.022994    
2018-10-27 23:03:44,827 - Epoch: [100][  200/  391]    Overall Loss 0.381376    Objective Loss 0.381376    Top1 86.785156    Top5 99.574219    LR 0.030000    Time 0.022954    
2018-10-27 23:03:45,968 - Epoch: [100][  250/  391]    Overall Loss 0.372632    Objective Loss 0.372632    Top1 87.075000    Top5 99.587500    LR 0.030000    Time 0.022923    
2018-10-27 23:03:47,110 - Epoch: [100][  300/  391]    Overall Loss 0.368796    Objective Loss 0.368796    Top1 87.234375    Top5 99.578125    LR 0.030000    Time 0.022906    
2018-10-27 23:03:48,252 - Epoch: [100][  350/  391]    Overall Loss 0.366458    Objective Loss 0.366458    Top1 87.332589    Top5 99.589286    LR 0.030000    Time 0.022892    
2018-10-27 23:03:49,267 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.51355 | -0.00377 |    0.19510 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15680 | -0.00340 |    0.03468 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.15041 |  0.00306 |    0.03787 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15257 | -0.00413 |    0.03679 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12371 | -0.00230 |    0.02493 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15905 | -0.00841 |    0.04535 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11897 | -0.00142 |    0.02773 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.17022 | -0.00449 |    0.06155 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13540 | -0.00336 |    0.04329 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.19882 | -0.01047 |    0.06508 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11142 | -0.00209 |    0.03143 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09273 | -0.00212 |    0.02304 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12198 | -0.00328 |    0.03670 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09344 | -0.00531 |    0.02450 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11627 | -0.00231 |    0.03973 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10859 | -0.00518 |    0.03828 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11322 | -0.00078 |    0.02973 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09882 | -0.00355 |    0.03135 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.08054 | -0.00299 |    0.02143 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07525 | -0.00021 |    0.02048 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.05024 |  0.00187 |    0.00925 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.47157 | -0.08004 |    0.24086 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:03:49,267 - Total sparsity: 88.62

2018-10-27 23:03:49,267 - --- validate (epoch=100)-----------
2018-10-27 23:03:49,267 - 10000 samples (128 per mini-batch)
2018-10-27 23:03:49,998 - Epoch: [100][   50/   78]    Loss 0.414755    Top1 86.375000    Top5 99.406250    
2018-10-27 23:03:50,391 - ==> Top1: 86.400    Top5: 99.510    Loss: 0.408

2018-10-27 23:03:50,392 - ==> Best Top1: 86.400   On Epoch: 100

2018-10-27 23:03:50,392 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:03:50,413 - 

2018-10-27 23:03:50,413 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:03:51,586 - Epoch: [101][   50/  391]    Overall Loss 0.360602    Objective Loss 0.360602    Top1 87.078125    Top5 99.531250    LR 0.030000    Time 0.023414    
2018-10-27 23:03:52,727 - Epoch: [101][  100/  391]    Overall Loss 0.341560    Objective Loss 0.341560    Top1 87.898438    Top5 99.578125    LR 0.030000    Time 0.023106    
2018-10-27 23:03:53,869 - Epoch: [101][  150/  391]    Overall Loss 0.343436    Objective Loss 0.343436    Top1 87.854167    Top5 99.635417    LR 0.030000    Time 0.023012    
2018-10-27 23:03:55,011 - Epoch: [101][  200/  391]    Overall Loss 0.341385    Objective Loss 0.341385    Top1 88.007812    Top5 99.621094    LR 0.030000    Time 0.022960    
2018-10-27 23:03:56,152 - Epoch: [101][  250/  391]    Overall Loss 0.337762    Objective Loss 0.337762    Top1 88.143750    Top5 99.631250    LR 0.030000    Time 0.022928    
2018-10-27 23:03:57,295 - Epoch: [101][  300/  391]    Overall Loss 0.335361    Objective Loss 0.335361    Top1 88.265625    Top5 99.638021    LR 0.030000    Time 0.022910    
2018-10-27 23:03:58,437 - Epoch: [101][  350/  391]    Overall Loss 0.332670    Objective Loss 0.332670    Top1 88.421875    Top5 99.654018    LR 0.030000    Time 0.022897    
2018-10-27 23:03:59,449 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.50812 | -0.00595 |    0.19306 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15513 | -0.00344 |    0.03434 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.14881 |  0.00286 |    0.03760 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.15092 | -0.00433 |    0.03644 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12238 | -0.00231 |    0.02461 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15734 | -0.00817 |    0.04496 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11769 | -0.00140 |    0.02743 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.16841 | -0.00431 |    0.06094 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13395 | -0.00333 |    0.04280 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.19673 | -0.01021 |    0.06442 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.11024 | -0.00208 |    0.03116 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09174 | -0.00206 |    0.02281 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.12069 | -0.00321 |    0.03630 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09244 | -0.00532 |    0.02427 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11503 | -0.00237 |    0.03936 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10743 | -0.00521 |    0.03788 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11201 | -0.00066 |    0.02948 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09777 | -0.00359 |    0.03105 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.07969 | -0.00297 |    0.02122 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07445 | -0.00029 |    0.02027 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04971 |  0.00186 |    0.00917 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.47807 | -0.08024 |    0.24401 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:03:59,450 - Total sparsity: 88.62

2018-10-27 23:03:59,450 - --- validate (epoch=101)-----------
2018-10-27 23:03:59,450 - 10000 samples (128 per mini-batch)
2018-10-27 23:04:00,177 - Epoch: [101][   50/   78]    Loss 0.413151    Top1 86.640625    Top5 99.468750    
2018-10-27 23:04:00,571 - ==> Top1: 86.660    Top5: 99.580    Loss: 0.404

2018-10-27 23:04:00,572 - ==> Best Top1: 86.660   On Epoch: 101

2018-10-27 23:04:00,572 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:04:00,588 - 

2018-10-27 23:04:00,588 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:04:01,758 - Epoch: [102][   50/  391]    Overall Loss 0.325863    Objective Loss 0.325863    Top1 88.953125    Top5 99.562500    LR 0.030000    Time 0.023377    
2018-10-27 23:04:02,902 - Epoch: [102][  100/  391]    Overall Loss 0.318068    Objective Loss 0.318068    Top1 89.101562    Top5 99.648438    LR 0.030000    Time 0.023108    
2018-10-27 23:04:04,044 - Epoch: [102][  150/  391]    Overall Loss 0.322975    Objective Loss 0.322975    Top1 88.932292    Top5 99.619792    LR 0.030000    Time 0.023011    
2018-10-27 23:04:05,189 - Epoch: [102][  200/  391]    Overall Loss 0.317500    Objective Loss 0.317500    Top1 89.121094    Top5 99.640625    LR 0.030000    Time 0.022977    
2018-10-27 23:04:06,332 - Epoch: [102][  250/  391]    Overall Loss 0.316700    Objective Loss 0.316700    Top1 89.103125    Top5 99.646875    LR 0.030000    Time 0.022950    
2018-10-27 23:04:07,475 - Epoch: [102][  300/  391]    Overall Loss 0.319327    Objective Loss 0.319327    Top1 89.013021    Top5 99.643229    LR 0.030000    Time 0.022931    
2018-10-27 23:04:08,619 - Epoch: [102][  350/  391]    Overall Loss 0.320016    Objective Loss 0.320016    Top1 89.029018    Top5 99.629464    LR 0.030000    Time 0.022919    
2018-10-27 23:04:09,634 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.50283 | -0.00395 |    0.19077 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15348 | -0.00370 |    0.03402 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.14722 |  0.00281 |    0.03715 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.14931 | -0.00436 |    0.03608 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.12107 | -0.00230 |    0.02440 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15565 | -0.00828 |    0.04438 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11643 | -0.00150 |    0.02719 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.16663 | -0.00431 |    0.06037 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13253 | -0.00342 |    0.04234 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.19467 | -0.00994 |    0.06372 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.10908 | -0.00200 |    0.03084 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.09078 | -0.00200 |    0.02258 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.11942 | -0.00318 |    0.03599 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09147 | -0.00526 |    0.02405 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11382 | -0.00249 |    0.03898 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10629 | -0.00521 |    0.03749 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.11081 | -0.00073 |    0.02917 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09674 | -0.00361 |    0.03074 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.07885 | -0.00294 |    0.02099 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07367 | -0.00033 |    0.02006 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04918 |  0.00183 |    0.00908 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.48239 | -0.08030 |    0.24611 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:04:09,635 - Total sparsity: 88.62

2018-10-27 23:04:09,635 - --- validate (epoch=102)-----------
2018-10-27 23:04:09,635 - 10000 samples (128 per mini-batch)
2018-10-27 23:04:10,359 - Epoch: [102][   50/   78]    Loss 0.410431    Top1 86.671875    Top5 99.390625    
2018-10-27 23:04:10,753 - ==> Top1: 86.610    Top5: 99.490    Loss: 0.403

2018-10-27 23:04:10,754 - ==> Best Top1: 86.660   On Epoch: 101

2018-10-27 23:04:10,754 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:04:10,764 - 

2018-10-27 23:04:10,765 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:04:11,962 - Epoch: [103][   50/  391]    Overall Loss 0.309378    Objective Loss 0.309378    Top1 89.234375    Top5 99.718750    LR 0.030000    Time 0.023915    
2018-10-27 23:04:13,101 - Epoch: [103][  100/  391]    Overall Loss 0.308556    Objective Loss 0.308556    Top1 89.250000    Top5 99.640625    LR 0.030000    Time 0.023335    
2018-10-27 23:04:14,241 - Epoch: [103][  150/  391]    Overall Loss 0.314522    Objective Loss 0.314522    Top1 89.119792    Top5 99.635417    LR 0.030000    Time 0.023147    
2018-10-27 23:04:15,380 - Epoch: [103][  200/  391]    Overall Loss 0.315506    Objective Loss 0.315506    Top1 89.003906    Top5 99.640625    LR 0.030000    Time 0.023049    
2018-10-27 23:04:16,521 - Epoch: [103][  250/  391]    Overall Loss 0.313841    Objective Loss 0.313841    Top1 89.106250    Top5 99.656250    LR 0.030000    Time 0.022996    
2018-10-27 23:04:17,659 - Epoch: [103][  300/  391]    Overall Loss 0.313051    Objective Loss 0.313051    Top1 89.151042    Top5 99.658854    LR 0.030000    Time 0.022953    
2018-10-27 23:04:18,802 - Epoch: [103][  350/  391]    Overall Loss 0.312184    Objective Loss 0.312184    Top1 89.225446    Top5 99.669643    LR 0.030000    Time 0.022935    
2018-10-27 23:04:19,816 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.49766 | -0.00650 |    0.18908 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15187 | -0.00387 |    0.03368 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.14567 |  0.00268 |    0.03673 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.14774 | -0.00427 |    0.03567 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.11978 | -0.00233 |    0.02417 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15401 | -0.00800 |    0.04388 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11520 | -0.00144 |    0.02692 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.16489 | -0.00417 |    0.05974 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.13114 | -0.00337 |    0.04190 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.19265 | -0.00942 |    0.06308 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.10795 | -0.00193 |    0.03057 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.08983 | -0.00193 |    0.02233 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.11817 | -0.00321 |    0.03566 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.09052 | -0.00514 |    0.02381 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11263 | -0.00245 |    0.03860 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10518 | -0.00518 |    0.03711 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.10964 | -0.00079 |    0.02891 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09574 | -0.00364 |    0.03045 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.07804 | -0.00291 |    0.02077 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07290 | -0.00040 |    0.01986 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04867 |  0.00180 |    0.00900 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.48685 | -0.08031 |    0.24816 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:04:19,816 - Total sparsity: 88.62

2018-10-27 23:04:19,816 - --- validate (epoch=103)-----------
2018-10-27 23:04:19,816 - 10000 samples (128 per mini-batch)
2018-10-27 23:04:20,534 - Epoch: [103][   50/   78]    Loss 0.403069    Top1 87.125000    Top5 99.468750    
2018-10-27 23:04:20,919 - ==> Top1: 87.040    Top5: 99.550    Loss: 0.396

2018-10-27 23:04:20,919 - ==> Best Top1: 87.040   On Epoch: 103

2018-10-27 23:04:20,920 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:04:20,937 - 

2018-10-27 23:04:20,938 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:04:22,110 - Epoch: [104][   50/  391]    Overall Loss 0.316885    Objective Loss 0.316885    Top1 89.156250    Top5 99.656250    LR 0.030000    Time 0.023413    
2018-10-27 23:04:23,252 - Epoch: [104][  100/  391]    Overall Loss 0.305871    Objective Loss 0.305871    Top1 89.437500    Top5 99.679688    LR 0.030000    Time 0.023114    
2018-10-27 23:04:24,395 - Epoch: [104][  150/  391]    Overall Loss 0.306030    Objective Loss 0.306030    Top1 89.453125    Top5 99.651042    LR 0.030000    Time 0.023016    
2018-10-27 23:04:25,535 - Epoch: [104][  200/  391]    Overall Loss 0.305784    Objective Loss 0.305784    Top1 89.406250    Top5 99.675781    LR 0.030000    Time 0.022956    
2018-10-27 23:04:26,674 - Epoch: [104][  250/  391]    Overall Loss 0.305796    Objective Loss 0.305796    Top1 89.381250    Top5 99.659375    LR 0.030000    Time 0.022917    
2018-10-27 23:04:27,815 - Epoch: [104][  300/  391]    Overall Loss 0.308762    Objective Loss 0.308762    Top1 89.296875    Top5 99.658854    LR 0.030000    Time 0.022885    
2018-10-27 23:04:28,956 - Epoch: [104][  350/  391]    Overall Loss 0.308447    Objective Loss 0.308447    Top1 89.337054    Top5 99.658482    LR 0.030000    Time 0.022869    
2018-10-27 23:04:29,971 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.49249 | -0.00460 |    0.18707 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.15030 | -0.00370 |    0.03340 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.14415 |  0.00264 |    0.03642 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.14620 | -0.00432 |    0.03546 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.11854 | -0.00230 |    0.02393 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15240 | -0.00785 |    0.04333 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11399 | -0.00141 |    0.02668 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.16319 | -0.00419 |    0.05910 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.12979 | -0.00345 |    0.04144 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.19064 | -0.00972 |    0.06243 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.10685 | -0.00189 |    0.03029 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.08891 | -0.00189 |    0.02210 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.11697 | -0.00313 |    0.03528 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.08960 | -0.00505 |    0.02355 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11146 | -0.00237 |    0.03820 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10409 | -0.00513 |    0.03672 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.10848 | -0.00085 |    0.02863 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09476 | -0.00359 |    0.03014 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.07724 | -0.00291 |    0.02056 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07215 | -0.00043 |    0.01965 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04816 |  0.00176 |    0.00891 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.48855 | -0.08023 |    0.24891 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:04:29,972 - Total sparsity: 88.62

2018-10-27 23:04:29,972 - --- validate (epoch=104)-----------
2018-10-27 23:04:29,972 - 10000 samples (128 per mini-batch)
2018-10-27 23:04:30,693 - Epoch: [104][   50/   78]    Loss 0.403666    Top1 86.921875    Top5 99.453125    
2018-10-27 23:04:31,083 - ==> Top1: 86.940    Top5: 99.520    Loss: 0.398

2018-10-27 23:04:31,084 - ==> Best Top1: 87.040   On Epoch: 103

2018-10-27 23:04:31,084 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:04:31,097 - 

2018-10-27 23:04:31,098 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:04:32,295 - Epoch: [105][   50/  391]    Overall Loss 0.297111    Objective Loss 0.297111    Top1 89.390625    Top5 99.734375    LR 0.030000    Time 0.023919    
2018-10-27 23:04:33,440 - Epoch: [105][  100/  391]    Overall Loss 0.297882    Objective Loss 0.297882    Top1 89.273438    Top5 99.750000    LR 0.030000    Time 0.023389    
2018-10-27 23:04:34,582 - Epoch: [105][  150/  391]    Overall Loss 0.301267    Objective Loss 0.301267    Top1 89.291667    Top5 99.739583    LR 0.030000    Time 0.023197    
2018-10-27 23:04:35,726 - Epoch: [105][  200/  391]    Overall Loss 0.299814    Objective Loss 0.299814    Top1 89.445312    Top5 99.750000    LR 0.030000    Time 0.023115    
2018-10-27 23:04:36,872 - Epoch: [105][  250/  391]    Overall Loss 0.301809    Objective Loss 0.301809    Top1 89.421875    Top5 99.731250    LR 0.030000    Time 0.023068    
2018-10-27 23:04:38,014 - Epoch: [105][  300/  391]    Overall Loss 0.303007    Objective Loss 0.303007    Top1 89.354167    Top5 99.729167    LR 0.030000    Time 0.023027    
2018-10-27 23:04:39,156 - Epoch: [105][  350/  391]    Overall Loss 0.303472    Objective Loss 0.303472    Top1 89.328125    Top5 99.707589    LR 0.030000    Time 0.022996    
2018-10-27 23:04:40,173 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.48749 | -0.00446 |    0.18537 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.14877 | -0.00359 |    0.03310 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.14267 |  0.00253 |    0.03597 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.14471 | -0.00431 |    0.03511 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.11732 | -0.00206 |    0.02365 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.15084 | -0.00774 |    0.04296 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11281 | -0.00146 |    0.02637 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.16153 | -0.00403 |    0.05846 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.12846 | -0.00338 |    0.04097 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.18865 | -0.01008 |    0.06190 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.10577 | -0.00177 |    0.02993 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.08801 | -0.00187 |    0.02189 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.11579 | -0.00299 |    0.03491 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.08869 | -0.00507 |    0.02330 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.11032 | -0.00235 |    0.03780 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10303 | -0.00510 |    0.03637 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.10734 | -0.00071 |    0.02834 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09380 | -0.00355 |    0.02984 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.07646 | -0.00289 |    0.02033 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07143 | -0.00046 |    0.01946 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04767 |  0.00174 |    0.00882 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.49085 | -0.08030 |    0.24992 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:04:40,173 - Total sparsity: 88.62

2018-10-27 23:04:40,173 - --- validate (epoch=105)-----------
2018-10-27 23:04:40,173 - 10000 samples (128 per mini-batch)
2018-10-27 23:04:40,897 - Epoch: [105][   50/   78]    Loss 0.397918    Top1 87.359375    Top5 99.500000    
2018-10-27 23:04:41,285 - ==> Top1: 87.300    Top5: 99.540    Loss: 0.394

2018-10-27 23:04:41,286 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:04:41,286 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:04:41,306 - 

2018-10-27 23:04:41,307 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:04:42,479 - Epoch: [106][   50/  391]    Overall Loss 0.303880    Objective Loss 0.303880    Top1 89.234375    Top5 99.625000    LR 0.030000    Time 0.023408    
2018-10-27 23:04:43,621 - Epoch: [106][  100/  391]    Overall Loss 0.303563    Objective Loss 0.303563    Top1 89.257812    Top5 99.679688    LR 0.030000    Time 0.023113    
2018-10-27 23:04:44,763 - Epoch: [106][  150/  391]    Overall Loss 0.307044    Objective Loss 0.307044    Top1 89.177083    Top5 99.687500    LR 0.030000    Time 0.023010    
2018-10-27 23:04:45,905 - Epoch: [106][  200/  391]    Overall Loss 0.303685    Objective Loss 0.303685    Top1 89.316406    Top5 99.703125    LR 0.030000    Time 0.022963    
2018-10-27 23:04:47,047 - Epoch: [106][  250/  391]    Overall Loss 0.302328    Objective Loss 0.302328    Top1 89.415625    Top5 99.718750    LR 0.030000    Time 0.022934    
2018-10-27 23:04:48,190 - Epoch: [106][  300/  391]    Overall Loss 0.302693    Objective Loss 0.302693    Top1 89.401042    Top5 99.729167    LR 0.030000    Time 0.022917    
2018-10-27 23:04:49,334 - Epoch: [106][  350/  391]    Overall Loss 0.303732    Objective Loss 0.303732    Top1 89.341518    Top5 99.709821    LR 0.030000    Time 0.022907    
2018-10-27 23:04:50,347 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.48259 | -0.00423 |    0.18343 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.14727 | -0.00360 |    0.03264 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.14121 |  0.00260 |    0.03566 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.14325 | -0.00428 |    0.03477 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.11611 | -0.00211 |    0.02336 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.14931 | -0.00749 |    0.04254 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11165 | -0.00145 |    0.02611 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.15990 | -0.00409 |    0.05790 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.12716 | -0.00331 |    0.04052 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.18673 | -0.00988 |    0.06107 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.10471 | -0.00169 |    0.02967 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.08712 | -0.00183 |    0.02163 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.11463 | -0.00296 |    0.03451 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.08780 | -0.00506 |    0.02310 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.10921 | -0.00227 |    0.03744 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10199 | -0.00505 |    0.03600 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.10623 | -0.00083 |    0.02804 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09287 | -0.00356 |    0.02954 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.07570 | -0.00286 |    0.02012 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07072 | -0.00048 |    0.01926 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04719 |  0.00169 |    0.00873 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.49260 | -0.07991 |    0.25064 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:04:50,347 - Total sparsity: 88.62

2018-10-27 23:04:50,347 - --- validate (epoch=106)-----------
2018-10-27 23:04:50,347 - 10000 samples (128 per mini-batch)
2018-10-27 23:04:51,060 - Epoch: [106][   50/   78]    Loss 0.415724    Top1 86.921875    Top5 99.484375    
2018-10-27 23:04:51,448 - ==> Top1: 86.750    Top5: 99.520    Loss: 0.412

2018-10-27 23:04:51,449 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:04:51,449 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:04:51,460 - 

2018-10-27 23:04:51,460 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:04:52,656 - Epoch: [107][   50/  391]    Overall Loss 0.287925    Objective Loss 0.287925    Top1 89.890625    Top5 99.703125    LR 0.030000    Time 0.023878    
2018-10-27 23:04:53,798 - Epoch: [107][  100/  391]    Overall Loss 0.290244    Objective Loss 0.290244    Top1 89.773438    Top5 99.742188    LR 0.030000    Time 0.023345    
2018-10-27 23:04:54,938 - Epoch: [107][  150/  391]    Overall Loss 0.290355    Objective Loss 0.290355    Top1 89.838542    Top5 99.744792    LR 0.030000    Time 0.023157    
2018-10-27 23:04:56,078 - Epoch: [107][  200/  391]    Overall Loss 0.289067    Objective Loss 0.289067    Top1 89.945312    Top5 99.746094    LR 0.030000    Time 0.023061    
2018-10-27 23:04:57,218 - Epoch: [107][  250/  391]    Overall Loss 0.293581    Objective Loss 0.293581    Top1 89.712500    Top5 99.740625    LR 0.030000    Time 0.023005    
2018-10-27 23:04:58,358 - Epoch: [107][  300/  391]    Overall Loss 0.296040    Objective Loss 0.296040    Top1 89.619792    Top5 99.736979    LR 0.030000    Time 0.022965    
2018-10-27 23:04:59,498 - Epoch: [107][  350/  391]    Overall Loss 0.295521    Objective Loss 0.295521    Top1 89.642857    Top5 99.743304    LR 0.030000    Time 0.022938    
2018-10-27 23:05:00,516 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.47775 | -0.00584 |    0.18141 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.14580 | -0.00360 |    0.03227 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.13980 |  0.00244 |    0.03523 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.14182 | -0.00433 |    0.03435 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.11494 | -0.00213 |    0.02313 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.14779 | -0.00749 |    0.04217 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.11051 | -0.00150 |    0.02585 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.15829 | -0.00389 |    0.05728 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.12588 | -0.00321 |    0.04015 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.18482 | -0.00988 |    0.06059 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.10366 | -0.00173 |    0.02936 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.08626 | -0.00183 |    0.02142 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.11350 | -0.00292 |    0.03417 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.08693 | -0.00494 |    0.02285 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.10811 | -0.00226 |    0.03707 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.10097 | -0.00500 |    0.03564 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.10513 | -0.00077 |    0.02777 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09195 | -0.00354 |    0.02925 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.07495 | -0.00283 |    0.01991 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.07002 | -0.00048 |    0.01906 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04671 |  0.00168 |    0.00864 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.49523 | -0.08018 |    0.25190 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:05:00,516 - Total sparsity: 88.62

2018-10-27 23:05:00,516 - --- validate (epoch=107)-----------
2018-10-27 23:05:00,516 - 10000 samples (128 per mini-batch)
2018-10-27 23:05:01,243 - Epoch: [107][   50/   78]    Loss 0.401725    Top1 87.093750    Top5 99.468750    
2018-10-27 23:05:01,637 - ==> Top1: 87.060    Top5: 99.550    Loss: 0.396

2018-10-27 23:05:01,638 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:05:01,638 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:05:01,648 - 

2018-10-27 23:05:01,648 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:05:02,847 - Epoch: [108][   50/  391]    Overall Loss 0.285924    Objective Loss 0.285924    Top1 89.812500    Top5 99.781250    LR 0.030000    Time 0.023930    
2018-10-27 23:05:03,988 - Epoch: [108][  100/  391]    Overall Loss 0.289120    Objective Loss 0.289120    Top1 89.890625    Top5 99.757812    LR 0.030000    Time 0.023367    
2018-10-27 23:05:05,131 - Epoch: [108][  150/  391]    Overall Loss 0.290543    Objective Loss 0.290543    Top1 89.822917    Top5 99.744792    LR 0.030000    Time 0.023192    
2018-10-27 23:05:06,272 - Epoch: [108][  200/  391]    Overall Loss 0.292248    Objective Loss 0.292248    Top1 89.667969    Top5 99.757812    LR 0.030000    Time 0.023092    
2018-10-27 23:05:07,414 - Epoch: [108][  250/  391]    Overall Loss 0.293808    Objective Loss 0.293808    Top1 89.696875    Top5 99.750000    LR 0.030000    Time 0.023034    
2018-10-27 23:05:08,558 - Epoch: [108][  300/  391]    Overall Loss 0.295553    Objective Loss 0.295553    Top1 89.570312    Top5 99.752604    LR 0.030000    Time 0.023005    
2018-10-27 23:05:09,702 - Epoch: [108][  350/  391]    Overall Loss 0.294363    Objective Loss 0.294363    Top1 89.680804    Top5 99.736607    LR 0.030000    Time 0.022982    
2018-10-27 23:05:10,717 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.47304 | -0.00591 |    0.17935 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.14437 | -0.00378 |    0.03196 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.13840 |  0.00247 |    0.03488 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.14042 | -0.00430 |    0.03404 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.11380 | -0.00205 |    0.02293 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.14633 | -0.00740 |    0.04168 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.10941 | -0.00148 |    0.02554 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.15672 | -0.00405 |    0.05676 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.12463 | -0.00326 |    0.03970 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.18300 | -0.00986 |    0.06014 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.10266 | -0.00168 |    0.02909 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.08541 | -0.00172 |    0.02118 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.11239 | -0.00281 |    0.03378 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.08608 | -0.00485 |    0.02261 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.10704 | -0.00226 |    0.03670 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.09998 | -0.00490 |    0.03526 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.10406 | -0.00071 |    0.02748 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09106 | -0.00347 |    0.02897 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.07422 | -0.00281 |    0.01972 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.06934 | -0.00053 |    0.01886 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04625 |  0.00168 |    0.00856 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.49733 | -0.07995 |    0.25278 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:05:10,718 - Total sparsity: 88.62

2018-10-27 23:05:10,718 - --- validate (epoch=108)-----------
2018-10-27 23:05:10,718 - 10000 samples (128 per mini-batch)
2018-10-27 23:05:11,441 - Epoch: [108][   50/   78]    Loss 0.392871    Top1 87.000000    Top5 99.515625    
2018-10-27 23:05:11,833 - ==> Top1: 87.280    Top5: 99.530    Loss: 0.389

2018-10-27 23:05:11,834 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:05:11,834 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:05:11,844 - 

2018-10-27 23:05:11,844 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:05:13,043 - Epoch: [109][   50/  391]    Overall Loss 0.294829    Objective Loss 0.294829    Top1 89.234375    Top5 99.781250    LR 0.030000    Time 0.023933    
2018-10-27 23:05:14,183 - Epoch: [109][  100/  391]    Overall Loss 0.290119    Objective Loss 0.290119    Top1 89.531250    Top5 99.750000    LR 0.030000    Time 0.023357    
2018-10-27 23:05:15,325 - Epoch: [109][  150/  391]    Overall Loss 0.289484    Objective Loss 0.289484    Top1 89.598958    Top5 99.750000    LR 0.030000    Time 0.023175    
2018-10-27 23:05:16,468 - Epoch: [109][  200/  391]    Overall Loss 0.287809    Objective Loss 0.287809    Top1 89.675781    Top5 99.746094    LR 0.030000    Time 0.023088    
2018-10-27 23:05:17,611 - Epoch: [109][  250/  391]    Overall Loss 0.292355    Objective Loss 0.292355    Top1 89.581250    Top5 99.737500    LR 0.030000    Time 0.023036    
2018-10-27 23:05:18,753 - Epoch: [109][  300/  391]    Overall Loss 0.290739    Objective Loss 0.290739    Top1 89.638021    Top5 99.744792    LR 0.030000    Time 0.023000    
2018-10-27 23:05:19,894 - Epoch: [109][  350/  391]    Overall Loss 0.292045    Objective Loss 0.292045    Top1 89.651786    Top5 99.736607    LR 0.030000    Time 0.022972    
2018-10-27 23:05:20,909 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.46853 | -0.00614 |    0.17800 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.14295 | -0.00370 |    0.03165 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.13704 |  0.00246 |    0.03455 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.13905 | -0.00441 |    0.03366 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.11268 | -0.00194 |    0.02270 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.14491 | -0.00712 |    0.04136 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.10832 | -0.00153 |    0.02528 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.15518 | -0.00390 |    0.05604 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.12340 | -0.00336 |    0.03923 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.18119 | -0.00978 |    0.05974 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.10166 | -0.00167 |    0.02883 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.08458 | -0.00175 |    0.02099 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.11130 | -0.00281 |    0.03345 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.08524 | -0.00485 |    0.02240 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.10599 | -0.00222 |    0.03639 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.09901 | -0.00485 |    0.03493 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.10300 | -0.00071 |    0.02717 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.09019 | -0.00344 |    0.02868 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.07351 | -0.00281 |    0.01953 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.06868 | -0.00053 |    0.01867 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04580 |  0.00166 |    0.00848 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.49857 | -0.07996 |    0.25332 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:05:20,909 - Total sparsity: 88.62

2018-10-27 23:05:20,910 - --- validate (epoch=109)-----------
2018-10-27 23:05:20,910 - 10000 samples (128 per mini-batch)
2018-10-27 23:05:21,631 - Epoch: [109][   50/   78]    Loss 0.401801    Top1 87.093750    Top5 99.562500    
2018-10-27 23:05:22,021 - ==> Top1: 87.180    Top5: 99.580    Loss: 0.393

2018-10-27 23:05:22,022 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:05:22,022 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:05:22,036 - 

2018-10-27 23:05:22,037 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:05:23,240 - Epoch: [110][   50/  391]    Overall Loss 0.283380    Objective Loss 0.283380    Top1 90.000000    Top5 99.656250    LR 0.030000    Time 0.024034    
2018-10-27 23:05:24,381 - Epoch: [110][  100/  391]    Overall Loss 0.290220    Objective Loss 0.290220    Top1 89.718750    Top5 99.703125    LR 0.030000    Time 0.023419    
2018-10-27 23:05:25,521 - Epoch: [110][  150/  391]    Overall Loss 0.286112    Objective Loss 0.286112    Top1 90.015625    Top5 99.718750    LR 0.030000    Time 0.023199    
2018-10-27 23:05:26,663 - Epoch: [110][  200/  391]    Overall Loss 0.288405    Objective Loss 0.288405    Top1 89.953125    Top5 99.722656    LR 0.030000    Time 0.023104    
2018-10-27 23:05:27,806 - Epoch: [110][  250/  391]    Overall Loss 0.288838    Objective Loss 0.288838    Top1 89.946875    Top5 99.743750    LR 0.030000    Time 0.023051    
2018-10-27 23:05:28,948 - Epoch: [110][  300/  391]    Overall Loss 0.290496    Objective Loss 0.290496    Top1 89.916667    Top5 99.752604    LR 0.030000    Time 0.023012    
2018-10-27 23:05:30,091 - Epoch: [110][  350/  391]    Overall Loss 0.290047    Objective Loss 0.290047    Top1 89.892857    Top5 99.756696    LR 0.030000    Time 0.022984    
2018-10-27 23:05:31,104 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.46415 | -0.00630 |    0.17612 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.14156 | -0.00384 |    0.03128 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.13570 |  0.00268 |    0.03426 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.13771 | -0.00422 |    0.03336 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.11157 | -0.00193 |    0.02248 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.14352 | -0.00700 |    0.04088 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.10726 | -0.00133 |    0.02511 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.15366 | -0.00380 |    0.05551 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.12219 | -0.00331 |    0.03886 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.17942 | -0.00971 |    0.05910 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.10069 | -0.00160 |    0.02859 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.08376 | -0.00170 |    0.02079 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.11024 | -0.00275 |    0.03311 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.08443 | -0.00480 |    0.02218 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.10496 | -0.00224 |    0.03604 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.09805 | -0.00485 |    0.03460 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.10197 | -0.00073 |    0.02685 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.08934 | -0.00340 |    0.02840 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.07282 | -0.00279 |    0.01933 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.06804 | -0.00054 |    0.01849 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04535 |  0.00163 |    0.00839 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.50040 | -0.07967 |    0.25391 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:05:31,104 - Total sparsity: 88.62

2018-10-27 23:05:31,104 - --- validate (epoch=110)-----------
2018-10-27 23:05:31,104 - 10000 samples (128 per mini-batch)
2018-10-27 23:05:31,833 - Epoch: [110][   50/   78]    Loss 0.407114    Top1 86.906250    Top5 99.484375    
2018-10-27 23:05:32,229 - ==> Top1: 86.840    Top5: 99.590    Loss: 0.399

2018-10-27 23:05:32,230 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:05:32,230 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:05:32,240 - 

2018-10-27 23:05:32,241 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:05:33,441 - Epoch: [111][   50/  391]    Overall Loss 0.275014    Objective Loss 0.275014    Top1 90.453125    Top5 99.703125    LR 0.030000    Time 0.023975    
2018-10-27 23:05:34,581 - Epoch: [111][  100/  391]    Overall Loss 0.291461    Objective Loss 0.291461    Top1 89.843750    Top5 99.679688    LR 0.030000    Time 0.023373    
2018-10-27 23:05:35,720 - Epoch: [111][  150/  391]    Overall Loss 0.286736    Objective Loss 0.286736    Top1 89.947917    Top5 99.734375    LR 0.030000    Time 0.023170    
2018-10-27 23:05:36,860 - Epoch: [111][  200/  391]    Overall Loss 0.287487    Objective Loss 0.287487    Top1 89.812500    Top5 99.738281    LR 0.030000    Time 0.023066    
2018-10-27 23:05:38,003 - Epoch: [111][  250/  391]    Overall Loss 0.286859    Objective Loss 0.286859    Top1 89.925000    Top5 99.731250    LR 0.030000    Time 0.023020    
2018-10-27 23:05:39,145 - Epoch: [111][  300/  391]    Overall Loss 0.289422    Objective Loss 0.289422    Top1 89.846354    Top5 99.734375    LR 0.030000    Time 0.022986    
2018-10-27 23:05:40,288 - Epoch: [111][  350/  391]    Overall Loss 0.287412    Objective Loss 0.287412    Top1 89.926339    Top5 99.750000    LR 0.030000    Time 0.022966    
2018-10-27 23:05:41,307 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.45971 | -0.00566 |    0.17423 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.14021 | -0.00377 |    0.03107 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.13439 |  0.00252 |    0.03396 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.13643 | -0.00421 |    0.03304 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.11050 | -0.00192 |    0.02232 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.14215 | -0.00697 |    0.04052 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.10623 | -0.00142 |    0.02484 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.15218 | -0.00408 |    0.05492 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.12101 | -0.00316 |    0.03844 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.17768 | -0.00943 |    0.05851 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.09973 | -0.00158 |    0.02834 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.08296 | -0.00159 |    0.02057 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.10920 | -0.00266 |    0.03279 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.08363 | -0.00468 |    0.02196 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.10396 | -0.00223 |    0.03566 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.09712 | -0.00482 |    0.03424 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.10096 | -0.00060 |    0.02659 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.08851 | -0.00337 |    0.02810 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.07214 | -0.00277 |    0.01914 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.06741 | -0.00053 |    0.01830 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04492 |  0.00160 |    0.00832 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.50153 | -0.07939 |    0.25439 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:05:41,307 - Total sparsity: 88.62

2018-10-27 23:05:41,307 - --- validate (epoch=111)-----------
2018-10-27 23:05:41,307 - 10000 samples (128 per mini-batch)
2018-10-27 23:05:42,034 - Epoch: [111][   50/   78]    Loss 0.401002    Top1 87.093750    Top5 99.390625    
2018-10-27 23:05:42,427 - ==> Top1: 87.170    Top5: 99.530    Loss: 0.394

2018-10-27 23:05:42,428 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:05:42,428 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:05:42,439 - 

2018-10-27 23:05:42,439 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:05:43,638 - Epoch: [112][   50/  391]    Overall Loss 0.296492    Objective Loss 0.296492    Top1 89.390625    Top5 99.734375    LR 0.030000    Time 0.023939    
2018-10-27 23:05:44,780 - Epoch: [112][  100/  391]    Overall Loss 0.291708    Objective Loss 0.291708    Top1 89.710938    Top5 99.734375    LR 0.030000    Time 0.023377    
2018-10-27 23:05:45,922 - Epoch: [112][  150/  391]    Overall Loss 0.294981    Objective Loss 0.294981    Top1 89.536458    Top5 99.739583    LR 0.030000    Time 0.023188    
2018-10-27 23:05:47,066 - Epoch: [112][  200/  391]    Overall Loss 0.293244    Objective Loss 0.293244    Top1 89.640625    Top5 99.726562    LR 0.030000    Time 0.023104    
2018-10-27 23:05:48,211 - Epoch: [112][  250/  391]    Overall Loss 0.294633    Objective Loss 0.294633    Top1 89.590625    Top5 99.709375    LR 0.030000    Time 0.023061    
2018-10-27 23:05:49,355 - Epoch: [112][  300/  391]    Overall Loss 0.293677    Objective Loss 0.293677    Top1 89.606771    Top5 99.690104    LR 0.030000    Time 0.023024    
2018-10-27 23:05:50,499 - Epoch: [112][  350/  391]    Overall Loss 0.293266    Objective Loss 0.293266    Top1 89.654018    Top5 99.696429    LR 0.030000    Time 0.023001    
2018-10-27 23:05:51,512 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.45537 | -0.00540 |    0.17279 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.13890 | -0.00377 |    0.03085 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.13310 |  0.00238 |    0.03369 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.13514 | -0.00406 |    0.03265 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.10944 | -0.00192 |    0.02204 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.14081 | -0.00692 |    0.04023 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.10521 | -0.00161 |    0.02458 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.15073 | -0.00421 |    0.05430 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.11986 | -0.00305 |    0.03801 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.17596 | -0.00926 |    0.05775 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.09879 | -0.00154 |    0.02806 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.08218 | -0.00146 |    0.02035 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.10819 | -0.00267 |    0.03248 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.08286 | -0.00465 |    0.02173 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.10297 | -0.00221 |    0.03533 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.09621 | -0.00476 |    0.03389 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.09997 | -0.00042 |    0.02635 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.08770 | -0.00334 |    0.02785 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.07148 | -0.00278 |    0.01895 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.06680 | -0.00057 |    0.01812 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04450 |  0.00157 |    0.00824 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.50139 | -0.07920 |    0.25432 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:05:51,512 - Total sparsity: 88.62

2018-10-27 23:05:51,512 - --- validate (epoch=112)-----------
2018-10-27 23:05:51,512 - 10000 samples (128 per mini-batch)
2018-10-27 23:05:52,238 - Epoch: [112][   50/   78]    Loss 0.410080    Top1 86.906250    Top5 99.468750    
2018-10-27 23:05:52,632 - ==> Top1: 86.910    Top5: 99.530    Loss: 0.406

2018-10-27 23:05:52,632 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:05:52,632 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:05:52,643 - 

2018-10-27 23:05:52,643 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:05:53,841 - Epoch: [113][   50/  391]    Overall Loss 0.272887    Objective Loss 0.272887    Top1 90.703125    Top5 99.781250    LR 0.030000    Time 0.023921    
2018-10-27 23:05:54,981 - Epoch: [113][  100/  391]    Overall Loss 0.277270    Objective Loss 0.277270    Top1 90.437500    Top5 99.796875    LR 0.030000    Time 0.023348    
2018-10-27 23:05:56,123 - Epoch: [113][  150/  391]    Overall Loss 0.279235    Objective Loss 0.279235    Top1 90.328125    Top5 99.755208    LR 0.030000    Time 0.023165    
2018-10-27 23:05:57,264 - Epoch: [113][  200/  391]    Overall Loss 0.280580    Objective Loss 0.280580    Top1 90.257812    Top5 99.738281    LR 0.030000    Time 0.023074    
2018-10-27 23:05:58,405 - Epoch: [113][  250/  391]    Overall Loss 0.278551    Objective Loss 0.278551    Top1 90.300000    Top5 99.740625    LR 0.030000    Time 0.023018    
2018-10-27 23:05:59,546 - Epoch: [113][  300/  391]    Overall Loss 0.281542    Objective Loss 0.281542    Top1 90.244792    Top5 99.729167    LR 0.030000    Time 0.022980    
2018-10-27 23:06:00,691 - Epoch: [113][  350/  391]    Overall Loss 0.281108    Objective Loss 0.281108    Top1 90.252232    Top5 99.736607    LR 0.030000    Time 0.022965    
2018-10-27 23:06:01,709 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.45116 | -0.00666 |    0.17054 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.13761 | -0.00378 |    0.03068 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.13185 |  0.00244 |    0.03331 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.13389 | -0.00419 |    0.03231 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.10839 | -0.00197 |    0.02181 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.13950 | -0.00684 |    0.03986 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.10422 | -0.00136 |    0.02438 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.14931 | -0.00419 |    0.05378 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.11873 | -0.00306 |    0.03765 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.17427 | -0.00950 |    0.05729 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.09788 | -0.00145 |    0.02777 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.08142 | -0.00148 |    0.02012 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.10719 | -0.00270 |    0.03219 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.08209 | -0.00456 |    0.02149 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.10201 | -0.00207 |    0.03496 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.09532 | -0.00471 |    0.03355 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.09900 | -0.00028 |    0.02608 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.08691 | -0.00330 |    0.02758 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.07085 | -0.00271 |    0.01876 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.06620 | -0.00056 |    0.01795 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04408 |  0.00156 |    0.00815 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.50333 | -0.07944 |    0.25511 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:06:01,709 - Total sparsity: 88.62

2018-10-27 23:06:01,709 - --- validate (epoch=113)-----------
2018-10-27 23:06:01,709 - 10000 samples (128 per mini-batch)
2018-10-27 23:06:02,428 - Epoch: [113][   50/   78]    Loss 0.396671    Top1 87.171875    Top5 99.515625    
2018-10-27 23:06:02,817 - ==> Top1: 87.190    Top5: 99.550    Loss: 0.392

2018-10-27 23:06:02,818 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:06:02,818 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:06:02,829 - 

2018-10-27 23:06:02,829 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:06:04,030 - Epoch: [114][   50/  391]    Overall Loss 0.270846    Objective Loss 0.270846    Top1 90.390625    Top5 99.750000    LR 0.030000    Time 0.023980    
2018-10-27 23:06:05,174 - Epoch: [114][  100/  391]    Overall Loss 0.270958    Objective Loss 0.270958    Top1 90.507812    Top5 99.765625    LR 0.030000    Time 0.023413    
2018-10-27 23:06:06,318 - Epoch: [114][  150/  391]    Overall Loss 0.271473    Objective Loss 0.271473    Top1 90.604167    Top5 99.796875    LR 0.030000    Time 0.023233    
2018-10-27 23:06:07,465 - Epoch: [114][  200/  391]    Overall Loss 0.276061    Objective Loss 0.276061    Top1 90.406250    Top5 99.761719    LR 0.030000    Time 0.023152    
2018-10-27 23:06:08,608 - Epoch: [114][  250/  391]    Overall Loss 0.277642    Objective Loss 0.277642    Top1 90.300000    Top5 99.753125    LR 0.030000    Time 0.023072    
2018-10-27 23:06:09,751 - Epoch: [114][  300/  391]    Overall Loss 0.276976    Objective Loss 0.276976    Top1 90.380208    Top5 99.760417    LR 0.030000    Time 0.023033    
2018-10-27 23:06:10,894 - Epoch: [114][  350/  391]    Overall Loss 0.277869    Objective Loss 0.277869    Top1 90.352679    Top5 99.754464    LR 0.030000    Time 0.023004    
2018-10-27 23:06:11,911 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.44718 | -0.00523 |    0.16921 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.13634 | -0.00373 |    0.03031 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.13062 |  0.00247 |    0.03300 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.13267 | -0.00426 |    0.03206 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.10738 | -0.00197 |    0.02158 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.13823 | -0.00653 |    0.03937 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.10324 | -0.00130 |    0.02418 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.14792 | -0.00418 |    0.05331 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.11762 | -0.00295 |    0.03736 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.17260 | -0.00915 |    0.05663 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.09700 | -0.00139 |    0.02755 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.08067 | -0.00146 |    0.01993 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.10623 | -0.00264 |    0.03192 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.08135 | -0.00449 |    0.02126 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.10106 | -0.00199 |    0.03462 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.09445 | -0.00467 |    0.03325 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.09804 | -0.00034 |    0.02582 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.08614 | -0.00323 |    0.02732 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.07022 | -0.00267 |    0.01858 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.06563 | -0.00058 |    0.01778 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04367 |  0.00155 |    0.00807 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.50515 | -0.07932 |    0.25584 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:06:11,911 - Total sparsity: 88.62

2018-10-27 23:06:11,911 - --- validate (epoch=114)-----------
2018-10-27 23:06:11,911 - 10000 samples (128 per mini-batch)
2018-10-27 23:06:12,631 - Epoch: [114][   50/   78]    Loss 0.403115    Top1 86.921875    Top5 99.468750    
2018-10-27 23:06:13,025 - ==> Top1: 86.810    Top5: 99.510    Loss: 0.400

2018-10-27 23:06:13,025 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:06:13,025 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:06:13,035 - 

2018-10-27 23:06:13,035 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:06:14,236 - Epoch: [115][   50/  391]    Overall Loss 0.269600    Objective Loss 0.269600    Top1 90.125000    Top5 99.828125    LR 0.030000    Time 0.023976    
2018-10-27 23:06:15,380 - Epoch: [115][  100/  391]    Overall Loss 0.269451    Objective Loss 0.269451    Top1 90.320312    Top5 99.796875    LR 0.030000    Time 0.023415    
2018-10-27 23:06:16,524 - Epoch: [115][  150/  391]    Overall Loss 0.275717    Objective Loss 0.275717    Top1 90.192708    Top5 99.770833    LR 0.030000    Time 0.023229    
2018-10-27 23:06:17,667 - Epoch: [115][  200/  391]    Overall Loss 0.274332    Objective Loss 0.274332    Top1 90.246094    Top5 99.781250    LR 0.030000    Time 0.023130    
2018-10-27 23:06:18,808 - Epoch: [115][  250/  391]    Overall Loss 0.280241    Objective Loss 0.280241    Top1 90.125000    Top5 99.768750    LR 0.030000    Time 0.023062    
2018-10-27 23:06:19,952 - Epoch: [115][  300/  391]    Overall Loss 0.283279    Objective Loss 0.283279    Top1 90.007812    Top5 99.776042    LR 0.030000    Time 0.023015    
2018-10-27 23:06:21,095 - Epoch: [115][  350/  391]    Overall Loss 0.283428    Objective Loss 0.283428    Top1 90.029018    Top5 99.761161    LR 0.030000    Time 0.022989    
2018-10-27 23:06:22,111 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.44320 | -0.00497 |    0.16788 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.13512 | -0.00378 |    0.02989 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.12942 |  0.00214 |    0.03277 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.13148 | -0.00386 |    0.03164 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.10638 | -0.00198 |    0.02141 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.13699 | -0.00650 |    0.03891 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.10229 | -0.00138 |    0.02397 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.14656 | -0.00437 |    0.05282 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.11655 | -0.00298 |    0.03702 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.17098 | -0.00931 |    0.05603 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.09614 | -0.00142 |    0.02728 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.07995 | -0.00143 |    0.01976 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.10530 | -0.00252 |    0.03159 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.08064 | -0.00444 |    0.02107 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.10015 | -0.00197 |    0.03432 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.09361 | -0.00454 |    0.03293 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.09711 | -0.00036 |    0.02557 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.08540 | -0.00319 |    0.02707 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06962 | -0.00262 |    0.01840 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.06507 | -0.00057 |    0.01763 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04328 |  0.00154 |    0.00799 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.50509 | -0.07911 |    0.25564 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:06:22,111 - Total sparsity: 88.62

2018-10-27 23:06:22,111 - --- validate (epoch=115)-----------
2018-10-27 23:06:22,112 - 10000 samples (128 per mini-batch)
2018-10-27 23:06:22,838 - Epoch: [115][   50/   78]    Loss 0.403900    Top1 87.000000    Top5 99.406250    
2018-10-27 23:06:23,229 - ==> Top1: 86.910    Top5: 99.500    Loss: 0.401

2018-10-27 23:06:23,230 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:06:23,230 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:06:23,240 - 

2018-10-27 23:06:23,240 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:06:24,438 - Epoch: [116][   50/  391]    Overall Loss 0.274967    Objective Loss 0.274967    Top1 90.328125    Top5 99.781250    LR 0.030000    Time 0.023922    
2018-10-27 23:06:25,579 - Epoch: [116][  100/  391]    Overall Loss 0.278016    Objective Loss 0.278016    Top1 90.414062    Top5 99.750000    LR 0.030000    Time 0.023352    
2018-10-27 23:06:26,718 - Epoch: [116][  150/  391]    Overall Loss 0.277023    Objective Loss 0.277023    Top1 90.411458    Top5 99.750000    LR 0.030000    Time 0.023158    
2018-10-27 23:06:27,857 - Epoch: [116][  200/  391]    Overall Loss 0.282231    Objective Loss 0.282231    Top1 90.160156    Top5 99.734375    LR 0.030000    Time 0.023057    
2018-10-27 23:06:28,999 - Epoch: [116][  250/  391]    Overall Loss 0.280088    Objective Loss 0.280088    Top1 90.218750    Top5 99.718750    LR 0.030000    Time 0.023005    
2018-10-27 23:06:30,138 - Epoch: [116][  300/  391]    Overall Loss 0.280805    Objective Loss 0.280805    Top1 90.166667    Top5 99.744792    LR 0.030000    Time 0.022966    
2018-10-27 23:06:31,281 - Epoch: [116][  350/  391]    Overall Loss 0.280552    Objective Loss 0.280552    Top1 90.198661    Top5 99.756696    LR 0.030000    Time 0.022945    
2018-10-27 23:06:32,297 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.43931 | -0.00542 |    0.16573 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.13391 | -0.00384 |    0.02965 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.12825 |  0.00228 |    0.03240 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.13028 | -0.00384 |    0.03130 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.10540 | -0.00181 |    0.02123 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.13578 | -0.00623 |    0.03860 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.10136 | -0.00127 |    0.02377 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.14525 | -0.00414 |    0.05237 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.11550 | -0.00283 |    0.03666 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.16943 | -0.00839 |    0.05560 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.09530 | -0.00148 |    0.02701 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.07924 | -0.00139 |    0.01957 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.10438 | -0.00253 |    0.03136 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.07993 | -0.00432 |    0.02086 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.09925 | -0.00195 |    0.03402 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.09279 | -0.00448 |    0.03261 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.09619 | -0.00038 |    0.02529 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.08467 | -0.00318 |    0.02683 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06902 | -0.00260 |    0.01823 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.06452 | -0.00059 |    0.01747 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04289 |  0.00152 |    0.00791 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.50657 | -0.07905 |    0.25608 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:06:32,297 - Total sparsity: 88.62

2018-10-27 23:06:32,297 - --- validate (epoch=116)-----------
2018-10-27 23:06:32,297 - 10000 samples (128 per mini-batch)
2018-10-27 23:06:33,019 - Epoch: [116][   50/   78]    Loss 0.401048    Top1 87.328125    Top5 99.468750    
2018-10-27 23:06:33,410 - ==> Top1: 87.190    Top5: 99.570    Loss: 0.395

2018-10-27 23:06:33,410 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:06:33,411 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:06:33,427 - 

2018-10-27 23:06:33,427 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:06:34,601 - Epoch: [117][   50/  391]    Overall Loss 0.280122    Objective Loss 0.280122    Top1 89.984375    Top5 99.796875    LR 0.030000    Time 0.023439    
2018-10-27 23:06:35,742 - Epoch: [117][  100/  391]    Overall Loss 0.277403    Objective Loss 0.277403    Top1 90.187500    Top5 99.773438    LR 0.030000    Time 0.023120    
2018-10-27 23:06:36,884 - Epoch: [117][  150/  391]    Overall Loss 0.280235    Objective Loss 0.280235    Top1 90.109375    Top5 99.760417    LR 0.030000    Time 0.023017    
2018-10-27 23:06:38,024 - Epoch: [117][  200/  391]    Overall Loss 0.277405    Objective Loss 0.277405    Top1 90.250000    Top5 99.765625    LR 0.030000    Time 0.022956    
2018-10-27 23:06:39,166 - Epoch: [117][  250/  391]    Overall Loss 0.280073    Objective Loss 0.280073    Top1 90.181250    Top5 99.759375    LR 0.030000    Time 0.022928    
2018-10-27 23:06:40,308 - Epoch: [117][  300/  391]    Overall Loss 0.279458    Objective Loss 0.279458    Top1 90.158854    Top5 99.770833    LR 0.030000    Time 0.022909    
2018-10-27 23:06:41,449 - Epoch: [117][  350/  391]    Overall Loss 0.281366    Objective Loss 0.281366    Top1 90.145089    Top5 99.761161    LR 0.030000    Time 0.022894    
2018-10-27 23:06:42,460 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.43560 | -0.00483 |    0.16448 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.13272 | -0.00398 |    0.02934 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.12710 |  0.00214 |    0.03217 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.12914 | -0.00406 |    0.03101 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.10447 | -0.00179 |    0.02097 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.13458 | -0.00621 |    0.03819 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.10044 | -0.00135 |    0.02350 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.14396 | -0.00410 |    0.05190 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.11446 | -0.00278 |    0.03629 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.16786 | -0.00867 |    0.05512 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.09448 | -0.00143 |    0.02674 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.07855 | -0.00137 |    0.01940 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.10349 | -0.00250 |    0.03105 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.07925 | -0.00425 |    0.02068 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.09838 | -0.00183 |    0.03366 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.09199 | -0.00448 |    0.03232 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.09528 | -0.00036 |    0.02505 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.08396 | -0.00311 |    0.02661 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06845 | -0.00258 |    0.01805 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.06400 | -0.00056 |    0.01733 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04251 |  0.00150 |    0.00784 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.50685 | -0.07900 |    0.25632 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:06:42,460 - Total sparsity: 88.62

2018-10-27 23:06:42,461 - --- validate (epoch=117)-----------
2018-10-27 23:06:42,461 - 10000 samples (128 per mini-batch)
2018-10-27 23:06:43,176 - Epoch: [117][   50/   78]    Loss 0.405196    Top1 86.953125    Top5 99.562500    
2018-10-27 23:06:43,558 - ==> Top1: 86.810    Top5: 99.630    Loss: 0.403

2018-10-27 23:06:43,559 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:06:43,559 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:06:43,570 - 

2018-10-27 23:06:43,570 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:06:44,766 - Epoch: [118][   50/  391]    Overall Loss 0.262441    Objective Loss 0.262441    Top1 91.000000    Top5 99.750000    LR 0.030000    Time 0.023889    
2018-10-27 23:06:45,905 - Epoch: [118][  100/  391]    Overall Loss 0.272870    Objective Loss 0.272870    Top1 90.500000    Top5 99.789062    LR 0.030000    Time 0.023321    
2018-10-27 23:06:47,046 - Epoch: [118][  150/  391]    Overall Loss 0.276983    Objective Loss 0.276983    Top1 90.395833    Top5 99.796875    LR 0.030000    Time 0.023142    
2018-10-27 23:06:48,188 - Epoch: [118][  200/  391]    Overall Loss 0.276524    Objective Loss 0.276524    Top1 90.371094    Top5 99.789062    LR 0.030000    Time 0.023061    
2018-10-27 23:06:49,330 - Epoch: [118][  250/  391]    Overall Loss 0.278062    Objective Loss 0.278062    Top1 90.256250    Top5 99.784375    LR 0.030000    Time 0.023012    
2018-10-27 23:06:50,466 - Epoch: [118][  300/  391]    Overall Loss 0.280554    Objective Loss 0.280554    Top1 90.216146    Top5 99.770833    LR 0.030000    Time 0.022961    
2018-10-27 23:06:51,607 - Epoch: [118][  350/  391]    Overall Loss 0.279975    Objective Loss 0.279975    Top1 90.261161    Top5 99.763393    LR 0.030000    Time 0.022936    
2018-10-27 23:06:52,619 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.43183 | -0.00587 |    0.16298 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.13157 | -0.00387 |    0.02913 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.12599 |  0.00215 |    0.03182 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.12804 | -0.00392 |    0.03075 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.10353 | -0.00182 |    0.02074 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.13340 | -0.00621 |    0.03780 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.09953 | -0.00122 |    0.02332 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.14269 | -0.00409 |    0.05151 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.11346 | -0.00278 |    0.03595 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.16637 | -0.00823 |    0.05449 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.09369 | -0.00146 |    0.02656 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.07787 | -0.00151 |    0.01926 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.10263 | -0.00242 |    0.03076 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.07858 | -0.00414 |    0.02049 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.09752 | -0.00181 |    0.03337 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.09120 | -0.00443 |    0.03202 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.09438 | -0.00033 |    0.02482 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.08327 | -0.00307 |    0.02636 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06789 | -0.00253 |    0.01789 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.06348 | -0.00054 |    0.01718 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04214 |  0.00147 |    0.00777 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.50793 | -0.07906 |    0.25675 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:06:52,619 - Total sparsity: 88.62

2018-10-27 23:06:52,619 - --- validate (epoch=118)-----------
2018-10-27 23:06:52,619 - 10000 samples (128 per mini-batch)
2018-10-27 23:06:53,343 - Epoch: [118][   50/   78]    Loss 0.395970    Top1 87.171875    Top5 99.515625    
2018-10-27 23:06:53,735 - ==> Top1: 87.220    Top5: 99.610    Loss: 0.392

2018-10-27 23:06:53,736 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:06:53,736 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:06:53,746 - 

2018-10-27 23:06:53,747 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:06:54,942 - Epoch: [119][   50/  391]    Overall Loss 0.272864    Objective Loss 0.272864    Top1 90.437500    Top5 99.796875    LR 0.030000    Time 0.023883    
2018-10-27 23:06:56,083 - Epoch: [119][  100/  391]    Overall Loss 0.276696    Objective Loss 0.276696    Top1 90.085938    Top5 99.757812    LR 0.030000    Time 0.023331    
2018-10-27 23:06:57,225 - Epoch: [119][  150/  391]    Overall Loss 0.278763    Objective Loss 0.278763    Top1 90.010417    Top5 99.744792    LR 0.030000    Time 0.023159    
2018-10-27 23:06:58,366 - Epoch: [119][  200/  391]    Overall Loss 0.280025    Objective Loss 0.280025    Top1 90.066406    Top5 99.738281    LR 0.030000    Time 0.023066    
2018-10-27 23:06:59,506 - Epoch: [119][  250/  391]    Overall Loss 0.280673    Objective Loss 0.280673    Top1 90.071875    Top5 99.750000    LR 0.030000    Time 0.023008    
2018-10-27 23:07:00,649 - Epoch: [119][  300/  391]    Overall Loss 0.281331    Objective Loss 0.281331    Top1 90.078125    Top5 99.752604    LR 0.030000    Time 0.022980    
2018-10-27 23:07:01,808 - Epoch: [119][  350/  391]    Overall Loss 0.281996    Objective Loss 0.281996    Top1 90.111607    Top5 99.736607    LR 0.030000    Time 0.023004    
2018-10-27 23:07:02,856 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.42836 | -0.00567 |    0.16144 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.13046 | -0.00388 |    0.02891 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.12491 |  0.00229 |    0.03154 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.12698 | -0.00367 |    0.03046 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.10263 | -0.00188 |    0.02058 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.13229 | -0.00590 |    0.03737 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.09867 | -0.00126 |    0.02309 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.14148 | -0.00407 |    0.05116 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.11248 | -0.00283 |    0.03563 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.16486 | -0.00833 |    0.05397 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.09292 | -0.00156 |    0.02633 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.07722 | -0.00148 |    0.01907 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.10178 | -0.00241 |    0.03050 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.07793 | -0.00412 |    0.02032 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.09670 | -0.00174 |    0.03308 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.09045 | -0.00434 |    0.03172 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.09350 | -0.00028 |    0.02454 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.08261 | -0.00303 |    0.02612 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06736 | -0.00251 |    0.01775 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.06299 | -0.00052 |    0.01702 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04179 |  0.00144 |    0.00770 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.50791 | -0.07877 |    0.25667 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:07:02,856 - Total sparsity: 88.62

2018-10-27 23:07:02,856 - --- validate (epoch=119)-----------
2018-10-27 23:07:02,856 - 10000 samples (128 per mini-batch)
2018-10-27 23:07:03,586 - Epoch: [119][   50/   78]    Loss 0.397866    Top1 87.250000    Top5 99.453125    
2018-10-27 23:07:03,983 - ==> Top1: 87.260    Top5: 99.590    Loss: 0.395

2018-10-27 23:07:03,983 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:07:03,984 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:07:03,993 - 

2018-10-27 23:07:03,994 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:07:05,226 - Epoch: [120][   50/  391]    Overall Loss 0.263838    Objective Loss 0.263838    Top1 90.718750    Top5 99.906250    LR 0.030000    Time 0.024610    
2018-10-27 23:07:06,397 - Epoch: [120][  100/  391]    Overall Loss 0.269507    Objective Loss 0.269507    Top1 90.648438    Top5 99.796875    LR 0.030000    Time 0.024001    
2018-10-27 23:07:07,572 - Epoch: [120][  150/  391]    Overall Loss 0.271660    Objective Loss 0.271660    Top1 90.541667    Top5 99.765625    LR 0.030000    Time 0.023822    
2018-10-27 23:07:08,748 - Epoch: [120][  200/  391]    Overall Loss 0.269126    Objective Loss 0.269126    Top1 90.582031    Top5 99.773438    LR 0.030000    Time 0.023740    
2018-10-27 23:07:09,922 - Epoch: [120][  250/  391]    Overall Loss 0.272364    Objective Loss 0.272364    Top1 90.446875    Top5 99.771875    LR 0.030000    Time 0.023685    
2018-10-27 23:07:11,099 - Epoch: [120][  300/  391]    Overall Loss 0.274948    Objective Loss 0.274948    Top1 90.354167    Top5 99.760417    LR 0.030000    Time 0.023655    
2018-10-27 23:07:12,275 - Epoch: [120][  350/  391]    Overall Loss 0.275163    Objective Loss 0.275163    Top1 90.334821    Top5 99.765625    LR 0.030000    Time 0.023631    
2018-10-27 23:07:13,319 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.42476 | -0.00570 |    0.16024 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.12941 | -0.00389 |    0.02856 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.12387 |  0.00198 |    0.03126 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.12593 | -0.00393 |    0.03016 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.10173 | -0.00186 |    0.02036 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.13121 | -0.00584 |    0.03709 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.09783 | -0.00129 |    0.02283 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.14028 | -0.00410 |    0.05063 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.11153 | -0.00285 |    0.03535 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.16346 | -0.00772 |    0.05344 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.09216 | -0.00151 |    0.02611 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.07659 | -0.00140 |    0.01890 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.10097 | -0.00240 |    0.03023 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.07730 | -0.00408 |    0.02011 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.09589 | -0.00175 |    0.03278 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.08971 | -0.00423 |    0.03144 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.09265 | -0.00025 |    0.02428 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.08196 | -0.00302 |    0.02590 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06683 | -0.00246 |    0.01759 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.06251 | -0.00054 |    0.01686 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04144 |  0.00144 |    0.00762 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.50917 | -0.07894 |    0.25710 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:07:13,320 - Total sparsity: 88.62

2018-10-27 23:07:13,320 - --- validate (epoch=120)-----------
2018-10-27 23:07:13,320 - 10000 samples (128 per mini-batch)
2018-10-27 23:07:14,051 - Epoch: [120][   50/   78]    Loss 0.397380    Top1 86.968750    Top5 99.546875    
2018-10-27 23:07:14,443 - ==> Top1: 86.920    Top5: 99.580    Loss: 0.400

2018-10-27 23:07:14,444 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:07:14,444 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:07:14,460 - 

2018-10-27 23:07:14,460 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:07:15,662 - Epoch: [121][   50/  391]    Overall Loss 0.275513    Objective Loss 0.275513    Top1 90.375000    Top5 99.781250    LR 0.030000    Time 0.023994    
2018-10-27 23:07:16,835 - Epoch: [121][  100/  391]    Overall Loss 0.279338    Objective Loss 0.279338    Top1 90.281250    Top5 99.781250    LR 0.030000    Time 0.023716    
2018-10-27 23:07:18,009 - Epoch: [121][  150/  391]    Overall Loss 0.282073    Objective Loss 0.282073    Top1 90.130208    Top5 99.755208    LR 0.030000    Time 0.023628    
2018-10-27 23:07:19,184 - Epoch: [121][  200/  391]    Overall Loss 0.278856    Objective Loss 0.278856    Top1 90.152344    Top5 99.781250    LR 0.030000    Time 0.023587    
2018-10-27 23:07:20,356 - Epoch: [121][  250/  391]    Overall Loss 0.278564    Objective Loss 0.278564    Top1 90.203125    Top5 99.768750    LR 0.030000    Time 0.023551    
2018-10-27 23:07:21,526 - Epoch: [121][  300/  391]    Overall Loss 0.279974    Objective Loss 0.279974    Top1 90.145833    Top5 99.763021    LR 0.030000    Time 0.023510    
2018-10-27 23:07:22,699 - Epoch: [121][  350/  391]    Overall Loss 0.279256    Objective Loss 0.279256    Top1 90.162946    Top5 99.750000    LR 0.030000    Time 0.023499    
2018-10-27 23:07:23,739 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.42141 | -0.00467 |    0.15934 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.12837 | -0.00400 |    0.02828 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.12285 |  0.00207 |    0.03097 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.12490 | -0.00361 |    0.02991 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.10090 | -0.00188 |    0.02023 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.13017 | -0.00565 |    0.03678 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.09702 | -0.00111 |    0.02262 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.13911 | -0.00423 |    0.05009 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.11062 | -0.00279 |    0.03503 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.16204 | -0.00767 |    0.05292 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.09145 | -0.00145 |    0.02590 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.07598 | -0.00132 |    0.01877 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.10018 | -0.00237 |    0.02994 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.07670 | -0.00400 |    0.01992 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.09510 | -0.00172 |    0.03252 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.08899 | -0.00422 |    0.03120 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.09182 | -0.00021 |    0.02405 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.08134 | -0.00291 |    0.02568 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06633 | -0.00243 |    0.01747 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.06205 | -0.00051 |    0.01673 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04110 |  0.00143 |    0.00755 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.50987 | -0.07878 |    0.25743 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:07:23,739 - Total sparsity: 88.62

2018-10-27 23:07:23,740 - --- validate (epoch=121)-----------
2018-10-27 23:07:23,740 - 10000 samples (128 per mini-batch)
2018-10-27 23:07:24,470 - Epoch: [121][   50/   78]    Loss 0.396770    Top1 87.281250    Top5 99.515625    
2018-10-27 23:07:24,867 - ==> Top1: 87.100    Top5: 99.580    Loss: 0.398

2018-10-27 23:07:24,868 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:07:24,868 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:07:24,885 - 

2018-10-27 23:07:24,885 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:07:26,087 - Epoch: [122][   50/  391]    Overall Loss 0.273507    Objective Loss 0.273507    Top1 90.609375    Top5 99.796875    LR 0.030000    Time 0.024013    
2018-10-27 23:07:27,265 - Epoch: [122][  100/  391]    Overall Loss 0.267776    Objective Loss 0.267776    Top1 90.726562    Top5 99.851562    LR 0.030000    Time 0.023765    
2018-10-27 23:07:28,438 - Epoch: [122][  150/  391]    Overall Loss 0.274763    Objective Loss 0.274763    Top1 90.364583    Top5 99.843750    LR 0.030000    Time 0.023656    
2018-10-27 23:07:29,609 - Epoch: [122][  200/  391]    Overall Loss 0.275129    Objective Loss 0.275129    Top1 90.343750    Top5 99.789062    LR 0.030000    Time 0.023591    
2018-10-27 23:07:30,783 - Epoch: [122][  250/  391]    Overall Loss 0.277059    Objective Loss 0.277059    Top1 90.318750    Top5 99.778125    LR 0.030000    Time 0.023561    
2018-10-27 23:07:31,951 - Epoch: [122][  300/  391]    Overall Loss 0.278185    Objective Loss 0.278185    Top1 90.315104    Top5 99.752604    LR 0.030000    Time 0.023525    
2018-10-27 23:07:33,121 - Epoch: [122][  350/  391]    Overall Loss 0.279033    Objective Loss 0.279033    Top1 90.343750    Top5 99.752232    LR 0.030000    Time 0.023502    
2018-10-27 23:07:34,165 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.41785 | -0.00567 |    0.15753 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.12732 | -0.00392 |    0.02814 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.12182 |  0.00188 |    0.03071 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.12390 | -0.00353 |    0.02967 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.10004 | -0.00180 |    0.01995 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.12913 | -0.00566 |    0.03651 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.09622 | -0.00124 |    0.02238 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.13797 | -0.00398 |    0.04960 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.10971 | -0.00265 |    0.03477 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.16062 | -0.00816 |    0.05241 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.09073 | -0.00135 |    0.02571 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.07539 | -0.00130 |    0.01857 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.09941 | -0.00234 |    0.02974 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.07610 | -0.00394 |    0.01972 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.09433 | -0.00166 |    0.03225 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.08830 | -0.00413 |    0.03093 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.09099 | -0.00021 |    0.02381 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.08073 | -0.00286 |    0.02546 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06583 | -0.00244 |    0.01733 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.06160 | -0.00055 |    0.01661 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04077 |  0.00142 |    0.00748 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.50999 | -0.07876 |    0.25727 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:07:34,165 - Total sparsity: 88.62

2018-10-27 23:07:34,165 - --- validate (epoch=122)-----------
2018-10-27 23:07:34,166 - 10000 samples (128 per mini-batch)
2018-10-27 23:07:34,893 - Epoch: [122][   50/   78]    Loss 0.393714    Top1 87.078125    Top5 99.531250    
2018-10-27 23:07:35,284 - ==> Top1: 86.860    Top5: 99.600    Loss: 0.395

2018-10-27 23:07:35,285 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:07:35,285 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:07:35,295 - 

2018-10-27 23:07:35,296 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:07:36,525 - Epoch: [123][   50/  391]    Overall Loss 0.267086    Objective Loss 0.267086    Top1 90.390625    Top5 99.765625    LR 0.030000    Time 0.024544    
2018-10-27 23:07:37,701 - Epoch: [123][  100/  391]    Overall Loss 0.272168    Objective Loss 0.272168    Top1 90.320312    Top5 99.710938    LR 0.030000    Time 0.024015    
2018-10-27 23:07:38,875 - Epoch: [123][  150/  391]    Overall Loss 0.272887    Objective Loss 0.272887    Top1 90.442708    Top5 99.755208    LR 0.030000    Time 0.023829    
2018-10-27 23:07:40,048 - Epoch: [123][  200/  391]    Overall Loss 0.273662    Objective Loss 0.273662    Top1 90.539062    Top5 99.753906    LR 0.030000    Time 0.023732    
2018-10-27 23:07:41,223 - Epoch: [123][  250/  391]    Overall Loss 0.277364    Objective Loss 0.277364    Top1 90.365625    Top5 99.740625    LR 0.030000    Time 0.023680    
2018-10-27 23:07:42,401 - Epoch: [123][  300/  391]    Overall Loss 0.276323    Objective Loss 0.276323    Top1 90.380208    Top5 99.726562    LR 0.030000    Time 0.023655    
2018-10-27 23:07:43,574 - Epoch: [123][  350/  391]    Overall Loss 0.278954    Objective Loss 0.278954    Top1 90.238839    Top5 99.720982    LR 0.030000    Time 0.023624    
2018-10-27 23:07:44,620 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.41441 | -0.00720 |    0.15642 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.12631 | -0.00392 |    0.02795 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.12084 |  0.00179 |    0.03046 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.12294 | -0.00341 |    0.02948 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.09924 | -0.00192 |    0.01980 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.12814 | -0.00538 |    0.03617 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.09545 | -0.00113 |    0.02217 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.13687 | -0.00414 |    0.04915 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.10883 | -0.00266 |    0.03447 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.15932 | -0.00777 |    0.05203 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.09006 | -0.00135 |    0.02548 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.07481 | -0.00134 |    0.01841 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.09867 | -0.00233 |    0.02952 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.07553 | -0.00396 |    0.01958 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.09360 | -0.00164 |    0.03195 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.08762 | -0.00412 |    0.03068 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.09020 | -0.00010 |    0.02362 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.08013 | -0.00285 |    0.02528 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06536 | -0.00242 |    0.01719 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.06117 | -0.00051 |    0.01648 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04045 |  0.00143 |    0.00741 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51066 | -0.07850 |    0.25744 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:07:44,620 - Total sparsity: 88.62

2018-10-27 23:07:44,620 - --- validate (epoch=123)-----------
2018-10-27 23:07:44,621 - 10000 samples (128 per mini-batch)
2018-10-27 23:07:45,352 - Epoch: [123][   50/   78]    Loss 0.410346    Top1 86.609375    Top5 99.421875    
2018-10-27 23:07:45,750 - ==> Top1: 86.570    Top5: 99.480    Loss: 0.409

2018-10-27 23:07:45,750 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:07:45,750 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:07:45,762 - 

2018-10-27 23:07:45,762 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:07:46,986 - Epoch: [124][   50/  391]    Overall Loss 0.289667    Objective Loss 0.289667    Top1 89.890625    Top5 99.765625    LR 0.030000    Time 0.024453    
2018-10-27 23:07:48,157 - Epoch: [124][  100/  391]    Overall Loss 0.278802    Objective Loss 0.278802    Top1 90.078125    Top5 99.781250    LR 0.030000    Time 0.023923    
2018-10-27 23:07:49,330 - Epoch: [124][  150/  391]    Overall Loss 0.275588    Objective Loss 0.275588    Top1 90.125000    Top5 99.807292    LR 0.030000    Time 0.023760    
2018-10-27 23:07:50,505 - Epoch: [124][  200/  391]    Overall Loss 0.275079    Objective Loss 0.275079    Top1 90.265625    Top5 99.800781    LR 0.030000    Time 0.023688    
2018-10-27 23:07:51,683 - Epoch: [124][  250/  391]    Overall Loss 0.275917    Objective Loss 0.275917    Top1 90.271875    Top5 99.768750    LR 0.030000    Time 0.023654    
2018-10-27 23:07:52,856 - Epoch: [124][  300/  391]    Overall Loss 0.276437    Objective Loss 0.276437    Top1 90.283854    Top5 99.765625    LR 0.030000    Time 0.023618    
2018-10-27 23:07:54,030 - Epoch: [124][  350/  391]    Overall Loss 0.277414    Objective Loss 0.277414    Top1 90.234375    Top5 99.767857    LR 0.030000    Time 0.023596    
2018-10-27 23:07:55,075 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.41125 | -0.00584 |    0.15535 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.12535 | -0.00400 |    0.02777 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.11989 |  0.00166 |    0.03026 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.12200 | -0.00360 |    0.02930 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.09843 | -0.00200 |    0.01958 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.12719 | -0.00527 |    0.03584 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.09469 | -0.00109 |    0.02201 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.13579 | -0.00406 |    0.04877 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.10798 | -0.00262 |    0.03416 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.15796 | -0.00814 |    0.05150 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08941 | -0.00138 |    0.02530 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.07425 | -0.00132 |    0.01829 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.09796 | -0.00231 |    0.02925 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.07497 | -0.00398 |    0.01944 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.09287 | -0.00168 |    0.03172 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.08696 | -0.00410 |    0.03040 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.08939 | -0.00018 |    0.02341 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07957 | -0.00276 |    0.02510 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06489 | -0.00241 |    0.01706 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.06074 | -0.00052 |    0.01633 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.04014 |  0.00144 |    0.00735 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51138 | -0.07828 |    0.25781 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:07:55,075 - Total sparsity: 88.62

2018-10-27 23:07:55,075 - --- validate (epoch=124)-----------
2018-10-27 23:07:55,075 - 10000 samples (128 per mini-batch)
2018-10-27 23:07:55,823 - Epoch: [124][   50/   78]    Loss 0.405979    Top1 86.718750    Top5 99.515625    
2018-10-27 23:07:56,225 - ==> Top1: 86.880    Top5: 99.580    Loss: 0.405

2018-10-27 23:07:56,226 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:07:56,226 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:07:56,240 - 

2018-10-27 23:07:56,241 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:07:57,468 - Epoch: [125][   50/  391]    Overall Loss 0.278327    Objective Loss 0.278327    Top1 90.093750    Top5 99.828125    LR 0.030000    Time 0.024505    
2018-10-27 23:07:58,638 - Epoch: [125][  100/  391]    Overall Loss 0.279631    Objective Loss 0.279631    Top1 90.054688    Top5 99.820312    LR 0.030000    Time 0.023940    
2018-10-27 23:07:59,812 - Epoch: [125][  150/  391]    Overall Loss 0.274845    Objective Loss 0.274845    Top1 90.244792    Top5 99.807292    LR 0.030000    Time 0.023775    
2018-10-27 23:08:00,989 - Epoch: [125][  200/  391]    Overall Loss 0.271995    Objective Loss 0.271995    Top1 90.402344    Top5 99.808594    LR 0.030000    Time 0.023712    
2018-10-27 23:08:02,163 - Epoch: [125][  250/  391]    Overall Loss 0.274045    Objective Loss 0.274045    Top1 90.418750    Top5 99.809375    LR 0.030000    Time 0.023659    
2018-10-27 23:08:03,341 - Epoch: [125][  300/  391]    Overall Loss 0.274641    Objective Loss 0.274641    Top1 90.335938    Top5 99.794271    LR 0.030000    Time 0.023637    
2018-10-27 23:08:04,512 - Epoch: [125][  350/  391]    Overall Loss 0.275850    Objective Loss 0.275850    Top1 90.328125    Top5 99.770089    LR 0.030000    Time 0.023604    
2018-10-27 23:08:05,559 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.40809 | -0.00578 |    0.15401 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.12443 | -0.00390 |    0.02758 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.11896 |  0.00165 |    0.02998 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.12110 | -0.00349 |    0.02903 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.09767 | -0.00207 |    0.01951 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.12626 | -0.00518 |    0.03553 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.09398 | -0.00119 |    0.02177 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.13475 | -0.00403 |    0.04842 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.10715 | -0.00266 |    0.03386 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.15672 | -0.00804 |    0.05101 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08878 | -0.00141 |    0.02512 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.07372 | -0.00130 |    0.01815 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.09726 | -0.00230 |    0.02907 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.07442 | -0.00409 |    0.01928 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.09216 | -0.00173 |    0.03143 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.08634 | -0.00404 |    0.03014 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.08861 | -0.00011 |    0.02322 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07903 | -0.00269 |    0.02490 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06445 | -0.00241 |    0.01692 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.06034 | -0.00052 |    0.01623 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03984 |  0.00142 |    0.00729 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51247 | -0.07817 |    0.25821 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:08:05,559 - Total sparsity: 88.62

2018-10-27 23:08:05,559 - --- validate (epoch=125)-----------
2018-10-27 23:08:05,559 - 10000 samples (128 per mini-batch)
2018-10-27 23:08:06,295 - Epoch: [125][   50/   78]    Loss 0.435828    Top1 85.953125    Top5 99.437500    
2018-10-27 23:08:06,693 - ==> Top1: 86.070    Top5: 99.500    Loss: 0.432

2018-10-27 23:08:06,694 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:08:06,694 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:08:06,704 - 

2018-10-27 23:08:06,705 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:08:07,934 - Epoch: [126][   50/  391]    Overall Loss 0.275374    Objective Loss 0.275374    Top1 90.375000    Top5 99.796875    LR 0.030000    Time 0.024541    
2018-10-27 23:08:09,107 - Epoch: [126][  100/  391]    Overall Loss 0.274193    Objective Loss 0.274193    Top1 90.406250    Top5 99.820312    LR 0.030000    Time 0.023991    
2018-10-27 23:08:10,279 - Epoch: [126][  150/  391]    Overall Loss 0.276468    Objective Loss 0.276468    Top1 90.322917    Top5 99.812500    LR 0.030000    Time 0.023797    
2018-10-27 23:08:11,452 - Epoch: [126][  200/  391]    Overall Loss 0.277690    Objective Loss 0.277690    Top1 90.273438    Top5 99.804688    LR 0.030000    Time 0.023706    
2018-10-27 23:08:12,626 - Epoch: [126][  250/  391]    Overall Loss 0.278136    Objective Loss 0.278136    Top1 90.250000    Top5 99.784375    LR 0.030000    Time 0.023655    
2018-10-27 23:08:13,795 - Epoch: [126][  300/  391]    Overall Loss 0.279080    Objective Loss 0.279080    Top1 90.210938    Top5 99.778646    LR 0.030000    Time 0.023606    
2018-10-27 23:08:14,966 - Epoch: [126][  350/  391]    Overall Loss 0.280769    Objective Loss 0.280769    Top1 90.176339    Top5 99.772321    LR 0.030000    Time 0.023577    
2018-10-27 23:08:16,007 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.40532 | -0.00521 |    0.15261 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.12352 | -0.00409 |    0.02740 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.11809 |  0.00156 |    0.02974 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.12023 | -0.00309 |    0.02874 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.09695 | -0.00203 |    0.01936 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.12536 | -0.00533 |    0.03535 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.09328 | -0.00147 |    0.02163 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.13374 | -0.00410 |    0.04811 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.10637 | -0.00270 |    0.03358 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.15546 | -0.00837 |    0.05042 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08817 | -0.00147 |    0.02488 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.07320 | -0.00129 |    0.01804 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.09659 | -0.00229 |    0.02890 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.07391 | -0.00400 |    0.01911 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.09149 | -0.00162 |    0.03121 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.08573 | -0.00398 |    0.02992 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.08786 | -0.00019 |    0.02306 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07851 | -0.00265 |    0.02471 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06403 | -0.00240 |    0.01679 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05996 | -0.00048 |    0.01612 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03955 |  0.00140 |    0.00723 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51219 | -0.07799 |    0.25794 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:08:16,007 - Total sparsity: 88.62

2018-10-27 23:08:16,007 - --- validate (epoch=126)-----------
2018-10-27 23:08:16,008 - 10000 samples (128 per mini-batch)
2018-10-27 23:08:16,744 - Epoch: [126][   50/   78]    Loss 0.403624    Top1 86.546875    Top5 99.562500    
2018-10-27 23:08:17,143 - ==> Top1: 86.650    Top5: 99.590    Loss: 0.404

2018-10-27 23:08:17,144 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:08:17,144 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:08:17,155 - 

2018-10-27 23:08:17,155 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:08:18,384 - Epoch: [127][   50/  391]    Overall Loss 0.269959    Objective Loss 0.269959    Top1 90.265625    Top5 99.843750    LR 0.030000    Time 0.024549    
2018-10-27 23:08:19,558 - Epoch: [127][  100/  391]    Overall Loss 0.266300    Objective Loss 0.266300    Top1 90.718750    Top5 99.835938    LR 0.030000    Time 0.023995    
2018-10-27 23:08:20,732 - Epoch: [127][  150/  391]    Overall Loss 0.267667    Objective Loss 0.267667    Top1 90.588542    Top5 99.828125    LR 0.030000    Time 0.023816    
2018-10-27 23:08:21,908 - Epoch: [127][  200/  391]    Overall Loss 0.269896    Objective Loss 0.269896    Top1 90.562500    Top5 99.820312    LR 0.030000    Time 0.023736    
2018-10-27 23:08:23,080 - Epoch: [127][  250/  391]    Overall Loss 0.272662    Objective Loss 0.272662    Top1 90.437500    Top5 99.812500    LR 0.030000    Time 0.023671    
2018-10-27 23:08:24,252 - Epoch: [127][  300/  391]    Overall Loss 0.274900    Objective Loss 0.274900    Top1 90.406250    Top5 99.809896    LR 0.030000    Time 0.023627    
2018-10-27 23:08:25,426 - Epoch: [127][  350/  391]    Overall Loss 0.275181    Objective Loss 0.275181    Top1 90.404018    Top5 99.803571    LR 0.030000    Time 0.023603    
2018-10-27 23:08:26,469 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.40217 | -0.00628 |    0.15112 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.12266 | -0.00402 |    0.02718 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.11720 |  0.00185 |    0.02948 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.11937 | -0.00351 |    0.02852 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.09621 | -0.00190 |    0.01925 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.12451 | -0.00525 |    0.03507 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.09260 | -0.00136 |    0.02151 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.13278 | -0.00370 |    0.04767 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.10559 | -0.00259 |    0.03330 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.15425 | -0.00835 |    0.05006 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08758 | -0.00144 |    0.02476 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.07267 | -0.00124 |    0.01789 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.09594 | -0.00240 |    0.02871 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.07342 | -0.00387 |    0.01894 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.09082 | -0.00169 |    0.03094 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.08513 | -0.00400 |    0.02969 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.08711 | -0.00010 |    0.02281 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07800 | -0.00269 |    0.02455 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06362 | -0.00233 |    0.01668 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05959 | -0.00052 |    0.01601 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03926 |  0.00140 |    0.00715 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51210 | -0.07756 |    0.25776 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:08:26,469 - Total sparsity: 88.62

2018-10-27 23:08:26,469 - --- validate (epoch=127)-----------
2018-10-27 23:08:26,469 - 10000 samples (128 per mini-batch)
2018-10-27 23:08:27,193 - Epoch: [127][   50/   78]    Loss 0.424923    Top1 86.375000    Top5 99.406250    
2018-10-27 23:08:27,581 - ==> Top1: 86.400    Top5: 99.470    Loss: 0.427

2018-10-27 23:08:27,582 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:08:27,582 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:08:27,599 - 

2018-10-27 23:08:27,599 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:08:28,801 - Epoch: [128][   50/  391]    Overall Loss 0.267399    Objective Loss 0.267399    Top1 90.437500    Top5 99.703125    LR 0.030000    Time 0.024004    
2018-10-27 23:08:29,974 - Epoch: [128][  100/  391]    Overall Loss 0.272976    Objective Loss 0.272976    Top1 90.445312    Top5 99.734375    LR 0.030000    Time 0.023723    
2018-10-27 23:08:31,140 - Epoch: [128][  150/  391]    Overall Loss 0.268455    Objective Loss 0.268455    Top1 90.619792    Top5 99.770833    LR 0.030000    Time 0.023575    
2018-10-27 23:08:32,307 - Epoch: [128][  200/  391]    Overall Loss 0.270000    Objective Loss 0.270000    Top1 90.578125    Top5 99.761719    LR 0.030000    Time 0.023509    
2018-10-27 23:08:33,478 - Epoch: [128][  250/  391]    Overall Loss 0.267758    Objective Loss 0.267758    Top1 90.662500    Top5 99.775000    LR 0.030000    Time 0.023486    
2018-10-27 23:08:34,649 - Epoch: [128][  300/  391]    Overall Loss 0.271420    Objective Loss 0.271420    Top1 90.460938    Top5 99.763021    LR 0.030000    Time 0.023470    
2018-10-27 23:08:35,818 - Epoch: [128][  350/  391]    Overall Loss 0.272771    Objective Loss 0.272771    Top1 90.379464    Top5 99.770089    LR 0.030000    Time 0.023454    
2018-10-27 23:08:36,861 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.39927 | -0.00325 |    0.14999 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.12181 | -0.00405 |    0.02694 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.11630 |  0.00214 |    0.02915 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.11854 | -0.00346 |    0.02837 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.09552 | -0.00171 |    0.01907 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.12367 | -0.00498 |    0.03489 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.09195 | -0.00117 |    0.02136 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.13181 | -0.00374 |    0.04734 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.10482 | -0.00271 |    0.03302 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.15303 | -0.00870 |    0.04965 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08699 | -0.00144 |    0.02460 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.07218 | -0.00118 |    0.01772 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.09530 | -0.00242 |    0.02848 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.07293 | -0.00383 |    0.01882 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.09017 | -0.00158 |    0.03072 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.08455 | -0.00396 |    0.02948 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.08636 |  0.00005 |    0.02261 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07750 | -0.00261 |    0.02436 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06321 | -0.00232 |    0.01657 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05922 | -0.00052 |    0.01591 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03897 |  0.00139 |    0.00709 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51361 | -0.07782 |    0.25838 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:08:36,861 - Total sparsity: 88.62

2018-10-27 23:08:36,861 - --- validate (epoch=128)-----------
2018-10-27 23:08:36,861 - 10000 samples (128 per mini-batch)
2018-10-27 23:08:37,594 - Epoch: [128][   50/   78]    Loss 0.412227    Top1 86.703125    Top5 99.515625    
2018-10-27 23:08:37,992 - ==> Top1: 86.590    Top5: 99.540    Loss: 0.417

2018-10-27 23:08:37,993 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:08:37,993 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:08:38,004 - 

2018-10-27 23:08:38,004 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:08:39,233 - Epoch: [129][   50/  391]    Overall Loss 0.277570    Objective Loss 0.277570    Top1 90.515625    Top5 99.875000    LR 0.030000    Time 0.024531    
2018-10-27 23:08:40,406 - Epoch: [129][  100/  391]    Overall Loss 0.276459    Objective Loss 0.276459    Top1 90.562500    Top5 99.796875    LR 0.030000    Time 0.023987    
2018-10-27 23:08:41,578 - Epoch: [129][  150/  391]    Overall Loss 0.281920    Objective Loss 0.281920    Top1 90.296875    Top5 99.760417    LR 0.030000    Time 0.023791    
2018-10-27 23:08:42,749 - Epoch: [129][  200/  391]    Overall Loss 0.280451    Objective Loss 0.280451    Top1 90.300781    Top5 99.746094    LR 0.030000    Time 0.023692    
2018-10-27 23:08:43,924 - Epoch: [129][  250/  391]    Overall Loss 0.280720    Objective Loss 0.280720    Top1 90.193750    Top5 99.743750    LR 0.030000    Time 0.023648    
2018-10-27 23:08:45,095 - Epoch: [129][  300/  391]    Overall Loss 0.280811    Objective Loss 0.280811    Top1 90.247396    Top5 99.736979    LR 0.030000    Time 0.023607    
2018-10-27 23:08:46,268 - Epoch: [129][  350/  391]    Overall Loss 0.281074    Objective Loss 0.281074    Top1 90.209821    Top5 99.741071    LR 0.030000    Time 0.023582    
2018-10-27 23:08:47,311 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.39655 | -0.00423 |    0.14901 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.12100 | -0.00416 |    0.02672 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.11549 |  0.00196 |    0.02901 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.11776 | -0.00338 |    0.02814 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.09484 | -0.00189 |    0.01893 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.12287 | -0.00502 |    0.03466 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.09131 | -0.00127 |    0.02121 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.13090 | -0.00375 |    0.04699 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.10409 | -0.00267 |    0.03283 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.15193 | -0.00821 |    0.04939 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08646 | -0.00146 |    0.02445 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.07172 | -0.00122 |    0.01761 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.09471 | -0.00230 |    0.02834 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.07246 | -0.00391 |    0.01871 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08956 | -0.00148 |    0.03057 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.08400 | -0.00390 |    0.02927 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.08567 |  0.00020 |    0.02238 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07703 | -0.00254 |    0.02418 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06283 | -0.00231 |    0.01648 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05889 | -0.00056 |    0.01580 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03871 |  0.00135 |    0.00704 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51297 | -0.07782 |    0.25807 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:08:47,311 - Total sparsity: 88.62

2018-10-27 23:08:47,311 - --- validate (epoch=129)-----------
2018-10-27 23:08:47,311 - 10000 samples (128 per mini-batch)
2018-10-27 23:08:48,051 - Epoch: [129][   50/   78]    Loss 0.406668    Top1 87.015625    Top5 99.453125    
2018-10-27 23:08:48,447 - ==> Top1: 86.880    Top5: 99.550    Loss: 0.409

2018-10-27 23:08:48,448 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:08:48,448 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:08:48,459 - 

2018-10-27 23:08:48,459 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:08:49,687 - Epoch: [130][   50/  391]    Overall Loss 0.273533    Objective Loss 0.273533    Top1 90.312500    Top5 99.750000    LR 0.030000    Time 0.024508    
2018-10-27 23:08:50,860 - Epoch: [130][  100/  391]    Overall Loss 0.270706    Objective Loss 0.270706    Top1 90.515625    Top5 99.734375    LR 0.030000    Time 0.023972    
2018-10-27 23:08:52,027 - Epoch: [130][  150/  391]    Overall Loss 0.268270    Objective Loss 0.268270    Top1 90.541667    Top5 99.744792    LR 0.030000    Time 0.023757    
2018-10-27 23:08:53,197 - Epoch: [130][  200/  391]    Overall Loss 0.269931    Objective Loss 0.269931    Top1 90.425781    Top5 99.742188    LR 0.030000    Time 0.023658    
2018-10-27 23:08:54,364 - Epoch: [130][  250/  391]    Overall Loss 0.270420    Objective Loss 0.270420    Top1 90.353125    Top5 99.746875    LR 0.030000    Time 0.023590    
2018-10-27 23:08:55,542 - Epoch: [130][  300/  391]    Overall Loss 0.272821    Objective Loss 0.272821    Top1 90.289062    Top5 99.750000    LR 0.030000    Time 0.023579    
2018-10-27 23:08:56,711 - Epoch: [130][  350/  391]    Overall Loss 0.276170    Objective Loss 0.276170    Top1 90.247768    Top5 99.756696    LR 0.030000    Time 0.023546    
2018-10-27 23:08:57,755 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.39368 | -0.00568 |    0.14817 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.12017 | -0.00402 |    0.02657 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.11465 |  0.00168 |    0.02883 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.11698 | -0.00309 |    0.02808 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.09419 | -0.00184 |    0.01895 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.12210 | -0.00493 |    0.03441 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.09069 | -0.00121 |    0.02099 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.13001 | -0.00388 |    0.04664 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.10339 | -0.00258 |    0.03254 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.15083 | -0.00826 |    0.04902 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08595 | -0.00144 |    0.02430 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.07127 | -0.00124 |    0.01751 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.09412 | -0.00222 |    0.02818 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.07200 | -0.00385 |    0.01863 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08895 | -0.00150 |    0.03034 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.08346 | -0.00385 |    0.02908 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.08498 |  0.00002 |    0.02228 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07657 | -0.00256 |    0.02405 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06247 | -0.00226 |    0.01636 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05856 | -0.00049 |    0.01571 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03845 |  0.00138 |    0.00699 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51379 | -0.07774 |    0.25841 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:08:57,755 - Total sparsity: 88.62

2018-10-27 23:08:57,755 - --- validate (epoch=130)-----------
2018-10-27 23:08:57,756 - 10000 samples (128 per mini-batch)
2018-10-27 23:08:58,547 - Epoch: [130][   50/   78]    Loss 0.410968    Top1 86.734375    Top5 99.484375    
2018-10-27 23:08:58,937 - ==> Top1: 86.780    Top5: 99.510    Loss: 0.411

2018-10-27 23:08:58,938 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:08:58,939 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:08:58,953 - 

2018-10-27 23:08:58,953 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:09:00,186 - Epoch: [131][   50/  391]    Overall Loss 0.284303    Objective Loss 0.284303    Top1 89.890625    Top5 99.718750    LR 0.030000    Time 0.024624    
2018-10-27 23:09:01,358 - Epoch: [131][  100/  391]    Overall Loss 0.277197    Objective Loss 0.277197    Top1 90.179688    Top5 99.734375    LR 0.030000    Time 0.024013    
2018-10-27 23:09:02,528 - Epoch: [131][  150/  391]    Overall Loss 0.276733    Objective Loss 0.276733    Top1 90.312500    Top5 99.718750    LR 0.030000    Time 0.023799    
2018-10-27 23:09:03,699 - Epoch: [131][  200/  391]    Overall Loss 0.279409    Objective Loss 0.279409    Top1 90.117188    Top5 99.746094    LR 0.030000    Time 0.023696    
2018-10-27 23:09:04,875 - Epoch: [131][  250/  391]    Overall Loss 0.278536    Objective Loss 0.278536    Top1 90.134375    Top5 99.725000    LR 0.030000    Time 0.023654    
2018-10-27 23:09:06,047 - Epoch: [131][  300/  391]    Overall Loss 0.278273    Objective Loss 0.278273    Top1 90.096354    Top5 99.736979    LR 0.030000    Time 0.023617    
2018-10-27 23:09:07,223 - Epoch: [131][  350/  391]    Overall Loss 0.279025    Objective Loss 0.279025    Top1 90.055804    Top5 99.725446    LR 0.030000    Time 0.023598    
2018-10-27 23:09:08,265 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.39123 | -0.00543 |    0.14703 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.11938 | -0.00420 |    0.02637 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.11387 |  0.00178 |    0.02865 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.11625 | -0.00286 |    0.02778 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.09354 | -0.00199 |    0.01879 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.12132 | -0.00472 |    0.03412 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.09005 | -0.00125 |    0.02090 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.12915 | -0.00375 |    0.04640 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.10272 | -0.00256 |    0.03230 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.14970 | -0.00844 |    0.04872 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08545 | -0.00148 |    0.02416 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.07084 | -0.00122 |    0.01739 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.09356 | -0.00211 |    0.02798 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.07156 | -0.00388 |    0.01851 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08836 | -0.00160 |    0.03013 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.08295 | -0.00379 |    0.02891 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.08431 | -0.00003 |    0.02207 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07615 | -0.00248 |    0.02390 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06212 | -0.00223 |    0.01626 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05825 | -0.00044 |    0.01562 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03820 |  0.00139 |    0.00694 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51346 | -0.07818 |    0.25830 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:09:08,266 - Total sparsity: 88.62

2018-10-27 23:09:08,266 - --- validate (epoch=131)-----------
2018-10-27 23:09:08,266 - 10000 samples (128 per mini-batch)
2018-10-27 23:09:09,001 - Epoch: [131][   50/   78]    Loss 0.430744    Top1 86.375000    Top5 99.437500    
2018-10-27 23:09:09,396 - ==> Top1: 86.260    Top5: 99.430    Loss: 0.428

2018-10-27 23:09:09,397 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:09:09,397 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:09:09,407 - 

2018-10-27 23:09:09,407 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:09:10,632 - Epoch: [132][   50/  391]    Overall Loss 0.273090    Objective Loss 0.273090    Top1 90.671875    Top5 99.734375    LR 0.030000    Time 0.024453    
2018-10-27 23:09:11,798 - Epoch: [132][  100/  391]    Overall Loss 0.263660    Objective Loss 0.263660    Top1 91.007812    Top5 99.796875    LR 0.030000    Time 0.023873    
2018-10-27 23:09:12,971 - Epoch: [132][  150/  391]    Overall Loss 0.269536    Objective Loss 0.269536    Top1 90.739583    Top5 99.812500    LR 0.030000    Time 0.023729    
2018-10-27 23:09:14,147 - Epoch: [132][  200/  391]    Overall Loss 0.272994    Objective Loss 0.272994    Top1 90.593750    Top5 99.785156    LR 0.030000    Time 0.023671    
2018-10-27 23:09:15,322 - Epoch: [132][  250/  391]    Overall Loss 0.276391    Objective Loss 0.276391    Top1 90.431250    Top5 99.771875    LR 0.030000    Time 0.023631    
2018-10-27 23:09:16,494 - Epoch: [132][  300/  391]    Overall Loss 0.276699    Objective Loss 0.276699    Top1 90.351562    Top5 99.773438    LR 0.030000    Time 0.023593    
2018-10-27 23:09:17,668 - Epoch: [132][  350/  391]    Overall Loss 0.279619    Objective Loss 0.279619    Top1 90.183036    Top5 99.745536    LR 0.030000    Time 0.023574    
2018-10-27 23:09:18,718 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.38871 | -0.00410 |    0.14641 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.11864 | -0.00401 |    0.02618 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.11310 |  0.00165 |    0.02844 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.11553 | -0.00303 |    0.02760 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.09292 | -0.00185 |    0.01869 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.12054 | -0.00484 |    0.03393 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08945 | -0.00138 |    0.02068 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.12832 | -0.00366 |    0.04594 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.10204 | -0.00268 |    0.03216 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.14859 | -0.00860 |    0.04825 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08495 | -0.00141 |    0.02399 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.07040 | -0.00114 |    0.01730 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.09299 | -0.00205 |    0.02780 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.07113 | -0.00393 |    0.01836 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08779 | -0.00163 |    0.02993 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.08244 | -0.00378 |    0.02869 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.08365 | -0.00008 |    0.02190 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07572 | -0.00247 |    0.02374 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06178 | -0.00221 |    0.01616 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05795 | -0.00047 |    0.01555 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03796 |  0.00137 |    0.00689 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51304 | -0.07784 |    0.25810 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:09:18,718 - Total sparsity: 88.62

2018-10-27 23:09:18,718 - --- validate (epoch=132)-----------
2018-10-27 23:09:18,718 - 10000 samples (128 per mini-batch)
2018-10-27 23:09:19,448 - Epoch: [132][   50/   78]    Loss 0.407830    Top1 86.359375    Top5 99.500000    
2018-10-27 23:09:19,838 - ==> Top1: 86.520    Top5: 99.540    Loss: 0.402

2018-10-27 23:09:19,839 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:09:19,839 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:09:19,849 - 

2018-10-27 23:09:19,849 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:09:21,075 - Epoch: [133][   50/  391]    Overall Loss 0.257618    Objective Loss 0.257618    Top1 91.093750    Top5 99.875000    LR 0.030000    Time 0.024470    
2018-10-27 23:09:22,245 - Epoch: [133][  100/  391]    Overall Loss 0.271857    Objective Loss 0.271857    Top1 90.632812    Top5 99.812500    LR 0.030000    Time 0.023921    
2018-10-27 23:09:23,418 - Epoch: [133][  150/  391]    Overall Loss 0.273721    Objective Loss 0.273721    Top1 90.500000    Top5 99.755208    LR 0.030000    Time 0.023759    
2018-10-27 23:09:24,591 - Epoch: [133][  200/  391]    Overall Loss 0.274680    Objective Loss 0.274680    Top1 90.394531    Top5 99.765625    LR 0.030000    Time 0.023678    
2018-10-27 23:09:25,762 - Epoch: [133][  250/  391]    Overall Loss 0.274916    Objective Loss 0.274916    Top1 90.387500    Top5 99.775000    LR 0.030000    Time 0.023617    
2018-10-27 23:09:26,934 - Epoch: [133][  300/  391]    Overall Loss 0.275141    Objective Loss 0.275141    Top1 90.356771    Top5 99.776042    LR 0.030000    Time 0.023584    
2018-10-27 23:09:28,105 - Epoch: [133][  350/  391]    Overall Loss 0.277755    Objective Loss 0.277755    Top1 90.252232    Top5 99.776786    LR 0.030000    Time 0.023558    
2018-10-27 23:09:29,148 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.38612 | -0.00459 |    0.14543 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.11793 | -0.00389 |    0.02598 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.11234 |  0.00156 |    0.02818 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.11481 | -0.00286 |    0.02739 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.09229 | -0.00180 |    0.01847 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.11983 | -0.00443 |    0.03364 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08887 | -0.00131 |    0.02056 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.12749 | -0.00355 |    0.04552 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.10140 | -0.00254 |    0.03195 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.14754 | -0.00810 |    0.04792 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08449 | -0.00136 |    0.02384 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06998 | -0.00119 |    0.01719 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.09244 | -0.00205 |    0.02764 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.07071 | -0.00383 |    0.01825 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08724 | -0.00161 |    0.02973 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.08195 | -0.00376 |    0.02853 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.08300 |  0.00003 |    0.02170 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07530 | -0.00246 |    0.02360 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06144 | -0.00221 |    0.01607 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05766 | -0.00052 |    0.01546 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03773 |  0.00137 |    0.00684 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51401 | -0.07797 |    0.25839 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:09:29,148 - Total sparsity: 88.62

2018-10-27 23:09:29,148 - --- validate (epoch=133)-----------
2018-10-27 23:09:29,148 - 10000 samples (128 per mini-batch)
2018-10-27 23:09:29,877 - Epoch: [133][   50/   78]    Loss 0.429643    Top1 85.984375    Top5 99.515625    
2018-10-27 23:09:30,279 - ==> Top1: 85.960    Top5: 99.550    Loss: 0.428

2018-10-27 23:09:30,279 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:09:30,280 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:09:30,290 - 

2018-10-27 23:09:30,290 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:09:31,522 - Epoch: [134][   50/  391]    Overall Loss 0.273877    Objective Loss 0.273877    Top1 90.125000    Top5 99.828125    LR 0.030000    Time 0.024588    
2018-10-27 23:09:32,697 - Epoch: [134][  100/  391]    Overall Loss 0.270588    Objective Loss 0.270588    Top1 90.453125    Top5 99.796875    LR 0.030000    Time 0.024034    
2018-10-27 23:09:33,873 - Epoch: [134][  150/  391]    Overall Loss 0.275532    Objective Loss 0.275532    Top1 90.270833    Top5 99.781250    LR 0.030000    Time 0.023853    
2018-10-27 23:09:35,049 - Epoch: [134][  200/  391]    Overall Loss 0.277360    Objective Loss 0.277360    Top1 90.144531    Top5 99.781250    LR 0.030000    Time 0.023765    
2018-10-27 23:09:36,221 - Epoch: [134][  250/  391]    Overall Loss 0.276163    Objective Loss 0.276163    Top1 90.212500    Top5 99.790625    LR 0.030000    Time 0.023695    
2018-10-27 23:09:37,396 - Epoch: [134][  300/  391]    Overall Loss 0.277683    Objective Loss 0.277683    Top1 90.166667    Top5 99.773438    LR 0.030000    Time 0.023655    
2018-10-27 23:09:38,569 - Epoch: [134][  350/  391]    Overall Loss 0.279670    Objective Loss 0.279670    Top1 90.064732    Top5 99.758929    LR 0.030000    Time 0.023625    
2018-10-27 23:09:39,606 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.38373 | -0.00736 |    0.14459 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.11721 | -0.00392 |    0.02590 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.11161 |  0.00158 |    0.02809 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.11413 | -0.00288 |    0.02725 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.09168 | -0.00168 |    0.01838 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.11914 | -0.00453 |    0.03353 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08833 | -0.00151 |    0.02037 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.12670 | -0.00363 |    0.04527 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.10077 | -0.00251 |    0.03177 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.14658 | -0.00839 |    0.04751 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08403 | -0.00127 |    0.02370 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06957 | -0.00122 |    0.01707 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.09193 | -0.00203 |    0.02747 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.07031 | -0.00388 |    0.01813 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08671 | -0.00161 |    0.02959 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.08149 | -0.00367 |    0.02835 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.08239 |  0.00006 |    0.02154 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07491 | -0.00246 |    0.02345 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06113 | -0.00215 |    0.01597 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05739 | -0.00049 |    0.01538 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03751 |  0.00135 |    0.00680 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51432 | -0.07820 |    0.25877 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:09:39,607 - Total sparsity: 88.62

2018-10-27 23:09:39,607 - --- validate (epoch=134)-----------
2018-10-27 23:09:39,607 - 10000 samples (128 per mini-batch)
2018-10-27 23:09:40,335 - Epoch: [134][   50/   78]    Loss 0.419326    Top1 86.281250    Top5 99.484375    
2018-10-27 23:09:40,729 - ==> Top1: 86.450    Top5: 99.490    Loss: 0.416

2018-10-27 23:09:40,730 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:09:40,730 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:09:40,744 - 

2018-10-27 23:09:40,745 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:09:41,975 - Epoch: [135][   50/  391]    Overall Loss 0.269191    Objective Loss 0.269191    Top1 90.671875    Top5 99.734375    LR 0.030000    Time 0.024568    
2018-10-27 23:09:43,149 - Epoch: [135][  100/  391]    Overall Loss 0.275443    Objective Loss 0.275443    Top1 90.171875    Top5 99.750000    LR 0.030000    Time 0.024005    
2018-10-27 23:09:44,322 - Epoch: [135][  150/  391]    Overall Loss 0.275731    Objective Loss 0.275731    Top1 90.250000    Top5 99.739583    LR 0.030000    Time 0.023813    
2018-10-27 23:09:45,496 - Epoch: [135][  200/  391]    Overall Loss 0.275648    Objective Loss 0.275648    Top1 90.238281    Top5 99.746094    LR 0.030000    Time 0.023726    
2018-10-27 23:09:46,670 - Epoch: [135][  250/  391]    Overall Loss 0.277550    Objective Loss 0.277550    Top1 90.159375    Top5 99.737500    LR 0.030000    Time 0.023670    
2018-10-27 23:09:47,842 - Epoch: [135][  300/  391]    Overall Loss 0.278164    Objective Loss 0.278164    Top1 90.138021    Top5 99.734375    LR 0.030000    Time 0.023628    
2018-10-27 23:09:49,011 - Epoch: [135][  350/  391]    Overall Loss 0.278993    Objective Loss 0.278993    Top1 90.071429    Top5 99.734375    LR 0.030000    Time 0.023589    
2018-10-27 23:09:50,057 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.38165 | -0.00497 |    0.14351 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.11661 | -0.00394 |    0.02583 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.11092 |  0.00181 |    0.02786 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.11351 | -0.00312 |    0.02714 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.09109 | -0.00178 |    0.01821 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.11849 | -0.00460 |    0.03330 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08779 | -0.00151 |    0.02029 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.12595 | -0.00354 |    0.04493 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.10018 | -0.00243 |    0.03159 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.14556 | -0.00848 |    0.04726 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08359 | -0.00120 |    0.02358 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06918 | -0.00120 |    0.01695 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.09145 | -0.00190 |    0.02727 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06992 | -0.00384 |    0.01801 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08620 | -0.00149 |    0.02939 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.08103 | -0.00366 |    0.02816 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.08178 |  0.00023 |    0.02134 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07453 | -0.00239 |    0.02334 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06083 | -0.00212 |    0.01588 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05713 | -0.00045 |    0.01531 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03730 |  0.00135 |    0.00676 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51490 | -0.07774 |    0.25885 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:09:50,057 - Total sparsity: 88.62

2018-10-27 23:09:50,057 - --- validate (epoch=135)-----------
2018-10-27 23:09:50,057 - 10000 samples (128 per mini-batch)
2018-10-27 23:09:50,799 - Epoch: [135][   50/   78]    Loss 0.401945    Top1 87.000000    Top5 99.546875    
2018-10-27 23:09:51,196 - ==> Top1: 86.950    Top5: 99.590    Loss: 0.403

2018-10-27 23:09:51,197 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:09:51,197 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:09:51,208 - 

2018-10-27 23:09:51,208 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:09:52,435 - Epoch: [136][   50/  391]    Overall Loss 0.273217    Objective Loss 0.273217    Top1 90.546875    Top5 99.687500    LR 0.030000    Time 0.024494    
2018-10-27 23:09:53,611 - Epoch: [136][  100/  391]    Overall Loss 0.275885    Objective Loss 0.275885    Top1 90.609375    Top5 99.671875    LR 0.030000    Time 0.023989    
2018-10-27 23:09:54,782 - Epoch: [136][  150/  391]    Overall Loss 0.273784    Objective Loss 0.273784    Top1 90.447917    Top5 99.734375    LR 0.030000    Time 0.023793    
2018-10-27 23:09:55,950 - Epoch: [136][  200/  391]    Overall Loss 0.271667    Objective Loss 0.271667    Top1 90.484375    Top5 99.761719    LR 0.030000    Time 0.023660    
2018-10-27 23:09:57,116 - Epoch: [136][  250/  391]    Overall Loss 0.272375    Objective Loss 0.272375    Top1 90.515625    Top5 99.756250    LR 0.030000    Time 0.023587    
2018-10-27 23:09:58,286 - Epoch: [136][  300/  391]    Overall Loss 0.273370    Objective Loss 0.273370    Top1 90.460938    Top5 99.760417    LR 0.030000    Time 0.023551    
2018-10-27 23:09:59,459 - Epoch: [136][  350/  391]    Overall Loss 0.274526    Objective Loss 0.274526    Top1 90.441964    Top5 99.754464    LR 0.030000    Time 0.023535    
2018-10-27 23:10:00,505 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.37948 | -0.00491 |    0.14259 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.11595 | -0.00399 |    0.02559 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.11021 |  0.00189 |    0.02765 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.11283 | -0.00328 |    0.02691 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.09051 | -0.00186 |    0.01798 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.11778 | -0.00443 |    0.03312 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08726 | -0.00135 |    0.02012 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.12517 | -0.00351 |    0.04469 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09957 | -0.00263 |    0.03138 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.14461 | -0.00836 |    0.04668 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08314 | -0.00119 |    0.02345 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06880 | -0.00116 |    0.01689 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.09097 | -0.00206 |    0.02717 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06954 | -0.00388 |    0.01788 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08570 | -0.00159 |    0.02916 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.08059 | -0.00358 |    0.02801 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.08119 |  0.00023 |    0.02122 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07416 | -0.00231 |    0.02319 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06054 | -0.00210 |    0.01580 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05687 | -0.00046 |    0.01524 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03709 |  0.00135 |    0.00671 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51604 | -0.07767 |    0.25918 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:10:00,505 - Total sparsity: 88.62

2018-10-27 23:10:00,505 - --- validate (epoch=136)-----------
2018-10-27 23:10:00,505 - 10000 samples (128 per mini-batch)
2018-10-27 23:10:01,237 - Epoch: [136][   50/   78]    Loss 0.412328    Top1 86.656250    Top5 99.593750    
2018-10-27 23:10:01,621 - ==> Top1: 86.690    Top5: 99.600    Loss: 0.412

2018-10-27 23:10:01,622 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:10:01,622 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:10:01,635 - 

2018-10-27 23:10:01,636 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:10:02,835 - Epoch: [137][   50/  391]    Overall Loss 0.281961    Objective Loss 0.281961    Top1 90.218750    Top5 99.718750    LR 0.030000    Time 0.023942    
2018-10-27 23:10:03,976 - Epoch: [137][  100/  391]    Overall Loss 0.276368    Objective Loss 0.276368    Top1 90.414062    Top5 99.734375    LR 0.030000    Time 0.023371    
2018-10-27 23:10:05,119 - Epoch: [137][  150/  391]    Overall Loss 0.279520    Objective Loss 0.279520    Top1 90.250000    Top5 99.739583    LR 0.030000    Time 0.023187    
2018-10-27 23:10:06,260 - Epoch: [137][  200/  391]    Overall Loss 0.278013    Objective Loss 0.278013    Top1 90.304688    Top5 99.742188    LR 0.030000    Time 0.023090    
2018-10-27 23:10:07,404 - Epoch: [137][  250/  391]    Overall Loss 0.277445    Objective Loss 0.277445    Top1 90.321875    Top5 99.746875    LR 0.030000    Time 0.023042    
2018-10-27 23:10:08,547 - Epoch: [137][  300/  391]    Overall Loss 0.275027    Objective Loss 0.275027    Top1 90.367188    Top5 99.763021    LR 0.030000    Time 0.023008    
2018-10-27 23:10:09,691 - Epoch: [137][  350/  391]    Overall Loss 0.274491    Objective Loss 0.274491    Top1 90.392857    Top5 99.752232    LR 0.030000    Time 0.022987    
2018-10-27 23:10:10,710 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.37722 | -0.00364 |    0.14223 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.11536 | -0.00402 |    0.02550 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10957 |  0.00183 |    0.02743 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.11223 | -0.00307 |    0.02664 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08997 | -0.00165 |    0.01796 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.11719 | -0.00436 |    0.03286 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08674 | -0.00136 |    0.02002 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.12445 | -0.00367 |    0.04431 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09900 | -0.00251 |    0.03117 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.14365 | -0.00809 |    0.04617 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08274 | -0.00108 |    0.02329 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06843 | -0.00123 |    0.01676 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.09049 | -0.00204 |    0.02701 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06917 | -0.00383 |    0.01777 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08522 | -0.00155 |    0.02898 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.08017 | -0.00358 |    0.02782 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.08062 |  0.00011 |    0.02104 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07380 | -0.00234 |    0.02304 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06025 | -0.00213 |    0.01573 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05662 | -0.00041 |    0.01514 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03688 |  0.00131 |    0.00666 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51700 | -0.07735 |    0.25944 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:10:10,711 - Total sparsity: 88.62

2018-10-27 23:10:10,711 - --- validate (epoch=137)-----------
2018-10-27 23:10:10,711 - 10000 samples (128 per mini-batch)
2018-10-27 23:10:11,439 - Epoch: [137][   50/   78]    Loss 0.415151    Top1 86.859375    Top5 99.421875    
2018-10-27 23:10:11,829 - ==> Top1: 86.940    Top5: 99.500    Loss: 0.412

2018-10-27 23:10:11,830 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:10:11,831 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:10:11,840 - 

2018-10-27 23:10:11,841 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:10:13,040 - Epoch: [138][   50/  391]    Overall Loss 0.271220    Objective Loss 0.271220    Top1 90.625000    Top5 99.796875    LR 0.030000    Time 0.023934    
2018-10-27 23:10:14,181 - Epoch: [138][  100/  391]    Overall Loss 0.269054    Objective Loss 0.269054    Top1 90.476562    Top5 99.812500    LR 0.030000    Time 0.023364    
2018-10-27 23:10:15,321 - Epoch: [138][  150/  391]    Overall Loss 0.274375    Objective Loss 0.274375    Top1 90.286458    Top5 99.791667    LR 0.030000    Time 0.023169    
2018-10-27 23:10:16,463 - Epoch: [138][  200/  391]    Overall Loss 0.275415    Objective Loss 0.275415    Top1 90.285156    Top5 99.792969    LR 0.030000    Time 0.023078    
2018-10-27 23:10:17,603 - Epoch: [138][  250/  391]    Overall Loss 0.275921    Objective Loss 0.275921    Top1 90.246875    Top5 99.781250    LR 0.030000    Time 0.023019    
2018-10-27 23:10:18,747 - Epoch: [138][  300/  391]    Overall Loss 0.274405    Objective Loss 0.274405    Top1 90.244792    Top5 99.783854    LR 0.030000    Time 0.022978    
2018-10-27 23:10:19,889 - Epoch: [138][  350/  391]    Overall Loss 0.276460    Objective Loss 0.276460    Top1 90.209821    Top5 99.787946    LR 0.030000    Time 0.022955    
2018-10-27 23:10:20,901 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.37523 | -0.00492 |    0.14171 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.11480 | -0.00371 |    0.02544 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10897 |  0.00201 |    0.02730 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.11167 | -0.00322 |    0.02664 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08943 | -0.00168 |    0.01776 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.11663 | -0.00446 |    0.03293 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08626 | -0.00155 |    0.01989 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.12376 | -0.00375 |    0.04395 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09845 | -0.00253 |    0.03100 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.14277 | -0.00841 |    0.04625 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08234 | -0.00117 |    0.02310 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06808 | -0.00115 |    0.01671 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.09003 | -0.00220 |    0.02688 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06882 | -0.00380 |    0.01769 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08475 | -0.00151 |    0.02882 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07976 | -0.00360 |    0.02770 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.08003 |  0.00001 |    0.02082 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07346 | -0.00235 |    0.02291 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.06000 | -0.00210 |    0.01566 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05639 | -0.00042 |    0.01508 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03668 |  0.00133 |    0.00663 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51744 | -0.07765 |    0.25970 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:10:20,901 - Total sparsity: 88.62

2018-10-27 23:10:20,901 - --- validate (epoch=138)-----------
2018-10-27 23:10:20,901 - 10000 samples (128 per mini-batch)
2018-10-27 23:10:21,625 - Epoch: [138][   50/   78]    Loss 0.448650    Top1 85.453125    Top5 99.406250    
2018-10-27 23:10:22,019 - ==> Top1: 85.620    Top5: 99.400    Loss: 0.448

2018-10-27 23:10:22,020 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:10:22,020 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:10:22,036 - 

2018-10-27 23:10:22,037 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:10:23,211 - Epoch: [139][   50/  391]    Overall Loss 0.290732    Objective Loss 0.290732    Top1 89.843750    Top5 99.781250    LR 0.030000    Time 0.023447    
2018-10-27 23:10:24,354 - Epoch: [139][  100/  391]    Overall Loss 0.283753    Objective Loss 0.283753    Top1 90.226562    Top5 99.789062    LR 0.030000    Time 0.023146    
2018-10-27 23:10:25,496 - Epoch: [139][  150/  391]    Overall Loss 0.286000    Objective Loss 0.286000    Top1 90.104167    Top5 99.776042    LR 0.030000    Time 0.023036    
2018-10-27 23:10:26,637 - Epoch: [139][  200/  391]    Overall Loss 0.284896    Objective Loss 0.284896    Top1 90.113281    Top5 99.785156    LR 0.030000    Time 0.022972    
2018-10-27 23:10:27,777 - Epoch: [139][  250/  391]    Overall Loss 0.282972    Objective Loss 0.282972    Top1 90.131250    Top5 99.759375    LR 0.030000    Time 0.022934    
2018-10-27 23:10:28,916 - Epoch: [139][  300/  391]    Overall Loss 0.281676    Objective Loss 0.281676    Top1 90.234375    Top5 99.755208    LR 0.030000    Time 0.022903    
2018-10-27 23:10:30,059 - Epoch: [139][  350/  391]    Overall Loss 0.281924    Objective Loss 0.281924    Top1 90.162946    Top5 99.756696    LR 0.030000    Time 0.022883    
2018-10-27 23:10:31,074 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.37332 | -0.00429 |    0.14096 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.11426 | -0.00367 |    0.02527 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10839 |  0.00178 |    0.02700 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.11120 | -0.00319 |    0.02659 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08898 | -0.00187 |    0.01762 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.11608 | -0.00450 |    0.03266 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08581 | -0.00146 |    0.01975 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.12312 | -0.00353 |    0.04375 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09794 | -0.00252 |    0.03084 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.14195 | -0.00790 |    0.04603 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08198 | -0.00119 |    0.02304 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06775 | -0.00108 |    0.01659 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08960 | -0.00209 |    0.02672 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06848 | -0.00383 |    0.01758 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08431 | -0.00149 |    0.02863 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07938 | -0.00360 |    0.02755 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07950 |  0.00014 |    0.02063 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07316 | -0.00234 |    0.02281 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05975 | -0.00206 |    0.01562 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05618 | -0.00041 |    0.01501 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03650 |  0.00135 |    0.00658 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51730 | -0.07732 |    0.25935 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:10:31,074 - Total sparsity: 88.62

2018-10-27 23:10:31,075 - --- validate (epoch=139)-----------
2018-10-27 23:10:31,075 - 10000 samples (128 per mini-batch)
2018-10-27 23:10:31,799 - Epoch: [139][   50/   78]    Loss 0.429642    Top1 86.203125    Top5 99.343750    
2018-10-27 23:10:32,192 - ==> Top1: 86.250    Top5: 99.440    Loss: 0.429

2018-10-27 23:10:32,193 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:10:32,193 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:10:32,209 - 

2018-10-27 23:10:32,210 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:10:33,381 - Epoch: [140][   50/  391]    Overall Loss 0.257041    Objective Loss 0.257041    Top1 91.015625    Top5 99.781250    LR 0.030000    Time 0.023401    
2018-10-27 23:10:34,521 - Epoch: [140][  100/  391]    Overall Loss 0.272697    Objective Loss 0.272697    Top1 90.500000    Top5 99.742188    LR 0.030000    Time 0.023087    
2018-10-27 23:10:35,663 - Epoch: [140][  150/  391]    Overall Loss 0.276593    Objective Loss 0.276593    Top1 90.385417    Top5 99.739583    LR 0.030000    Time 0.022996    
2018-10-27 23:10:36,806 - Epoch: [140][  200/  391]    Overall Loss 0.278180    Objective Loss 0.278180    Top1 90.226562    Top5 99.757812    LR 0.030000    Time 0.022952    
2018-10-27 23:10:37,950 - Epoch: [140][  250/  391]    Overall Loss 0.280519    Objective Loss 0.280519    Top1 90.150000    Top5 99.737500    LR 0.030000    Time 0.022932    
2018-10-27 23:10:39,092 - Epoch: [140][  300/  391]    Overall Loss 0.282723    Objective Loss 0.282723    Top1 90.039062    Top5 99.755208    LR 0.030000    Time 0.022913    
2018-10-27 23:10:40,236 - Epoch: [140][  350/  391]    Overall Loss 0.282816    Objective Loss 0.282816    Top1 90.049107    Top5 99.758929    LR 0.030000    Time 0.022895    
2018-10-27 23:10:41,251 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.37139 | -0.00447 |    0.13977 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.11365 | -0.00384 |    0.02508 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10781 |  0.00184 |    0.02677 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.11065 | -0.00290 |    0.02629 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08852 | -0.00195 |    0.01761 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.11551 | -0.00470 |    0.03260 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08537 | -0.00115 |    0.01973 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.12250 | -0.00382 |    0.04349 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09745 | -0.00245 |    0.03063 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.14107 | -0.00803 |    0.04568 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08163 | -0.00131 |    0.02291 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06744 | -0.00116 |    0.01649 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08918 | -0.00196 |    0.02663 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06816 | -0.00368 |    0.01751 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08389 | -0.00145 |    0.02849 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07902 | -0.00353 |    0.02743 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07897 | -0.00001 |    0.02050 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07285 | -0.00229 |    0.02270 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05950 | -0.00204 |    0.01553 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05598 | -0.00036 |    0.01495 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03632 |  0.00132 |    0.00655 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51729 | -0.07738 |    0.25943 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:10:41,251 - Total sparsity: 88.62

2018-10-27 23:10:41,251 - --- validate (epoch=140)-----------
2018-10-27 23:10:41,251 - 10000 samples (128 per mini-batch)
2018-10-27 23:10:41,979 - Epoch: [140][   50/   78]    Loss 0.431708    Top1 86.078125    Top5 99.312500    
2018-10-27 23:10:42,374 - ==> Top1: 86.370    Top5: 99.440    Loss: 0.421

2018-10-27 23:10:42,375 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:10:42,376 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:10:42,393 - 

2018-10-27 23:10:42,393 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:10:43,564 - Epoch: [141][   50/  391]    Overall Loss 0.262530    Objective Loss 0.262530    Top1 90.859375    Top5 99.875000    LR 0.030000    Time 0.023369    
2018-10-27 23:10:44,705 - Epoch: [141][  100/  391]    Overall Loss 0.269866    Objective Loss 0.269866    Top1 90.585938    Top5 99.820312    LR 0.030000    Time 0.023086    
2018-10-27 23:10:45,846 - Epoch: [141][  150/  391]    Overall Loss 0.272706    Objective Loss 0.272706    Top1 90.401042    Top5 99.781250    LR 0.030000    Time 0.022987    
2018-10-27 23:10:46,989 - Epoch: [141][  200/  391]    Overall Loss 0.276367    Objective Loss 0.276367    Top1 90.238281    Top5 99.781250    LR 0.030000    Time 0.022948    
2018-10-27 23:10:48,131 - Epoch: [141][  250/  391]    Overall Loss 0.278022    Objective Loss 0.278022    Top1 90.121875    Top5 99.784375    LR 0.030000    Time 0.022924    
2018-10-27 23:10:49,275 - Epoch: [141][  300/  391]    Overall Loss 0.278518    Objective Loss 0.278518    Top1 90.091146    Top5 99.773438    LR 0.030000    Time 0.022911    
2018-10-27 23:10:50,417 - Epoch: [141][  350/  391]    Overall Loss 0.280963    Objective Loss 0.280963    Top1 90.064732    Top5 99.750000    LR 0.030000    Time 0.022897    
2018-10-27 23:10:51,432 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.36946 | -0.00355 |    0.13902 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.11305 | -0.00394 |    0.02497 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10724 |  0.00152 |    0.02670 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.11009 | -0.00281 |    0.02603 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08808 | -0.00196 |    0.01744 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.11495 | -0.00457 |    0.03253 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08492 | -0.00123 |    0.01964 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.12191 | -0.00348 |    0.04322 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09697 | -0.00235 |    0.03045 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.14028 | -0.00804 |    0.04526 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08132 | -0.00131 |    0.02284 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06714 | -0.00100 |    0.01645 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08879 | -0.00180 |    0.02650 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06784 | -0.00379 |    0.01745 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08348 | -0.00142 |    0.02837 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07867 | -0.00353 |    0.02728 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07845 |  0.00001 |    0.02035 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07257 | -0.00223 |    0.02259 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05928 | -0.00207 |    0.01547 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05579 | -0.00040 |    0.01488 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03615 |  0.00133 |    0.00652 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51636 | -0.07736 |    0.25885 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:10:51,432 - Total sparsity: 88.62

2018-10-27 23:10:51,432 - --- validate (epoch=141)-----------
2018-10-27 23:10:51,432 - 10000 samples (128 per mini-batch)
2018-10-27 23:10:52,159 - Epoch: [141][   50/   78]    Loss 0.430320    Top1 86.203125    Top5 99.390625    
2018-10-27 23:10:52,553 - ==> Top1: 86.130    Top5: 99.470    Loss: 0.422

2018-10-27 23:10:52,554 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:10:52,554 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:10:52,564 - 

2018-10-27 23:10:52,565 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:10:53,764 - Epoch: [142][   50/  391]    Overall Loss 0.282068    Objective Loss 0.282068    Top1 89.906250    Top5 99.843750    LR 0.030000    Time 0.023954    
2018-10-27 23:10:54,907 - Epoch: [142][  100/  391]    Overall Loss 0.275378    Objective Loss 0.275378    Top1 90.257812    Top5 99.742188    LR 0.030000    Time 0.023387    
2018-10-27 23:10:56,050 - Epoch: [142][  150/  391]    Overall Loss 0.278186    Objective Loss 0.278186    Top1 89.979167    Top5 99.739583    LR 0.030000    Time 0.023207    
2018-10-27 23:10:57,193 - Epoch: [142][  200/  391]    Overall Loss 0.276389    Objective Loss 0.276389    Top1 90.089844    Top5 99.742188    LR 0.030000    Time 0.023109    
2018-10-27 23:10:58,334 - Epoch: [142][  250/  391]    Overall Loss 0.274711    Objective Loss 0.274711    Top1 90.259375    Top5 99.740625    LR 0.030000    Time 0.023048    
2018-10-27 23:10:59,475 - Epoch: [142][  300/  391]    Overall Loss 0.275914    Objective Loss 0.275914    Top1 90.252604    Top5 99.750000    LR 0.030000    Time 0.023007    
2018-10-27 23:11:00,619 - Epoch: [142][  350/  391]    Overall Loss 0.277030    Objective Loss 0.277030    Top1 90.200893    Top5 99.745536    LR 0.030000    Time 0.022984    
2018-10-27 23:11:01,642 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.36743 | -0.00399 |    0.13845 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.11249 | -0.00415 |    0.02498 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10668 |  0.00162 |    0.02652 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10953 | -0.00296 |    0.02594 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08761 | -0.00197 |    0.01745 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.11437 | -0.00462 |    0.03212 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08450 | -0.00106 |    0.01950 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.12130 | -0.00355 |    0.04312 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09650 | -0.00239 |    0.03028 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.13946 | -0.00772 |    0.04513 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08102 | -0.00134 |    0.02274 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06685 | -0.00094 |    0.01637 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08841 | -0.00180 |    0.02636 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06755 | -0.00364 |    0.01735 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08308 | -0.00138 |    0.02818 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07832 | -0.00343 |    0.02716 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07794 |  0.00001 |    0.02036 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07227 | -0.00220 |    0.02249 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05905 | -0.00201 |    0.01540 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05560 | -0.00037 |    0.01482 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03598 |  0.00132 |    0.00648 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51766 | -0.07711 |    0.25927 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:11:01,642 - Total sparsity: 88.62

2018-10-27 23:11:01,642 - --- validate (epoch=142)-----------
2018-10-27 23:11:01,642 - 10000 samples (128 per mini-batch)
2018-10-27 23:11:02,369 - Epoch: [142][   50/   78]    Loss 0.448775    Top1 86.015625    Top5 99.328125    
2018-10-27 23:11:02,763 - ==> Top1: 86.020    Top5: 99.400    Loss: 0.443

2018-10-27 23:11:02,764 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:11:02,764 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:11:02,775 - 

2018-10-27 23:11:02,775 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:11:03,975 - Epoch: [143][   50/  391]    Overall Loss 0.277226    Objective Loss 0.277226    Top1 90.359375    Top5 99.750000    LR 0.030000    Time 0.023965    
2018-10-27 23:11:05,115 - Epoch: [143][  100/  391]    Overall Loss 0.269899    Objective Loss 0.269899    Top1 90.648438    Top5 99.765625    LR 0.030000    Time 0.023375    
2018-10-27 23:11:06,259 - Epoch: [143][  150/  391]    Overall Loss 0.267257    Objective Loss 0.267257    Top1 90.833333    Top5 99.776042    LR 0.030000    Time 0.023200    
2018-10-27 23:11:07,403 - Epoch: [143][  200/  391]    Overall Loss 0.270250    Objective Loss 0.270250    Top1 90.625000    Top5 99.789062    LR 0.030000    Time 0.023114    
2018-10-27 23:11:08,546 - Epoch: [143][  250/  391]    Overall Loss 0.273988    Objective Loss 0.273988    Top1 90.481250    Top5 99.781250    LR 0.030000    Time 0.023056    
2018-10-27 23:11:09,687 - Epoch: [143][  300/  391]    Overall Loss 0.274846    Objective Loss 0.274846    Top1 90.388021    Top5 99.773438    LR 0.030000    Time 0.023013    
2018-10-27 23:11:10,829 - Epoch: [143][  350/  391]    Overall Loss 0.276238    Objective Loss 0.276238    Top1 90.323661    Top5 99.774554    LR 0.030000    Time 0.022985    
2018-10-27 23:11:11,847 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.36568 | -0.00525 |    0.13764 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.11204 | -0.00395 |    0.02480 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10618 |  0.00166 |    0.02633 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10907 | -0.00298 |    0.02582 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08716 | -0.00200 |    0.01725 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.11391 | -0.00421 |    0.03212 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08413 | -0.00094 |    0.01928 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.12073 | -0.00357 |    0.04302 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09605 | -0.00230 |    0.03016 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.13862 | -0.00788 |    0.04483 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08077 | -0.00128 |    0.02268 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06659 | -0.00097 |    0.01627 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08806 | -0.00170 |    0.02621 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06726 | -0.00380 |    0.01723 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08270 | -0.00144 |    0.02805 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07800 | -0.00342 |    0.02704 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07747 |  0.00005 |    0.02021 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07200 | -0.00213 |    0.02240 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05884 | -0.00201 |    0.01534 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05543 | -0.00037 |    0.01477 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03582 |  0.00133 |    0.00645 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51739 | -0.07738 |    0.25916 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:11:11,847 - Total sparsity: 88.62

2018-10-27 23:11:11,847 - --- validate (epoch=143)-----------
2018-10-27 23:11:11,847 - 10000 samples (128 per mini-batch)
2018-10-27 23:11:12,573 - Epoch: [143][   50/   78]    Loss 0.434582    Top1 86.156250    Top5 99.500000    
2018-10-27 23:11:12,958 - ==> Top1: 86.110    Top5: 99.520    Loss: 0.427

2018-10-27 23:11:12,958 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:11:12,958 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:11:12,968 - 

2018-10-27 23:11:12,969 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:11:14,168 - Epoch: [144][   50/  391]    Overall Loss 0.275661    Objective Loss 0.275661    Top1 90.781250    Top5 99.734375    LR 0.030000    Time 0.023944    
2018-10-27 23:11:15,309 - Epoch: [144][  100/  391]    Overall Loss 0.277703    Objective Loss 0.277703    Top1 90.484375    Top5 99.726562    LR 0.030000    Time 0.023370    
2018-10-27 23:11:16,450 - Epoch: [144][  150/  391]    Overall Loss 0.281347    Objective Loss 0.281347    Top1 90.348958    Top5 99.692708    LR 0.030000    Time 0.023178    
2018-10-27 23:11:17,593 - Epoch: [144][  200/  391]    Overall Loss 0.284949    Objective Loss 0.284949    Top1 90.132812    Top5 99.707031    LR 0.030000    Time 0.023093    
2018-10-27 23:11:18,734 - Epoch: [144][  250/  391]    Overall Loss 0.281939    Objective Loss 0.281939    Top1 90.200000    Top5 99.740625    LR 0.030000    Time 0.023032    
2018-10-27 23:11:19,879 - Epoch: [144][  300/  391]    Overall Loss 0.281598    Objective Loss 0.281598    Top1 90.216146    Top5 99.747396    LR 0.030000    Time 0.022994    
2018-10-27 23:11:21,021 - Epoch: [144][  350/  391]    Overall Loss 0.282090    Objective Loss 0.282090    Top1 90.160714    Top5 99.747768    LR 0.030000    Time 0.022971    
2018-10-27 23:11:22,037 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.36453 | -0.00436 |    0.13748 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.11157 | -0.00378 |    0.02467 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10565 |  0.00171 |    0.02619 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10859 | -0.00306 |    0.02577 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08673 | -0.00184 |    0.01717 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.11338 | -0.00409 |    0.03195 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08372 | -0.00117 |    0.01926 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.12017 | -0.00360 |    0.04274 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09564 | -0.00232 |    0.03010 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.13788 | -0.00730 |    0.04456 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08049 | -0.00122 |    0.02256 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06630 | -0.00086 |    0.01620 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08771 | -0.00193 |    0.02615 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06700 | -0.00371 |    0.01718 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08233 | -0.00157 |    0.02790 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07770 | -0.00329 |    0.02691 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07701 | -0.00001 |    0.02007 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07174 | -0.00213 |    0.02234 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05863 | -0.00203 |    0.01528 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05527 | -0.00037 |    0.01473 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03568 |  0.00133 |    0.00643 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51766 | -0.07740 |    0.25933 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:11:22,037 - Total sparsity: 88.62

2018-10-27 23:11:22,037 - --- validate (epoch=144)-----------
2018-10-27 23:11:22,037 - 10000 samples (128 per mini-batch)
2018-10-27 23:11:22,757 - Epoch: [144][   50/   78]    Loss 0.436214    Top1 85.765625    Top5 99.359375    
2018-10-27 23:11:23,149 - ==> Top1: 85.800    Top5: 99.370    Loss: 0.432

2018-10-27 23:11:23,150 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:11:23,150 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:11:23,166 - 

2018-10-27 23:11:23,166 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:11:24,337 - Epoch: [145][   50/  391]    Overall Loss 0.265123    Objective Loss 0.265123    Top1 91.156250    Top5 99.812500    LR 0.030000    Time 0.023387    
2018-10-27 23:11:25,479 - Epoch: [145][  100/  391]    Overall Loss 0.263468    Objective Loss 0.263468    Top1 90.984375    Top5 99.828125    LR 0.030000    Time 0.023095    
2018-10-27 23:11:26,620 - Epoch: [145][  150/  391]    Overall Loss 0.265123    Objective Loss 0.265123    Top1 90.973958    Top5 99.765625    LR 0.030000    Time 0.022994    
2018-10-27 23:11:27,762 - Epoch: [145][  200/  391]    Overall Loss 0.266932    Objective Loss 0.266932    Top1 90.816406    Top5 99.773438    LR 0.030000    Time 0.022949    
2018-10-27 23:11:28,902 - Epoch: [145][  250/  391]    Overall Loss 0.269978    Objective Loss 0.269978    Top1 90.596875    Top5 99.781250    LR 0.030000    Time 0.022915    
2018-10-27 23:11:30,044 - Epoch: [145][  300/  391]    Overall Loss 0.273346    Objective Loss 0.273346    Top1 90.437500    Top5 99.773438    LR 0.030000    Time 0.022899    
2018-10-27 23:11:31,187 - Epoch: [145][  350/  391]    Overall Loss 0.275261    Objective Loss 0.275261    Top1 90.366071    Top5 99.779018    LR 0.030000    Time 0.022888    
2018-10-27 23:11:32,203 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.36274 | -0.00477 |    0.13627 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.11108 | -0.00382 |    0.02455 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10516 |  0.00122 |    0.02601 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10814 | -0.00286 |    0.02569 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08628 | -0.00197 |    0.01704 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.11292 | -0.00387 |    0.03177 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08339 | -0.00121 |    0.01919 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11967 | -0.00341 |    0.04257 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09524 | -0.00218 |    0.02994 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.13713 | -0.00745 |    0.04443 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08022 | -0.00125 |    0.02247 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06605 | -0.00089 |    0.01614 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08737 | -0.00191 |    0.02610 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06674 | -0.00362 |    0.01711 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08198 | -0.00146 |    0.02779 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07741 | -0.00323 |    0.02680 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07653 |  0.00016 |    0.01989 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07149 | -0.00215 |    0.02226 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05845 | -0.00200 |    0.01522 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05512 | -0.00030 |    0.01468 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03553 |  0.00135 |    0.00639 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51814 | -0.07742 |    0.25950 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:11:32,204 - Total sparsity: 88.62

2018-10-27 23:11:32,204 - --- validate (epoch=145)-----------
2018-10-27 23:11:32,204 - 10000 samples (128 per mini-batch)
2018-10-27 23:11:32,922 - Epoch: [145][   50/   78]    Loss 0.427286    Top1 86.078125    Top5 99.406250    
2018-10-27 23:11:33,309 - ==> Top1: 86.190    Top5: 99.450    Loss: 0.426

2018-10-27 23:11:33,310 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:11:33,310 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:11:33,330 - 

2018-10-27 23:11:33,330 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:11:34,503 - Epoch: [146][   50/  391]    Overall Loss 0.274316    Objective Loss 0.274316    Top1 90.609375    Top5 99.843750    LR 0.030000    Time 0.023431    
2018-10-27 23:11:35,646 - Epoch: [146][  100/  391]    Overall Loss 0.279382    Objective Loss 0.279382    Top1 90.296875    Top5 99.812500    LR 0.030000    Time 0.023131    
2018-10-27 23:11:36,787 - Epoch: [146][  150/  391]    Overall Loss 0.278516    Objective Loss 0.278516    Top1 90.286458    Top5 99.776042    LR 0.030000    Time 0.023015    
2018-10-27 23:11:37,928 - Epoch: [146][  200/  391]    Overall Loss 0.278863    Objective Loss 0.278863    Top1 90.265625    Top5 99.769531    LR 0.030000    Time 0.022960    
2018-10-27 23:11:39,069 - Epoch: [146][  250/  391]    Overall Loss 0.276947    Objective Loss 0.276947    Top1 90.268750    Top5 99.762500    LR 0.030000    Time 0.022928    
2018-10-27 23:11:40,209 - Epoch: [146][  300/  391]    Overall Loss 0.275135    Objective Loss 0.275135    Top1 90.320312    Top5 99.768229    LR 0.030000    Time 0.022903    
2018-10-27 23:11:41,350 - Epoch: [146][  350/  391]    Overall Loss 0.275699    Objective Loss 0.275699    Top1 90.312500    Top5 99.761161    LR 0.030000    Time 0.022887    
2018-10-27 23:11:42,367 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.36120 | -0.00333 |    0.13611 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.11064 | -0.00385 |    0.02447 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10472 |  0.00129 |    0.02606 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10769 | -0.00278 |    0.02560 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08584 | -0.00196 |    0.01699 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.11244 | -0.00395 |    0.03163 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08303 | -0.00124 |    0.01904 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11917 | -0.00330 |    0.04239 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09485 | -0.00224 |    0.02983 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.13636 | -0.00730 |    0.04424 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.08000 | -0.00124 |    0.02247 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06582 | -0.00091 |    0.01604 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08703 | -0.00198 |    0.02594 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06647 | -0.00363 |    0.01706 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08165 | -0.00148 |    0.02765 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07713 | -0.00319 |    0.02670 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07609 |  0.00029 |    0.01974 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07126 | -0.00213 |    0.02213 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05827 | -0.00202 |    0.01517 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05497 | -0.00029 |    0.01464 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03538 |  0.00134 |    0.00636 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51766 | -0.07726 |    0.25913 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:11:42,367 - Total sparsity: 88.62

2018-10-27 23:11:42,367 - --- validate (epoch=146)-----------
2018-10-27 23:11:42,367 - 10000 samples (128 per mini-batch)
2018-10-27 23:11:43,088 - Epoch: [146][   50/   78]    Loss 0.417178    Top1 86.687500    Top5 99.437500    
2018-10-27 23:11:43,481 - ==> Top1: 86.730    Top5: 99.490    Loss: 0.409

2018-10-27 23:11:43,482 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:11:43,482 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:11:43,493 - 

2018-10-27 23:11:43,493 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:11:44,693 - Epoch: [147][   50/  391]    Overall Loss 0.272797    Objective Loss 0.272797    Top1 90.281250    Top5 99.843750    LR 0.030000    Time 0.023959    
2018-10-27 23:11:45,833 - Epoch: [147][  100/  391]    Overall Loss 0.277017    Objective Loss 0.277017    Top1 90.289062    Top5 99.804688    LR 0.030000    Time 0.023365    
2018-10-27 23:11:46,975 - Epoch: [147][  150/  391]    Overall Loss 0.276684    Objective Loss 0.276684    Top1 90.286458    Top5 99.817708    LR 0.030000    Time 0.023183    
2018-10-27 23:11:48,116 - Epoch: [147][  200/  391]    Overall Loss 0.276197    Objective Loss 0.276197    Top1 90.335938    Top5 99.812500    LR 0.030000    Time 0.023087    
2018-10-27 23:11:49,258 - Epoch: [147][  250/  391]    Overall Loss 0.278616    Objective Loss 0.278616    Top1 90.312500    Top5 99.806250    LR 0.030000    Time 0.023030    
2018-10-27 23:11:50,400 - Epoch: [147][  300/  391]    Overall Loss 0.278653    Objective Loss 0.278653    Top1 90.299479    Top5 99.789062    LR 0.030000    Time 0.022994    
2018-10-27 23:11:51,542 - Epoch: [147][  350/  391]    Overall Loss 0.278116    Objective Loss 0.278116    Top1 90.334821    Top5 99.787946    LR 0.030000    Time 0.022969    
2018-10-27 23:11:52,557 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.35984 | -0.00415 |    0.13615 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.11023 | -0.00364 |    0.02442 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10427 |  0.00129 |    0.02596 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10722 | -0.00317 |    0.02542 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08542 | -0.00213 |    0.01680 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.11195 | -0.00427 |    0.03145 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08266 | -0.00118 |    0.01889 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11865 | -0.00380 |    0.04232 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09445 | -0.00224 |    0.02965 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.13558 | -0.00765 |    0.04385 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07976 | -0.00126 |    0.02236 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06559 | -0.00097 |    0.01596 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08671 | -0.00185 |    0.02583 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06623 | -0.00371 |    0.01697 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08132 | -0.00160 |    0.02752 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07686 | -0.00320 |    0.02659 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07566 |  0.00007 |    0.01972 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07103 | -0.00208 |    0.02207 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05810 | -0.00198 |    0.01512 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05482 | -0.00031 |    0.01457 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03524 |  0.00133 |    0.00634 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51795 | -0.07721 |    0.25923 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:11:52,558 - Total sparsity: 88.62

2018-10-27 23:11:52,558 - --- validate (epoch=147)-----------
2018-10-27 23:11:52,558 - 10000 samples (128 per mini-batch)
2018-10-27 23:11:53,277 - Epoch: [147][   50/   78]    Loss 0.441610    Top1 85.593750    Top5 99.468750    
2018-10-27 23:11:53,669 - ==> Top1: 85.750    Top5: 99.510    Loss: 0.438

2018-10-27 23:11:53,670 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:11:53,670 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:11:53,680 - 

2018-10-27 23:11:53,681 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:11:54,877 - Epoch: [148][   50/  391]    Overall Loss 0.281162    Objective Loss 0.281162    Top1 90.187500    Top5 99.750000    LR 0.030000    Time 0.023894    
2018-10-27 23:11:56,017 - Epoch: [148][  100/  391]    Overall Loss 0.279659    Objective Loss 0.279659    Top1 90.281250    Top5 99.773438    LR 0.030000    Time 0.023327    
2018-10-27 23:11:57,157 - Epoch: [148][  150/  391]    Overall Loss 0.279466    Objective Loss 0.279466    Top1 90.250000    Top5 99.796875    LR 0.030000    Time 0.023143    
2018-10-27 23:11:58,297 - Epoch: [148][  200/  391]    Overall Loss 0.279010    Objective Loss 0.279010    Top1 90.253906    Top5 99.796875    LR 0.030000    Time 0.023054    
2018-10-27 23:11:59,438 - Epoch: [148][  250/  391]    Overall Loss 0.276170    Objective Loss 0.276170    Top1 90.381250    Top5 99.803125    LR 0.030000    Time 0.023002    
2018-10-27 23:12:00,583 - Epoch: [148][  300/  391]    Overall Loss 0.278002    Objective Loss 0.278002    Top1 90.255208    Top5 99.807292    LR 0.030000    Time 0.022979    
2018-10-27 23:12:01,725 - Epoch: [148][  350/  391]    Overall Loss 0.278869    Objective Loss 0.278869    Top1 90.205357    Top5 99.819196    LR 0.030000    Time 0.022957    
2018-10-27 23:12:02,744 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.35847 | -0.00432 |    0.13516 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10979 | -0.00351 |    0.02429 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10384 |  0.00164 |    0.02583 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10684 | -0.00317 |    0.02557 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08505 | -0.00196 |    0.01678 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.11155 | -0.00425 |    0.03139 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08237 | -0.00121 |    0.01883 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11817 | -0.00374 |    0.04207 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09408 | -0.00230 |    0.02950 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.13497 | -0.00724 |    0.04361 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07953 | -0.00111 |    0.02226 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06536 | -0.00092 |    0.01592 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08641 | -0.00186 |    0.02574 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06599 | -0.00358 |    0.01689 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08102 | -0.00144 |    0.02744 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07660 | -0.00317 |    0.02648 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07523 | -0.00004 |    0.01962 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07082 | -0.00209 |    0.02204 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05794 | -0.00196 |    0.01509 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05469 | -0.00040 |    0.01453 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03510 |  0.00130 |    0.00630 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51850 | -0.07706 |    0.25952 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:12:02,745 - Total sparsity: 88.62

2018-10-27 23:12:02,745 - --- validate (epoch=148)-----------
2018-10-27 23:12:02,745 - 10000 samples (128 per mini-batch)
2018-10-27 23:12:03,469 - Epoch: [148][   50/   78]    Loss 0.442368    Top1 86.093750    Top5 99.343750    
2018-10-27 23:12:03,861 - ==> Top1: 86.120    Top5: 99.440    Loss: 0.443

2018-10-27 23:12:03,862 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:12:03,862 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:12:03,872 - 

2018-10-27 23:12:03,872 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:12:05,071 - Epoch: [149][   50/  391]    Overall Loss 0.257964    Objective Loss 0.257964    Top1 90.796875    Top5 99.843750    LR 0.030000    Time 0.023934    
2018-10-27 23:12:06,213 - Epoch: [149][  100/  391]    Overall Loss 0.268815    Objective Loss 0.268815    Top1 90.578125    Top5 99.757812    LR 0.030000    Time 0.023373    
2018-10-27 23:12:07,356 - Epoch: [149][  150/  391]    Overall Loss 0.272073    Objective Loss 0.272073    Top1 90.505208    Top5 99.755208    LR 0.030000    Time 0.023193    
2018-10-27 23:12:08,500 - Epoch: [149][  200/  391]    Overall Loss 0.273939    Objective Loss 0.273939    Top1 90.417969    Top5 99.761719    LR 0.030000    Time 0.023108    
2018-10-27 23:12:09,644 - Epoch: [149][  250/  391]    Overall Loss 0.275663    Objective Loss 0.275663    Top1 90.309375    Top5 99.771875    LR 0.030000    Time 0.023058    
2018-10-27 23:12:10,788 - Epoch: [149][  300/  391]    Overall Loss 0.277821    Objective Loss 0.277821    Top1 90.156250    Top5 99.765625    LR 0.030000    Time 0.023025    
2018-10-27 23:12:11,933 - Epoch: [149][  350/  391]    Overall Loss 0.278559    Objective Loss 0.278559    Top1 90.160714    Top5 99.770089    LR 0.030000    Time 0.023004    
2018-10-27 23:12:12,953 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.35700 | -0.00454 |    0.13447 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10947 | -0.00378 |    0.02423 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10351 |  0.00126 |    0.02565 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10648 | -0.00301 |    0.02542 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08469 | -0.00196 |    0.01672 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.11118 | -0.00409 |    0.03125 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08209 | -0.00121 |    0.01882 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11772 | -0.00355 |    0.04206 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09373 | -0.00231 |    0.02939 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.13427 | -0.00692 |    0.04322 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07930 | -0.00123 |    0.02218 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06516 | -0.00087 |    0.01582 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08614 | -0.00195 |    0.02561 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06579 | -0.00352 |    0.01686 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08073 | -0.00142 |    0.02737 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07637 | -0.00312 |    0.02638 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07482 |  0.00008 |    0.01950 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07062 | -0.00208 |    0.02198 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05780 | -0.00191 |    0.01506 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05457 | -0.00028 |    0.01451 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03498 |  0.00129 |    0.00628 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51755 | -0.07661 |    0.25892 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:12:12,954 - Total sparsity: 88.62

2018-10-27 23:12:12,954 - --- validate (epoch=149)-----------
2018-10-27 23:12:12,954 - 10000 samples (128 per mini-batch)
2018-10-27 23:12:13,675 - Epoch: [149][   50/   78]    Loss 0.470889    Top1 85.187500    Top5 99.312500    
2018-10-27 23:12:14,065 - ==> Top1: 85.340    Top5: 99.320    Loss: 0.462

2018-10-27 23:12:14,066 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:12:14,066 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:12:14,077 - 

2018-10-27 23:12:14,077 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:12:15,276 - Epoch: [150][   50/  391]    Overall Loss 0.257637    Objective Loss 0.257637    Top1 90.859375    Top5 99.734375    LR 0.030000    Time 0.023948    
2018-10-27 23:12:16,420 - Epoch: [150][  100/  391]    Overall Loss 0.266594    Objective Loss 0.266594    Top1 90.460938    Top5 99.726562    LR 0.030000    Time 0.023403    
2018-10-27 23:12:17,565 - Epoch: [150][  150/  391]    Overall Loss 0.269245    Objective Loss 0.269245    Top1 90.500000    Top5 99.744792    LR 0.030000    Time 0.023226    
2018-10-27 23:12:18,709 - Epoch: [150][  200/  391]    Overall Loss 0.273098    Objective Loss 0.273098    Top1 90.347656    Top5 99.761719    LR 0.030000    Time 0.023134    
2018-10-27 23:12:19,855 - Epoch: [150][  250/  391]    Overall Loss 0.274908    Objective Loss 0.274908    Top1 90.328125    Top5 99.756250    LR 0.030000    Time 0.023083    
2018-10-27 23:12:20,999 - Epoch: [150][  300/  391]    Overall Loss 0.278671    Objective Loss 0.278671    Top1 90.208333    Top5 99.760417    LR 0.030000    Time 0.023046    
2018-10-27 23:12:22,143 - Epoch: [150][  350/  391]    Overall Loss 0.280309    Objective Loss 0.280309    Top1 90.100446    Top5 99.758929    LR 0.030000    Time 0.023017    
2018-10-27 23:12:23,160 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.35553 | -0.00349 |    0.13404 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10915 | -0.00359 |    0.02414 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10313 |  0.00122 |    0.02557 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10611 | -0.00277 |    0.02525 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08436 | -0.00190 |    0.01671 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.11086 | -0.00413 |    0.03121 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08184 | -0.00084 |    0.01856 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11728 | -0.00362 |    0.04184 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09343 | -0.00220 |    0.02934 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.13366 | -0.00684 |    0.04329 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07911 | -0.00128 |    0.02213 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06496 | -0.00091 |    0.01577 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08589 | -0.00196 |    0.02555 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06558 | -0.00353 |    0.01681 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08046 | -0.00120 |    0.02723 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07615 | -0.00308 |    0.02629 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07445 | -0.00005 |    0.01935 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07044 | -0.00209 |    0.02193 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05766 | -0.00192 |    0.01502 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05447 | -0.00026 |    0.01447 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03487 |  0.00129 |    0.00626 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51691 | -0.07654 |    0.25860 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:12:23,160 - Total sparsity: 88.62

2018-10-27 23:12:23,160 - --- validate (epoch=150)-----------
2018-10-27 23:12:23,160 - 10000 samples (128 per mini-batch)
2018-10-27 23:12:23,879 - Epoch: [150][   50/   78]    Loss 0.448912    Top1 85.453125    Top5 99.453125    
2018-10-27 23:12:24,268 - ==> Top1: 85.500    Top5: 99.440    Loss: 0.443

2018-10-27 23:12:24,268 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:12:24,269 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:12:24,285 - 

2018-10-27 23:12:24,285 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:12:25,460 - Epoch: [151][   50/  391]    Overall Loss 0.288224    Objective Loss 0.288224    Top1 89.906250    Top5 99.796875    LR 0.030000    Time 0.023456    
2018-10-27 23:12:26,598 - Epoch: [151][  100/  391]    Overall Loss 0.278900    Objective Loss 0.278900    Top1 90.101562    Top5 99.804688    LR 0.030000    Time 0.023095    
2018-10-27 23:12:27,740 - Epoch: [151][  150/  391]    Overall Loss 0.278243    Objective Loss 0.278243    Top1 90.145833    Top5 99.812500    LR 0.030000    Time 0.023001    
2018-10-27 23:12:28,884 - Epoch: [151][  200/  391]    Overall Loss 0.278664    Objective Loss 0.278664    Top1 90.183594    Top5 99.769531    LR 0.030000    Time 0.022968    
2018-10-27 23:12:30,027 - Epoch: [151][  250/  391]    Overall Loss 0.281872    Objective Loss 0.281872    Top1 90.031250    Top5 99.775000    LR 0.030000    Time 0.022940    
2018-10-27 23:12:31,171 - Epoch: [151][  300/  391]    Overall Loss 0.283320    Objective Loss 0.283320    Top1 89.963542    Top5 99.786458    LR 0.030000    Time 0.022926    
2018-10-27 23:12:32,315 - Epoch: [151][  350/  391]    Overall Loss 0.282738    Objective Loss 0.282738    Top1 89.988839    Top5 99.785714    LR 0.030000    Time 0.022915    
2018-10-27 23:12:33,335 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.35425 | -0.00434 |    0.13339 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10879 | -0.00366 |    0.02420 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10276 |  0.00123 |    0.02554 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10574 | -0.00274 |    0.02529 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08403 | -0.00189 |    0.01666 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.11055 | -0.00437 |    0.03123 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08161 | -0.00104 |    0.01853 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11688 | -0.00358 |    0.04172 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09312 | -0.00229 |    0.02920 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.13305 | -0.00669 |    0.04309 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07893 | -0.00124 |    0.02208 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06475 | -0.00103 |    0.01578 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08565 | -0.00196 |    0.02552 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06537 | -0.00360 |    0.01673 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.08019 | -0.00136 |    0.02713 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07593 | -0.00308 |    0.02621 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07407 |  0.00005 |    0.01929 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07025 | -0.00207 |    0.02186 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05753 | -0.00192 |    0.01499 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05437 | -0.00026 |    0.01444 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03477 |  0.00130 |    0.00624 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51750 | -0.07640 |    0.25877 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:12:33,335 - Total sparsity: 88.62

2018-10-27 23:12:33,335 - --- validate (epoch=151)-----------
2018-10-27 23:12:33,336 - 10000 samples (128 per mini-batch)
2018-10-27 23:12:34,063 - Epoch: [151][   50/   78]    Loss 0.467411    Top1 85.140625    Top5 99.312500    
2018-10-27 23:12:34,457 - ==> Top1: 85.500    Top5: 99.320    Loss: 0.456

2018-10-27 23:12:34,458 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:12:34,458 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:12:34,469 - 

2018-10-27 23:12:34,470 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:12:35,669 - Epoch: [152][   50/  391]    Overall Loss 0.276207    Objective Loss 0.276207    Top1 90.187500    Top5 99.843750    LR 0.030000    Time 0.023947    
2018-10-27 23:12:36,812 - Epoch: [152][  100/  391]    Overall Loss 0.277946    Objective Loss 0.277946    Top1 90.070312    Top5 99.828125    LR 0.030000    Time 0.023389    
2018-10-27 23:12:37,954 - Epoch: [152][  150/  391]    Overall Loss 0.281613    Objective Loss 0.281613    Top1 89.973958    Top5 99.802083    LR 0.030000    Time 0.023202    
2018-10-27 23:12:39,096 - Epoch: [152][  200/  391]    Overall Loss 0.279027    Objective Loss 0.279027    Top1 90.074219    Top5 99.804688    LR 0.030000    Time 0.023085    
2018-10-27 23:12:40,238 - Epoch: [152][  250/  391]    Overall Loss 0.281790    Objective Loss 0.281790    Top1 90.000000    Top5 99.790625    LR 0.030000    Time 0.023029    
2018-10-27 23:12:41,380 - Epoch: [152][  300/  391]    Overall Loss 0.282979    Objective Loss 0.282979    Top1 89.979167    Top5 99.776042    LR 0.030000    Time 0.022996    
2018-10-27 23:12:42,524 - Epoch: [152][  350/  391]    Overall Loss 0.282761    Objective Loss 0.282761    Top1 90.026786    Top5 99.767857    LR 0.030000    Time 0.022974    
2018-10-27 23:12:43,544 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.35298 | -0.00516 |    0.13283 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10838 | -0.00403 |    0.02406 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10241 |  0.00147 |    0.02542 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10533 | -0.00271 |    0.02512 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08369 | -0.00193 |    0.01660 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.11025 | -0.00440 |    0.03120 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08133 | -0.00128 |    0.01858 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11647 | -0.00360 |    0.04151 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09282 | -0.00227 |    0.02906 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.13246 | -0.00660 |    0.04277 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07871 | -0.00144 |    0.02202 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06455 | -0.00092 |    0.01573 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08539 | -0.00204 |    0.02547 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06518 | -0.00347 |    0.01665 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07992 | -0.00123 |    0.02699 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07571 | -0.00301 |    0.02615 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07370 |  0.00018 |    0.01914 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.07007 | -0.00201 |    0.02180 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05739 | -0.00192 |    0.01495 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05426 | -0.00027 |    0.01439 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03465 |  0.00131 |    0.00624 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51729 | -0.07599 |    0.25866 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:12:43,544 - Total sparsity: 88.62

2018-10-27 23:12:43,544 - --- validate (epoch=152)-----------
2018-10-27 23:12:43,545 - 10000 samples (128 per mini-batch)
2018-10-27 23:12:44,260 - Epoch: [152][   50/   78]    Loss 0.425686    Top1 86.328125    Top5 99.421875    
2018-10-27 23:12:44,651 - ==> Top1: 86.530    Top5: 99.520    Loss: 0.420

2018-10-27 23:12:44,652 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:12:44,652 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:12:44,663 - 

2018-10-27 23:12:44,663 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:12:45,860 - Epoch: [153][   50/  391]    Overall Loss 0.276843    Objective Loss 0.276843    Top1 90.421875    Top5 99.843750    LR 0.030000    Time 0.023909    
2018-10-27 23:12:47,002 - Epoch: [153][  100/  391]    Overall Loss 0.284258    Objective Loss 0.284258    Top1 90.093750    Top5 99.851562    LR 0.030000    Time 0.023355    
2018-10-27 23:12:48,142 - Epoch: [153][  150/  391]    Overall Loss 0.281201    Objective Loss 0.281201    Top1 90.197917    Top5 99.838542    LR 0.030000    Time 0.023165    
2018-10-27 23:12:49,284 - Epoch: [153][  200/  391]    Overall Loss 0.276653    Objective Loss 0.276653    Top1 90.316406    Top5 99.851562    LR 0.030000    Time 0.023076    
2018-10-27 23:12:50,426 - Epoch: [153][  250/  391]    Overall Loss 0.277226    Objective Loss 0.277226    Top1 90.284375    Top5 99.843750    LR 0.030000    Time 0.023023    
2018-10-27 23:12:51,566 - Epoch: [153][  300/  391]    Overall Loss 0.279149    Objective Loss 0.279149    Top1 90.244792    Top5 99.825521    LR 0.030000    Time 0.022980    
2018-10-27 23:12:52,706 - Epoch: [153][  350/  391]    Overall Loss 0.278893    Objective Loss 0.278893    Top1 90.252232    Top5 99.812500    LR 0.030000    Time 0.022952    
2018-10-27 23:12:53,719 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.35155 | -0.00503 |    0.13191 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10805 | -0.00379 |    0.02400 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10208 |  0.00145 |    0.02536 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10495 | -0.00291 |    0.02489 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08336 | -0.00186 |    0.01651 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.11001 | -0.00385 |    0.03102 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08108 | -0.00128 |    0.01857 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11605 | -0.00368 |    0.04131 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09252 | -0.00234 |    0.02900 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.13177 | -0.00678 |    0.04258 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07851 | -0.00126 |    0.02193 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06433 | -0.00094 |    0.01568 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08518 | -0.00195 |    0.02540 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06500 | -0.00345 |    0.01658 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07966 | -0.00137 |    0.02691 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07550 | -0.00301 |    0.02607 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07337 |  0.00004 |    0.01906 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06990 | -0.00205 |    0.02172 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05726 | -0.00196 |    0.01488 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05416 | -0.00027 |    0.01434 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03454 |  0.00128 |    0.00620 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51787 | -0.07605 |    0.25872 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:12:53,719 - Total sparsity: 88.62

2018-10-27 23:12:53,720 - --- validate (epoch=153)-----------
2018-10-27 23:12:53,720 - 10000 samples (128 per mini-batch)
2018-10-27 23:12:54,441 - Epoch: [153][   50/   78]    Loss 0.442716    Top1 85.828125    Top5 99.453125    
2018-10-27 23:12:54,835 - ==> Top1: 86.200    Top5: 99.500    Loss: 0.427

2018-10-27 23:12:54,836 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:12:54,836 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:12:54,846 - 

2018-10-27 23:12:54,846 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:12:56,045 - Epoch: [154][   50/  391]    Overall Loss 0.268793    Objective Loss 0.268793    Top1 90.656250    Top5 99.734375    LR 0.030000    Time 0.023938    
2018-10-27 23:12:57,187 - Epoch: [154][  100/  391]    Overall Loss 0.271051    Objective Loss 0.271051    Top1 90.570312    Top5 99.789062    LR 0.030000    Time 0.023378    
2018-10-27 23:12:58,327 - Epoch: [154][  150/  391]    Overall Loss 0.274354    Objective Loss 0.274354    Top1 90.364583    Top5 99.765625    LR 0.030000    Time 0.023178    
2018-10-27 23:12:59,468 - Epoch: [154][  200/  391]    Overall Loss 0.277621    Objective Loss 0.277621    Top1 90.292969    Top5 99.738281    LR 0.030000    Time 0.023080    
2018-10-27 23:13:00,616 - Epoch: [154][  250/  391]    Overall Loss 0.279649    Objective Loss 0.279649    Top1 90.218750    Top5 99.743750    LR 0.030000    Time 0.023049    
2018-10-27 23:13:01,761 - Epoch: [154][  300/  391]    Overall Loss 0.280123    Objective Loss 0.280123    Top1 90.156250    Top5 99.752604    LR 0.030000    Time 0.023008    
2018-10-27 23:13:02,903 - Epoch: [154][  350/  391]    Overall Loss 0.280619    Objective Loss 0.280619    Top1 90.127232    Top5 99.743304    LR 0.030000    Time 0.022981    
2018-10-27 23:13:03,918 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.35057 | -0.00514 |    0.13202 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10775 | -0.00376 |    0.02394 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10177 |  0.00166 |    0.02523 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10469 | -0.00305 |    0.02501 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08307 | -0.00194 |    0.01656 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10970 | -0.00405 |    0.03092 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08087 | -0.00116 |    0.01854 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11563 | -0.00383 |    0.04124 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09223 | -0.00224 |    0.02893 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.13128 | -0.00653 |    0.04235 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07832 | -0.00129 |    0.02188 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06414 | -0.00085 |    0.01558 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08497 | -0.00191 |    0.02532 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06484 | -0.00344 |    0.01653 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07942 | -0.00129 |    0.02681 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07530 | -0.00303 |    0.02600 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07303 |  0.00012 |    0.01895 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06974 | -0.00202 |    0.02166 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05715 | -0.00189 |    0.01486 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05408 | -0.00024 |    0.01432 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03444 |  0.00129 |    0.00618 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51815 | -0.07600 |    0.25873 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:13:03,919 - Total sparsity: 88.62

2018-10-27 23:13:03,919 - --- validate (epoch=154)-----------
2018-10-27 23:13:03,919 - 10000 samples (128 per mini-batch)
2018-10-27 23:13:04,639 - Epoch: [154][   50/   78]    Loss 0.434198    Top1 86.171875    Top5 99.312500    
2018-10-27 23:13:05,031 - ==> Top1: 86.420    Top5: 99.400    Loss: 0.426

2018-10-27 23:13:05,032 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:13:05,032 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:13:05,042 - 

2018-10-27 23:13:05,043 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:13:06,239 - Epoch: [155][   50/  391]    Overall Loss 0.277966    Objective Loss 0.277966    Top1 89.968750    Top5 99.703125    LR 0.030000    Time 0.023896    
2018-10-27 23:13:07,382 - Epoch: [155][  100/  391]    Overall Loss 0.278333    Objective Loss 0.278333    Top1 90.054688    Top5 99.750000    LR 0.030000    Time 0.023359    
2018-10-27 23:13:08,521 - Epoch: [155][  150/  391]    Overall Loss 0.278882    Objective Loss 0.278882    Top1 90.088542    Top5 99.734375    LR 0.030000    Time 0.023162    
2018-10-27 23:13:09,662 - Epoch: [155][  200/  391]    Overall Loss 0.283713    Objective Loss 0.283713    Top1 89.945312    Top5 99.746094    LR 0.030000    Time 0.023071    
2018-10-27 23:13:10,806 - Epoch: [155][  250/  391]    Overall Loss 0.283168    Objective Loss 0.283168    Top1 89.959375    Top5 99.753125    LR 0.030000    Time 0.023024    
2018-10-27 23:13:11,949 - Epoch: [155][  300/  391]    Overall Loss 0.284947    Objective Loss 0.284947    Top1 89.885417    Top5 99.750000    LR 0.030000    Time 0.022994    
2018-10-27 23:13:13,091 - Epoch: [155][  350/  391]    Overall Loss 0.284311    Objective Loss 0.284311    Top1 89.991071    Top5 99.738839    LR 0.030000    Time 0.022958    
2018-10-27 23:13:14,109 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.34958 | -0.00411 |    0.13139 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10743 | -0.00359 |    0.02385 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10146 |  0.00164 |    0.02515 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10446 | -0.00300 |    0.02473 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08279 | -0.00194 |    0.01638 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10944 | -0.00396 |    0.03090 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08068 | -0.00120 |    0.01852 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11529 | -0.00383 |    0.04100 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09197 | -0.00228 |    0.02881 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.13070 | -0.00642 |    0.04176 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07815 | -0.00128 |    0.02183 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06395 | -0.00090 |    0.01554 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08474 | -0.00188 |    0.02522 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06467 | -0.00346 |    0.01653 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07918 | -0.00129 |    0.02672 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07511 | -0.00304 |    0.02594 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07270 |  0.00042 |    0.01886 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06959 | -0.00205 |    0.02159 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05705 | -0.00190 |    0.01482 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05398 | -0.00023 |    0.01431 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03433 |  0.00131 |    0.00616 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51798 | -0.07595 |    0.25876 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:13:14,109 - Total sparsity: 88.62

2018-10-27 23:13:14,109 - --- validate (epoch=155)-----------
2018-10-27 23:13:14,109 - 10000 samples (128 per mini-batch)
2018-10-27 23:13:14,831 - Epoch: [155][   50/   78]    Loss 0.449803    Top1 85.765625    Top5 99.312500    
2018-10-27 23:13:15,239 - ==> Top1: 86.050    Top5: 99.480    Loss: 0.440

2018-10-27 23:13:15,240 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:13:15,240 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:13:15,256 - 

2018-10-27 23:13:15,257 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:13:16,427 - Epoch: [156][   50/  391]    Overall Loss 0.272400    Objective Loss 0.272400    Top1 90.546875    Top5 99.812500    LR 0.030000    Time 0.023368    
2018-10-27 23:13:17,567 - Epoch: [156][  100/  391]    Overall Loss 0.267902    Objective Loss 0.267902    Top1 90.632812    Top5 99.781250    LR 0.030000    Time 0.023077    
2018-10-27 23:13:18,706 - Epoch: [156][  150/  391]    Overall Loss 0.270806    Objective Loss 0.270806    Top1 90.416667    Top5 99.781250    LR 0.030000    Time 0.022971    
2018-10-27 23:13:19,846 - Epoch: [156][  200/  391]    Overall Loss 0.274324    Objective Loss 0.274324    Top1 90.246094    Top5 99.777344    LR 0.030000    Time 0.022921    
2018-10-27 23:13:20,986 - Epoch: [156][  250/  391]    Overall Loss 0.281885    Objective Loss 0.281885    Top1 89.968750    Top5 99.765625    LR 0.030000    Time 0.022890    
2018-10-27 23:13:22,128 - Epoch: [156][  300/  391]    Overall Loss 0.281830    Objective Loss 0.281830    Top1 89.976562    Top5 99.773438    LR 0.030000    Time 0.022877    
2018-10-27 23:13:23,269 - Epoch: [156][  350/  391]    Overall Loss 0.282983    Objective Loss 0.282983    Top1 89.924107    Top5 99.781250    LR 0.030000    Time 0.022865    
2018-10-27 23:13:24,279 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.34825 | -0.00365 |    0.13080 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10714 | -0.00387 |    0.02361 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10115 |  0.00167 |    0.02501 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10419 | -0.00289 |    0.02473 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08255 | -0.00174 |    0.01640 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10919 | -0.00411 |    0.03082 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08052 | -0.00107 |    0.01842 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11494 | -0.00416 |    0.04110 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09173 | -0.00224 |    0.02872 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.13021 | -0.00592 |    0.04169 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07800 | -0.00126 |    0.02176 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06380 | -0.00098 |    0.01552 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08454 | -0.00185 |    0.02515 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06451 | -0.00339 |    0.01652 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07896 | -0.00130 |    0.02659 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07493 | -0.00302 |    0.02586 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07240 |  0.00015 |    0.01876 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06945 | -0.00205 |    0.02157 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05694 | -0.00187 |    0.01477 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05389 | -0.00019 |    0.01428 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03424 |  0.00129 |    0.00615 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51699 | -0.07613 |    0.25827 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:13:24,279 - Total sparsity: 88.62

2018-10-27 23:13:24,280 - --- validate (epoch=156)-----------
2018-10-27 23:13:24,280 - 10000 samples (128 per mini-batch)
2018-10-27 23:13:25,007 - Epoch: [156][   50/   78]    Loss 0.447477    Top1 85.328125    Top5 99.500000    
2018-10-27 23:13:25,401 - ==> Top1: 85.580    Top5: 99.470    Loss: 0.445

2018-10-27 23:13:25,402 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:13:25,402 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:13:25,412 - 

2018-10-27 23:13:25,412 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:13:26,609 - Epoch: [157][   50/  391]    Overall Loss 0.261246    Objective Loss 0.261246    Top1 90.937500    Top5 99.765625    LR 0.030000    Time 0.023895    
2018-10-27 23:13:27,751 - Epoch: [157][  100/  391]    Overall Loss 0.268460    Objective Loss 0.268460    Top1 90.539062    Top5 99.765625    LR 0.030000    Time 0.023354    
2018-10-27 23:13:28,893 - Epoch: [157][  150/  391]    Overall Loss 0.275756    Objective Loss 0.275756    Top1 90.442708    Top5 99.734375    LR 0.030000    Time 0.023173    
2018-10-27 23:13:30,034 - Epoch: [157][  200/  391]    Overall Loss 0.282033    Objective Loss 0.282033    Top1 90.167969    Top5 99.734375    LR 0.030000    Time 0.023062    
2018-10-27 23:13:31,177 - Epoch: [157][  250/  391]    Overall Loss 0.281695    Objective Loss 0.281695    Top1 90.081250    Top5 99.759375    LR 0.030000    Time 0.023015    
2018-10-27 23:13:32,319 - Epoch: [157][  300/  391]    Overall Loss 0.283884    Objective Loss 0.283884    Top1 89.976562    Top5 99.755208    LR 0.030000    Time 0.022980    
2018-10-27 23:13:33,460 - Epoch: [157][  350/  391]    Overall Loss 0.283057    Objective Loss 0.283057    Top1 90.006696    Top5 99.767857    LR 0.030000    Time 0.022955    
2018-10-27 23:13:34,476 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.34737 | -0.00473 |    0.13066 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10688 | -0.00380 |    0.02367 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10084 |  0.00140 |    0.02502 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10396 | -0.00277 |    0.02451 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08234 | -0.00194 |    0.01623 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10889 | -0.00389 |    0.03060 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08031 | -0.00113 |    0.01836 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11461 | -0.00368 |    0.04078 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09150 | -0.00216 |    0.02860 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12962 | -0.00609 |    0.04181 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07785 | -0.00125 |    0.02175 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06364 | -0.00088 |    0.01547 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08436 | -0.00170 |    0.02508 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06436 | -0.00333 |    0.01652 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07876 | -0.00135 |    0.02659 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07475 | -0.00303 |    0.02579 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07207 |  0.00021 |    0.01870 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06932 | -0.00199 |    0.02153 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05683 | -0.00194 |    0.01473 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05378 | -0.00019 |    0.01423 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03415 |  0.00132 |    0.00612 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51755 | -0.07601 |    0.25860 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:13:34,476 - Total sparsity: 88.62

2018-10-27 23:13:34,476 - --- validate (epoch=157)-----------
2018-10-27 23:13:34,476 - 10000 samples (128 per mini-batch)
2018-10-27 23:13:35,199 - Epoch: [157][   50/   78]    Loss 0.437119    Top1 86.156250    Top5 99.406250    
2018-10-27 23:13:35,589 - ==> Top1: 86.350    Top5: 99.510    Loss: 0.428

2018-10-27 23:13:35,590 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:13:35,590 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:13:35,601 - 

2018-10-27 23:13:35,602 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:13:36,799 - Epoch: [158][   50/  391]    Overall Loss 0.274483    Objective Loss 0.274483    Top1 90.546875    Top5 99.765625    LR 0.030000    Time 0.023912    
2018-10-27 23:13:37,939 - Epoch: [158][  100/  391]    Overall Loss 0.274147    Objective Loss 0.274147    Top1 90.546875    Top5 99.765625    LR 0.030000    Time 0.023344    
2018-10-27 23:13:39,082 - Epoch: [158][  150/  391]    Overall Loss 0.278132    Objective Loss 0.278132    Top1 90.338542    Top5 99.765625    LR 0.030000    Time 0.023172    
2018-10-27 23:13:40,224 - Epoch: [158][  200/  391]    Overall Loss 0.279802    Objective Loss 0.279802    Top1 90.269531    Top5 99.773438    LR 0.030000    Time 0.023081    
2018-10-27 23:13:41,366 - Epoch: [158][  250/  391]    Overall Loss 0.282039    Objective Loss 0.282039    Top1 90.165625    Top5 99.768750    LR 0.030000    Time 0.023031    
2018-10-27 23:13:42,508 - Epoch: [158][  300/  391]    Overall Loss 0.281605    Objective Loss 0.281605    Top1 90.158854    Top5 99.765625    LR 0.030000    Time 0.022994    
2018-10-27 23:13:43,651 - Epoch: [158][  350/  391]    Overall Loss 0.281899    Objective Loss 0.281899    Top1 90.171875    Top5 99.767857    LR 0.030000    Time 0.022971    
2018-10-27 23:13:44,669 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.34647 | -0.00512 |    0.13066 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10663 | -0.00365 |    0.02350 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10056 |  0.00150 |    0.02486 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10375 | -0.00287 |    0.02450 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08213 | -0.00188 |    0.01626 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10865 | -0.00402 |    0.03061 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.08008 | -0.00108 |    0.01822 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11430 | -0.00375 |    0.04062 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09124 | -0.00241 |    0.02854 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12908 | -0.00624 |    0.04158 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07769 | -0.00118 |    0.02166 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06347 | -0.00087 |    0.01536 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08415 | -0.00172 |    0.02498 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06421 | -0.00332 |    0.01645 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07855 | -0.00132 |    0.02649 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07458 | -0.00301 |    0.02574 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07178 |  0.00007 |    0.01861 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06919 | -0.00190 |    0.02149 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05673 | -0.00191 |    0.01470 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05370 | -0.00015 |    0.01421 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03406 |  0.00131 |    0.00611 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51732 | -0.07615 |    0.25834 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:13:44,669 - Total sparsity: 88.62

2018-10-27 23:13:44,669 - --- validate (epoch=158)-----------
2018-10-27 23:13:44,669 - 10000 samples (128 per mini-batch)
2018-10-27 23:13:45,387 - Epoch: [158][   50/   78]    Loss 0.456511    Top1 85.390625    Top5 99.421875    
2018-10-27 23:13:45,773 - ==> Top1: 85.570    Top5: 99.480    Loss: 0.447

2018-10-27 23:13:45,774 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:13:45,774 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:13:45,783 - 

2018-10-27 23:13:45,784 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:13:46,983 - Epoch: [159][   50/  391]    Overall Loss 0.259892    Objective Loss 0.259892    Top1 91.046875    Top5 99.875000    LR 0.030000    Time 0.023941    
2018-10-27 23:13:48,125 - Epoch: [159][  100/  391]    Overall Loss 0.274657    Objective Loss 0.274657    Top1 90.335938    Top5 99.812500    LR 0.030000    Time 0.023379    
2018-10-27 23:13:49,266 - Epoch: [159][  150/  391]    Overall Loss 0.275097    Objective Loss 0.275097    Top1 90.270833    Top5 99.817708    LR 0.030000    Time 0.023187    
2018-10-27 23:13:50,407 - Epoch: [159][  200/  391]    Overall Loss 0.283086    Objective Loss 0.283086    Top1 90.136719    Top5 99.773438    LR 0.030000    Time 0.023089    
2018-10-27 23:13:51,548 - Epoch: [159][  250/  391]    Overall Loss 0.281811    Objective Loss 0.281811    Top1 90.187500    Top5 99.790625    LR 0.030000    Time 0.023027    
2018-10-27 23:13:52,690 - Epoch: [159][  300/  391]    Overall Loss 0.282142    Objective Loss 0.282142    Top1 90.174479    Top5 99.796875    LR 0.030000    Time 0.022991    
2018-10-27 23:13:53,834 - Epoch: [159][  350/  391]    Overall Loss 0.283261    Objective Loss 0.283261    Top1 90.118304    Top5 99.790179    LR 0.030000    Time 0.022973    
2018-10-27 23:13:54,852 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.34522 | -0.00586 |    0.13030 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10634 | -0.00375 |    0.02341 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.10024 |  0.00138 |    0.02487 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10347 | -0.00265 |    0.02443 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08190 | -0.00180 |    0.01623 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10841 | -0.00402 |    0.03054 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07990 | -0.00116 |    0.01817 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11398 | -0.00372 |    0.04045 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09099 | -0.00250 |    0.02842 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12852 | -0.00677 |    0.04154 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07755 | -0.00117 |    0.02159 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06332 | -0.00087 |    0.01532 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08395 | -0.00175 |    0.02494 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06407 | -0.00333 |    0.01639 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07835 | -0.00122 |    0.02641 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07442 | -0.00302 |    0.02569 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07151 |  0.00019 |    0.01852 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06905 | -0.00198 |    0.02144 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05664 | -0.00195 |    0.01470 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05362 | -0.00021 |    0.01420 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03397 |  0.00131 |    0.00609 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51763 | -0.07603 |    0.25844 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:13:54,853 - Total sparsity: 88.62

2018-10-27 23:13:54,853 - --- validate (epoch=159)-----------
2018-10-27 23:13:54,853 - 10000 samples (128 per mini-batch)
2018-10-27 23:13:55,579 - Epoch: [159][   50/   78]    Loss 0.479429    Top1 84.609375    Top5 99.406250    
2018-10-27 23:13:55,978 - ==> Top1: 84.500    Top5: 99.510    Loss: 0.481

2018-10-27 23:13:55,978 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:13:55,979 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:13:55,989 - 

2018-10-27 23:13:55,990 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:13:57,187 - Epoch: [160][   50/  391]    Overall Loss 0.268040    Objective Loss 0.268040    Top1 90.609375    Top5 99.796875    LR 0.030000    Time 0.023914    
2018-10-27 23:13:58,329 - Epoch: [160][  100/  391]    Overall Loss 0.277588    Objective Loss 0.277588    Top1 90.218750    Top5 99.757812    LR 0.030000    Time 0.023360    
2018-10-27 23:13:59,471 - Epoch: [160][  150/  391]    Overall Loss 0.277214    Objective Loss 0.277214    Top1 90.208333    Top5 99.781250    LR 0.030000    Time 0.023179    
2018-10-27 23:14:00,617 - Epoch: [160][  200/  391]    Overall Loss 0.278775    Objective Loss 0.278775    Top1 90.125000    Top5 99.765625    LR 0.030000    Time 0.023110    
2018-10-27 23:14:01,761 - Epoch: [160][  250/  391]    Overall Loss 0.284079    Objective Loss 0.284079    Top1 89.915625    Top5 99.768750    LR 0.030000    Time 0.023058    
2018-10-27 23:14:02,905 - Epoch: [160][  300/  391]    Overall Loss 0.282360    Objective Loss 0.282360    Top1 89.953125    Top5 99.773438    LR 0.030000    Time 0.023025    
2018-10-27 23:14:04,047 - Epoch: [160][  350/  391]    Overall Loss 0.283126    Objective Loss 0.283126    Top1 89.901786    Top5 99.758929    LR 0.030000    Time 0.022985    
2018-10-27 23:14:05,062 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.34472 | -0.00362 |    0.13016 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10620 | -0.00378 |    0.02346 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09999 |  0.00146 |    0.02474 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10325 | -0.00262 |    0.02450 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08165 | -0.00192 |    0.01631 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10816 | -0.00401 |    0.03045 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07971 | -0.00121 |    0.01811 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11370 | -0.00373 |    0.04024 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09078 | -0.00238 |    0.02835 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12807 | -0.00665 |    0.04127 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07747 | -0.00112 |    0.02156 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06321 | -0.00086 |    0.01532 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08383 | -0.00174 |    0.02487 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06398 | -0.00346 |    0.01633 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07817 | -0.00118 |    0.02629 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07428 | -0.00294 |    0.02562 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07125 |  0.00024 |    0.01853 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06893 | -0.00199 |    0.02141 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05656 | -0.00191 |    0.01466 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05357 | -0.00018 |    0.01417 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03391 |  0.00129 |    0.00608 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51721 | -0.07559 |    0.25821 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:14:05,062 - Total sparsity: 88.62

2018-10-27 23:14:05,062 - --- validate (epoch=160)-----------
2018-10-27 23:14:05,062 - 10000 samples (128 per mini-batch)
2018-10-27 23:14:05,778 - Epoch: [160][   50/   78]    Loss 0.438209    Top1 85.828125    Top5 99.468750    
2018-10-27 23:14:06,165 - ==> Top1: 85.930    Top5: 99.470    Loss: 0.429

2018-10-27 23:14:06,166 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:14:06,166 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:14:06,176 - 

2018-10-27 23:14:06,177 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:14:07,377 - Epoch: [161][   50/  391]    Overall Loss 0.266530    Objective Loss 0.266530    Top1 90.937500    Top5 99.656250    LR 0.030000    Time 0.023970    
2018-10-27 23:14:08,520 - Epoch: [161][  100/  391]    Overall Loss 0.273803    Objective Loss 0.273803    Top1 90.445312    Top5 99.679688    LR 0.030000    Time 0.023399    
2018-10-27 23:14:09,662 - Epoch: [161][  150/  391]    Overall Loss 0.276251    Objective Loss 0.276251    Top1 90.255208    Top5 99.723958    LR 0.030000    Time 0.023207    
2018-10-27 23:14:10,803 - Epoch: [161][  200/  391]    Overall Loss 0.281333    Objective Loss 0.281333    Top1 90.046875    Top5 99.734375    LR 0.030000    Time 0.023102    
2018-10-27 23:14:11,943 - Epoch: [161][  250/  391]    Overall Loss 0.280256    Objective Loss 0.280256    Top1 90.090625    Top5 99.750000    LR 0.030000    Time 0.023038    
2018-10-27 23:14:13,085 - Epoch: [161][  300/  391]    Overall Loss 0.279121    Objective Loss 0.279121    Top1 90.101562    Top5 99.752604    LR 0.030000    Time 0.022999    
2018-10-27 23:14:14,225 - Epoch: [161][  350/  391]    Overall Loss 0.280347    Objective Loss 0.280347    Top1 90.138393    Top5 99.747768    LR 0.030000    Time 0.022967    
2018-10-27 23:14:15,241 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.34412 | -0.00420 |    0.12936 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10597 | -0.00372 |    0.02340 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09975 |  0.00139 |    0.02477 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10306 | -0.00239 |    0.02435 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08146 | -0.00212 |    0.01623 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10786 | -0.00420 |    0.03030 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07949 | -0.00096 |    0.01815 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11342 | -0.00355 |    0.04019 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09057 | -0.00224 |    0.02823 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12757 | -0.00652 |    0.04117 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07735 | -0.00125 |    0.02157 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06307 | -0.00094 |    0.01526 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08366 | -0.00184 |    0.02483 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06386 | -0.00348 |    0.01633 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07797 | -0.00114 |    0.02623 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07413 | -0.00297 |    0.02556 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07103 |  0.00027 |    0.01852 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06884 | -0.00199 |    0.02138 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05648 | -0.00193 |    0.01464 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05350 | -0.00016 |    0.01417 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03383 |  0.00130 |    0.00606 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51744 | -0.07571 |    0.25838 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:14:15,241 - Total sparsity: 88.62

2018-10-27 23:14:15,242 - --- validate (epoch=161)-----------
2018-10-27 23:14:15,242 - 10000 samples (128 per mini-batch)
2018-10-27 23:14:15,959 - Epoch: [161][   50/   78]    Loss 0.469542    Top1 84.953125    Top5 99.343750    
2018-10-27 23:14:16,351 - ==> Top1: 84.990    Top5: 99.440    Loss: 0.464

2018-10-27 23:14:16,352 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:14:16,352 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:14:16,363 - 

2018-10-27 23:14:16,363 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:14:17,560 - Epoch: [162][   50/  391]    Overall Loss 0.271347    Objective Loss 0.271347    Top1 90.578125    Top5 99.765625    LR 0.030000    Time 0.023887    
2018-10-27 23:14:18,698 - Epoch: [162][  100/  391]    Overall Loss 0.278341    Objective Loss 0.278341    Top1 90.328125    Top5 99.765625    LR 0.030000    Time 0.023318    
2018-10-27 23:14:19,839 - Epoch: [162][  150/  391]    Overall Loss 0.275456    Objective Loss 0.275456    Top1 90.223958    Top5 99.786458    LR 0.030000    Time 0.023143    
2018-10-27 23:14:20,980 - Epoch: [162][  200/  391]    Overall Loss 0.276970    Objective Loss 0.276970    Top1 90.144531    Top5 99.796875    LR 0.030000    Time 0.023036    
2018-10-27 23:14:22,121 - Epoch: [162][  250/  391]    Overall Loss 0.277851    Objective Loss 0.277851    Top1 90.178125    Top5 99.784375    LR 0.030000    Time 0.022987    
2018-10-27 23:14:23,262 - Epoch: [162][  300/  391]    Overall Loss 0.280299    Objective Loss 0.280299    Top1 90.091146    Top5 99.794271    LR 0.030000    Time 0.022955    
2018-10-27 23:14:24,402 - Epoch: [162][  350/  391]    Overall Loss 0.281875    Objective Loss 0.281875    Top1 90.051339    Top5 99.799107    LR 0.030000    Time 0.022929    
2018-10-27 23:14:25,420 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.34349 | -0.00394 |    0.12966 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10572 | -0.00346 |    0.02338 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09949 |  0.00129 |    0.02474 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10285 | -0.00236 |    0.02442 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08130 | -0.00230 |    0.01625 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10759 | -0.00403 |    0.03015 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07928 | -0.00102 |    0.01818 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11313 | -0.00359 |    0.04016 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09036 | -0.00222 |    0.02813 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12711 | -0.00706 |    0.04101 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07725 | -0.00116 |    0.02155 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06297 | -0.00098 |    0.01526 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08352 | -0.00175 |    0.02480 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06375 | -0.00338 |    0.01629 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07780 | -0.00123 |    0.02620 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07401 | -0.00297 |    0.02549 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07078 |  0.00023 |    0.01836 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06875 | -0.00183 |    0.02134 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05642 | -0.00189 |    0.01462 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05345 | -0.00017 |    0.01416 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03375 |  0.00129 |    0.00604 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51773 | -0.07571 |    0.25828 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:14:25,421 - Total sparsity: 88.62

2018-10-27 23:14:25,421 - --- validate (epoch=162)-----------
2018-10-27 23:14:25,421 - 10000 samples (128 per mini-batch)
2018-10-27 23:14:26,147 - Epoch: [162][   50/   78]    Loss 0.460607    Top1 84.984375    Top5 99.328125    
2018-10-27 23:14:26,541 - ==> Top1: 85.540    Top5: 99.410    Loss: 0.442

2018-10-27 23:14:26,542 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:14:26,542 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:14:26,552 - 

2018-10-27 23:14:26,553 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:14:27,752 - Epoch: [163][   50/  391]    Overall Loss 0.279190    Objective Loss 0.279190    Top1 90.515625    Top5 99.718750    LR 0.030000    Time 0.023960    
2018-10-27 23:14:28,896 - Epoch: [163][  100/  391]    Overall Loss 0.276441    Objective Loss 0.276441    Top1 90.585938    Top5 99.742188    LR 0.030000    Time 0.023398    
2018-10-27 23:14:30,040 - Epoch: [163][  150/  391]    Overall Loss 0.277427    Objective Loss 0.277427    Top1 90.526042    Top5 99.744792    LR 0.030000    Time 0.023217    
2018-10-27 23:14:31,184 - Epoch: [163][  200/  391]    Overall Loss 0.282384    Objective Loss 0.282384    Top1 90.316406    Top5 99.734375    LR 0.030000    Time 0.023128    
2018-10-27 23:14:32,328 - Epoch: [163][  250/  391]    Overall Loss 0.284518    Objective Loss 0.284518    Top1 90.128125    Top5 99.746875    LR 0.030000    Time 0.023073    
2018-10-27 23:14:33,473 - Epoch: [163][  300/  391]    Overall Loss 0.283617    Objective Loss 0.283617    Top1 90.156250    Top5 99.757812    LR 0.030000    Time 0.023040    
2018-10-27 23:14:34,618 - Epoch: [163][  350/  391]    Overall Loss 0.285220    Objective Loss 0.285220    Top1 90.078125    Top5 99.747768    LR 0.030000    Time 0.023015    
2018-10-27 23:14:35,634 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.34281 | -0.00520 |    0.12953 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10551 | -0.00348 |    0.02325 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09921 |  0.00129 |    0.02457 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10263 | -0.00246 |    0.02448 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08112 | -0.00224 |    0.01614 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10735 | -0.00421 |    0.03012 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07912 | -0.00108 |    0.01805 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11289 | -0.00366 |    0.04005 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.09018 | -0.00223 |    0.02808 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12671 | -0.00676 |    0.04108 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07717 | -0.00132 |    0.02150 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06285 | -0.00098 |    0.01519 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08341 | -0.00168 |    0.02480 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06365 | -0.00346 |    0.01624 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07764 | -0.00108 |    0.02613 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07391 | -0.00294 |    0.02546 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07058 |  0.00016 |    0.01832 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06868 | -0.00182 |    0.02132 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05637 | -0.00191 |    0.01460 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05342 | -0.00023 |    0.01413 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03370 |  0.00129 |    0.00603 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51725 | -0.07539 |    0.25802 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:14:35,634 - Total sparsity: 88.62

2018-10-27 23:14:35,634 - --- validate (epoch=163)-----------
2018-10-27 23:14:35,634 - 10000 samples (128 per mini-batch)
2018-10-27 23:14:36,357 - Epoch: [163][   50/   78]    Loss 0.438195    Top1 85.500000    Top5 99.390625    
2018-10-27 23:14:36,746 - ==> Top1: 85.650    Top5: 99.430    Loss: 0.438

2018-10-27 23:14:36,747 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:14:36,747 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:14:36,760 - 

2018-10-27 23:14:36,761 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:14:37,960 - Epoch: [164][   50/  391]    Overall Loss 0.263330    Objective Loss 0.263330    Top1 90.515625    Top5 99.890625    LR 0.030000    Time 0.023936    
2018-10-27 23:14:39,100 - Epoch: [164][  100/  391]    Overall Loss 0.270320    Objective Loss 0.270320    Top1 90.460938    Top5 99.828125    LR 0.030000    Time 0.023358    
2018-10-27 23:14:40,241 - Epoch: [164][  150/  391]    Overall Loss 0.276900    Objective Loss 0.276900    Top1 90.229167    Top5 99.822917    LR 0.030000    Time 0.023167    
2018-10-27 23:14:41,381 - Epoch: [164][  200/  391]    Overall Loss 0.280035    Objective Loss 0.280035    Top1 90.058594    Top5 99.800781    LR 0.030000    Time 0.023072    
2018-10-27 23:14:42,522 - Epoch: [164][  250/  391]    Overall Loss 0.281925    Objective Loss 0.281925    Top1 90.003125    Top5 99.781250    LR 0.030000    Time 0.023015    
2018-10-27 23:14:43,661 - Epoch: [164][  300/  391]    Overall Loss 0.283461    Objective Loss 0.283461    Top1 89.906250    Top5 99.786458    LR 0.030000    Time 0.022971    
2018-10-27 23:14:44,803 - Epoch: [164][  350/  391]    Overall Loss 0.283180    Objective Loss 0.283180    Top1 89.890625    Top5 99.781250    LR 0.030000    Time 0.022949    
2018-10-27 23:14:45,820 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.34209 | -0.00322 |    0.12881 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10523 | -0.00388 |    0.02330 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09897 |  0.00115 |    0.02449 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10244 | -0.00248 |    0.02455 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08091 | -0.00214 |    0.01624 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10713 | -0.00394 |    0.02997 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07900 | -0.00094 |    0.01809 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11268 | -0.00335 |    0.04007 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08999 | -0.00215 |    0.02804 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12623 | -0.00652 |    0.04059 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07707 | -0.00132 |    0.02148 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06275 | -0.00101 |    0.01514 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08329 | -0.00161 |    0.02479 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06355 | -0.00348 |    0.01621 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07750 | -0.00108 |    0.02606 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07381 | -0.00292 |    0.02542 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07036 |  0.00019 |    0.01820 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06860 | -0.00188 |    0.02126 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05631 | -0.00197 |    0.01459 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05338 | -0.00021 |    0.01411 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03365 |  0.00130 |    0.00601 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51705 | -0.07540 |    0.25788 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:14:45,820 - Total sparsity: 88.62

2018-10-27 23:14:45,820 - --- validate (epoch=164)-----------
2018-10-27 23:14:45,821 - 10000 samples (128 per mini-batch)
2018-10-27 23:14:46,543 - Epoch: [164][   50/   78]    Loss 0.451665    Top1 86.109375    Top5 99.390625    
2018-10-27 23:14:46,933 - ==> Top1: 85.940    Top5: 99.460    Loss: 0.451

2018-10-27 23:14:46,934 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:14:46,934 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:14:46,954 - 

2018-10-27 23:14:46,955 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:14:48,127 - Epoch: [165][   50/  391]    Overall Loss 0.283945    Objective Loss 0.283945    Top1 89.906250    Top5 99.781250    LR 0.030000    Time 0.023406    
2018-10-27 23:14:49,268 - Epoch: [165][  100/  391]    Overall Loss 0.274824    Objective Loss 0.274824    Top1 90.382812    Top5 99.804688    LR 0.030000    Time 0.023105    
2018-10-27 23:14:50,410 - Epoch: [165][  150/  391]    Overall Loss 0.276658    Objective Loss 0.276658    Top1 90.369792    Top5 99.791667    LR 0.030000    Time 0.023004    
2018-10-27 23:14:51,552 - Epoch: [165][  200/  391]    Overall Loss 0.282178    Objective Loss 0.282178    Top1 90.171875    Top5 99.773438    LR 0.030000    Time 0.022957    
2018-10-27 23:14:52,693 - Epoch: [165][  250/  391]    Overall Loss 0.283424    Objective Loss 0.283424    Top1 90.118750    Top5 99.768750    LR 0.030000    Time 0.022926    
2018-10-27 23:14:53,837 - Epoch: [165][  300/  391]    Overall Loss 0.285703    Objective Loss 0.285703    Top1 89.994792    Top5 99.770833    LR 0.030000    Time 0.022913    
2018-10-27 23:14:54,978 - Epoch: [165][  350/  391]    Overall Loss 0.284720    Objective Loss 0.284720    Top1 90.035714    Top5 99.765625    LR 0.030000    Time 0.022886    
2018-10-27 23:14:55,993 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.34138 | -0.00499 |    0.12903 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10502 | -0.00385 |    0.02337 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09875 |  0.00105 |    0.02433 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10222 | -0.00259 |    0.02442 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08073 | -0.00201 |    0.01610 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10689 | -0.00421 |    0.03002 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07881 | -0.00105 |    0.01802 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11245 | -0.00362 |    0.03987 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08981 | -0.00225 |    0.02798 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12586 | -0.00675 |    0.04052 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07697 | -0.00118 |    0.02146 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06264 | -0.00106 |    0.01508 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08316 | -0.00177 |    0.02471 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06346 | -0.00337 |    0.01619 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07735 | -0.00095 |    0.02602 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07370 | -0.00292 |    0.02538 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.07009 |  0.00029 |    0.01806 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06852 | -0.00190 |    0.02129 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05626 | -0.00193 |    0.01457 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05334 | -0.00022 |    0.01410 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03359 |  0.00132 |    0.00600 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51755 | -0.07531 |    0.25819 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:14:55,993 - Total sparsity: 88.62

2018-10-27 23:14:55,993 - --- validate (epoch=165)-----------
2018-10-27 23:14:55,993 - 10000 samples (128 per mini-batch)
2018-10-27 23:14:56,711 - Epoch: [165][   50/   78]    Loss 0.456385    Top1 85.437500    Top5 99.359375    
2018-10-27 23:14:57,101 - ==> Top1: 85.770    Top5: 99.420    Loss: 0.448

2018-10-27 23:14:57,102 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:14:57,102 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:14:57,118 - 

2018-10-27 23:14:57,119 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:14:58,290 - Epoch: [166][   50/  391]    Overall Loss 0.273328    Objective Loss 0.273328    Top1 90.187500    Top5 99.812500    LR 0.030000    Time 0.023391    
2018-10-27 23:14:59,432 - Epoch: [166][  100/  391]    Overall Loss 0.278584    Objective Loss 0.278584    Top1 90.109375    Top5 99.828125    LR 0.030000    Time 0.023100    
2018-10-27 23:15:00,578 - Epoch: [166][  150/  391]    Overall Loss 0.283981    Objective Loss 0.283981    Top1 89.859375    Top5 99.791667    LR 0.030000    Time 0.023033    
2018-10-27 23:15:01,736 - Epoch: [166][  200/  391]    Overall Loss 0.282317    Objective Loss 0.282317    Top1 89.996094    Top5 99.777344    LR 0.030000    Time 0.023056    
2018-10-27 23:15:02,908 - Epoch: [166][  250/  391]    Overall Loss 0.283103    Objective Loss 0.283103    Top1 90.012500    Top5 99.756250    LR 0.030000    Time 0.023128    
2018-10-27 23:15:04,079 - Epoch: [166][  300/  391]    Overall Loss 0.284128    Objective Loss 0.284128    Top1 89.916667    Top5 99.768229    LR 0.030000    Time 0.023171    
2018-10-27 23:15:05,253 - Epoch: [166][  350/  391]    Overall Loss 0.285589    Objective Loss 0.285589    Top1 89.875000    Top5 99.783482    LR 0.030000    Time 0.023211    
2018-10-27 23:15:06,294 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.34083 | -0.00439 |    0.12897 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10485 | -0.00368 |    0.02335 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09850 |  0.00102 |    0.02422 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10205 | -0.00305 |    0.02433 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08057 | -0.00199 |    0.01599 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10675 | -0.00419 |    0.02994 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07869 | -0.00091 |    0.01791 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11227 | -0.00360 |    0.03970 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08966 | -0.00228 |    0.02785 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12559 | -0.00625 |    0.04007 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07690 | -0.00105 |    0.02138 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06255 | -0.00104 |    0.01508 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08306 | -0.00169 |    0.02463 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06339 | -0.00339 |    0.01620 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07723 | -0.00108 |    0.02600 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07362 | -0.00293 |    0.02535 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06987 |  0.00038 |    0.01801 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06847 | -0.00186 |    0.02123 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05621 | -0.00191 |    0.01455 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05329 | -0.00019 |    0.01410 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03352 |  0.00131 |    0.00599 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51760 | -0.07549 |    0.25820 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:15:06,295 - Total sparsity: 88.62

2018-10-27 23:15:06,295 - --- validate (epoch=166)-----------
2018-10-27 23:15:06,295 - 10000 samples (128 per mini-batch)
2018-10-27 23:15:07,030 - Epoch: [166][   50/   78]    Loss 0.418496    Top1 86.187500    Top5 99.343750    
2018-10-27 23:15:07,425 - ==> Top1: 86.350    Top5: 99.400    Loss: 0.413

2018-10-27 23:15:07,426 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:15:07,426 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:15:07,436 - 

2018-10-27 23:15:07,437 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:15:08,662 - Epoch: [167][   50/  391]    Overall Loss 0.265804    Objective Loss 0.265804    Top1 90.875000    Top5 99.734375    LR 0.030000    Time 0.024481    
2018-10-27 23:15:09,827 - Epoch: [167][  100/  391]    Overall Loss 0.270744    Objective Loss 0.270744    Top1 90.585938    Top5 99.750000    LR 0.030000    Time 0.023872    
2018-10-27 23:15:10,999 - Epoch: [167][  150/  391]    Overall Loss 0.274024    Objective Loss 0.274024    Top1 90.411458    Top5 99.776042    LR 0.030000    Time 0.023716    
2018-10-27 23:15:12,170 - Epoch: [167][  200/  391]    Overall Loss 0.280421    Objective Loss 0.280421    Top1 90.082031    Top5 99.789062    LR 0.030000    Time 0.023639    
2018-10-27 23:15:13,346 - Epoch: [167][  250/  391]    Overall Loss 0.281044    Objective Loss 0.281044    Top1 90.131250    Top5 99.793750    LR 0.030000    Time 0.023607    
2018-10-27 23:15:14,521 - Epoch: [167][  300/  391]    Overall Loss 0.281413    Objective Loss 0.281413    Top1 90.111979    Top5 99.802083    LR 0.030000    Time 0.023585    
2018-10-27 23:15:15,691 - Epoch: [167][  350/  391]    Overall Loss 0.283623    Objective Loss 0.283623    Top1 89.986607    Top5 99.803571    LR 0.030000    Time 0.023556    
2018-10-27 23:15:16,734 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.34028 | -0.00366 |    0.12844 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10464 | -0.00368 |    0.02331 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09830 |  0.00127 |    0.02414 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10186 | -0.00290 |    0.02419 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08043 | -0.00192 |    0.01599 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10654 | -0.00428 |    0.02981 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07855 | -0.00086 |    0.01791 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11209 | -0.00357 |    0.03976 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08953 | -0.00220 |    0.02781 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12523 | -0.00624 |    0.04026 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07682 | -0.00110 |    0.02141 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06247 | -0.00103 |    0.01508 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08296 | -0.00169 |    0.02458 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06332 | -0.00338 |    0.01615 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07710 | -0.00110 |    0.02594 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07352 | -0.00296 |    0.02529 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06969 |  0.00017 |    0.01801 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06840 | -0.00190 |    0.02121 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05617 | -0.00191 |    0.01455 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05327 | -0.00025 |    0.01408 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03347 |  0.00133 |    0.00597 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51717 | -0.07543 |    0.25803 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:15:16,734 - Total sparsity: 88.62

2018-10-27 23:15:16,734 - --- validate (epoch=167)-----------
2018-10-27 23:15:16,734 - 10000 samples (128 per mini-batch)
2018-10-27 23:15:17,505 - Epoch: [167][   50/   78]    Loss 0.454476    Top1 85.593750    Top5 99.453125    
2018-10-27 23:15:17,912 - ==> Top1: 85.640    Top5: 99.530    Loss: 0.449

2018-10-27 23:15:17,913 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:15:17,913 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:15:17,924 - 

2018-10-27 23:15:17,924 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:15:19,154 - Epoch: [168][   50/  391]    Overall Loss 0.264280    Objective Loss 0.264280    Top1 91.000000    Top5 99.750000    LR 0.030000    Time 0.024563    
2018-10-27 23:15:20,329 - Epoch: [168][  100/  391]    Overall Loss 0.275300    Objective Loss 0.275300    Top1 90.531250    Top5 99.773438    LR 0.030000    Time 0.024012    
2018-10-27 23:15:21,499 - Epoch: [168][  150/  391]    Overall Loss 0.281938    Objective Loss 0.281938    Top1 90.265625    Top5 99.770833    LR 0.030000    Time 0.023801    
2018-10-27 23:15:22,669 - Epoch: [168][  200/  391]    Overall Loss 0.281085    Objective Loss 0.281085    Top1 90.226562    Top5 99.761719    LR 0.030000    Time 0.023695    
2018-10-27 23:15:23,843 - Epoch: [168][  250/  391]    Overall Loss 0.281934    Objective Loss 0.281934    Top1 90.190625    Top5 99.771875    LR 0.030000    Time 0.023645    
2018-10-27 23:15:25,013 - Epoch: [168][  300/  391]    Overall Loss 0.281119    Objective Loss 0.281119    Top1 90.229167    Top5 99.773438    LR 0.030000    Time 0.023598    
2018-10-27 23:15:26,183 - Epoch: [168][  350/  391]    Overall Loss 0.284245    Objective Loss 0.284245    Top1 90.091518    Top5 99.758929    LR 0.030000    Time 0.023567    
2018-10-27 23:15:27,219 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33951 | -0.00380 |    0.12827 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10441 | -0.00404 |    0.02316 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09808 |  0.00118 |    0.02422 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10168 | -0.00250 |    0.02418 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08028 | -0.00193 |    0.01597 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10635 | -0.00425 |    0.02969 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07844 | -0.00093 |    0.01789 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11189 | -0.00388 |    0.03960 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08937 | -0.00209 |    0.02772 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12490 | -0.00661 |    0.04013 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07673 | -0.00108 |    0.02145 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06238 | -0.00099 |    0.01509 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08283 | -0.00180 |    0.02449 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06323 | -0.00337 |    0.01615 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07697 | -0.00116 |    0.02594 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07343 | -0.00299 |    0.02526 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06948 |  0.00029 |    0.01795 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06833 | -0.00195 |    0.02119 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05612 | -0.00192 |    0.01452 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05325 | -0.00018 |    0.01406 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03342 |  0.00135 |    0.00596 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51637 | -0.07551 |    0.25759 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:15:27,220 - Total sparsity: 88.62

2018-10-27 23:15:27,220 - --- validate (epoch=168)-----------
2018-10-27 23:15:27,220 - 10000 samples (128 per mini-batch)
2018-10-27 23:15:27,949 - Epoch: [168][   50/   78]    Loss 0.436215    Top1 85.718750    Top5 99.406250    
2018-10-27 23:15:28,343 - ==> Top1: 86.090    Top5: 99.490    Loss: 0.430

2018-10-27 23:15:28,344 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:15:28,344 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:15:28,361 - 

2018-10-27 23:15:28,361 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:15:29,560 - Epoch: [169][   50/  391]    Overall Loss 0.285239    Objective Loss 0.285239    Top1 89.968750    Top5 99.750000    LR 0.030000    Time 0.023931    
2018-10-27 23:15:30,735 - Epoch: [169][  100/  391]    Overall Loss 0.283607    Objective Loss 0.283607    Top1 90.125000    Top5 99.718750    LR 0.030000    Time 0.023706    
2018-10-27 23:15:31,911 - Epoch: [169][  150/  391]    Overall Loss 0.285023    Objective Loss 0.285023    Top1 90.083333    Top5 99.739583    LR 0.030000    Time 0.023632    
2018-10-27 23:15:33,086 - Epoch: [169][  200/  391]    Overall Loss 0.285365    Objective Loss 0.285365    Top1 90.000000    Top5 99.769531    LR 0.030000    Time 0.023594    
2018-10-27 23:15:34,261 - Epoch: [169][  250/  391]    Overall Loss 0.285286    Objective Loss 0.285286    Top1 89.971875    Top5 99.765625    LR 0.030000    Time 0.023568    
2018-10-27 23:15:35,442 - Epoch: [169][  300/  391]    Overall Loss 0.284191    Objective Loss 0.284191    Top1 89.979167    Top5 99.760417    LR 0.030000    Time 0.023575    
2018-10-27 23:15:36,618 - Epoch: [169][  350/  391]    Overall Loss 0.284718    Objective Loss 0.284718    Top1 89.962054    Top5 99.752232    LR 0.030000    Time 0.023563    
2018-10-27 23:15:37,660 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33856 | -0.00359 |    0.12756 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10428 | -0.00384 |    0.02321 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09792 |  0.00100 |    0.02410 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10151 | -0.00254 |    0.02403 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.08015 | -0.00190 |    0.01595 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10613 | -0.00427 |    0.02977 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07827 | -0.00074 |    0.01781 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11173 | -0.00356 |    0.03948 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08921 | -0.00221 |    0.02770 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12452 | -0.00670 |    0.04006 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07662 | -0.00111 |    0.02141 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06228 | -0.00107 |    0.01506 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08272 | -0.00183 |    0.02447 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06313 | -0.00328 |    0.01607 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07685 | -0.00114 |    0.02590 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07335 | -0.00303 |    0.02521 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06926 |  0.00014 |    0.01784 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06826 | -0.00192 |    0.02116 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05608 | -0.00192 |    0.01451 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05322 | -0.00027 |    0.01404 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03337 |  0.00136 |    0.00595 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51704 | -0.07545 |    0.25792 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:15:37,660 - Total sparsity: 88.62

2018-10-27 23:15:37,660 - --- validate (epoch=169)-----------
2018-10-27 23:15:37,660 - 10000 samples (128 per mini-batch)
2018-10-27 23:15:38,387 - Epoch: [169][   50/   78]    Loss 0.438034    Top1 86.218750    Top5 99.250000    
2018-10-27 23:15:38,781 - ==> Top1: 86.240    Top5: 99.390    Loss: 0.431

2018-10-27 23:15:38,782 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:15:38,782 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:15:38,793 - 

2018-10-27 23:15:38,793 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:15:40,028 - Epoch: [170][   50/  391]    Overall Loss 0.254566    Objective Loss 0.254566    Top1 91.281250    Top5 99.718750    LR 0.030000    Time 0.024650    
2018-10-27 23:15:41,199 - Epoch: [170][  100/  391]    Overall Loss 0.268494    Objective Loss 0.268494    Top1 90.562500    Top5 99.734375    LR 0.030000    Time 0.024025    
2018-10-27 23:15:42,371 - Epoch: [170][  150/  391]    Overall Loss 0.274596    Objective Loss 0.274596    Top1 90.244792    Top5 99.744792    LR 0.030000    Time 0.023821    
2018-10-27 23:15:43,545 - Epoch: [170][  200/  391]    Overall Loss 0.280139    Objective Loss 0.280139    Top1 90.093750    Top5 99.738281    LR 0.030000    Time 0.023730    
2018-10-27 23:15:44,721 - Epoch: [170][  250/  391]    Overall Loss 0.280426    Objective Loss 0.280426    Top1 90.103125    Top5 99.753125    LR 0.030000    Time 0.023680    
2018-10-27 23:15:45,893 - Epoch: [170][  300/  391]    Overall Loss 0.283182    Objective Loss 0.283182    Top1 90.002604    Top5 99.747396    LR 0.030000    Time 0.023634    
2018-10-27 23:15:47,059 - Epoch: [170][  350/  391]    Overall Loss 0.285781    Objective Loss 0.285781    Top1 89.912946    Top5 99.745536    LR 0.030000    Time 0.023588    
2018-10-27 23:15:48,103 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33812 | -0.00280 |    0.12707 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10419 | -0.00332 |    0.02314 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09785 |  0.00091 |    0.02409 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10133 | -0.00268 |    0.02399 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07994 | -0.00193 |    0.01590 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10600 | -0.00439 |    0.02966 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07812 | -0.00109 |    0.01776 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11160 | -0.00356 |    0.03932 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08909 | -0.00211 |    0.02770 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12419 | -0.00702 |    0.04007 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07657 | -0.00112 |    0.02144 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06220 | -0.00099 |    0.01504 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08265 | -0.00183 |    0.02445 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06305 | -0.00328 |    0.01602 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07674 | -0.00115 |    0.02585 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07328 | -0.00295 |    0.02520 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06905 |  0.00011 |    0.01780 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06819 | -0.00190 |    0.02111 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05605 | -0.00192 |    0.01449 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05321 | -0.00021 |    0.01405 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03333 |  0.00135 |    0.00595 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51690 | -0.07495 |    0.25759 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:15:48,103 - Total sparsity: 88.62

2018-10-27 23:15:48,103 - --- validate (epoch=170)-----------
2018-10-27 23:15:48,103 - 10000 samples (128 per mini-batch)
2018-10-27 23:15:48,834 - Epoch: [170][   50/   78]    Loss 0.484222    Top1 85.000000    Top5 99.312500    
2018-10-27 23:15:49,229 - ==> Top1: 85.080    Top5: 99.390    Loss: 0.479

2018-10-27 23:15:49,230 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:15:49,230 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:15:49,241 - 

2018-10-27 23:15:49,241 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:15:50,468 - Epoch: [171][   50/  391]    Overall Loss 0.284402    Objective Loss 0.284402    Top1 90.062500    Top5 99.796875    LR 0.030000    Time 0.024496    
2018-10-27 23:15:51,640 - Epoch: [171][  100/  391]    Overall Loss 0.273960    Objective Loss 0.273960    Top1 90.617188    Top5 99.820312    LR 0.030000    Time 0.023958    
2018-10-27 23:15:52,812 - Epoch: [171][  150/  391]    Overall Loss 0.279420    Objective Loss 0.279420    Top1 90.281250    Top5 99.817708    LR 0.030000    Time 0.023775    
2018-10-27 23:15:53,986 - Epoch: [171][  200/  391]    Overall Loss 0.285510    Objective Loss 0.285510    Top1 90.074219    Top5 99.781250    LR 0.030000    Time 0.023698    
2018-10-27 23:15:55,162 - Epoch: [171][  250/  391]    Overall Loss 0.285840    Objective Loss 0.285840    Top1 90.093750    Top5 99.753125    LR 0.030000    Time 0.023657    
2018-10-27 23:15:56,337 - Epoch: [171][  300/  391]    Overall Loss 0.286435    Objective Loss 0.286435    Top1 90.018229    Top5 99.750000    LR 0.030000    Time 0.023624    
2018-10-27 23:15:57,514 - Epoch: [171][  350/  391]    Overall Loss 0.285272    Objective Loss 0.285272    Top1 90.011161    Top5 99.758929    LR 0.030000    Time 0.023608    
2018-10-27 23:15:58,557 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33793 | -0.00321 |    0.12732 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10407 | -0.00359 |    0.02313 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09770 |  0.00085 |    0.02403 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10121 | -0.00243 |    0.02414 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07980 | -0.00204 |    0.01580 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10581 | -0.00443 |    0.02958 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07798 | -0.00103 |    0.01774 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11141 | -0.00377 |    0.03940 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08898 | -0.00206 |    0.02762 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12398 | -0.00687 |    0.03952 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07653 | -0.00119 |    0.02138 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06215 | -0.00100 |    0.01501 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08261 | -0.00184 |    0.02445 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06298 | -0.00335 |    0.01603 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07664 | -0.00117 |    0.02581 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07322 | -0.00293 |    0.02518 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06887 | -0.00012 |    0.01776 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06813 | -0.00187 |    0.02108 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05601 | -0.00188 |    0.01448 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05318 | -0.00027 |    0.01404 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03330 |  0.00132 |    0.00594 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51694 | -0.07527 |    0.25770 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:15:58,557 - Total sparsity: 88.62

2018-10-27 23:15:58,557 - --- validate (epoch=171)-----------
2018-10-27 23:15:58,558 - 10000 samples (128 per mini-batch)
2018-10-27 23:15:59,280 - Epoch: [171][   50/   78]    Loss 0.457706    Top1 85.359375    Top5 99.421875    
2018-10-27 23:15:59,670 - ==> Top1: 85.510    Top5: 99.500    Loss: 0.450

2018-10-27 23:15:59,671 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:15:59,671 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:15:59,686 - 

2018-10-27 23:15:59,687 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:16:00,919 - Epoch: [172][   50/  391]    Overall Loss 0.289742    Objective Loss 0.289742    Top1 89.953125    Top5 99.734375    LR 0.030000    Time 0.024595    
2018-10-27 23:16:02,092 - Epoch: [172][  100/  391]    Overall Loss 0.284863    Objective Loss 0.284863    Top1 89.796875    Top5 99.773438    LR 0.030000    Time 0.024016    
2018-10-27 23:16:03,266 - Epoch: [172][  150/  391]    Overall Loss 0.284375    Objective Loss 0.284375    Top1 89.921875    Top5 99.770833    LR 0.030000    Time 0.023828    
2018-10-27 23:16:04,439 - Epoch: [172][  200/  391]    Overall Loss 0.283302    Objective Loss 0.283302    Top1 90.062500    Top5 99.777344    LR 0.030000    Time 0.023733    
2018-10-27 23:16:05,608 - Epoch: [172][  250/  391]    Overall Loss 0.285004    Objective Loss 0.285004    Top1 90.043750    Top5 99.781250    LR 0.030000    Time 0.023656    
2018-10-27 23:16:06,777 - Epoch: [172][  300/  391]    Overall Loss 0.287587    Objective Loss 0.287587    Top1 89.992188    Top5 99.786458    LR 0.030000    Time 0.023603    
2018-10-27 23:16:07,948 - Epoch: [172][  350/  391]    Overall Loss 0.286462    Objective Loss 0.286462    Top1 90.017857    Top5 99.794643    LR 0.030000    Time 0.023574    
2018-10-27 23:16:08,988 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33713 | -0.00376 |    0.12643 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10391 | -0.00371 |    0.02289 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09754 |  0.00057 |    0.02405 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10095 | -0.00252 |    0.02404 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07964 | -0.00180 |    0.01579 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10562 | -0.00441 |    0.02956 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07782 | -0.00100 |    0.01770 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11125 | -0.00375 |    0.03940 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08884 | -0.00223 |    0.02762 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12363 | -0.00719 |    0.03973 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07644 | -0.00115 |    0.02132 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06207 | -0.00101 |    0.01501 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08252 | -0.00188 |    0.02442 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06292 | -0.00340 |    0.01610 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07653 | -0.00120 |    0.02581 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07312 | -0.00292 |    0.02512 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06869 |  0.00010 |    0.01782 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06805 | -0.00194 |    0.02107 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05596 | -0.00188 |    0.01447 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05317 | -0.00029 |    0.01406 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03326 |  0.00131 |    0.00594 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51650 | -0.07504 |    0.25724 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:16:08,988 - Total sparsity: 88.62

2018-10-27 23:16:08,988 - --- validate (epoch=172)-----------
2018-10-27 23:16:08,989 - 10000 samples (128 per mini-batch)
2018-10-27 23:16:09,717 - Epoch: [172][   50/   78]    Loss 0.439801    Top1 85.843750    Top5 99.406250    
2018-10-27 23:16:10,109 - ==> Top1: 85.950    Top5: 99.430    Loss: 0.441

2018-10-27 23:16:10,110 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:16:10,110 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:16:10,120 - 

2018-10-27 23:16:10,120 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:16:11,347 - Epoch: [173][   50/  391]    Overall Loss 0.291810    Objective Loss 0.291810    Top1 89.734375    Top5 99.703125    LR 0.030000    Time 0.024485    
2018-10-27 23:16:12,520 - Epoch: [173][  100/  391]    Overall Loss 0.284378    Objective Loss 0.284378    Top1 90.070312    Top5 99.710938    LR 0.030000    Time 0.023959    
2018-10-27 23:16:13,692 - Epoch: [173][  150/  391]    Overall Loss 0.282549    Objective Loss 0.282549    Top1 90.010417    Top5 99.723958    LR 0.030000    Time 0.023776    
2018-10-27 23:16:14,863 - Epoch: [173][  200/  391]    Overall Loss 0.279266    Objective Loss 0.279266    Top1 90.058594    Top5 99.742188    LR 0.030000    Time 0.023683    
2018-10-27 23:16:16,034 - Epoch: [173][  250/  391]    Overall Loss 0.280731    Objective Loss 0.280731    Top1 90.046875    Top5 99.750000    LR 0.030000    Time 0.023623    
2018-10-27 23:16:17,204 - Epoch: [173][  300/  391]    Overall Loss 0.282772    Objective Loss 0.282772    Top1 89.997396    Top5 99.750000    LR 0.030000    Time 0.023569    
2018-10-27 23:16:18,375 - Epoch: [173][  350/  391]    Overall Loss 0.285409    Objective Loss 0.285409    Top1 89.886161    Top5 99.752232    LR 0.030000    Time 0.023544    
2018-10-27 23:16:19,417 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33643 | -0.00427 |    0.12641 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10370 | -0.00339 |    0.02284 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09737 |  0.00068 |    0.02406 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10077 | -0.00272 |    0.02398 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07947 | -0.00180 |    0.01590 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10548 | -0.00438 |    0.02959 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07771 | -0.00108 |    0.01765 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11110 | -0.00338 |    0.03925 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08872 | -0.00227 |    0.02759 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12334 | -0.00715 |    0.03969 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07639 | -0.00102 |    0.02126 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06199 | -0.00098 |    0.01497 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08241 | -0.00186 |    0.02439 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06285 | -0.00328 |    0.01604 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07642 | -0.00111 |    0.02575 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07304 | -0.00301 |    0.02510 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06854 |  0.00007 |    0.01779 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06799 | -0.00188 |    0.02103 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05593 | -0.00189 |    0.01447 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05316 | -0.00024 |    0.01403 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03322 |  0.00132 |    0.00593 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51658 | -0.07495 |    0.25727 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:16:19,417 - Total sparsity: 88.62

2018-10-27 23:16:19,417 - --- validate (epoch=173)-----------
2018-10-27 23:16:19,417 - 10000 samples (128 per mini-batch)
2018-10-27 23:16:20,150 - Epoch: [173][   50/   78]    Loss 0.424813    Top1 86.406250    Top5 99.453125    
2018-10-27 23:16:20,547 - ==> Top1: 86.230    Top5: 99.470    Loss: 0.424

2018-10-27 23:16:20,548 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:16:20,548 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:16:20,564 - 

2018-10-27 23:16:20,564 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:16:21,768 - Epoch: [174][   50/  391]    Overall Loss 0.276149    Objective Loss 0.276149    Top1 90.265625    Top5 99.781250    LR 0.030000    Time 0.024032    
2018-10-27 23:16:22,940 - Epoch: [174][  100/  391]    Overall Loss 0.279282    Objective Loss 0.279282    Top1 90.375000    Top5 99.789062    LR 0.030000    Time 0.023719    
2018-10-27 23:16:24,108 - Epoch: [174][  150/  391]    Overall Loss 0.279450    Objective Loss 0.279450    Top1 90.296875    Top5 99.786458    LR 0.030000    Time 0.023591    
2018-10-27 23:16:25,283 - Epoch: [174][  200/  391]    Overall Loss 0.281948    Objective Loss 0.281948    Top1 90.136719    Top5 99.777344    LR 0.030000    Time 0.023561    
2018-10-27 23:16:26,456 - Epoch: [174][  250/  391]    Overall Loss 0.282837    Objective Loss 0.282837    Top1 90.115625    Top5 99.781250    LR 0.030000    Time 0.023538    
2018-10-27 23:16:27,632 - Epoch: [174][  300/  391]    Overall Loss 0.282779    Objective Loss 0.282779    Top1 90.101562    Top5 99.776042    LR 0.030000    Time 0.023530    
2018-10-27 23:16:28,804 - Epoch: [174][  350/  391]    Overall Loss 0.284830    Objective Loss 0.284830    Top1 90.029018    Top5 99.765625    LR 0.030000    Time 0.023513    
2018-10-27 23:16:29,845 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33615 | -0.00372 |    0.12626 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10360 | -0.00327 |    0.02276 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09724 |  0.00082 |    0.02389 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10067 | -0.00234 |    0.02389 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07934 | -0.00199 |    0.01581 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10533 | -0.00401 |    0.02953 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07754 | -0.00094 |    0.01748 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11095 | -0.00354 |    0.03921 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08861 | -0.00226 |    0.02753 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12308 | -0.00722 |    0.03931 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07637 | -0.00095 |    0.02127 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06192 | -0.00097 |    0.01492 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08232 | -0.00177 |    0.02440 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06277 | -0.00326 |    0.01605 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07632 | -0.00113 |    0.02574 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07296 | -0.00303 |    0.02508 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06840 |  0.00031 |    0.01761 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06794 | -0.00189 |    0.02100 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05590 | -0.00189 |    0.01446 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05315 | -0.00021 |    0.01403 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03319 |  0.00133 |    0.00593 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51686 | -0.07479 |    0.25737 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:16:29,845 - Total sparsity: 88.62

2018-10-27 23:16:29,845 - --- validate (epoch=174)-----------
2018-10-27 23:16:29,845 - 10000 samples (128 per mini-batch)
2018-10-27 23:16:30,569 - Epoch: [174][   50/   78]    Loss 0.422501    Top1 86.359375    Top5 99.546875    
2018-10-27 23:16:30,961 - ==> Top1: 86.200    Top5: 99.560    Loss: 0.425

2018-10-27 23:16:30,961 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:16:30,962 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:16:30,975 - 

2018-10-27 23:16:30,976 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:16:32,200 - Epoch: [175][   50/  391]    Overall Loss 0.281219    Objective Loss 0.281219    Top1 90.125000    Top5 99.703125    LR 0.030000    Time 0.024455    
2018-10-27 23:16:33,371 - Epoch: [175][  100/  391]    Overall Loss 0.272948    Objective Loss 0.272948    Top1 90.187500    Top5 99.796875    LR 0.030000    Time 0.023921    
2018-10-27 23:16:34,544 - Epoch: [175][  150/  391]    Overall Loss 0.274002    Objective Loss 0.274002    Top1 90.161458    Top5 99.807292    LR 0.030000    Time 0.023754    
2018-10-27 23:16:35,717 - Epoch: [175][  200/  391]    Overall Loss 0.276812    Objective Loss 0.276812    Top1 90.101562    Top5 99.816406    LR 0.030000    Time 0.023674    
2018-10-27 23:16:36,889 - Epoch: [175][  250/  391]    Overall Loss 0.280458    Objective Loss 0.280458    Top1 89.975000    Top5 99.825000    LR 0.030000    Time 0.023625    
2018-10-27 23:16:38,063 - Epoch: [175][  300/  391]    Overall Loss 0.277723    Objective Loss 0.277723    Top1 90.096354    Top5 99.809896    LR 0.030000    Time 0.023595    
2018-10-27 23:16:39,231 - Epoch: [175][  350/  391]    Overall Loss 0.277778    Objective Loss 0.277778    Top1 90.107143    Top5 99.799107    LR 0.030000    Time 0.023556    
2018-10-27 23:16:40,274 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33566 | -0.00479 |    0.12618 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10343 | -0.00335 |    0.02286 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09711 |  0.00096 |    0.02389 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10052 | -0.00246 |    0.02387 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07919 | -0.00182 |    0.01576 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10517 | -0.00406 |    0.02951 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07736 | -0.00095 |    0.01752 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11078 | -0.00366 |    0.03921 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08847 | -0.00225 |    0.02749 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12293 | -0.00709 |    0.03961 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07635 | -0.00097 |    0.02124 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06187 | -0.00087 |    0.01490 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08223 | -0.00170 |    0.02432 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06269 | -0.00333 |    0.01601 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07620 | -0.00126 |    0.02573 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07288 | -0.00295 |    0.02501 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06820 |  0.00034 |    0.01753 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06787 | -0.00184 |    0.02100 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05586 | -0.00189 |    0.01444 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05312 | -0.00019 |    0.01400 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03314 |  0.00132 |    0.00592 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51763 | -0.07508 |    0.25776 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:16:40,275 - Total sparsity: 88.62

2018-10-27 23:16:40,275 - --- validate (epoch=175)-----------
2018-10-27 23:16:40,275 - 10000 samples (128 per mini-batch)
2018-10-27 23:16:41,059 - Epoch: [175][   50/   78]    Loss 0.433058    Top1 86.250000    Top5 99.546875    
2018-10-27 23:16:41,513 - ==> Top1: 86.110    Top5: 99.580    Loss: 0.438

2018-10-27 23:16:41,514 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:16:41,514 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:16:41,525 - 

2018-10-27 23:16:41,525 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:16:42,755 - Epoch: [176][   50/  391]    Overall Loss 0.273148    Objective Loss 0.273148    Top1 90.203125    Top5 99.687500    LR 0.030000    Time 0.024568    
2018-10-27 23:16:43,924 - Epoch: [176][  100/  391]    Overall Loss 0.279954    Objective Loss 0.279954    Top1 90.195312    Top5 99.742188    LR 0.030000    Time 0.023957    
2018-10-27 23:16:45,098 - Epoch: [176][  150/  391]    Overall Loss 0.281015    Objective Loss 0.281015    Top1 90.229167    Top5 99.729167    LR 0.030000    Time 0.023790    
2018-10-27 23:16:46,271 - Epoch: [176][  200/  391]    Overall Loss 0.284552    Objective Loss 0.284552    Top1 89.992188    Top5 99.734375    LR 0.030000    Time 0.023700    
2018-10-27 23:16:47,444 - Epoch: [176][  250/  391]    Overall Loss 0.283648    Objective Loss 0.283648    Top1 89.987500    Top5 99.743750    LR 0.030000    Time 0.023646    
2018-10-27 23:16:48,618 - Epoch: [176][  300/  391]    Overall Loss 0.283924    Objective Loss 0.283924    Top1 90.007812    Top5 99.718750    LR 0.030000    Time 0.023615    
2018-10-27 23:16:49,790 - Epoch: [176][  350/  391]    Overall Loss 0.284855    Objective Loss 0.284855    Top1 89.941964    Top5 99.738839    LR 0.030000    Time 0.023587    
2018-10-27 23:16:50,838 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33598 | -0.00290 |    0.12676 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10341 | -0.00388 |    0.02283 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09705 |  0.00101 |    0.02395 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10039 | -0.00239 |    0.02378 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07904 | -0.00180 |    0.01575 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10507 | -0.00409 |    0.02935 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07723 | -0.00109 |    0.01742 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11068 | -0.00379 |    0.03899 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08839 | -0.00215 |    0.02750 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12271 | -0.00758 |    0.03926 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07634 | -0.00090 |    0.02121 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06182 | -0.00086 |    0.01499 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08217 | -0.00182 |    0.02428 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06265 | -0.00326 |    0.01601 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07613 | -0.00111 |    0.02570 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07283 | -0.00292 |    0.02498 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06808 |  0.00031 |    0.01745 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06783 | -0.00181 |    0.02099 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05585 | -0.00187 |    0.01446 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05313 | -0.00019 |    0.01401 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03312 |  0.00131 |    0.00593 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51709 | -0.07465 |    0.25744 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:16:50,839 - Total sparsity: 88.62

2018-10-27 23:16:50,839 - --- validate (epoch=176)-----------
2018-10-27 23:16:50,839 - 10000 samples (128 per mini-batch)
2018-10-27 23:16:51,569 - Epoch: [176][   50/   78]    Loss 0.466597    Top1 85.296875    Top5 99.390625    
2018-10-27 23:16:51,962 - ==> Top1: 85.170    Top5: 99.440    Loss: 0.465

2018-10-27 23:16:51,963 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:16:51,963 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:16:51,974 - 

2018-10-27 23:16:51,974 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:16:53,201 - Epoch: [177][   50/  391]    Overall Loss 0.296342    Objective Loss 0.296342    Top1 89.609375    Top5 99.734375    LR 0.030000    Time 0.024513    
2018-10-27 23:16:54,373 - Epoch: [177][  100/  391]    Overall Loss 0.282352    Objective Loss 0.282352    Top1 90.117188    Top5 99.695312    LR 0.030000    Time 0.023956    
2018-10-27 23:16:55,545 - Epoch: [177][  150/  391]    Overall Loss 0.283508    Objective Loss 0.283508    Top1 90.026042    Top5 99.729167    LR 0.030000    Time 0.023775    
2018-10-27 23:16:56,720 - Epoch: [177][  200/  391]    Overall Loss 0.284182    Objective Loss 0.284182    Top1 90.000000    Top5 99.750000    LR 0.030000    Time 0.023702    
2018-10-27 23:16:57,893 - Epoch: [177][  250/  391]    Overall Loss 0.286713    Objective Loss 0.286713    Top1 89.968750    Top5 99.746875    LR 0.030000    Time 0.023645    
2018-10-27 23:16:59,066 - Epoch: [177][  300/  391]    Overall Loss 0.287530    Objective Loss 0.287530    Top1 89.903646    Top5 99.736979    LR 0.030000    Time 0.023612    
2018-10-27 23:17:00,245 - Epoch: [177][  350/  391]    Overall Loss 0.286327    Objective Loss 0.286327    Top1 89.937500    Top5 99.741071    LR 0.030000    Time 0.023604    
2018-10-27 23:17:01,289 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33596 | -0.00350 |    0.12616 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10324 | -0.00373 |    0.02281 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09691 |  0.00099 |    0.02372 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10026 | -0.00231 |    0.02363 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07891 | -0.00201 |    0.01579 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10504 | -0.00419 |    0.02930 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07713 | -0.00132 |    0.01735 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11060 | -0.00362 |    0.03911 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08832 | -0.00221 |    0.02741 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12243 | -0.00737 |    0.03936 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07632 | -0.00097 |    0.02119 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06177 | -0.00089 |    0.01493 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08208 | -0.00175 |    0.02426 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06260 | -0.00323 |    0.01602 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07603 | -0.00116 |    0.02565 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07276 | -0.00293 |    0.02493 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06789 |  0.00032 |    0.01746 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06777 | -0.00187 |    0.02096 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05582 | -0.00185 |    0.01442 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05312 | -0.00019 |    0.01399 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03309 |  0.00128 |    0.00591 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51685 | -0.07449 |    0.25721 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:17:01,289 - Total sparsity: 88.62

2018-10-27 23:17:01,290 - --- validate (epoch=177)-----------
2018-10-27 23:17:01,290 - 10000 samples (128 per mini-batch)
2018-10-27 23:17:02,022 - Epoch: [177][   50/   78]    Loss 0.449654    Top1 85.656250    Top5 99.406250    
2018-10-27 23:17:02,414 - ==> Top1: 85.650    Top5: 99.490    Loss: 0.440

2018-10-27 23:17:02,415 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:17:02,415 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:17:02,426 - 

2018-10-27 23:17:02,426 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:17:03,652 - Epoch: [178][   50/  391]    Overall Loss 0.274397    Objective Loss 0.274397    Top1 90.484375    Top5 99.843750    LR 0.030000    Time 0.024484    
2018-10-27 23:17:04,828 - Epoch: [178][  100/  391]    Overall Loss 0.276698    Objective Loss 0.276698    Top1 90.156250    Top5 99.796875    LR 0.030000    Time 0.023984    
2018-10-27 23:17:06,001 - Epoch: [178][  150/  391]    Overall Loss 0.274754    Objective Loss 0.274754    Top1 90.494792    Top5 99.781250    LR 0.030000    Time 0.023804    
2018-10-27 23:17:07,173 - Epoch: [178][  200/  391]    Overall Loss 0.278183    Objective Loss 0.278183    Top1 90.253906    Top5 99.777344    LR 0.030000    Time 0.023705    
2018-10-27 23:17:08,347 - Epoch: [178][  250/  391]    Overall Loss 0.279516    Objective Loss 0.279516    Top1 90.262500    Top5 99.793750    LR 0.030000    Time 0.023654    
2018-10-27 23:17:09,520 - Epoch: [178][  300/  391]    Overall Loss 0.281341    Objective Loss 0.281341    Top1 90.109375    Top5 99.783854    LR 0.030000    Time 0.023618    
2018-10-27 23:17:10,692 - Epoch: [178][  350/  391]    Overall Loss 0.282272    Objective Loss 0.282272    Top1 90.040179    Top5 99.790179    LR 0.030000    Time 0.023590    
2018-10-27 23:17:11,734 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33528 | -0.00451 |    0.12606 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10314 | -0.00357 |    0.02281 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09682 |  0.00106 |    0.02378 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10020 | -0.00251 |    0.02375 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07878 | -0.00195 |    0.01580 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10491 | -0.00430 |    0.02915 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07709 | -0.00127 |    0.01736 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11053 | -0.00346 |    0.03910 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08824 | -0.00243 |    0.02748 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12217 | -0.00685 |    0.03952 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07630 | -0.00097 |    0.02112 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06170 | -0.00100 |    0.01490 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08204 | -0.00169 |    0.02416 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06255 | -0.00326 |    0.01600 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07594 | -0.00121 |    0.02566 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07271 | -0.00289 |    0.02491 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06774 |  0.00047 |    0.01749 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06772 | -0.00193 |    0.02096 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05578 | -0.00185 |    0.01442 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05311 | -0.00019 |    0.01399 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03306 |  0.00131 |    0.00590 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51629 | -0.07443 |    0.25683 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:17:11,734 - Total sparsity: 88.62

2018-10-27 23:17:11,735 - --- validate (epoch=178)-----------
2018-10-27 23:17:11,735 - 10000 samples (128 per mini-batch)
2018-10-27 23:17:12,475 - Epoch: [178][   50/   78]    Loss 0.431348    Top1 86.140625    Top5 99.484375    
2018-10-27 23:17:12,873 - ==> Top1: 86.150    Top5: 99.530    Loss: 0.428

2018-10-27 23:17:12,874 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:17:12,874 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:17:12,884 - 

2018-10-27 23:17:12,884 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:17:14,114 - Epoch: [179][   50/  391]    Overall Loss 0.280209    Objective Loss 0.280209    Top1 90.187500    Top5 99.875000    LR 0.030000    Time 0.024561    
2018-10-27 23:17:15,288 - Epoch: [179][  100/  391]    Overall Loss 0.281032    Objective Loss 0.281032    Top1 89.976562    Top5 99.820312    LR 0.030000    Time 0.024003    
2018-10-27 23:17:16,459 - Epoch: [179][  150/  391]    Overall Loss 0.283879    Objective Loss 0.283879    Top1 89.932292    Top5 99.776042    LR 0.030000    Time 0.023801    
2018-10-27 23:17:17,634 - Epoch: [179][  200/  391]    Overall Loss 0.282324    Objective Loss 0.282324    Top1 89.992188    Top5 99.781250    LR 0.030000    Time 0.023719    
2018-10-27 23:17:18,805 - Epoch: [179][  250/  391]    Overall Loss 0.283871    Objective Loss 0.283871    Top1 89.887500    Top5 99.800000    LR 0.030000    Time 0.023651    
2018-10-27 23:17:19,976 - Epoch: [179][  300/  391]    Overall Loss 0.285630    Objective Loss 0.285630    Top1 89.848958    Top5 99.794271    LR 0.030000    Time 0.023610    
2018-10-27 23:17:21,155 - Epoch: [179][  350/  391]    Overall Loss 0.286012    Objective Loss 0.286012    Top1 89.845982    Top5 99.792411    LR 0.030000    Time 0.023601    
2018-10-27 23:17:22,195 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33486 | -0.00316 |    0.12498 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10299 | -0.00362 |    0.02291 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09666 |  0.00109 |    0.02390 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10010 | -0.00238 |    0.02364 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07863 | -0.00223 |    0.01574 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10487 | -0.00425 |    0.02917 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07709 | -0.00103 |    0.01734 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11041 | -0.00357 |    0.03916 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08817 | -0.00240 |    0.02736 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12195 | -0.00675 |    0.03903 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07630 | -0.00111 |    0.02115 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06166 | -0.00094 |    0.01493 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08202 | -0.00173 |    0.02413 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06251 | -0.00334 |    0.01597 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07587 | -0.00115 |    0.02564 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07265 | -0.00294 |    0.02488 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06759 |  0.00045 |    0.01737 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06768 | -0.00190 |    0.02097 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05576 | -0.00179 |    0.01443 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05311 | -0.00021 |    0.01399 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03304 |  0.00131 |    0.00591 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51571 | -0.07435 |    0.25645 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:17:22,196 - Total sparsity: 88.62

2018-10-27 23:17:22,196 - --- validate (epoch=179)-----------
2018-10-27 23:17:22,196 - 10000 samples (128 per mini-batch)
2018-10-27 23:17:22,920 - Epoch: [179][   50/   78]    Loss 0.427738    Top1 86.046875    Top5 99.531250    
2018-10-27 23:17:23,313 - ==> Top1: 85.970    Top5: 99.520    Loss: 0.430

2018-10-27 23:17:23,314 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:17:23,314 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:17:23,329 - 

2018-10-27 23:17:23,329 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:17:24,532 - Epoch: [180][   50/  391]    Overall Loss 0.274651    Objective Loss 0.274651    Top1 90.453125    Top5 99.828125    LR 0.030000    Time 0.024020    
2018-10-27 23:17:25,705 - Epoch: [180][  100/  391]    Overall Loss 0.278638    Objective Loss 0.278638    Top1 90.351562    Top5 99.812500    LR 0.030000    Time 0.023732    
2018-10-27 23:17:26,877 - Epoch: [180][  150/  391]    Overall Loss 0.279527    Objective Loss 0.279527    Top1 90.302083    Top5 99.781250    LR 0.030000    Time 0.023626    
2018-10-27 23:17:28,054 - Epoch: [180][  200/  391]    Overall Loss 0.284883    Objective Loss 0.284883    Top1 90.128906    Top5 99.769531    LR 0.030000    Time 0.023596    
2018-10-27 23:17:29,229 - Epoch: [180][  250/  391]    Overall Loss 0.288453    Objective Loss 0.288453    Top1 89.943750    Top5 99.781250    LR 0.030000    Time 0.023571    
2018-10-27 23:17:30,405 - Epoch: [180][  300/  391]    Overall Loss 0.287922    Objective Loss 0.287922    Top1 89.932292    Top5 99.770833    LR 0.030000    Time 0.023557    
2018-10-27 23:17:31,579 - Epoch: [180][  350/  391]    Overall Loss 0.289146    Objective Loss 0.289146    Top1 89.857143    Top5 99.774554    LR 0.030000    Time 0.023542    
2018-10-27 23:17:32,628 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33463 | -0.00359 |    0.12533 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10290 | -0.00365 |    0.02286 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09661 |  0.00127 |    0.02371 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.10007 | -0.00249 |    0.02371 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07855 | -0.00206 |    0.01573 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10482 | -0.00408 |    0.02913 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07710 | -0.00113 |    0.01740 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11031 | -0.00375 |    0.03911 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08811 | -0.00208 |    0.02737 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12179 | -0.00652 |    0.03881 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07631 | -0.00103 |    0.02118 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06163 | -0.00086 |    0.01489 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08200 | -0.00176 |    0.02412 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06244 | -0.00338 |    0.01594 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07580 | -0.00117 |    0.02564 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07262 | -0.00281 |    0.02487 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06747 |  0.00048 |    0.01730 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06767 | -0.00189 |    0.02095 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05576 | -0.00175 |    0.01442 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05312 | -0.00020 |    0.01400 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03301 |  0.00133 |    0.00589 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51542 | -0.07418 |    0.25624 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:17:32,628 - Total sparsity: 88.62

2018-10-27 23:17:32,628 - --- validate (epoch=180)-----------
2018-10-27 23:17:32,628 - 10000 samples (128 per mini-batch)
2018-10-27 23:17:33,365 - Epoch: [180][   50/   78]    Loss 0.440446    Top1 85.578125    Top5 99.453125    
2018-10-27 23:17:33,765 - ==> Top1: 85.690    Top5: 99.480    Loss: 0.439

2018-10-27 23:17:33,766 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:17:33,766 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:17:33,777 - 

2018-10-27 23:17:33,777 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:17:35,006 - Epoch: [181][   50/  391]    Overall Loss 0.271206    Objective Loss 0.271206    Top1 90.859375    Top5 99.828125    LR 0.030000    Time 0.024546    
2018-10-27 23:17:36,178 - Epoch: [181][  100/  391]    Overall Loss 0.270189    Objective Loss 0.270189    Top1 90.539062    Top5 99.835938    LR 0.030000    Time 0.023977    
2018-10-27 23:17:37,343 - Epoch: [181][  150/  391]    Overall Loss 0.277616    Objective Loss 0.277616    Top1 90.468750    Top5 99.807292    LR 0.030000    Time 0.023744    
2018-10-27 23:17:38,513 - Epoch: [181][  200/  391]    Overall Loss 0.279750    Objective Loss 0.279750    Top1 90.347656    Top5 99.804688    LR 0.030000    Time 0.023647    
2018-10-27 23:17:39,677 - Epoch: [181][  250/  391]    Overall Loss 0.282492    Objective Loss 0.282492    Top1 90.165625    Top5 99.796875    LR 0.030000    Time 0.023571    
2018-10-27 23:17:40,851 - Epoch: [181][  300/  391]    Overall Loss 0.283399    Objective Loss 0.283399    Top1 90.088542    Top5 99.781250    LR 0.030000    Time 0.023551    
2018-10-27 23:17:42,026 - Epoch: [181][  350/  391]    Overall Loss 0.284755    Objective Loss 0.284755    Top1 90.093750    Top5 99.790179    LR 0.030000    Time 0.023540    
2018-10-27 23:17:43,069 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33427 | -0.00452 |    0.12519 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10276 | -0.00386 |    0.02291 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09649 |  0.00105 |    0.02374 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09998 | -0.00254 |    0.02354 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07847 | -0.00209 |    0.01565 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10471 | -0.00412 |    0.02916 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07703 | -0.00129 |    0.01738 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11021 | -0.00331 |    0.03893 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08803 | -0.00219 |    0.02728 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12156 | -0.00641 |    0.03903 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07628 | -0.00109 |    0.02120 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06158 | -0.00076 |    0.01482 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08194 | -0.00169 |    0.02409 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06237 | -0.00334 |    0.01591 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07572 | -0.00123 |    0.02560 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07257 | -0.00287 |    0.02486 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06735 |  0.00046 |    0.01726 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06764 | -0.00184 |    0.02091 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05574 | -0.00178 |    0.01441 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05312 | -0.00026 |    0.01399 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03297 |  0.00132 |    0.00590 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51540 | -0.07398 |    0.25625 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:17:43,069 - Total sparsity: 88.62

2018-10-27 23:17:43,069 - --- validate (epoch=181)-----------
2018-10-27 23:17:43,069 - 10000 samples (128 per mini-batch)
2018-10-27 23:17:43,791 - Epoch: [181][   50/   78]    Loss 0.476104    Top1 84.593750    Top5 99.406250    
2018-10-27 23:17:44,184 - ==> Top1: 84.640    Top5: 99.430    Loss: 0.469

2018-10-27 23:17:44,185 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:17:44,185 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:17:44,196 - 

2018-10-27 23:17:44,197 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:17:45,428 - Epoch: [182][   50/  391]    Overall Loss 0.279519    Objective Loss 0.279519    Top1 90.421875    Top5 99.796875    LR 0.030000    Time 0.024593    
2018-10-27 23:17:46,597 - Epoch: [182][  100/  391]    Overall Loss 0.278889    Objective Loss 0.278889    Top1 90.257812    Top5 99.804688    LR 0.030000    Time 0.023975    
2018-10-27 23:17:47,774 - Epoch: [182][  150/  391]    Overall Loss 0.277298    Objective Loss 0.277298    Top1 90.197917    Top5 99.786458    LR 0.030000    Time 0.023820    
2018-10-27 23:17:48,947 - Epoch: [182][  200/  391]    Overall Loss 0.278611    Objective Loss 0.278611    Top1 90.214844    Top5 99.781250    LR 0.030000    Time 0.023705    
2018-10-27 23:17:50,118 - Epoch: [182][  250/  391]    Overall Loss 0.280225    Objective Loss 0.280225    Top1 90.175000    Top5 99.781250    LR 0.030000    Time 0.023645    
2018-10-27 23:17:51,291 - Epoch: [182][  300/  391]    Overall Loss 0.283253    Objective Loss 0.283253    Top1 90.028646    Top5 99.773438    LR 0.030000    Time 0.023609    
2018-10-27 23:17:52,466 - Epoch: [182][  350/  391]    Overall Loss 0.281779    Objective Loss 0.281779    Top1 90.066964    Top5 99.776786    LR 0.030000    Time 0.023590    
2018-10-27 23:17:53,511 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33408 | -0.00375 |    0.12561 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10268 | -0.00395 |    0.02288 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09641 |  0.00124 |    0.02369 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09980 | -0.00240 |    0.02351 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07832 | -0.00173 |    0.01564 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10462 | -0.00424 |    0.02915 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07691 | -0.00124 |    0.01736 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.11007 | -0.00336 |    0.03872 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08792 | -0.00225 |    0.02730 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12136 | -0.00691 |    0.03867 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07623 | -0.00105 |    0.02117 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06151 | -0.00077 |    0.01487 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08187 | -0.00168 |    0.02409 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06230 | -0.00327 |    0.01588 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07563 | -0.00118 |    0.02555 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07252 | -0.00287 |    0.02483 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06721 |  0.00051 |    0.01722 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06760 | -0.00180 |    0.02090 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05570 | -0.00180 |    0.01441 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05311 | -0.00024 |    0.01399 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03294 |  0.00134 |    0.00588 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51638 | -0.07379 |    0.25653 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:17:53,511 - Total sparsity: 88.62

2018-10-27 23:17:53,511 - --- validate (epoch=182)-----------
2018-10-27 23:17:53,511 - 10000 samples (128 per mini-batch)
2018-10-27 23:17:54,237 - Epoch: [182][   50/   78]    Loss 0.426242    Top1 86.093750    Top5 99.515625    
2018-10-27 23:17:54,631 - ==> Top1: 86.140    Top5: 99.550    Loss: 0.421

2018-10-27 23:17:54,631 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:17:54,632 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:17:54,641 - 

2018-10-27 23:17:54,642 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:17:55,866 - Epoch: [183][   50/  391]    Overall Loss 0.267752    Objective Loss 0.267752    Top1 90.421875    Top5 99.781250    LR 0.030000    Time 0.024443    
2018-10-27 23:17:57,040 - Epoch: [183][  100/  391]    Overall Loss 0.283577    Objective Loss 0.283577    Top1 89.867188    Top5 99.773438    LR 0.030000    Time 0.023953    
2018-10-27 23:17:58,212 - Epoch: [183][  150/  391]    Overall Loss 0.284102    Objective Loss 0.284102    Top1 89.895833    Top5 99.781250    LR 0.030000    Time 0.023773    
2018-10-27 23:17:59,381 - Epoch: [183][  200/  391]    Overall Loss 0.288814    Objective Loss 0.288814    Top1 89.769531    Top5 99.738281    LR 0.030000    Time 0.023666    
2018-10-27 23:18:00,558 - Epoch: [183][  250/  391]    Overall Loss 0.286677    Objective Loss 0.286677    Top1 89.840625    Top5 99.743750    LR 0.030000    Time 0.023637    
2018-10-27 23:18:01,714 - Epoch: [183][  300/  391]    Overall Loss 0.286989    Objective Loss 0.286989    Top1 89.789062    Top5 99.757812    LR 0.030000    Time 0.023546    
2018-10-27 23:18:02,858 - Epoch: [183][  350/  391]    Overall Loss 0.286094    Objective Loss 0.286094    Top1 89.796875    Top5 99.767857    LR 0.030000    Time 0.023446    
2018-10-27 23:18:03,871 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33359 | -0.00402 |    0.12552 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10257 | -0.00377 |    0.02273 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09633 |  0.00122 |    0.02365 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09971 | -0.00238 |    0.02365 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07822 | -0.00189 |    0.01553 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10459 | -0.00403 |    0.02894 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07688 | -0.00115 |    0.01720 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10998 | -0.00331 |    0.03863 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08784 | -0.00226 |    0.02731 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12120 | -0.00633 |    0.03853 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07618 | -0.00124 |    0.02117 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06146 | -0.00083 |    0.01482 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08185 | -0.00160 |    0.02409 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06226 | -0.00325 |    0.01587 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07557 | -0.00124 |    0.02551 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07249 | -0.00292 |    0.02481 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06706 |  0.00036 |    0.01723 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06760 | -0.00182 |    0.02089 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05569 | -0.00182 |    0.01441 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05312 | -0.00024 |    0.01400 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03292 |  0.00135 |    0.00587 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51663 | -0.07350 |    0.25657 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:18:03,872 - Total sparsity: 88.62

2018-10-27 23:18:03,872 - --- validate (epoch=183)-----------
2018-10-27 23:18:03,872 - 10000 samples (128 per mini-batch)
2018-10-27 23:18:04,598 - Epoch: [183][   50/   78]    Loss 0.451702    Top1 85.765625    Top5 99.343750    
2018-10-27 23:18:04,993 - ==> Top1: 85.890    Top5: 99.340    Loss: 0.448

2018-10-27 23:18:04,994 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:18:04,994 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:18:05,005 - 

2018-10-27 23:18:05,005 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:18:06,204 - Epoch: [184][   50/  391]    Overall Loss 0.271548    Objective Loss 0.271548    Top1 90.265625    Top5 99.796875    LR 0.030000    Time 0.023937    
2018-10-27 23:18:07,344 - Epoch: [184][  100/  391]    Overall Loss 0.271771    Objective Loss 0.271771    Top1 90.390625    Top5 99.757812    LR 0.030000    Time 0.023358    
2018-10-27 23:18:08,484 - Epoch: [184][  150/  391]    Overall Loss 0.277902    Objective Loss 0.277902    Top1 90.192708    Top5 99.765625    LR 0.030000    Time 0.023161    
2018-10-27 23:18:09,623 - Epoch: [184][  200/  391]    Overall Loss 0.279156    Objective Loss 0.279156    Top1 90.234375    Top5 99.796875    LR 0.030000    Time 0.023058    
2018-10-27 23:18:10,763 - Epoch: [184][  250/  391]    Overall Loss 0.280827    Objective Loss 0.280827    Top1 90.096875    Top5 99.806250    LR 0.030000    Time 0.023002    
2018-10-27 23:18:11,905 - Epoch: [184][  300/  391]    Overall Loss 0.282650    Objective Loss 0.282650    Top1 90.044271    Top5 99.778646    LR 0.030000    Time 0.022971    
2018-10-27 23:18:13,051 - Epoch: [184][  350/  391]    Overall Loss 0.283603    Objective Loss 0.283603    Top1 89.995536    Top5 99.779018    LR 0.030000    Time 0.022959    
2018-10-27 23:18:14,069 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33342 | -0.00331 |    0.12520 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10247 | -0.00352 |    0.02266 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09623 |  0.00107 |    0.02370 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09956 | -0.00250 |    0.02348 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07809 | -0.00180 |    0.01549 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10448 | -0.00407 |    0.02898 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07681 | -0.00124 |    0.01716 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10995 | -0.00312 |    0.03858 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08780 | -0.00242 |    0.02734 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12098 | -0.00686 |    0.03846 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07615 | -0.00122 |    0.02111 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06144 | -0.00087 |    0.01475 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08182 | -0.00185 |    0.02414 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06222 | -0.00326 |    0.01589 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07551 | -0.00126 |    0.02548 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07246 | -0.00293 |    0.02480 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06694 |  0.00037 |    0.01717 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06758 | -0.00184 |    0.02089 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05567 | -0.00183 |    0.01440 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05313 | -0.00019 |    0.01402 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03289 |  0.00136 |    0.00586 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51617 | -0.07346 |    0.25614 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:18:14,069 - Total sparsity: 88.62

2018-10-27 23:18:14,069 - --- validate (epoch=184)-----------
2018-10-27 23:18:14,069 - 10000 samples (128 per mini-batch)
2018-10-27 23:18:14,778 - Epoch: [184][   50/   78]    Loss 0.461257    Top1 85.359375    Top5 99.343750    
2018-10-27 23:18:15,163 - ==> Top1: 85.370    Top5: 99.400    Loss: 0.459

2018-10-27 23:18:15,164 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:18:15,164 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:18:15,177 - 

2018-10-27 23:18:15,177 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:18:16,375 - Epoch: [185][   50/  391]    Overall Loss 0.274398    Objective Loss 0.274398    Top1 90.390625    Top5 99.687500    LR 0.030000    Time 0.023922    
2018-10-27 23:18:17,519 - Epoch: [185][  100/  391]    Overall Loss 0.271410    Objective Loss 0.271410    Top1 90.562500    Top5 99.726562    LR 0.030000    Time 0.023382    
2018-10-27 23:18:18,661 - Epoch: [185][  150/  391]    Overall Loss 0.273702    Objective Loss 0.273702    Top1 90.343750    Top5 99.734375    LR 0.030000    Time 0.023194    
2018-10-27 23:18:19,807 - Epoch: [185][  200/  391]    Overall Loss 0.278624    Objective Loss 0.278624    Top1 90.257812    Top5 99.746094    LR 0.030000    Time 0.023119    
2018-10-27 23:18:20,952 - Epoch: [185][  250/  391]    Overall Loss 0.281994    Objective Loss 0.281994    Top1 90.178125    Top5 99.759375    LR 0.030000    Time 0.023070    
2018-10-27 23:18:22,097 - Epoch: [185][  300/  391]    Overall Loss 0.283179    Objective Loss 0.283179    Top1 90.119792    Top5 99.765625    LR 0.030000    Time 0.023036    
2018-10-27 23:18:23,239 - Epoch: [185][  350/  391]    Overall Loss 0.284942    Objective Loss 0.284942    Top1 90.006696    Top5 99.772321    LR 0.030000    Time 0.023004    
2018-10-27 23:18:24,258 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33293 | -0.00384 |    0.12518 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10241 | -0.00353 |    0.02265 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09617 |  0.00122 |    0.02365 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09956 | -0.00242 |    0.02339 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07804 | -0.00194 |    0.01535 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10446 | -0.00417 |    0.02896 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07675 | -0.00137 |    0.01712 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10991 | -0.00308 |    0.03856 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08775 | -0.00235 |    0.02735 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12079 | -0.00634 |    0.03835 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07608 | -0.00114 |    0.02113 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06138 | -0.00082 |    0.01480 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08178 | -0.00174 |    0.02410 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06220 | -0.00317 |    0.01587 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07545 | -0.00124 |    0.02545 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07244 | -0.00291 |    0.02481 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06685 |  0.00063 |    0.01708 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06756 | -0.00176 |    0.02085 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05566 | -0.00179 |    0.01442 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05314 | -0.00017 |    0.01399 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03287 |  0.00136 |    0.00586 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51619 | -0.07364 |    0.25606 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:18:24,258 - Total sparsity: 88.62

2018-10-27 23:18:24,259 - --- validate (epoch=185)-----------
2018-10-27 23:18:24,259 - 10000 samples (128 per mini-batch)
2018-10-27 23:18:24,983 - Epoch: [185][   50/   78]    Loss 0.456432    Top1 85.281250    Top5 99.640625    
2018-10-27 23:18:25,373 - ==> Top1: 85.220    Top5: 99.620    Loss: 0.455

2018-10-27 23:18:25,374 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:18:25,374 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:18:25,384 - 

2018-10-27 23:18:25,384 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:18:26,581 - Epoch: [186][   50/  391]    Overall Loss 0.272321    Objective Loss 0.272321    Top1 90.406250    Top5 99.703125    LR 0.030000    Time 0.023904    
2018-10-27 23:18:27,721 - Epoch: [186][  100/  391]    Overall Loss 0.278229    Objective Loss 0.278229    Top1 90.335938    Top5 99.710938    LR 0.030000    Time 0.023340    
2018-10-27 23:18:28,861 - Epoch: [186][  150/  391]    Overall Loss 0.279800    Objective Loss 0.279800    Top1 90.187500    Top5 99.723958    LR 0.030000    Time 0.023149    
2018-10-27 23:18:30,002 - Epoch: [186][  200/  391]    Overall Loss 0.278660    Objective Loss 0.278660    Top1 90.125000    Top5 99.753906    LR 0.030000    Time 0.023062    
2018-10-27 23:18:31,142 - Epoch: [186][  250/  391]    Overall Loss 0.280222    Objective Loss 0.280222    Top1 90.087500    Top5 99.750000    LR 0.030000    Time 0.023003    
2018-10-27 23:18:32,282 - Epoch: [186][  300/  391]    Overall Loss 0.281319    Objective Loss 0.281319    Top1 90.036458    Top5 99.760417    LR 0.030000    Time 0.022966    
2018-10-27 23:18:33,422 - Epoch: [186][  350/  391]    Overall Loss 0.282590    Objective Loss 0.282590    Top1 89.975446    Top5 99.754464    LR 0.030000    Time 0.022939    
2018-10-27 23:18:34,437 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33286 | -0.00283 |    0.12510 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10229 | -0.00361 |    0.02263 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09608 |  0.00115 |    0.02345 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09943 | -0.00222 |    0.02350 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07797 | -0.00184 |    0.01541 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10439 | -0.00395 |    0.02895 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07666 | -0.00128 |    0.01706 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10978 | -0.00339 |    0.03855 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08767 | -0.00223 |    0.02728 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12062 | -0.00670 |    0.03838 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07602 | -0.00114 |    0.02110 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06132 | -0.00067 |    0.01481 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08174 | -0.00169 |    0.02407 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06215 | -0.00316 |    0.01582 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07540 | -0.00127 |    0.02542 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07242 | -0.00277 |    0.02475 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06678 |  0.00033 |    0.01707 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06756 | -0.00171 |    0.02082 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05566 | -0.00179 |    0.01443 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05315 | -0.00019 |    0.01401 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03285 |  0.00135 |    0.00586 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51675 | -0.07340 |    0.25625 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:18:34,437 - Total sparsity: 88.62

2018-10-27 23:18:34,437 - --- validate (epoch=186)-----------
2018-10-27 23:18:34,437 - 10000 samples (128 per mini-batch)
2018-10-27 23:18:35,155 - Epoch: [186][   50/   78]    Loss 0.467497    Top1 85.015625    Top5 99.359375    
2018-10-27 23:18:35,545 - ==> Top1: 85.120    Top5: 99.350    Loss: 0.461

2018-10-27 23:18:35,546 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:18:35,546 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:18:35,557 - 

2018-10-27 23:18:35,557 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:18:36,757 - Epoch: [187][   50/  391]    Overall Loss 0.295026    Objective Loss 0.295026    Top1 89.500000    Top5 99.812500    LR 0.030000    Time 0.023965    
2018-10-27 23:18:37,899 - Epoch: [187][  100/  391]    Overall Loss 0.287529    Objective Loss 0.287529    Top1 89.640625    Top5 99.781250    LR 0.030000    Time 0.023387    
2018-10-27 23:18:39,041 - Epoch: [187][  150/  391]    Overall Loss 0.280737    Objective Loss 0.280737    Top1 90.026042    Top5 99.812500    LR 0.030000    Time 0.023194    
2018-10-27 23:18:40,181 - Epoch: [187][  200/  391]    Overall Loss 0.281854    Objective Loss 0.281854    Top1 89.976562    Top5 99.789062    LR 0.030000    Time 0.023092    
2018-10-27 23:18:41,320 - Epoch: [187][  250/  391]    Overall Loss 0.281061    Objective Loss 0.281061    Top1 90.071875    Top5 99.787500    LR 0.030000    Time 0.023025    
2018-10-27 23:18:42,460 - Epoch: [187][  300/  391]    Overall Loss 0.281902    Objective Loss 0.281902    Top1 90.075521    Top5 99.791667    LR 0.030000    Time 0.022982    
2018-10-27 23:18:43,602 - Epoch: [187][  350/  391]    Overall Loss 0.283208    Objective Loss 0.283208    Top1 90.008929    Top5 99.783482    LR 0.030000    Time 0.022956    
2018-10-27 23:18:44,616 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33239 | -0.00271 |    0.12467 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10229 | -0.00366 |    0.02259 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09605 |  0.00104 |    0.02344 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09939 | -0.00220 |    0.02353 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07790 | -0.00187 |    0.01548 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10435 | -0.00427 |    0.02891 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07660 | -0.00125 |    0.01707 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10970 | -0.00345 |    0.03869 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08761 | -0.00211 |    0.02728 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12054 | -0.00660 |    0.03819 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07596 | -0.00112 |    0.02104 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06126 | -0.00064 |    0.01480 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08173 | -0.00184 |    0.02413 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06211 | -0.00323 |    0.01577 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07533 | -0.00127 |    0.02535 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07238 | -0.00278 |    0.02476 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06669 |  0.00034 |    0.01692 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06754 | -0.00177 |    0.02082 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05565 | -0.00176 |    0.01443 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05316 | -0.00016 |    0.01401 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03283 |  0.00138 |    0.00585 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51682 | -0.07320 |    0.25643 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:18:44,616 - Total sparsity: 88.62

2018-10-27 23:18:44,616 - --- validate (epoch=187)-----------
2018-10-27 23:18:44,617 - 10000 samples (128 per mini-batch)
2018-10-27 23:18:45,339 - Epoch: [187][   50/   78]    Loss 0.495006    Top1 84.687500    Top5 99.390625    
2018-10-27 23:18:45,718 - ==> Top1: 84.760    Top5: 99.430    Loss: 0.487

2018-10-27 23:18:45,719 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:18:45,719 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:18:45,730 - 

2018-10-27 23:18:45,730 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:18:46,927 - Epoch: [188][   50/  391]    Overall Loss 0.274581    Objective Loss 0.274581    Top1 90.343750    Top5 99.765625    LR 0.030000    Time 0.023903    
2018-10-27 23:18:48,065 - Epoch: [188][  100/  391]    Overall Loss 0.270810    Objective Loss 0.270810    Top1 90.500000    Top5 99.804688    LR 0.030000    Time 0.023317    
2018-10-27 23:18:49,206 - Epoch: [188][  150/  391]    Overall Loss 0.274655    Objective Loss 0.274655    Top1 90.375000    Top5 99.770833    LR 0.030000    Time 0.023147    
2018-10-27 23:18:50,347 - Epoch: [188][  200/  391]    Overall Loss 0.277595    Objective Loss 0.277595    Top1 90.226562    Top5 99.750000    LR 0.030000    Time 0.023055    
2018-10-27 23:18:51,487 - Epoch: [188][  250/  391]    Overall Loss 0.278981    Objective Loss 0.278981    Top1 90.209375    Top5 99.753125    LR 0.030000    Time 0.023001    
2018-10-27 23:18:52,627 - Epoch: [188][  300/  391]    Overall Loss 0.278442    Objective Loss 0.278442    Top1 90.221354    Top5 99.773438    LR 0.030000    Time 0.022963    
2018-10-27 23:18:53,770 - Epoch: [188][  350/  391]    Overall Loss 0.280638    Objective Loss 0.280638    Top1 90.075893    Top5 99.770089    LR 0.030000    Time 0.022942    
2018-10-27 23:18:54,783 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33192 | -0.00356 |    0.12487 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10223 | -0.00340 |    0.02269 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09597 |  0.00101 |    0.02349 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09931 | -0.00226 |    0.02344 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07785 | -0.00187 |    0.01558 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10436 | -0.00393 |    0.02892 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07652 | -0.00116 |    0.01714 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10965 | -0.00328 |    0.03866 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08756 | -0.00204 |    0.02729 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12040 | -0.00667 |    0.03841 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07589 | -0.00115 |    0.02110 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06122 | -0.00078 |    0.01482 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08169 | -0.00185 |    0.02417 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06207 | -0.00329 |    0.01578 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07529 | -0.00128 |    0.02527 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07235 | -0.00277 |    0.02473 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06662 |  0.00033 |    0.01685 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06753 | -0.00179 |    0.02087 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05564 | -0.00176 |    0.01443 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05317 | -0.00016 |    0.01401 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03282 |  0.00133 |    0.00585 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51694 | -0.07316 |    0.25623 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:18:54,783 - Total sparsity: 88.62

2018-10-27 23:18:54,783 - --- validate (epoch=188)-----------
2018-10-27 23:18:54,784 - 10000 samples (128 per mini-batch)
2018-10-27 23:18:55,505 - Epoch: [188][   50/   78]    Loss 0.444868    Top1 85.593750    Top5 99.468750    
2018-10-27 23:18:55,897 - ==> Top1: 85.690    Top5: 99.460    Loss: 0.439

2018-10-27 23:18:55,898 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:18:55,898 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:18:55,908 - 

2018-10-27 23:18:55,908 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:18:57,110 - Epoch: [189][   50/  391]    Overall Loss 0.275123    Objective Loss 0.275123    Top1 90.484375    Top5 99.812500    LR 0.030000    Time 0.023998    
2018-10-27 23:18:58,253 - Epoch: [189][  100/  391]    Overall Loss 0.279090    Objective Loss 0.279090    Top1 90.390625    Top5 99.773438    LR 0.030000    Time 0.023412    
2018-10-27 23:18:59,396 - Epoch: [189][  150/  391]    Overall Loss 0.285702    Objective Loss 0.285702    Top1 90.098958    Top5 99.729167    LR 0.030000    Time 0.023220    
2018-10-27 23:19:00,541 - Epoch: [189][  200/  391]    Overall Loss 0.285813    Objective Loss 0.285813    Top1 90.035156    Top5 99.753906    LR 0.030000    Time 0.023132    
2018-10-27 23:19:01,684 - Epoch: [189][  250/  391]    Overall Loss 0.284439    Objective Loss 0.284439    Top1 90.015625    Top5 99.775000    LR 0.030000    Time 0.023072    
2018-10-27 23:19:02,826 - Epoch: [189][  300/  391]    Overall Loss 0.284453    Objective Loss 0.284453    Top1 90.010417    Top5 99.789062    LR 0.030000    Time 0.023031    
2018-10-27 23:19:03,969 - Epoch: [189][  350/  391]    Overall Loss 0.285921    Objective Loss 0.285921    Top1 90.026786    Top5 99.794643    LR 0.030000    Time 0.023002    
2018-10-27 23:19:04,984 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33210 | -0.00349 |    0.12495 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10218 | -0.00345 |    0.02251 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09596 |  0.00083 |    0.02343 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09925 | -0.00255 |    0.02351 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07779 | -0.00181 |    0.01543 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10433 | -0.00391 |    0.02891 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07647 | -0.00134 |    0.01706 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10959 | -0.00361 |    0.03856 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08749 | -0.00202 |    0.02718 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12029 | -0.00632 |    0.03844 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07585 | -0.00115 |    0.02116 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06119 | -0.00071 |    0.01480 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08169 | -0.00189 |    0.02412 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06206 | -0.00324 |    0.01575 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07527 | -0.00116 |    0.02521 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07233 | -0.00283 |    0.02470 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06659 |  0.00045 |    0.01697 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06752 | -0.00175 |    0.02087 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05563 | -0.00178 |    0.01444 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05318 | -0.00013 |    0.01401 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03282 |  0.00132 |    0.00585 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51666 | -0.07324 |    0.25618 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:19:04,984 - Total sparsity: 88.62

2018-10-27 23:19:04,984 - --- validate (epoch=189)-----------
2018-10-27 23:19:04,984 - 10000 samples (128 per mini-batch)
2018-10-27 23:19:05,710 - Epoch: [189][   50/   78]    Loss 0.433439    Top1 85.906250    Top5 99.531250    
2018-10-27 23:19:06,092 - ==> Top1: 85.990    Top5: 99.550    Loss: 0.426

2018-10-27 23:19:06,092 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:19:06,092 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:19:06,109 - 

2018-10-27 23:19:06,109 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:19:07,283 - Epoch: [190][   50/  391]    Overall Loss 0.264094    Objective Loss 0.264094    Top1 90.812500    Top5 99.812500    LR 0.030000    Time 0.023442    
2018-10-27 23:19:08,426 - Epoch: [190][  100/  391]    Overall Loss 0.273197    Objective Loss 0.273197    Top1 90.164062    Top5 99.812500    LR 0.030000    Time 0.023143    
2018-10-27 23:19:09,568 - Epoch: [190][  150/  391]    Overall Loss 0.276076    Objective Loss 0.276076    Top1 90.166667    Top5 99.796875    LR 0.030000    Time 0.023031    
2018-10-27 23:19:10,709 - Epoch: [190][  200/  391]    Overall Loss 0.278597    Objective Loss 0.278597    Top1 90.214844    Top5 99.773438    LR 0.030000    Time 0.022973    
2018-10-27 23:19:11,851 - Epoch: [190][  250/  391]    Overall Loss 0.280281    Objective Loss 0.280281    Top1 90.153125    Top5 99.765625    LR 0.030000    Time 0.022939    
2018-10-27 23:19:12,991 - Epoch: [190][  300/  391]    Overall Loss 0.283298    Objective Loss 0.283298    Top1 90.028646    Top5 99.760417    LR 0.030000    Time 0.022913    
2018-10-27 23:19:14,132 - Epoch: [190][  350/  391]    Overall Loss 0.282395    Objective Loss 0.282395    Top1 90.084821    Top5 99.763393    LR 0.030000    Time 0.022895    
2018-10-27 23:19:15,149 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33191 | -0.00310 |    0.12491 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10212 | -0.00339 |    0.02264 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09588 |  0.00102 |    0.02338 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09927 | -0.00251 |    0.02357 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07769 | -0.00202 |    0.01542 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10429 | -0.00351 |    0.02896 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07641 | -0.00124 |    0.01709 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10952 | -0.00352 |    0.03854 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08743 | -0.00193 |    0.02712 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12017 | -0.00666 |    0.03835 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07579 | -0.00119 |    0.02116 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06116 | -0.00064 |    0.01477 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08164 | -0.00191 |    0.02412 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06202 | -0.00312 |    0.01577 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07522 | -0.00107 |    0.02517 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07230 | -0.00278 |    0.02470 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06651 |  0.00035 |    0.01696 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06750 | -0.00177 |    0.02085 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05560 | -0.00184 |    0.01440 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05317 | -0.00010 |    0.01402 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03279 |  0.00133 |    0.00584 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51732 | -0.07340 |    0.25647 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:19:15,149 - Total sparsity: 88.62

2018-10-27 23:19:15,149 - --- validate (epoch=190)-----------
2018-10-27 23:19:15,149 - 10000 samples (128 per mini-batch)
2018-10-27 23:19:15,875 - Epoch: [190][   50/   78]    Loss 0.471968    Top1 84.937500    Top5 99.453125    
2018-10-27 23:19:16,269 - ==> Top1: 84.860    Top5: 99.500    Loss: 0.469

2018-10-27 23:19:16,270 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:19:16,270 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:19:16,286 - 

2018-10-27 23:19:16,286 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:19:17,461 - Epoch: [191][   50/  391]    Overall Loss 0.272336    Objective Loss 0.272336    Top1 90.140625    Top5 99.796875    LR 0.030000    Time 0.023465    
2018-10-27 23:19:18,606 - Epoch: [191][  100/  391]    Overall Loss 0.269649    Objective Loss 0.269649    Top1 90.406250    Top5 99.820312    LR 0.030000    Time 0.023164    
2018-10-27 23:19:19,748 - Epoch: [191][  150/  391]    Overall Loss 0.270249    Objective Loss 0.270249    Top1 90.437500    Top5 99.828125    LR 0.030000    Time 0.023051    
2018-10-27 23:19:20,891 - Epoch: [191][  200/  391]    Overall Loss 0.275920    Objective Loss 0.275920    Top1 90.246094    Top5 99.816406    LR 0.030000    Time 0.022993    
2018-10-27 23:19:22,031 - Epoch: [191][  250/  391]    Overall Loss 0.276886    Objective Loss 0.276886    Top1 90.228125    Top5 99.825000    LR 0.030000    Time 0.022949    
2018-10-27 23:19:23,172 - Epoch: [191][  300/  391]    Overall Loss 0.280373    Objective Loss 0.280373    Top1 90.140625    Top5 99.820312    LR 0.030000    Time 0.022923    
2018-10-27 23:19:24,312 - Epoch: [191][  350/  391]    Overall Loss 0.283775    Objective Loss 0.283775    Top1 90.066964    Top5 99.794643    LR 0.030000    Time 0.022903    
2018-10-27 23:19:25,327 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33178 | -0.00281 |    0.12452 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10215 | -0.00349 |    0.02270 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09586 |  0.00104 |    0.02330 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09927 | -0.00222 |    0.02359 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07766 | -0.00179 |    0.01551 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10430 | -0.00355 |    0.02888 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07635 | -0.00114 |    0.01701 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10950 | -0.00354 |    0.03860 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08740 | -0.00192 |    0.02704 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.12002 | -0.00657 |    0.03840 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07574 | -0.00119 |    0.02108 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06113 | -0.00062 |    0.01474 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08163 | -0.00209 |    0.02416 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06199 | -0.00323 |    0.01576 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07520 | -0.00101 |    0.02516 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07229 | -0.00276 |    0.02468 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06644 |  0.00040 |    0.01691 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06748 | -0.00175 |    0.02084 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05560 | -0.00181 |    0.01437 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05317 | -0.00010 |    0.01402 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03277 |  0.00134 |    0.00584 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51669 | -0.07309 |    0.25593 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:19:25,327 - Total sparsity: 88.62

2018-10-27 23:19:25,327 - --- validate (epoch=191)-----------
2018-10-27 23:19:25,327 - 10000 samples (128 per mini-batch)
2018-10-27 23:19:26,043 - Epoch: [191][   50/   78]    Loss 0.455920    Top1 85.125000    Top5 99.437500    
2018-10-27 23:19:26,431 - ==> Top1: 85.440    Top5: 99.420    Loss: 0.454

2018-10-27 23:19:26,432 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:19:26,432 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:19:26,443 - 

2018-10-27 23:19:26,443 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:19:27,638 - Epoch: [192][   50/  391]    Overall Loss 0.274091    Objective Loss 0.274091    Top1 90.140625    Top5 99.796875    LR 0.030000    Time 0.023870    
2018-10-27 23:19:28,778 - Epoch: [192][  100/  391]    Overall Loss 0.281317    Objective Loss 0.281317    Top1 89.937500    Top5 99.804688    LR 0.030000    Time 0.023323    
2018-10-27 23:19:29,919 - Epoch: [192][  150/  391]    Overall Loss 0.283706    Objective Loss 0.283706    Top1 89.921875    Top5 99.786458    LR 0.030000    Time 0.023145    
2018-10-27 23:19:31,060 - Epoch: [192][  200/  391]    Overall Loss 0.287049    Objective Loss 0.287049    Top1 89.777344    Top5 99.777344    LR 0.030000    Time 0.023058    
2018-10-27 23:19:32,200 - Epoch: [192][  250/  391]    Overall Loss 0.288745    Objective Loss 0.288745    Top1 89.734375    Top5 99.756250    LR 0.030000    Time 0.023000    
2018-10-27 23:19:33,340 - Epoch: [192][  300/  391]    Overall Loss 0.287994    Objective Loss 0.287994    Top1 89.721354    Top5 99.778646    LR 0.030000    Time 0.022962    
2018-10-27 23:19:34,482 - Epoch: [192][  350/  391]    Overall Loss 0.285281    Objective Loss 0.285281    Top1 89.790179    Top5 99.781250    LR 0.030000    Time 0.022942    
2018-10-27 23:19:35,500 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33152 | -0.00362 |    0.12442 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10219 | -0.00351 |    0.02267 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09579 |  0.00119 |    0.02330 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09925 | -0.00245 |    0.02372 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07762 | -0.00182 |    0.01547 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10423 | -0.00394 |    0.02879 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07633 | -0.00124 |    0.01702 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10944 | -0.00314 |    0.03863 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08734 | -0.00197 |    0.02704 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11985 | -0.00655 |    0.03841 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07569 | -0.00111 |    0.02104 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06107 | -0.00063 |    0.01472 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08159 | -0.00197 |    0.02405 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06196 | -0.00317 |    0.01578 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07517 | -0.00095 |    0.02521 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07225 | -0.00282 |    0.02465 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06641 |  0.00042 |    0.01697 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06747 | -0.00169 |    0.02081 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05559 | -0.00180 |    0.01436 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05317 | -0.00009 |    0.01402 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03276 |  0.00130 |    0.00585 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51685 | -0.07331 |    0.25600 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:19:35,500 - Total sparsity: 88.62

2018-10-27 23:19:35,500 - --- validate (epoch=192)-----------
2018-10-27 23:19:35,500 - 10000 samples (128 per mini-batch)
2018-10-27 23:19:36,219 - Epoch: [192][   50/   78]    Loss 0.448676    Top1 85.437500    Top5 99.296875    
2018-10-27 23:19:36,609 - ==> Top1: 85.560    Top5: 99.390    Loss: 0.440

2018-10-27 23:19:36,610 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:19:36,610 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:19:36,620 - 

2018-10-27 23:19:36,620 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:19:37,818 - Epoch: [193][   50/  391]    Overall Loss 0.274318    Objective Loss 0.274318    Top1 90.390625    Top5 99.781250    LR 0.030000    Time 0.023927    
2018-10-27 23:19:38,960 - Epoch: [193][  100/  391]    Overall Loss 0.276900    Objective Loss 0.276900    Top1 90.242188    Top5 99.757812    LR 0.030000    Time 0.023361    
2018-10-27 23:19:40,103 - Epoch: [193][  150/  391]    Overall Loss 0.279274    Objective Loss 0.279274    Top1 90.046875    Top5 99.739583    LR 0.030000    Time 0.023189    
2018-10-27 23:19:41,245 - Epoch: [193][  200/  391]    Overall Loss 0.279881    Objective Loss 0.279881    Top1 90.097656    Top5 99.742188    LR 0.030000    Time 0.023097    
2018-10-27 23:19:42,387 - Epoch: [193][  250/  391]    Overall Loss 0.282661    Objective Loss 0.282661    Top1 90.021875    Top5 99.762500    LR 0.030000    Time 0.023040    
2018-10-27 23:19:43,529 - Epoch: [193][  300/  391]    Overall Loss 0.283053    Objective Loss 0.283053    Top1 90.010417    Top5 99.773438    LR 0.030000    Time 0.023001    
2018-10-27 23:19:44,672 - Epoch: [193][  350/  391]    Overall Loss 0.286811    Objective Loss 0.286811    Top1 89.875000    Top5 99.774554    LR 0.030000    Time 0.022977    
2018-10-27 23:19:45,690 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33154 | -0.00295 |    0.12427 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10219 | -0.00346 |    0.02269 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09579 |  0.00082 |    0.02326 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09921 | -0.00228 |    0.02368 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07761 | -0.00175 |    0.01536 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10424 | -0.00441 |    0.02874 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07634 | -0.00100 |    0.01694 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10941 | -0.00350 |    0.03858 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08733 | -0.00192 |    0.02701 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11985 | -0.00655 |    0.03814 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07570 | -0.00128 |    0.02101 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06108 | -0.00065 |    0.01475 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08162 | -0.00195 |    0.02415 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06198 | -0.00323 |    0.01582 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07516 | -0.00105 |    0.02522 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07225 | -0.00279 |    0.02470 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06635 |  0.00047 |    0.01694 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06748 | -0.00169 |    0.02080 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05561 | -0.00178 |    0.01438 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05320 | -0.00010 |    0.01403 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03276 |  0.00133 |    0.00584 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51588 | -0.07339 |    0.25563 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:19:45,690 - Total sparsity: 88.62

2018-10-27 23:19:45,690 - --- validate (epoch=193)-----------
2018-10-27 23:19:45,690 - 10000 samples (128 per mini-batch)
2018-10-27 23:19:46,413 - Epoch: [193][   50/   78]    Loss 0.444396    Top1 85.265625    Top5 99.390625    
2018-10-27 23:19:46,805 - ==> Top1: 85.300    Top5: 99.430    Loss: 0.446

2018-10-27 23:19:46,806 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:19:46,806 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:19:46,822 - 

2018-10-27 23:19:46,823 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:19:47,997 - Epoch: [194][   50/  391]    Overall Loss 0.270864    Objective Loss 0.270864    Top1 90.656250    Top5 99.796875    LR 0.030000    Time 0.023446    
2018-10-27 23:19:49,139 - Epoch: [194][  100/  391]    Overall Loss 0.276422    Objective Loss 0.276422    Top1 90.320312    Top5 99.765625    LR 0.030000    Time 0.023132    
2018-10-27 23:19:50,280 - Epoch: [194][  150/  391]    Overall Loss 0.279596    Objective Loss 0.279596    Top1 90.234375    Top5 99.734375    LR 0.030000    Time 0.023024    
2018-10-27 23:19:51,422 - Epoch: [194][  200/  391]    Overall Loss 0.284092    Objective Loss 0.284092    Top1 90.031250    Top5 99.707031    LR 0.030000    Time 0.022968    
2018-10-27 23:19:52,563 - Epoch: [194][  250/  391]    Overall Loss 0.284040    Objective Loss 0.284040    Top1 90.000000    Top5 99.718750    LR 0.030000    Time 0.022934    
2018-10-27 23:19:53,707 - Epoch: [194][  300/  391]    Overall Loss 0.285784    Objective Loss 0.285784    Top1 89.856771    Top5 99.736979    LR 0.030000    Time 0.022920    
2018-10-27 23:19:54,849 - Epoch: [194][  350/  391]    Overall Loss 0.284998    Objective Loss 0.284998    Top1 89.888393    Top5 99.743304    LR 0.030000    Time 0.022906    
2018-10-27 23:19:55,870 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33140 | -0.00352 |    0.12399 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10208 | -0.00335 |    0.02267 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09574 |  0.00105 |    0.02331 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09921 | -0.00209 |    0.02366 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07761 | -0.00185 |    0.01532 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10422 | -0.00383 |    0.02882 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07636 | -0.00097 |    0.01694 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10939 | -0.00286 |    0.03852 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08729 | -0.00205 |    0.02709 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11980 | -0.00696 |    0.03810 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07570 | -0.00116 |    0.02099 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06107 | -0.00072 |    0.01477 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08157 | -0.00191 |    0.02418 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06195 | -0.00323 |    0.01574 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07511 | -0.00117 |    0.02516 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07223 | -0.00277 |    0.02468 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06625 |  0.00039 |    0.01694 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06745 | -0.00168 |    0.02080 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05559 | -0.00183 |    0.01437 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05321 | -0.00015 |    0.01402 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03275 |  0.00134 |    0.00584 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51636 | -0.07318 |    0.25582 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:19:55,870 - Total sparsity: 88.62

2018-10-27 23:19:55,870 - --- validate (epoch=194)-----------
2018-10-27 23:19:55,870 - 10000 samples (128 per mini-batch)
2018-10-27 23:19:56,595 - Epoch: [194][   50/   78]    Loss 0.437643    Top1 86.031250    Top5 99.359375    
2018-10-27 23:19:56,985 - ==> Top1: 86.190    Top5: 99.430    Loss: 0.427

2018-10-27 23:19:56,985 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:19:56,986 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:19:57,002 - 

2018-10-27 23:19:57,002 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:19:58,174 - Epoch: [195][   50/  391]    Overall Loss 0.283651    Objective Loss 0.283651    Top1 90.093750    Top5 99.781250    LR 0.030000    Time 0.023406    
2018-10-27 23:19:59,315 - Epoch: [195][  100/  391]    Overall Loss 0.279273    Objective Loss 0.279273    Top1 90.171875    Top5 99.789062    LR 0.030000    Time 0.023101    
2018-10-27 23:20:00,460 - Epoch: [195][  150/  391]    Overall Loss 0.274702    Objective Loss 0.274702    Top1 90.333333    Top5 99.828125    LR 0.030000    Time 0.023022    
2018-10-27 23:20:01,602 - Epoch: [195][  200/  391]    Overall Loss 0.277128    Objective Loss 0.277128    Top1 90.351562    Top5 99.812500    LR 0.030000    Time 0.022971    
2018-10-27 23:20:02,743 - Epoch: [195][  250/  391]    Overall Loss 0.277871    Objective Loss 0.277871    Top1 90.212500    Top5 99.800000    LR 0.030000    Time 0.022936    
2018-10-27 23:20:03,885 - Epoch: [195][  300/  391]    Overall Loss 0.281928    Objective Loss 0.281928    Top1 90.059896    Top5 99.791667    LR 0.030000    Time 0.022915    
2018-10-27 23:20:05,027 - Epoch: [195][  350/  391]    Overall Loss 0.283173    Objective Loss 0.283173    Top1 90.029018    Top5 99.785714    LR 0.030000    Time 0.022901    
2018-10-27 23:20:06,042 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33096 | -0.00366 |    0.12390 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10205 | -0.00365 |    0.02255 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09572 |  0.00125 |    0.02325 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09918 | -0.00230 |    0.02369 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07757 | -0.00172 |    0.01536 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10422 | -0.00383 |    0.02892 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07638 | -0.00096 |    0.01704 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10934 | -0.00329 |    0.03840 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08727 | -0.00215 |    0.02700 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11988 | -0.00702 |    0.03819 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07569 | -0.00114 |    0.02101 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06106 | -0.00081 |    0.01479 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08156 | -0.00188 |    0.02417 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06194 | -0.00322 |    0.01572 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07510 | -0.00113 |    0.02521 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07223 | -0.00271 |    0.02468 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06616 |  0.00042 |    0.01692 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06746 | -0.00176 |    0.02084 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05560 | -0.00184 |    0.01438 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05323 | -0.00013 |    0.01402 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03274 |  0.00134 |    0.00583 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51623 | -0.07328 |    0.25566 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:20:06,042 - Total sparsity: 88.62

2018-10-27 23:20:06,042 - --- validate (epoch=195)-----------
2018-10-27 23:20:06,043 - 10000 samples (128 per mini-batch)
2018-10-27 23:20:06,763 - Epoch: [195][   50/   78]    Loss 0.487812    Top1 84.781250    Top5 99.125000    
2018-10-27 23:20:07,153 - ==> Top1: 84.860    Top5: 99.190    Loss: 0.482

2018-10-27 23:20:07,154 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:20:07,154 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:20:07,165 - 

2018-10-27 23:20:07,165 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:20:08,364 - Epoch: [196][   50/  391]    Overall Loss 0.268915    Objective Loss 0.268915    Top1 90.781250    Top5 99.781250    LR 0.030000    Time 0.023938    
2018-10-27 23:20:09,508 - Epoch: [196][  100/  391]    Overall Loss 0.273621    Objective Loss 0.273621    Top1 90.593750    Top5 99.828125    LR 0.030000    Time 0.023393    
2018-10-27 23:20:10,650 - Epoch: [196][  150/  391]    Overall Loss 0.284019    Objective Loss 0.284019    Top1 90.218750    Top5 99.770833    LR 0.030000    Time 0.023203    
2018-10-27 23:20:11,792 - Epoch: [196][  200/  391]    Overall Loss 0.282652    Objective Loss 0.282652    Top1 90.199219    Top5 99.777344    LR 0.030000    Time 0.023107    
2018-10-27 23:20:12,936 - Epoch: [196][  250/  391]    Overall Loss 0.283285    Objective Loss 0.283285    Top1 90.046875    Top5 99.784375    LR 0.030000    Time 0.023054    
2018-10-27 23:20:14,080 - Epoch: [196][  300/  391]    Overall Loss 0.283893    Objective Loss 0.283893    Top1 90.046875    Top5 99.786458    LR 0.030000    Time 0.023021    
2018-10-27 23:20:15,223 - Epoch: [196][  350/  391]    Overall Loss 0.283320    Objective Loss 0.283320    Top1 90.082589    Top5 99.774554    LR 0.030000    Time 0.022996    
2018-10-27 23:20:16,242 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33101 | -0.00384 |    0.12388 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10209 | -0.00348 |    0.02266 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09568 |  0.00099 |    0.02330 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09922 | -0.00248 |    0.02365 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07756 | -0.00186 |    0.01537 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10413 | -0.00413 |    0.02881 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07637 | -0.00114 |    0.01701 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10925 | -0.00320 |    0.03842 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08722 | -0.00215 |    0.02694 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11977 | -0.00703 |    0.03809 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07567 | -0.00109 |    0.02095 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06101 | -0.00079 |    0.01478 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08156 | -0.00189 |    0.02410 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06195 | -0.00325 |    0.01576 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07505 | -0.00111 |    0.02520 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07221 | -0.00268 |    0.02468 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06607 |  0.00034 |    0.01694 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06744 | -0.00180 |    0.02082 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05559 | -0.00179 |    0.01439 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05324 | -0.00010 |    0.01402 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03274 |  0.00133 |    0.00584 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51684 | -0.07320 |    0.25593 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:20:16,242 - Total sparsity: 88.62

2018-10-27 23:20:16,243 - --- validate (epoch=196)-----------
2018-10-27 23:20:16,243 - 10000 samples (128 per mini-batch)
2018-10-27 23:20:16,968 - Epoch: [196][   50/   78]    Loss 0.436553    Top1 85.593750    Top5 99.484375    
2018-10-27 23:20:17,361 - ==> Top1: 85.760    Top5: 99.540    Loss: 0.429

2018-10-27 23:20:17,361 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:20:17,361 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:20:17,372 - 

2018-10-27 23:20:17,373 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:20:18,572 - Epoch: [197][   50/  391]    Overall Loss 0.281751    Objective Loss 0.281751    Top1 90.140625    Top5 99.765625    LR 0.030000    Time 0.023949    
2018-10-27 23:20:19,712 - Epoch: [197][  100/  391]    Overall Loss 0.274913    Objective Loss 0.274913    Top1 90.187500    Top5 99.773438    LR 0.030000    Time 0.023359    
2018-10-27 23:20:20,852 - Epoch: [197][  150/  391]    Overall Loss 0.279502    Objective Loss 0.279502    Top1 90.031250    Top5 99.760417    LR 0.030000    Time 0.023168    
2018-10-27 23:20:21,992 - Epoch: [197][  200/  391]    Overall Loss 0.280348    Objective Loss 0.280348    Top1 89.984375    Top5 99.773438    LR 0.030000    Time 0.023071    
2018-10-27 23:20:23,135 - Epoch: [197][  250/  391]    Overall Loss 0.284317    Objective Loss 0.284317    Top1 89.775000    Top5 99.768750    LR 0.030000    Time 0.023022    
2018-10-27 23:20:24,277 - Epoch: [197][  300/  391]    Overall Loss 0.284639    Objective Loss 0.284639    Top1 89.750000    Top5 99.760417    LR 0.030000    Time 0.022988    
2018-10-27 23:20:25,418 - Epoch: [197][  350/  391]    Overall Loss 0.284794    Objective Loss 0.284794    Top1 89.729911    Top5 99.763393    LR 0.030000    Time 0.022961    
2018-10-27 23:20:26,436 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33106 | -0.00422 |    0.12396 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10208 | -0.00365 |    0.02268 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09566 |  0.00130 |    0.02342 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09926 | -0.00224 |    0.02364 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07755 | -0.00177 |    0.01536 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10415 | -0.00414 |    0.02895 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07637 | -0.00084 |    0.01710 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10918 | -0.00337 |    0.03847 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08720 | -0.00227 |    0.02698 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11974 | -0.00668 |    0.03823 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07565 | -0.00111 |    0.02099 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06098 | -0.00074 |    0.01477 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08156 | -0.00183 |    0.02408 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06195 | -0.00324 |    0.01579 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07502 | -0.00115 |    0.02516 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07220 | -0.00273 |    0.02471 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06601 |  0.00039 |    0.01689 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06744 | -0.00177 |    0.02082 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05559 | -0.00181 |    0.01438 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05324 | -0.00014 |    0.01402 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03272 |  0.00132 |    0.00584 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51640 | -0.07307 |    0.25572 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:20:26,436 - Total sparsity: 88.62

2018-10-27 23:20:26,436 - --- validate (epoch=197)-----------
2018-10-27 23:20:26,436 - 10000 samples (128 per mini-batch)
2018-10-27 23:20:27,169 - Epoch: [197][   50/   78]    Loss 0.462266    Top1 85.234375    Top5 99.328125    
2018-10-27 23:20:27,561 - ==> Top1: 85.320    Top5: 99.400    Loss: 0.459

2018-10-27 23:20:27,562 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:20:27,562 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:20:27,572 - 

2018-10-27 23:20:27,572 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:20:28,770 - Epoch: [198][   50/  391]    Overall Loss 0.286075    Objective Loss 0.286075    Top1 90.000000    Top5 99.734375    LR 0.030000    Time 0.023920    
2018-10-27 23:20:29,914 - Epoch: [198][  100/  391]    Overall Loss 0.285089    Objective Loss 0.285089    Top1 90.070312    Top5 99.757812    LR 0.030000    Time 0.023383    
2018-10-27 23:20:31,055 - Epoch: [198][  150/  391]    Overall Loss 0.282454    Objective Loss 0.282454    Top1 90.239583    Top5 99.750000    LR 0.030000    Time 0.023188    
2018-10-27 23:20:32,199 - Epoch: [198][  200/  391]    Overall Loss 0.277713    Objective Loss 0.277713    Top1 90.460938    Top5 99.765625    LR 0.030000    Time 0.023105    
2018-10-27 23:20:33,342 - Epoch: [198][  250/  391]    Overall Loss 0.279586    Objective Loss 0.279586    Top1 90.321875    Top5 99.756250    LR 0.030000    Time 0.023052    
2018-10-27 23:20:34,485 - Epoch: [198][  300/  391]    Overall Loss 0.281111    Objective Loss 0.281111    Top1 90.270833    Top5 99.768229    LR 0.030000    Time 0.023015    
2018-10-27 23:20:35,627 - Epoch: [198][  350/  391]    Overall Loss 0.282714    Objective Loss 0.282714    Top1 90.205357    Top5 99.776786    LR 0.030000    Time 0.022987    
2018-10-27 23:20:36,643 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33098 | -0.00210 |    0.12435 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10199 | -0.00356 |    0.02255 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09562 |  0.00121 |    0.02339 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09932 | -0.00217 |    0.02369 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07753 | -0.00187 |    0.01540 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10411 | -0.00404 |    0.02891 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07633 | -0.00097 |    0.01708 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10910 | -0.00316 |    0.03830 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08716 | -0.00232 |    0.02697 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11966 | -0.00670 |    0.03809 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07562 | -0.00103 |    0.02096 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06094 | -0.00072 |    0.01479 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08156 | -0.00175 |    0.02407 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06194 | -0.00328 |    0.01581 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07499 | -0.00106 |    0.02516 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07219 | -0.00275 |    0.02472 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06594 |  0.00034 |    0.01690 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06744 | -0.00181 |    0.02084 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05560 | -0.00184 |    0.01438 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05327 | -0.00016 |    0.01402 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03271 |  0.00131 |    0.00584 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51623 | -0.07294 |    0.25549 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:20:36,643 - Total sparsity: 88.62

2018-10-27 23:20:36,643 - --- validate (epoch=198)-----------
2018-10-27 23:20:36,644 - 10000 samples (128 per mini-batch)
2018-10-27 23:20:37,362 - Epoch: [198][   50/   78]    Loss 0.432742    Top1 85.718750    Top5 99.406250    
2018-10-27 23:20:37,753 - ==> Top1: 85.700    Top5: 99.450    Loss: 0.429

2018-10-27 23:20:37,754 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:20:37,754 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:20:37,773 - 

2018-10-27 23:20:37,774 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:20:38,948 - Epoch: [199][   50/  391]    Overall Loss 0.280115    Objective Loss 0.280115    Top1 89.968750    Top5 99.765625    LR 0.030000    Time 0.023444    
2018-10-27 23:20:40,089 - Epoch: [199][  100/  391]    Overall Loss 0.275176    Objective Loss 0.275176    Top1 90.132812    Top5 99.765625    LR 0.030000    Time 0.023124    
2018-10-27 23:20:41,232 - Epoch: [199][  150/  391]    Overall Loss 0.276743    Objective Loss 0.276743    Top1 90.026042    Top5 99.781250    LR 0.030000    Time 0.023024    
2018-10-27 23:20:42,375 - Epoch: [199][  200/  391]    Overall Loss 0.275904    Objective Loss 0.275904    Top1 90.074219    Top5 99.804688    LR 0.030000    Time 0.022975    
2018-10-27 23:20:43,518 - Epoch: [199][  250/  391]    Overall Loss 0.280031    Objective Loss 0.280031    Top1 90.028125    Top5 99.803125    LR 0.030000    Time 0.022945    
2018-10-27 23:20:44,660 - Epoch: [199][  300/  391]    Overall Loss 0.284392    Objective Loss 0.284392    Top1 89.880208    Top5 99.783854    LR 0.030000    Time 0.022924    
2018-10-27 23:20:45,803 - Epoch: [199][  350/  391]    Overall Loss 0.285078    Objective Loss 0.285078    Top1 89.879464    Top5 99.785714    LR 0.030000    Time 0.022911    
2018-10-27 23:20:46,824 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33088 | -0.00399 |    0.12402 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10200 | -0.00341 |    0.02267 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09560 |  0.00099 |    0.02345 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09938 | -0.00209 |    0.02373 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07754 | -0.00199 |    0.01542 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10404 | -0.00378 |    0.02889 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07628 | -0.00094 |    0.01710 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10903 | -0.00334 |    0.03831 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08710 | -0.00214 |    0.02693 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11963 | -0.00638 |    0.03805 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07560 | -0.00099 |    0.02093 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06090 | -0.00071 |    0.01477 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08152 | -0.00193 |    0.02409 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06192 | -0.00314 |    0.01581 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07499 | -0.00109 |    0.02516 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07220 | -0.00273 |    0.02471 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06588 |  0.00035 |    0.01687 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06745 | -0.00186 |    0.02083 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05561 | -0.00178 |    0.01439 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05329 | -0.00012 |    0.01402 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03270 |  0.00134 |    0.00584 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51572 | -0.07332 |    0.25537 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:20:46,824 - Total sparsity: 88.62

2018-10-27 23:20:46,824 - --- validate (epoch=199)-----------
2018-10-27 23:20:46,824 - 10000 samples (128 per mini-batch)
2018-10-27 23:20:47,553 - Epoch: [199][   50/   78]    Loss 0.441873    Top1 85.968750    Top5 99.359375    
2018-10-27 23:20:47,948 - ==> Top1: 85.780    Top5: 99.450    Loss: 0.444

2018-10-27 23:20:47,948 - ==> Best Top1: 87.300   On Epoch: 105

2018-10-27 23:20:47,948 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:20:47,965 - 

2018-10-27 23:20:47,965 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:20:49,139 - Epoch: [200][   50/  391]    Overall Loss 0.254150    Objective Loss 0.254150    Top1 90.953125    Top5 99.828125    LR 0.003000    Time 0.023446    
2018-10-27 23:20:50,280 - Epoch: [200][  100/  391]    Overall Loss 0.251052    Objective Loss 0.251052    Top1 91.242188    Top5 99.812500    LR 0.003000    Time 0.023115    
2018-10-27 23:20:51,422 - Epoch: [200][  150/  391]    Overall Loss 0.251184    Objective Loss 0.251184    Top1 91.255208    Top5 99.817708    LR 0.003000    Time 0.023017    
2018-10-27 23:20:52,564 - Epoch: [200][  200/  391]    Overall Loss 0.250925    Objective Loss 0.250925    Top1 91.230469    Top5 99.828125    LR 0.003000    Time 0.022965    
2018-10-27 23:20:53,705 - Epoch: [200][  250/  391]    Overall Loss 0.252054    Objective Loss 0.252054    Top1 91.243750    Top5 99.825000    LR 0.003000    Time 0.022934    
2018-10-27 23:20:54,847 - Epoch: [200][  300/  391]    Overall Loss 0.251080    Objective Loss 0.251080    Top1 91.234375    Top5 99.822917    LR 0.003000    Time 0.022912    
2018-10-27 23:20:55,989 - Epoch: [200][  350/  391]    Overall Loss 0.249859    Objective Loss 0.249859    Top1 91.287946    Top5 99.812500    LR 0.003000    Time 0.022900    
2018-10-27 23:20:57,002 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33055 | -0.00342 |    0.12383 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10190 | -0.00343 |    0.02262 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09550 |  0.00106 |    0.02339 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09928 | -0.00211 |    0.02370 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07746 | -0.00196 |    0.01540 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10393 | -0.00375 |    0.02888 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07620 | -0.00096 |    0.01710 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10892 | -0.00328 |    0.03826 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08701 | -0.00216 |    0.02691 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11951 | -0.00626 |    0.03798 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07552 | -0.00101 |    0.02092 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06084 | -0.00069 |    0.01476 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08143 | -0.00192 |    0.02406 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06185 | -0.00316 |    0.01579 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07491 | -0.00111 |    0.02514 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07212 | -0.00274 |    0.02469 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06581 |  0.00037 |    0.01686 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06738 | -0.00185 |    0.02081 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05555 | -0.00178 |    0.01437 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05323 | -0.00014 |    0.01401 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03267 |  0.00133 |    0.00583 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51645 | -0.07335 |    0.25572 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:20:57,003 - Total sparsity: 88.62

2018-10-27 23:20:57,003 - --- validate (epoch=200)-----------
2018-10-27 23:20:57,003 - 10000 samples (128 per mini-batch)
2018-10-27 23:20:57,721 - Epoch: [200][   50/   78]    Loss 0.377419    Top1 87.765625    Top5 99.453125    
2018-10-27 23:20:58,113 - ==> Top1: 87.790    Top5: 99.540    Loss: 0.377

2018-10-27 23:20:58,113 - ==> Best Top1: 87.790   On Epoch: 200

2018-10-27 23:20:58,113 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:20:58,129 - 

2018-10-27 23:20:58,129 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:20:59,299 - Epoch: [201][   50/  391]    Overall Loss 0.231453    Objective Loss 0.231453    Top1 92.375000    Top5 99.796875    LR 0.003000    Time 0.023367    
2018-10-27 23:21:00,444 - Epoch: [201][  100/  391]    Overall Loss 0.229758    Objective Loss 0.229758    Top1 92.468750    Top5 99.781250    LR 0.003000    Time 0.023115    
2018-10-27 23:21:01,588 - Epoch: [201][  150/  391]    Overall Loss 0.234210    Objective Loss 0.234210    Top1 92.093750    Top5 99.781250    LR 0.003000    Time 0.023029    
2018-10-27 23:21:02,727 - Epoch: [201][  200/  391]    Overall Loss 0.235586    Objective Loss 0.235586    Top1 91.910156    Top5 99.796875    LR 0.003000    Time 0.022961    
2018-10-27 23:21:03,872 - Epoch: [201][  250/  391]    Overall Loss 0.233855    Objective Loss 0.233855    Top1 91.893750    Top5 99.800000    LR 0.003000    Time 0.022941    
2018-10-27 23:21:05,013 - Epoch: [201][  300/  391]    Overall Loss 0.232784    Objective Loss 0.232784    Top1 91.861979    Top5 99.825521    LR 0.003000    Time 0.022919    
2018-10-27 23:21:06,157 - Epoch: [201][  350/  391]    Overall Loss 0.232871    Objective Loss 0.232871    Top1 91.859375    Top5 99.830357    LR 0.003000    Time 0.022910    
2018-10-27 23:21:07,173 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.33020 | -0.00304 |    0.12371 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10179 | -0.00344 |    0.02261 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09540 |  0.00105 |    0.02334 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09917 | -0.00214 |    0.02367 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07738 | -0.00197 |    0.01537 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10382 | -0.00377 |    0.02884 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07612 | -0.00096 |    0.01708 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10880 | -0.00332 |    0.03823 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08692 | -0.00218 |    0.02688 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11937 | -0.00634 |    0.03799 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07544 | -0.00104 |    0.02090 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06077 | -0.00068 |    0.01474 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08134 | -0.00191 |    0.02403 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06179 | -0.00316 |    0.01577 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07483 | -0.00111 |    0.02511 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07205 | -0.00275 |    0.02466 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06574 |  0.00035 |    0.01686 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06731 | -0.00186 |    0.02079 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05549 | -0.00178 |    0.01436 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05318 | -0.00015 |    0.01400 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03263 |  0.00132 |    0.00583 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51742 | -0.07336 |    0.25613 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:21:07,173 - Total sparsity: 88.62

2018-10-27 23:21:07,173 - --- validate (epoch=201)-----------
2018-10-27 23:21:07,173 - 10000 samples (128 per mini-batch)
2018-10-27 23:21:07,898 - Epoch: [201][   50/   78]    Loss 0.375018    Top1 87.656250    Top5 99.468750    
2018-10-27 23:21:08,288 - ==> Top1: 87.740    Top5: 99.530    Loss: 0.374

2018-10-27 23:21:08,289 - ==> Best Top1: 87.790   On Epoch: 200

2018-10-27 23:21:08,289 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:21:08,304 - 

2018-10-27 23:21:08,304 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:21:09,503 - Epoch: [202][   50/  391]    Overall Loss 0.229107    Objective Loss 0.229107    Top1 92.125000    Top5 99.828125    LR 0.003000    Time 0.023944    
2018-10-27 23:21:10,645 - Epoch: [202][  100/  391]    Overall Loss 0.227462    Objective Loss 0.227462    Top1 92.234375    Top5 99.859375    LR 0.003000    Time 0.023378    
2018-10-27 23:21:11,789 - Epoch: [202][  150/  391]    Overall Loss 0.227617    Objective Loss 0.227617    Top1 92.192708    Top5 99.864583    LR 0.003000    Time 0.023200    
2018-10-27 23:21:12,932 - Epoch: [202][  200/  391]    Overall Loss 0.228222    Objective Loss 0.228222    Top1 92.093750    Top5 99.867188    LR 0.003000    Time 0.023111    
2018-10-27 23:21:14,076 - Epoch: [202][  250/  391]    Overall Loss 0.229484    Objective Loss 0.229484    Top1 92.078125    Top5 99.865625    LR 0.003000    Time 0.023042    
2018-10-27 23:21:15,220 - Epoch: [202][  300/  391]    Overall Loss 0.228866    Objective Loss 0.228866    Top1 92.088542    Top5 99.859375    LR 0.003000    Time 0.023011    
2018-10-27 23:21:16,364 - Epoch: [202][  350/  391]    Overall Loss 0.229674    Objective Loss 0.229674    Top1 92.071429    Top5 99.857143    LR 0.003000    Time 0.022988    
2018-10-27 23:21:17,382 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32984 | -0.00301 |    0.12348 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10168 | -0.00344 |    0.02260 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09530 |  0.00105 |    0.02331 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09906 | -0.00214 |    0.02366 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07730 | -0.00198 |    0.01535 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10371 | -0.00373 |    0.02883 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07603 | -0.00097 |    0.01705 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10868 | -0.00331 |    0.03818 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08682 | -0.00218 |    0.02685 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11924 | -0.00636 |    0.03795 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07536 | -0.00105 |    0.02088 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06071 | -0.00068 |    0.01473 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08125 | -0.00194 |    0.02401 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06172 | -0.00317 |    0.01575 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07475 | -0.00111 |    0.02508 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07197 | -0.00276 |    0.02463 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06567 |  0.00036 |    0.01684 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06724 | -0.00185 |    0.02077 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05543 | -0.00178 |    0.01434 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05312 | -0.00016 |    0.01398 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03260 |  0.00131 |    0.00582 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51826 | -0.07341 |    0.25652 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:21:17,382 - Total sparsity: 88.62

2018-10-27 23:21:17,382 - --- validate (epoch=202)-----------
2018-10-27 23:21:17,383 - 10000 samples (128 per mini-batch)
2018-10-27 23:21:18,107 - Epoch: [202][   50/   78]    Loss 0.377063    Top1 87.765625    Top5 99.468750    
2018-10-27 23:21:18,498 - ==> Top1: 87.760    Top5: 99.550    Loss: 0.374

2018-10-27 23:21:18,499 - ==> Best Top1: 87.790   On Epoch: 200

2018-10-27 23:21:18,499 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:21:18,509 - 

2018-10-27 23:21:18,509 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:21:19,708 - Epoch: [203][   50/  391]    Overall Loss 0.232437    Objective Loss 0.232437    Top1 91.750000    Top5 99.843750    LR 0.003000    Time 0.023929    
2018-10-27 23:21:20,848 - Epoch: [203][  100/  391]    Overall Loss 0.233255    Objective Loss 0.233255    Top1 91.789062    Top5 99.843750    LR 0.003000    Time 0.023358    
2018-10-27 23:21:21,991 - Epoch: [203][  150/  391]    Overall Loss 0.230346    Objective Loss 0.230346    Top1 91.786458    Top5 99.859375    LR 0.003000    Time 0.023181    
2018-10-27 23:21:23,133 - Epoch: [203][  200/  391]    Overall Loss 0.227954    Objective Loss 0.227954    Top1 91.929688    Top5 99.871094    LR 0.003000    Time 0.023091    
2018-10-27 23:21:24,277 - Epoch: [203][  250/  391]    Overall Loss 0.228556    Objective Loss 0.228556    Top1 91.937500    Top5 99.865625    LR 0.003000    Time 0.023044    
2018-10-27 23:21:25,424 - Epoch: [203][  300/  391]    Overall Loss 0.229729    Objective Loss 0.229729    Top1 91.867188    Top5 99.872396    LR 0.003000    Time 0.023020    
2018-10-27 23:21:26,566 - Epoch: [203][  350/  391]    Overall Loss 0.229301    Objective Loss 0.229301    Top1 91.946429    Top5 99.859375    LR 0.003000    Time 0.022991    
2018-10-27 23:21:27,581 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32949 | -0.00284 |    0.12326 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10157 | -0.00340 |    0.02258 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09519 |  0.00104 |    0.02327 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09895 | -0.00218 |    0.02363 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07721 | -0.00194 |    0.01534 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10360 | -0.00368 |    0.02878 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07595 | -0.00095 |    0.01703 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10856 | -0.00329 |    0.03814 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08673 | -0.00217 |    0.02682 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11912 | -0.00630 |    0.03792 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07527 | -0.00108 |    0.02086 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06064 | -0.00068 |    0.01471 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08117 | -0.00195 |    0.02398 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06165 | -0.00316 |    0.01573 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07467 | -0.00110 |    0.02505 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07189 | -0.00275 |    0.02460 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06560 |  0.00037 |    0.01682 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06716 | -0.00184 |    0.02074 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05537 | -0.00178 |    0.01433 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05306 | -0.00016 |    0.01396 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03256 |  0.00131 |    0.00581 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51902 | -0.07344 |    0.25686 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:21:27,581 - Total sparsity: 88.62

2018-10-27 23:21:27,581 - --- validate (epoch=203)-----------
2018-10-27 23:21:27,581 - 10000 samples (128 per mini-batch)
2018-10-27 23:21:28,303 - Epoch: [203][   50/   78]    Loss 0.373061    Top1 87.890625    Top5 99.468750    
2018-10-27 23:21:28,693 - ==> Top1: 87.920    Top5: 99.530    Loss: 0.371

2018-10-27 23:21:28,694 - ==> Best Top1: 87.920   On Epoch: 203

2018-10-27 23:21:28,694 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:21:28,715 - 

2018-10-27 23:21:28,716 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:21:29,886 - Epoch: [204][   50/  391]    Overall Loss 0.217131    Objective Loss 0.217131    Top1 92.625000    Top5 99.812500    LR 0.003000    Time 0.023380    
2018-10-27 23:21:31,029 - Epoch: [204][  100/  391]    Overall Loss 0.222778    Objective Loss 0.222778    Top1 92.351562    Top5 99.835938    LR 0.003000    Time 0.023107    
2018-10-27 23:21:32,173 - Epoch: [204][  150/  391]    Overall Loss 0.223579    Objective Loss 0.223579    Top1 92.328125    Top5 99.859375    LR 0.003000    Time 0.023020    
2018-10-27 23:21:33,317 - Epoch: [204][  200/  391]    Overall Loss 0.225211    Objective Loss 0.225211    Top1 92.230469    Top5 99.847656    LR 0.003000    Time 0.022978    
2018-10-27 23:21:34,462 - Epoch: [204][  250/  391]    Overall Loss 0.226794    Objective Loss 0.226794    Top1 92.093750    Top5 99.868750    LR 0.003000    Time 0.022959    
2018-10-27 23:21:35,608 - Epoch: [204][  300/  391]    Overall Loss 0.226165    Objective Loss 0.226165    Top1 92.111979    Top5 99.867188    LR 0.003000    Time 0.022948    
2018-10-27 23:21:36,752 - Epoch: [204][  350/  391]    Overall Loss 0.226049    Objective Loss 0.226049    Top1 92.142857    Top5 99.868304    LR 0.003000    Time 0.022922    
2018-10-27 23:21:37,767 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32913 | -0.00312 |    0.12307 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10146 | -0.00341 |    0.02255 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09509 |  0.00105 |    0.02325 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09885 | -0.00213 |    0.02359 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07713 | -0.00194 |    0.01531 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10349 | -0.00368 |    0.02873 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07587 | -0.00093 |    0.01700 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10845 | -0.00327 |    0.03809 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08664 | -0.00219 |    0.02678 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11899 | -0.00637 |    0.03789 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07519 | -0.00107 |    0.02084 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06058 | -0.00067 |    0.01470 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08108 | -0.00193 |    0.02396 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06159 | -0.00318 |    0.01571 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07459 | -0.00109 |    0.02502 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07181 | -0.00276 |    0.02458 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06552 |  0.00034 |    0.01682 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06709 | -0.00183 |    0.02072 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05531 | -0.00178 |    0.01431 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05300 | -0.00018 |    0.01395 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03253 |  0.00130 |    0.00581 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.51980 | -0.07343 |    0.25725 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:21:37,767 - Total sparsity: 88.62

2018-10-27 23:21:37,767 - --- validate (epoch=204)-----------
2018-10-27 23:21:37,767 - 10000 samples (128 per mini-batch)
2018-10-27 23:21:38,491 - Epoch: [204][   50/   78]    Loss 0.373297    Top1 87.765625    Top5 99.453125    
2018-10-27 23:21:38,881 - ==> Top1: 87.870    Top5: 99.520    Loss: 0.369

2018-10-27 23:21:38,882 - ==> Best Top1: 87.920   On Epoch: 203

2018-10-27 23:21:38,882 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:21:38,899 - 

2018-10-27 23:21:38,899 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:21:40,071 - Epoch: [205][   50/  391]    Overall Loss 0.222599    Objective Loss 0.222599    Top1 92.281250    Top5 99.875000    LR 0.003000    Time 0.023404    
2018-10-27 23:21:41,213 - Epoch: [205][  100/  391]    Overall Loss 0.221726    Objective Loss 0.221726    Top1 92.398438    Top5 99.859375    LR 0.003000    Time 0.023105    
2018-10-27 23:21:42,352 - Epoch: [205][  150/  391]    Overall Loss 0.221752    Objective Loss 0.221752    Top1 92.473958    Top5 99.843750    LR 0.003000    Time 0.022990    
2018-10-27 23:21:43,495 - Epoch: [205][  200/  391]    Overall Loss 0.226098    Objective Loss 0.226098    Top1 92.285156    Top5 99.855469    LR 0.003000    Time 0.022953    
2018-10-27 23:21:44,639 - Epoch: [205][  250/  391]    Overall Loss 0.226704    Objective Loss 0.226704    Top1 92.190625    Top5 99.846875    LR 0.003000    Time 0.022931    
2018-10-27 23:21:45,784 - Epoch: [205][  300/  391]    Overall Loss 0.226273    Objective Loss 0.226273    Top1 92.226562    Top5 99.848958    LR 0.003000    Time 0.022922    
2018-10-27 23:21:46,928 - Epoch: [205][  350/  391]    Overall Loss 0.226119    Objective Loss 0.226119    Top1 92.263393    Top5 99.845982    LR 0.003000    Time 0.022912    
2018-10-27 23:21:47,943 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32878 | -0.00363 |    0.12298 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10135 | -0.00344 |    0.02252 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09499 |  0.00106 |    0.02321 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09874 | -0.00214 |    0.02356 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07705 | -0.00191 |    0.01530 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10338 | -0.00364 |    0.02871 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07579 | -0.00095 |    0.01699 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10833 | -0.00329 |    0.03804 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08654 | -0.00217 |    0.02676 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11886 | -0.00632 |    0.03783 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07511 | -0.00107 |    0.02081 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06051 | -0.00067 |    0.01469 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08099 | -0.00193 |    0.02392 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06152 | -0.00317 |    0.01569 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07451 | -0.00109 |    0.02499 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07174 | -0.00275 |    0.02455 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06545 |  0.00034 |    0.01680 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06702 | -0.00183 |    0.02070 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05525 | -0.00178 |    0.01430 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05295 | -0.00019 |    0.01393 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03249 |  0.00130 |    0.00580 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.52046 | -0.07346 |    0.25755 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:21:47,944 - Total sparsity: 88.62

2018-10-27 23:21:47,944 - --- validate (epoch=205)-----------
2018-10-27 23:21:47,944 - 10000 samples (128 per mini-batch)
2018-10-27 23:21:48,665 - Epoch: [205][   50/   78]    Loss 0.376280    Top1 87.500000    Top5 99.531250    
2018-10-27 23:21:49,055 - ==> Top1: 87.710    Top5: 99.540    Loss: 0.372

2018-10-27 23:21:49,056 - ==> Best Top1: 87.920   On Epoch: 203

2018-10-27 23:21:49,056 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:21:49,071 - 

2018-10-27 23:21:49,071 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:21:50,270 - Epoch: [206][   50/  391]    Overall Loss 0.223164    Objective Loss 0.223164    Top1 92.375000    Top5 99.828125    LR 0.003000    Time 0.023946    
2018-10-27 23:21:51,413 - Epoch: [206][  100/  391]    Overall Loss 0.221533    Objective Loss 0.221533    Top1 92.367188    Top5 99.835938    LR 0.003000    Time 0.023385    
2018-10-27 23:21:52,556 - Epoch: [206][  150/  391]    Overall Loss 0.219803    Objective Loss 0.219803    Top1 92.458333    Top5 99.838542    LR 0.003000    Time 0.023203    
2018-10-27 23:21:53,700 - Epoch: [206][  200/  391]    Overall Loss 0.218889    Objective Loss 0.218889    Top1 92.531250    Top5 99.832031    LR 0.003000    Time 0.023098    
2018-10-27 23:21:54,843 - Epoch: [206][  250/  391]    Overall Loss 0.220969    Objective Loss 0.220969    Top1 92.446875    Top5 99.834375    LR 0.003000    Time 0.023046    
2018-10-27 23:21:55,985 - Epoch: [206][  300/  391]    Overall Loss 0.219236    Objective Loss 0.219236    Top1 92.473958    Top5 99.848958    LR 0.003000    Time 0.023005    
2018-10-27 23:21:57,127 - Epoch: [206][  350/  391]    Overall Loss 0.219706    Objective Loss 0.219706    Top1 92.455357    Top5 99.845982    LR 0.003000    Time 0.022979    
2018-10-27 23:21:58,143 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32843 | -0.00317 |    0.12296 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10124 | -0.00344 |    0.02250 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09489 |  0.00105 |    0.02319 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09864 | -0.00218 |    0.02353 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07697 | -0.00193 |    0.01528 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10327 | -0.00367 |    0.02866 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07571 | -0.00092 |    0.01698 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10822 | -0.00325 |    0.03801 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08645 | -0.00219 |    0.02673 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11873 | -0.00634 |    0.03778 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07503 | -0.00109 |    0.02078 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06045 | -0.00067 |    0.01466 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08091 | -0.00194 |    0.02390 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06145 | -0.00318 |    0.01568 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07443 | -0.00109 |    0.02496 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07166 | -0.00275 |    0.02452 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06538 |  0.00034 |    0.01678 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06695 | -0.00183 |    0.02067 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05519 | -0.00178 |    0.01428 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05289 | -0.00020 |    0.01392 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03246 |  0.00129 |    0.00579 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.52122 | -0.07350 |    0.25789 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:21:58,144 - Total sparsity: 88.62

2018-10-27 23:21:58,144 - --- validate (epoch=206)-----------
2018-10-27 23:21:58,144 - 10000 samples (128 per mini-batch)
2018-10-27 23:21:58,863 - Epoch: [206][   50/   78]    Loss 0.375952    Top1 87.593750    Top5 99.515625    
2018-10-27 23:21:59,255 - ==> Top1: 87.770    Top5: 99.540    Loss: 0.371

2018-10-27 23:21:59,256 - ==> Best Top1: 87.920   On Epoch: 203

2018-10-27 23:21:59,256 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:21:59,267 - 

2018-10-27 23:21:59,267 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:22:00,469 - Epoch: [207][   50/  391]    Overall Loss 0.215028    Objective Loss 0.215028    Top1 92.671875    Top5 99.875000    LR 0.003000    Time 0.023990    
2018-10-27 23:22:01,610 - Epoch: [207][  100/  391]    Overall Loss 0.213456    Objective Loss 0.213456    Top1 92.804688    Top5 99.882812    LR 0.003000    Time 0.023393    
2018-10-27 23:22:02,753 - Epoch: [207][  150/  391]    Overall Loss 0.216874    Objective Loss 0.216874    Top1 92.567708    Top5 99.864583    LR 0.003000    Time 0.023207    
2018-10-27 23:22:03,895 - Epoch: [207][  200/  391]    Overall Loss 0.216926    Objective Loss 0.216926    Top1 92.582031    Top5 99.878906    LR 0.003000    Time 0.023109    
2018-10-27 23:22:05,036 - Epoch: [207][  250/  391]    Overall Loss 0.218217    Objective Loss 0.218217    Top1 92.453125    Top5 99.875000    LR 0.003000    Time 0.023032    
2018-10-27 23:22:06,181 - Epoch: [207][  300/  391]    Overall Loss 0.217005    Objective Loss 0.217005    Top1 92.445312    Top5 99.877604    LR 0.003000    Time 0.023006    
2018-10-27 23:22:07,323 - Epoch: [207][  350/  391]    Overall Loss 0.219069    Objective Loss 0.219069    Top1 92.361607    Top5 99.861607    LR 0.003000    Time 0.022980    
2018-10-27 23:22:08,342 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32808 | -0.00321 |    0.12270 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10114 | -0.00343 |    0.02247 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09479 |  0.00105 |    0.02315 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09853 | -0.00220 |    0.02351 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07689 | -0.00189 |    0.01526 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10316 | -0.00369 |    0.02861 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07563 | -0.00090 |    0.01696 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10810 | -0.00324 |    0.03797 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08636 | -0.00215 |    0.02670 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11861 | -0.00625 |    0.03772 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07495 | -0.00110 |    0.02076 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06038 | -0.00067 |    0.01465 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08082 | -0.00193 |    0.02387 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06139 | -0.00317 |    0.01566 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07435 | -0.00108 |    0.02493 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07158 | -0.00275 |    0.02450 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06531 |  0.00033 |    0.01676 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06688 | -0.00182 |    0.02065 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05513 | -0.00177 |    0.01427 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05283 | -0.00021 |    0.01390 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03242 |  0.00129 |    0.00579 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.52194 | -0.07350 |    0.25823 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:22:08,342 - Total sparsity: 88.62

2018-10-27 23:22:08,342 - --- validate (epoch=207)-----------
2018-10-27 23:22:08,343 - 10000 samples (128 per mini-batch)
2018-10-27 23:22:09,067 - Epoch: [207][   50/   78]    Loss 0.379273    Top1 87.734375    Top5 99.484375    
2018-10-27 23:22:09,457 - ==> Top1: 87.720    Top5: 99.530    Loss: 0.376

2018-10-27 23:22:09,458 - ==> Best Top1: 87.920   On Epoch: 203

2018-10-27 23:22:09,458 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:22:09,471 - 

2018-10-27 23:22:09,472 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:22:10,671 - Epoch: [208][   50/  391]    Overall Loss 0.214439    Objective Loss 0.214439    Top1 92.500000    Top5 99.843750    LR 0.003000    Time 0.023950    
2018-10-27 23:22:11,815 - Epoch: [208][  100/  391]    Overall Loss 0.215943    Objective Loss 0.215943    Top1 92.492188    Top5 99.851562    LR 0.003000    Time 0.023403    
2018-10-27 23:22:12,957 - Epoch: [208][  150/  391]    Overall Loss 0.217827    Objective Loss 0.217827    Top1 92.447917    Top5 99.864583    LR 0.003000    Time 0.023206    
2018-10-27 23:22:14,101 - Epoch: [208][  200/  391]    Overall Loss 0.216787    Objective Loss 0.216787    Top1 92.433594    Top5 99.871094    LR 0.003000    Time 0.023117    
2018-10-27 23:22:15,244 - Epoch: [208][  250/  391]    Overall Loss 0.217717    Objective Loss 0.217717    Top1 92.415625    Top5 99.862500    LR 0.003000    Time 0.023060    
2018-10-27 23:22:16,388 - Epoch: [208][  300/  391]    Overall Loss 0.219417    Objective Loss 0.219417    Top1 92.354167    Top5 99.869792    LR 0.003000    Time 0.023026    
2018-10-27 23:22:17,531 - Epoch: [208][  350/  391]    Overall Loss 0.220922    Objective Loss 0.220922    Top1 92.292411    Top5 99.861607    LR 0.003000    Time 0.022999    
2018-10-27 23:22:18,544 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32772 | -0.00334 |    0.12255 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10103 | -0.00346 |    0.02243 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09469 |  0.00102 |    0.02312 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09843 | -0.00217 |    0.02346 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07680 | -0.00191 |    0.01526 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10304 | -0.00372 |    0.02859 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07555 | -0.00097 |    0.01694 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10798 | -0.00327 |    0.03794 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08627 | -0.00217 |    0.02667 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11848 | -0.00616 |    0.03766 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07487 | -0.00111 |    0.02074 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06032 | -0.00067 |    0.01463 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08073 | -0.00192 |    0.02384 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06132 | -0.00318 |    0.01564 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07427 | -0.00109 |    0.02490 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07151 | -0.00275 |    0.02447 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06524 |  0.00035 |    0.01674 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06681 | -0.00182 |    0.02062 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05507 | -0.00177 |    0.01425 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05278 | -0.00021 |    0.01389 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03239 |  0.00128 |    0.00578 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.52261 | -0.07358 |    0.25854 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:22:18,544 - Total sparsity: 88.62

2018-10-27 23:22:18,544 - --- validate (epoch=208)-----------
2018-10-27 23:22:18,544 - 10000 samples (128 per mini-batch)
2018-10-27 23:22:19,265 - Epoch: [208][   50/   78]    Loss 0.376192    Top1 87.875000    Top5 99.500000    
2018-10-27 23:22:19,657 - ==> Top1: 87.930    Top5: 99.520    Loss: 0.371

2018-10-27 23:22:19,658 - ==> Best Top1: 87.930   On Epoch: 208

2018-10-27 23:22:19,658 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:22:19,679 - 

2018-10-27 23:22:19,679 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:22:20,848 - Epoch: [209][   50/  391]    Overall Loss 0.205855    Objective Loss 0.205855    Top1 92.671875    Top5 99.937500    LR 0.003000    Time 0.023348    
2018-10-27 23:22:21,988 - Epoch: [209][  100/  391]    Overall Loss 0.221352    Objective Loss 0.221352    Top1 92.203125    Top5 99.875000    LR 0.003000    Time 0.023058    
2018-10-27 23:22:23,131 - Epoch: [209][  150/  391]    Overall Loss 0.221662    Objective Loss 0.221662    Top1 92.276042    Top5 99.875000    LR 0.003000    Time 0.022981    
2018-10-27 23:22:24,274 - Epoch: [209][  200/  391]    Overall Loss 0.221684    Objective Loss 0.221684    Top1 92.320312    Top5 99.871094    LR 0.003000    Time 0.022944    
2018-10-27 23:22:25,417 - Epoch: [209][  250/  391]    Overall Loss 0.220486    Objective Loss 0.220486    Top1 92.384375    Top5 99.871875    LR 0.003000    Time 0.022926    
2018-10-27 23:22:26,560 - Epoch: [209][  300/  391]    Overall Loss 0.218674    Objective Loss 0.218674    Top1 92.442708    Top5 99.869792    LR 0.003000    Time 0.022909    
2018-10-27 23:22:27,702 - Epoch: [209][  350/  391]    Overall Loss 0.218879    Objective Loss 0.218879    Top1 92.473214    Top5 99.870536    LR 0.003000    Time 0.022885    
2018-10-27 23:22:28,717 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32738 | -0.00289 |    0.12243 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10092 | -0.00352 |    0.02241 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09459 |  0.00104 |    0.02310 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09832 | -0.00217 |    0.02343 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07672 | -0.00190 |    0.01524 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10294 | -0.00371 |    0.02857 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07547 | -0.00094 |    0.01692 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10787 | -0.00327 |    0.03789 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08617 | -0.00217 |    0.02663 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11836 | -0.00613 |    0.03760 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07479 | -0.00110 |    0.02072 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06026 | -0.00068 |    0.01461 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08065 | -0.00192 |    0.02381 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06126 | -0.00316 |    0.01562 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07419 | -0.00108 |    0.02488 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07143 | -0.00274 |    0.02444 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06517 |  0.00034 |    0.01672 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06674 | -0.00181 |    0.02059 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05502 | -0.00177 |    0.01423 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05272 | -0.00022 |    0.01387 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03235 |  0.00128 |    0.00577 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.52329 | -0.07359 |    0.25884 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:22:28,717 - Total sparsity: 88.62

2018-10-27 23:22:28,717 - --- validate (epoch=209)-----------
2018-10-27 23:22:28,717 - 10000 samples (128 per mini-batch)
2018-10-27 23:22:29,445 - Epoch: [209][   50/   78]    Loss 0.379421    Top1 87.828125    Top5 99.437500    
2018-10-27 23:22:29,837 - ==> Top1: 87.960    Top5: 99.500    Loss: 0.374

2018-10-27 23:22:29,837 - ==> Best Top1: 87.960   On Epoch: 209

2018-10-27 23:22:29,837 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:22:29,859 - 

2018-10-27 23:22:29,859 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:22:31,032 - Epoch: [210][   50/  391]    Overall Loss 0.215821    Objective Loss 0.215821    Top1 92.500000    Top5 99.875000    LR 0.003000    Time 0.023423    
2018-10-27 23:22:32,175 - Epoch: [210][  100/  391]    Overall Loss 0.211529    Objective Loss 0.211529    Top1 92.812500    Top5 99.875000    LR 0.003000    Time 0.023130    
2018-10-27 23:22:33,316 - Epoch: [210][  150/  391]    Overall Loss 0.215693    Objective Loss 0.215693    Top1 92.468750    Top5 99.875000    LR 0.003000    Time 0.023017    
2018-10-27 23:22:34,456 - Epoch: [210][  200/  391]    Overall Loss 0.209817    Objective Loss 0.209817    Top1 92.757812    Top5 99.890625    LR 0.003000    Time 0.022955    
2018-10-27 23:22:35,595 - Epoch: [210][  250/  391]    Overall Loss 0.211249    Objective Loss 0.211249    Top1 92.700000    Top5 99.890625    LR 0.003000    Time 0.022917    
2018-10-27 23:22:36,736 - Epoch: [210][  300/  391]    Overall Loss 0.213023    Objective Loss 0.213023    Top1 92.596354    Top5 99.877604    LR 0.003000    Time 0.022894    
2018-10-27 23:22:37,876 - Epoch: [210][  350/  391]    Overall Loss 0.212264    Objective Loss 0.212264    Top1 92.647321    Top5 99.875000    LR 0.003000    Time 0.022879    
2018-10-27 23:22:38,893 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32702 | -0.00350 |    0.12230 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10081 | -0.00352 |    0.02238 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09448 |  0.00104 |    0.02307 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09822 | -0.00220 |    0.02340 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07664 | -0.00189 |    0.01521 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10283 | -0.00367 |    0.02852 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07539 | -0.00093 |    0.01688 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10775 | -0.00323 |    0.03784 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08608 | -0.00217 |    0.02660 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11823 | -0.00613 |    0.03755 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07471 | -0.00109 |    0.02070 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06019 | -0.00068 |    0.01459 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08056 | -0.00193 |    0.02379 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06119 | -0.00315 |    0.01560 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07411 | -0.00109 |    0.02484 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07136 | -0.00273 |    0.02441 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06510 |  0.00034 |    0.01671 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06667 | -0.00180 |    0.02057 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05496 | -0.00176 |    0.01422 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05267 | -0.00023 |    0.01386 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03232 |  0.00128 |    0.00576 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.52407 | -0.07368 |    0.25923 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:22:38,893 - Total sparsity: 88.62

2018-10-27 23:22:38,893 - --- validate (epoch=210)-----------
2018-10-27 23:22:38,893 - 10000 samples (128 per mini-batch)
2018-10-27 23:22:39,613 - Epoch: [210][   50/   78]    Loss 0.377721    Top1 87.906250    Top5 99.484375    
2018-10-27 23:22:40,001 - ==> Top1: 87.920    Top5: 99.510    Loss: 0.372

2018-10-27 23:22:40,002 - ==> Best Top1: 87.960   On Epoch: 209

2018-10-27 23:22:40,003 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:22:40,017 - 

2018-10-27 23:22:40,017 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:22:41,218 - Epoch: [211][   50/  391]    Overall Loss 0.209057    Objective Loss 0.209057    Top1 92.531250    Top5 99.890625    LR 0.003000    Time 0.023969    
2018-10-27 23:22:42,361 - Epoch: [211][  100/  391]    Overall Loss 0.208720    Objective Loss 0.208720    Top1 92.664062    Top5 99.875000    LR 0.003000    Time 0.023400    
2018-10-27 23:22:43,504 - Epoch: [211][  150/  391]    Overall Loss 0.208094    Objective Loss 0.208094    Top1 92.640625    Top5 99.885417    LR 0.003000    Time 0.023212    
2018-10-27 23:22:44,644 - Epoch: [211][  200/  391]    Overall Loss 0.210163    Objective Loss 0.210163    Top1 92.566406    Top5 99.890625    LR 0.003000    Time 0.023101    
2018-10-27 23:22:45,784 - Epoch: [211][  250/  391]    Overall Loss 0.213629    Objective Loss 0.213629    Top1 92.446875    Top5 99.884375    LR 0.003000    Time 0.023037    
2018-10-27 23:22:46,923 - Epoch: [211][  300/  391]    Overall Loss 0.211678    Objective Loss 0.211678    Top1 92.460938    Top5 99.893229    LR 0.003000    Time 0.022990    
2018-10-27 23:22:48,063 - Epoch: [211][  350/  391]    Overall Loss 0.212887    Objective Loss 0.212887    Top1 92.410714    Top5 99.890625    LR 0.003000    Time 0.022961    
2018-10-27 23:22:49,080 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32668 | -0.00322 |    0.12222 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10070 | -0.00355 |    0.02236 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09438 |  0.00100 |    0.02303 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09811 | -0.00220 |    0.02338 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07656 | -0.00186 |    0.01519 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10272 | -0.00371 |    0.02848 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07531 | -0.00095 |    0.01687 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10764 | -0.00327 |    0.03780 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08599 | -0.00218 |    0.02658 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11810 | -0.00612 |    0.03748 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07464 | -0.00108 |    0.02068 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06013 | -0.00068 |    0.01457 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08048 | -0.00196 |    0.02376 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06113 | -0.00316 |    0.01559 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07403 | -0.00108 |    0.02482 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07128 | -0.00274 |    0.02438 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06503 |  0.00033 |    0.01669 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06660 | -0.00180 |    0.02054 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05490 | -0.00176 |    0.01420 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05261 | -0.00022 |    0.01384 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03229 |  0.00127 |    0.00576 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.52473 | -0.07368 |    0.25949 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:22:49,081 - Total sparsity: 88.62

2018-10-27 23:22:49,081 - --- validate (epoch=211)-----------
2018-10-27 23:22:49,081 - 10000 samples (128 per mini-batch)
2018-10-27 23:22:49,807 - Epoch: [211][   50/   78]    Loss 0.381612    Top1 87.609375    Top5 99.515625    
2018-10-27 23:22:50,199 - ==> Top1: 87.790    Top5: 99.560    Loss: 0.377

2018-10-27 23:22:50,200 - ==> Best Top1: 87.960   On Epoch: 209

2018-10-27 23:22:50,200 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:22:50,211 - 

2018-10-27 23:22:50,211 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:22:51,409 - Epoch: [212][   50/  391]    Overall Loss 0.218920    Objective Loss 0.218920    Top1 91.890625    Top5 99.921875    LR 0.003000    Time 0.023929    
2018-10-27 23:22:52,549 - Epoch: [212][  100/  391]    Overall Loss 0.216906    Objective Loss 0.216906    Top1 92.242188    Top5 99.921875    LR 0.003000    Time 0.023350    
2018-10-27 23:22:53,690 - Epoch: [212][  150/  391]    Overall Loss 0.217757    Objective Loss 0.217757    Top1 92.177083    Top5 99.906250    LR 0.003000    Time 0.023165    
2018-10-27 23:22:54,831 - Epoch: [212][  200/  391]    Overall Loss 0.218094    Objective Loss 0.218094    Top1 92.179688    Top5 99.886719    LR 0.003000    Time 0.023073    
2018-10-27 23:22:55,975 - Epoch: [212][  250/  391]    Overall Loss 0.214889    Objective Loss 0.214889    Top1 92.337500    Top5 99.881250    LR 0.003000    Time 0.023016    
2018-10-27 23:22:57,118 - Epoch: [212][  300/  391]    Overall Loss 0.213323    Objective Loss 0.213323    Top1 92.479167    Top5 99.869792    LR 0.003000    Time 0.022985    
2018-10-27 23:22:58,261 - Epoch: [212][  350/  391]    Overall Loss 0.215370    Objective Loss 0.215370    Top1 92.408482    Top5 99.863839    LR 0.003000    Time 0.022963    
2018-10-27 23:22:59,274 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32634 | -0.00278 |    0.12214 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10060 | -0.00354 |    0.02233 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09428 |  0.00100 |    0.02301 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09801 | -0.00223 |    0.02334 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07648 | -0.00187 |    0.01517 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10261 | -0.00373 |    0.02844 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07523 | -0.00095 |    0.01685 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10752 | -0.00329 |    0.03778 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08590 | -0.00217 |    0.02655 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11798 | -0.00612 |    0.03743 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07456 | -0.00108 |    0.02066 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06006 | -0.00067 |    0.01454 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08039 | -0.00196 |    0.02373 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06106 | -0.00315 |    0.01557 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07395 | -0.00107 |    0.02479 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07121 | -0.00273 |    0.02435 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06496 |  0.00033 |    0.01668 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06653 | -0.00179 |    0.02052 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05484 | -0.00175 |    0.01419 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05255 | -0.00023 |    0.01382 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03225 |  0.00126 |    0.00575 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.52534 | -0.07367 |    0.25977 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:22:59,274 - Total sparsity: 88.62

2018-10-27 23:22:59,275 - --- validate (epoch=212)-----------
2018-10-27 23:22:59,275 - 10000 samples (128 per mini-batch)
2018-10-27 23:22:59,981 - Epoch: [212][   50/   78]    Loss 0.379944    Top1 87.812500    Top5 99.531250    
2018-10-27 23:23:00,372 - ==> Top1: 87.970    Top5: 99.550    Loss: 0.375

2018-10-27 23:23:00,373 - ==> Best Top1: 87.970   On Epoch: 212

2018-10-27 23:23:00,373 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:23:00,387 - 

2018-10-27 23:23:00,388 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:23:01,597 - Epoch: [213][   50/  391]    Overall Loss 0.214340    Objective Loss 0.214340    Top1 92.359375    Top5 99.875000    LR 0.003000    Time 0.024158    
2018-10-27 23:23:02,769 - Epoch: [213][  100/  391]    Overall Loss 0.215806    Objective Loss 0.215806    Top1 92.445312    Top5 99.882812    LR 0.003000    Time 0.023787    
2018-10-27 23:23:03,939 - Epoch: [213][  150/  391]    Overall Loss 0.215839    Objective Loss 0.215839    Top1 92.505208    Top5 99.880208    LR 0.003000    Time 0.023645    
2018-10-27 23:23:05,112 - Epoch: [213][  200/  391]    Overall Loss 0.215886    Objective Loss 0.215886    Top1 92.527344    Top5 99.867188    LR 0.003000    Time 0.023594    
2018-10-27 23:23:06,292 - Epoch: [213][  250/  391]    Overall Loss 0.216681    Objective Loss 0.216681    Top1 92.515625    Top5 99.865625    LR 0.003000    Time 0.023588    
2018-10-27 23:23:07,465 - Epoch: [213][  300/  391]    Overall Loss 0.215571    Objective Loss 0.215571    Top1 92.544271    Top5 99.875000    LR 0.003000    Time 0.023564    
2018-10-27 23:23:08,637 - Epoch: [213][  350/  391]    Overall Loss 0.214957    Objective Loss 0.214957    Top1 92.524554    Top5 99.866071    LR 0.003000    Time 0.023541    
2018-10-27 23:23:09,676 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32600 | -0.00274 |    0.12196 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10049 | -0.00358 |    0.02231 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09418 |  0.00099 |    0.02300 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09790 | -0.00223 |    0.02331 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07640 | -0.00189 |    0.01515 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10250 | -0.00368 |    0.02842 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07515 | -0.00094 |    0.01682 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10741 | -0.00329 |    0.03774 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08581 | -0.00217 |    0.02652 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11785 | -0.00612 |    0.03742 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07448 | -0.00109 |    0.02064 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.06000 | -0.00068 |    0.01453 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08031 | -0.00195 |    0.02370 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06100 | -0.00316 |    0.01555 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07387 | -0.00108 |    0.02476 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07113 | -0.00273 |    0.02433 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06489 |  0.00033 |    0.01664 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06646 | -0.00178 |    0.02049 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05478 | -0.00175 |    0.01417 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05250 | -0.00023 |    0.01381 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03222 |  0.00126 |    0.00575 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.52601 | -0.07372 |    0.26011 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:23:09,676 - Total sparsity: 88.62

2018-10-27 23:23:09,676 - --- validate (epoch=213)-----------
2018-10-27 23:23:09,676 - 10000 samples (128 per mini-batch)
2018-10-27 23:23:10,401 - Epoch: [213][   50/   78]    Loss 0.380369    Top1 87.859375    Top5 99.531250    
2018-10-27 23:23:10,793 - ==> Top1: 87.940    Top5: 99.610    Loss: 0.376

2018-10-27 23:23:10,794 - ==> Best Top1: 87.970   On Epoch: 212

2018-10-27 23:23:10,794 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:23:10,811 - 

2018-10-27 23:23:10,811 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:23:12,013 - Epoch: [214][   50/  391]    Overall Loss 0.210254    Objective Loss 0.210254    Top1 92.687500    Top5 99.843750    LR 0.003000    Time 0.023996    
2018-10-27 23:23:13,184 - Epoch: [214][  100/  391]    Overall Loss 0.211521    Objective Loss 0.211521    Top1 92.609375    Top5 99.828125    LR 0.003000    Time 0.023691    
2018-10-27 23:23:14,359 - Epoch: [214][  150/  391]    Overall Loss 0.213862    Objective Loss 0.213862    Top1 92.645833    Top5 99.807292    LR 0.003000    Time 0.023622    
2018-10-27 23:23:15,535 - Epoch: [214][  200/  391]    Overall Loss 0.213650    Objective Loss 0.213650    Top1 92.589844    Top5 99.839844    LR 0.003000    Time 0.023587    
2018-10-27 23:23:16,711 - Epoch: [214][  250/  391]    Overall Loss 0.214805    Objective Loss 0.214805    Top1 92.581250    Top5 99.828125    LR 0.003000    Time 0.023570    
2018-10-27 23:23:17,887 - Epoch: [214][  300/  391]    Overall Loss 0.212970    Objective Loss 0.212970    Top1 92.614583    Top5 99.838542    LR 0.003000    Time 0.023555    
2018-10-27 23:23:19,057 - Epoch: [214][  350/  391]    Overall Loss 0.213059    Objective Loss 0.213059    Top1 92.618304    Top5 99.841518    LR 0.003000    Time 0.023531    
2018-10-27 23:23:20,099 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32565 | -0.00306 |    0.12182 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10038 | -0.00355 |    0.02230 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09408 |  0.00100 |    0.02296 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09780 | -0.00224 |    0.02328 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07632 | -0.00189 |    0.01510 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10239 | -0.00365 |    0.02840 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07507 | -0.00092 |    0.01680 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10730 | -0.00330 |    0.03772 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08572 | -0.00216 |    0.02649 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11773 | -0.00605 |    0.03734 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07440 | -0.00108 |    0.02062 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05994 | -0.00067 |    0.01451 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08022 | -0.00194 |    0.02368 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06093 | -0.00315 |    0.01554 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07380 | -0.00107 |    0.02473 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07106 | -0.00273 |    0.02430 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06482 |  0.00033 |    0.01662 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06639 | -0.00177 |    0.02046 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05473 | -0.00174 |    0.01416 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05244 | -0.00024 |    0.01379 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03218 |  0.00126 |    0.00574 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.52659 | -0.07376 |    0.26038 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:23:20,099 - Total sparsity: 88.62

2018-10-27 23:23:20,100 - --- validate (epoch=214)-----------
2018-10-27 23:23:20,100 - 10000 samples (128 per mini-batch)
2018-10-27 23:23:20,830 - Epoch: [214][   50/   78]    Loss 0.381759    Top1 87.875000    Top5 99.531250    
2018-10-27 23:23:21,225 - ==> Top1: 87.950    Top5: 99.540    Loss: 0.378

2018-10-27 23:23:21,226 - ==> Best Top1: 87.970   On Epoch: 212

2018-10-27 23:23:21,226 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:23:21,240 - 

2018-10-27 23:23:21,240 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:23:22,467 - Epoch: [215][   50/  391]    Overall Loss 0.223356    Objective Loss 0.223356    Top1 92.218750    Top5 99.890625    LR 0.003000    Time 0.024496    
2018-10-27 23:23:23,642 - Epoch: [215][  100/  391]    Overall Loss 0.226202    Objective Loss 0.226202    Top1 92.132812    Top5 99.875000    LR 0.003000    Time 0.023981    
2018-10-27 23:23:24,813 - Epoch: [215][  150/  391]    Overall Loss 0.218942    Objective Loss 0.218942    Top1 92.442708    Top5 99.885417    LR 0.003000    Time 0.023784    
2018-10-27 23:23:25,985 - Epoch: [215][  200/  391]    Overall Loss 0.218243    Objective Loss 0.218243    Top1 92.480469    Top5 99.882812    LR 0.003000    Time 0.023692    
2018-10-27 23:23:27,156 - Epoch: [215][  250/  391]    Overall Loss 0.217481    Objective Loss 0.217481    Top1 92.412500    Top5 99.878125    LR 0.003000    Time 0.023634    
2018-10-27 23:23:28,332 - Epoch: [215][  300/  391]    Overall Loss 0.215869    Objective Loss 0.215869    Top1 92.489583    Top5 99.869792    LR 0.003000    Time 0.023608    
2018-10-27 23:23:29,509 - Epoch: [215][  350/  391]    Overall Loss 0.214223    Objective Loss 0.214223    Top1 92.569196    Top5 99.861607    LR 0.003000    Time 0.023596    
2018-10-27 23:23:30,553 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32532 | -0.00253 |    0.12181 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10027 | -0.00361 |    0.02227 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09398 |  0.00100 |    0.02294 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09769 | -0.00228 |    0.02327 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07624 | -0.00184 |    0.01509 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10228 | -0.00369 |    0.02836 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07499 | -0.00093 |    0.01677 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10718 | -0.00331 |    0.03766 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08563 | -0.00216 |    0.02646 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11760 | -0.00613 |    0.03727 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07432 | -0.00110 |    0.02060 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05987 | -0.00067 |    0.01449 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08014 | -0.00195 |    0.02364 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06087 | -0.00316 |    0.01552 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07372 | -0.00107 |    0.02470 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07098 | -0.00271 |    0.02427 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06475 |  0.00033 |    0.01661 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06632 | -0.00176 |    0.02044 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05467 | -0.00174 |    0.01414 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05239 | -0.00025 |    0.01378 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03215 |  0.00125 |    0.00573 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.52710 | -0.07379 |    0.26061 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:23:30,554 - Total sparsity: 88.62

2018-10-27 23:23:30,554 - --- validate (epoch=215)-----------
2018-10-27 23:23:30,554 - 10000 samples (128 per mini-batch)
2018-10-27 23:23:31,284 - Epoch: [215][   50/   78]    Loss 0.382358    Top1 87.890625    Top5 99.515625    
2018-10-27 23:23:31,682 - ==> Top1: 87.960    Top5: 99.580    Loss: 0.379

2018-10-27 23:23:31,682 - ==> Best Top1: 87.970   On Epoch: 212

2018-10-27 23:23:31,682 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:23:31,699 - 

2018-10-27 23:23:31,699 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:23:32,903 - Epoch: [216][   50/  391]    Overall Loss 0.209761    Objective Loss 0.209761    Top1 92.937500    Top5 99.828125    LR 0.003000    Time 0.024051    
2018-10-27 23:23:34,075 - Epoch: [216][  100/  391]    Overall Loss 0.206411    Objective Loss 0.206411    Top1 92.734375    Top5 99.882812    LR 0.003000    Time 0.023723    
2018-10-27 23:23:35,249 - Epoch: [216][  150/  391]    Overall Loss 0.207901    Objective Loss 0.207901    Top1 92.635417    Top5 99.869792    LR 0.003000    Time 0.023633    
2018-10-27 23:23:36,425 - Epoch: [216][  200/  391]    Overall Loss 0.211530    Objective Loss 0.211530    Top1 92.550781    Top5 99.863281    LR 0.003000    Time 0.023600    
2018-10-27 23:23:37,595 - Epoch: [216][  250/  391]    Overall Loss 0.211095    Objective Loss 0.211095    Top1 92.596875    Top5 99.846875    LR 0.003000    Time 0.023556    
2018-10-27 23:23:38,766 - Epoch: [216][  300/  391]    Overall Loss 0.211188    Objective Loss 0.211188    Top1 92.593750    Top5 99.851562    LR 0.003000    Time 0.023528    
2018-10-27 23:23:39,935 - Epoch: [216][  350/  391]    Overall Loss 0.211427    Objective Loss 0.211427    Top1 92.629464    Top5 99.857143    LR 0.003000    Time 0.023501    
2018-10-27 23:23:40,980 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32497 | -0.00295 |    0.12168 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10017 | -0.00356 |    0.02226 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09389 |  0.00098 |    0.02289 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09759 | -0.00228 |    0.02324 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07616 | -0.00187 |    0.01507 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10218 | -0.00365 |    0.02835 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07491 | -0.00091 |    0.01675 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10707 | -0.00331 |    0.03762 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08554 | -0.00217 |    0.02643 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11747 | -0.00612 |    0.03719 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07424 | -0.00110 |    0.02058 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05981 | -0.00068 |    0.01448 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.08005 | -0.00193 |    0.02362 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06080 | -0.00316 |    0.01551 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07364 | -0.00107 |    0.02467 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07091 | -0.00271 |    0.02424 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06468 |  0.00033 |    0.01659 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06625 | -0.00175 |    0.02041 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05461 | -0.00173 |    0.01413 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05233 | -0.00026 |    0.01376 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03211 |  0.00125 |    0.00573 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.52766 | -0.07385 |    0.26084 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:23:40,981 - Total sparsity: 88.62

2018-10-27 23:23:40,981 - --- validate (epoch=216)-----------
2018-10-27 23:23:40,981 - 10000 samples (128 per mini-batch)
2018-10-27 23:23:41,705 - Epoch: [216][   50/   78]    Loss 0.385186    Top1 87.843750    Top5 99.531250    
2018-10-27 23:23:42,099 - ==> Top1: 87.970    Top5: 99.560    Loss: 0.383

2018-10-27 23:23:42,100 - ==> Best Top1: 87.970   On Epoch: 212

2018-10-27 23:23:42,100 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:23:42,114 - 

2018-10-27 23:23:42,115 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:23:43,346 - Epoch: [217][   50/  391]    Overall Loss 0.201281    Objective Loss 0.201281    Top1 92.968750    Top5 99.875000    LR 0.003000    Time 0.024584    
2018-10-27 23:23:44,518 - Epoch: [217][  100/  391]    Overall Loss 0.201452    Objective Loss 0.201452    Top1 92.882812    Top5 99.906250    LR 0.003000    Time 0.023995    
2018-10-27 23:23:45,692 - Epoch: [217][  150/  391]    Overall Loss 0.207377    Objective Loss 0.207377    Top1 92.708333    Top5 99.911458    LR 0.003000    Time 0.023818    
2018-10-27 23:23:46,862 - Epoch: [217][  200/  391]    Overall Loss 0.210266    Objective Loss 0.210266    Top1 92.664062    Top5 99.902344    LR 0.003000    Time 0.023704    
2018-10-27 23:23:48,033 - Epoch: [217][  250/  391]    Overall Loss 0.209838    Objective Loss 0.209838    Top1 92.609375    Top5 99.884375    LR 0.003000    Time 0.023644    
2018-10-27 23:23:49,209 - Epoch: [217][  300/  391]    Overall Loss 0.211410    Objective Loss 0.211410    Top1 92.562500    Top5 99.869792    LR 0.003000    Time 0.023619    
2018-10-27 23:23:50,382 - Epoch: [217][  350/  391]    Overall Loss 0.212662    Objective Loss 0.212662    Top1 92.533482    Top5 99.863839    LR 0.003000    Time 0.023593    
2018-10-27 23:23:51,424 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32463 | -0.00276 |    0.12142 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.10007 | -0.00352 |    0.02222 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09379 |  0.00100 |    0.02287 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09749 | -0.00230 |    0.02322 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07607 | -0.00190 |    0.01505 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10207 | -0.00364 |    0.02828 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07483 | -0.00092 |    0.01673 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10695 | -0.00332 |    0.03758 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08545 | -0.00215 |    0.02640 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11734 | -0.00616 |    0.03716 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07417 | -0.00110 |    0.02055 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05975 | -0.00067 |    0.01446 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07997 | -0.00193 |    0.02360 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06074 | -0.00314 |    0.01549 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07356 | -0.00108 |    0.02464 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07083 | -0.00270 |    0.02421 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06461 |  0.00033 |    0.01658 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06618 | -0.00175 |    0.02039 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05455 | -0.00174 |    0.01411 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05228 | -0.00027 |    0.01374 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03208 |  0.00125 |    0.00572 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.52818 | -0.07391 |    0.26110 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:23:51,425 - Total sparsity: 88.62

2018-10-27 23:23:51,425 - --- validate (epoch=217)-----------
2018-10-27 23:23:51,425 - 10000 samples (128 per mini-batch)
2018-10-27 23:23:52,151 - Epoch: [217][   50/   78]    Loss 0.382595    Top1 87.875000    Top5 99.500000    
2018-10-27 23:23:52,546 - ==> Top1: 87.940    Top5: 99.550    Loss: 0.377

2018-10-27 23:23:52,546 - ==> Best Top1: 87.970   On Epoch: 212

2018-10-27 23:23:52,546 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:23:52,557 - 

2018-10-27 23:23:52,558 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:23:53,786 - Epoch: [218][   50/  391]    Overall Loss 0.206977    Objective Loss 0.206977    Top1 92.468750    Top5 99.921875    LR 0.003000    Time 0.024522    
2018-10-27 23:23:54,959 - Epoch: [218][  100/  391]    Overall Loss 0.214196    Objective Loss 0.214196    Top1 92.343750    Top5 99.890625    LR 0.003000    Time 0.023977    
2018-10-27 23:23:56,129 - Epoch: [218][  150/  391]    Overall Loss 0.214523    Objective Loss 0.214523    Top1 92.442708    Top5 99.906250    LR 0.003000    Time 0.023778    
2018-10-27 23:23:57,305 - Epoch: [218][  200/  391]    Overall Loss 0.212546    Objective Loss 0.212546    Top1 92.480469    Top5 99.894531    LR 0.003000    Time 0.023709    
2018-10-27 23:23:58,477 - Epoch: [218][  250/  391]    Overall Loss 0.212068    Objective Loss 0.212068    Top1 92.509375    Top5 99.893750    LR 0.003000    Time 0.023648    
2018-10-27 23:23:59,653 - Epoch: [218][  300/  391]    Overall Loss 0.213398    Objective Loss 0.213398    Top1 92.471354    Top5 99.885417    LR 0.003000    Time 0.023623    
2018-10-27 23:24:00,830 - Epoch: [218][  350/  391]    Overall Loss 0.213273    Objective Loss 0.213273    Top1 92.486607    Top5 99.886161    LR 0.003000    Time 0.023607    
2018-10-27 23:24:01,876 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32429 | -0.00262 |    0.12140 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09996 | -0.00355 |    0.02219 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09369 |  0.00101 |    0.02285 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09739 | -0.00232 |    0.02319 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07600 | -0.00188 |    0.01504 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10196 | -0.00361 |    0.02822 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07475 | -0.00095 |    0.01673 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10684 | -0.00327 |    0.03753 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08536 | -0.00214 |    0.02636 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11722 | -0.00620 |    0.03712 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07409 | -0.00112 |    0.02053 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05969 | -0.00067 |    0.01444 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07989 | -0.00194 |    0.02357 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06068 | -0.00315 |    0.01546 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07348 | -0.00105 |    0.02461 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07076 | -0.00269 |    0.02418 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06454 |  0.00033 |    0.01657 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06611 | -0.00174 |    0.02036 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05450 | -0.00173 |    0.01410 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05222 | -0.00027 |    0.01373 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03205 |  0.00124 |    0.00571 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.52872 | -0.07392 |    0.26133 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:24:01,876 - Total sparsity: 88.62

2018-10-27 23:24:01,876 - --- validate (epoch=218)-----------
2018-10-27 23:24:01,876 - 10000 samples (128 per mini-batch)
2018-10-27 23:24:02,608 - Epoch: [218][   50/   78]    Loss 0.386203    Top1 88.031250    Top5 99.500000    
2018-10-27 23:24:03,007 - ==> Top1: 88.120    Top5: 99.560    Loss: 0.380

2018-10-27 23:24:03,008 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:24:03,008 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:24:03,021 - 

2018-10-27 23:24:03,021 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:24:04,249 - Epoch: [219][   50/  391]    Overall Loss 0.212835    Objective Loss 0.212835    Top1 92.531250    Top5 99.765625    LR 0.003000    Time 0.024503    
2018-10-27 23:24:05,422 - Epoch: [219][  100/  391]    Overall Loss 0.218532    Objective Loss 0.218532    Top1 92.250000    Top5 99.804688    LR 0.003000    Time 0.023966    
2018-10-27 23:24:06,591 - Epoch: [219][  150/  391]    Overall Loss 0.219076    Objective Loss 0.219076    Top1 92.270833    Top5 99.807292    LR 0.003000    Time 0.023766    
2018-10-27 23:24:07,767 - Epoch: [219][  200/  391]    Overall Loss 0.216130    Objective Loss 0.216130    Top1 92.355469    Top5 99.824219    LR 0.003000    Time 0.023696    
2018-10-27 23:24:08,940 - Epoch: [219][  250/  391]    Overall Loss 0.216203    Objective Loss 0.216203    Top1 92.428125    Top5 99.828125    LR 0.003000    Time 0.023643    
2018-10-27 23:24:10,113 - Epoch: [219][  300/  391]    Overall Loss 0.216299    Objective Loss 0.216299    Top1 92.492188    Top5 99.830729    LR 0.003000    Time 0.023607    
2018-10-27 23:24:11,286 - Epoch: [219][  350/  391]    Overall Loss 0.213626    Objective Loss 0.213626    Top1 92.535714    Top5 99.843750    LR 0.003000    Time 0.023584    
2018-10-27 23:24:12,325 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32394 | -0.00357 |    0.12115 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09986 | -0.00354 |    0.02215 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09359 |  0.00102 |    0.02281 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09729 | -0.00225 |    0.02315 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07592 | -0.00189 |    0.01503 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10186 | -0.00358 |    0.02821 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07468 | -0.00094 |    0.01671 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10673 | -0.00327 |    0.03748 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08527 | -0.00215 |    0.02633 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11709 | -0.00613 |    0.03711 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07401 | -0.00112 |    0.02051 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05962 | -0.00067 |    0.01443 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07980 | -0.00194 |    0.02354 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06061 | -0.00315 |    0.01544 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07341 | -0.00106 |    0.02458 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07068 | -0.00269 |    0.02415 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06447 |  0.00034 |    0.01654 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06604 | -0.00173 |    0.02034 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05444 | -0.00171 |    0.01408 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05217 | -0.00027 |    0.01371 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03201 |  0.00123 |    0.00571 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.52916 | -0.07391 |    0.26154 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:24:12,326 - Total sparsity: 88.62

2018-10-27 23:24:12,326 - --- validate (epoch=219)-----------
2018-10-27 23:24:12,326 - 10000 samples (128 per mini-batch)
2018-10-27 23:24:13,063 - Epoch: [219][   50/   78]    Loss 0.383347    Top1 87.812500    Top5 99.515625    
2018-10-27 23:24:13,461 - ==> Top1: 87.950    Top5: 99.540    Loss: 0.378

2018-10-27 23:24:13,462 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:24:13,462 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:24:13,479 - 

2018-10-27 23:24:13,479 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:24:14,683 - Epoch: [220][   50/  391]    Overall Loss 0.205095    Objective Loss 0.205095    Top1 93.062500    Top5 99.890625    LR 0.003000    Time 0.024052    
2018-10-27 23:24:15,858 - Epoch: [220][  100/  391]    Overall Loss 0.207968    Objective Loss 0.207968    Top1 92.796875    Top5 99.867188    LR 0.003000    Time 0.023757    
2018-10-27 23:24:17,033 - Epoch: [220][  150/  391]    Overall Loss 0.210911    Objective Loss 0.210911    Top1 92.677083    Top5 99.854167    LR 0.003000    Time 0.023666    
2018-10-27 23:24:18,211 - Epoch: [220][  200/  391]    Overall Loss 0.210881    Objective Loss 0.210881    Top1 92.523438    Top5 99.875000    LR 0.003000    Time 0.023630    
2018-10-27 23:24:19,388 - Epoch: [220][  250/  391]    Overall Loss 0.209093    Objective Loss 0.209093    Top1 92.640625    Top5 99.875000    LR 0.003000    Time 0.023606    
2018-10-27 23:24:20,560 - Epoch: [220][  300/  391]    Overall Loss 0.207615    Objective Loss 0.207615    Top1 92.747396    Top5 99.882812    LR 0.003000    Time 0.023574    
2018-10-27 23:24:21,728 - Epoch: [220][  350/  391]    Overall Loss 0.207415    Objective Loss 0.207415    Top1 92.734375    Top5 99.892857    LR 0.003000    Time 0.023542    
2018-10-27 23:24:22,774 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32361 | -0.00313 |    0.12107 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09975 | -0.00353 |    0.02212 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09349 |  0.00104 |    0.02279 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09718 | -0.00227 |    0.02313 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07583 | -0.00191 |    0.01502 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10175 | -0.00359 |    0.02817 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07460 | -0.00090 |    0.01668 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10662 | -0.00325 |    0.03745 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08518 | -0.00213 |    0.02630 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11697 | -0.00608 |    0.03707 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07393 | -0.00112 |    0.02048 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05956 | -0.00065 |    0.01441 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07972 | -0.00193 |    0.02352 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06055 | -0.00314 |    0.01543 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07333 | -0.00105 |    0.02456 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07061 | -0.00268 |    0.02412 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06440 |  0.00036 |    0.01652 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06597 | -0.00172 |    0.02031 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05438 | -0.00172 |    0.01407 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05211 | -0.00027 |    0.01369 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03198 |  0.00123 |    0.00570 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.52978 | -0.07395 |    0.26183 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:24:22,774 - Total sparsity: 88.62

2018-10-27 23:24:22,774 - --- validate (epoch=220)-----------
2018-10-27 23:24:22,774 - 10000 samples (128 per mini-batch)
2018-10-27 23:24:23,536 - Epoch: [220][   50/   78]    Loss 0.386164    Top1 87.734375    Top5 99.484375    
2018-10-27 23:24:23,929 - ==> Top1: 87.900    Top5: 99.530    Loss: 0.379

2018-10-27 23:24:23,930 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:24:23,930 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:24:23,940 - 

2018-10-27 23:24:23,940 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:24:25,168 - Epoch: [221][   50/  391]    Overall Loss 0.205979    Objective Loss 0.205979    Top1 92.625000    Top5 99.875000    LR 0.003000    Time 0.024527    
2018-10-27 23:24:26,342 - Epoch: [221][  100/  391]    Overall Loss 0.202793    Objective Loss 0.202793    Top1 92.867188    Top5 99.875000    LR 0.003000    Time 0.023987    
2018-10-27 23:24:27,513 - Epoch: [221][  150/  391]    Overall Loss 0.205362    Objective Loss 0.205362    Top1 92.776042    Top5 99.880208    LR 0.003000    Time 0.023787    
2018-10-27 23:24:28,682 - Epoch: [221][  200/  391]    Overall Loss 0.207554    Objective Loss 0.207554    Top1 92.664062    Top5 99.875000    LR 0.003000    Time 0.023681    
2018-10-27 23:24:29,852 - Epoch: [221][  250/  391]    Overall Loss 0.208955    Objective Loss 0.208955    Top1 92.653125    Top5 99.881250    LR 0.003000    Time 0.023620    
2018-10-27 23:24:31,029 - Epoch: [221][  300/  391]    Overall Loss 0.208573    Objective Loss 0.208573    Top1 92.679688    Top5 99.875000    LR 0.003000    Time 0.023600    
2018-10-27 23:24:32,200 - Epoch: [221][  350/  391]    Overall Loss 0.208425    Objective Loss 0.208425    Top1 92.736607    Top5 99.877232    LR 0.003000    Time 0.023570    
2018-10-27 23:24:33,247 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32327 | -0.00284 |    0.12094 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09965 | -0.00355 |    0.02211 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09339 |  0.00102 |    0.02276 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09708 | -0.00228 |    0.02312 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07576 | -0.00190 |    0.01500 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10165 | -0.00354 |    0.02812 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07452 | -0.00091 |    0.01665 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10651 | -0.00327 |    0.03739 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08509 | -0.00212 |    0.02627 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11685 | -0.00613 |    0.03701 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07386 | -0.00111 |    0.02045 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05950 | -0.00065 |    0.01439 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07964 | -0.00193 |    0.02350 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06049 | -0.00314 |    0.01541 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07325 | -0.00105 |    0.02453 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07054 | -0.00267 |    0.02410 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06433 |  0.00036 |    0.01651 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06591 | -0.00172 |    0.02029 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05433 | -0.00172 |    0.01405 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05206 | -0.00028 |    0.01368 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03194 |  0.00123 |    0.00569 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53026 | -0.07393 |    0.26202 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:24:33,248 - Total sparsity: 88.62

2018-10-27 23:24:33,248 - --- validate (epoch=221)-----------
2018-10-27 23:24:33,248 - 10000 samples (128 per mini-batch)
2018-10-27 23:24:33,977 - Epoch: [221][   50/   78]    Loss 0.386139    Top1 87.640625    Top5 99.500000    
2018-10-27 23:24:34,372 - ==> Top1: 87.800    Top5: 99.560    Loss: 0.378

2018-10-27 23:24:34,372 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:24:34,372 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:24:34,383 - 

2018-10-27 23:24:34,384 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:24:35,612 - Epoch: [222][   50/  391]    Overall Loss 0.214283    Objective Loss 0.214283    Top1 92.875000    Top5 99.906250    LR 0.003000    Time 0.024528    
2018-10-27 23:24:36,785 - Epoch: [222][  100/  391]    Overall Loss 0.206680    Objective Loss 0.206680    Top1 93.171875    Top5 99.890625    LR 0.003000    Time 0.023983    
2018-10-27 23:24:37,962 - Epoch: [222][  150/  391]    Overall Loss 0.213888    Objective Loss 0.213888    Top1 92.875000    Top5 99.890625    LR 0.003000    Time 0.023825    
2018-10-27 23:24:39,132 - Epoch: [222][  200/  391]    Overall Loss 0.209093    Objective Loss 0.209093    Top1 92.988281    Top5 99.890625    LR 0.003000    Time 0.023715    
2018-10-27 23:24:40,303 - Epoch: [222][  250/  391]    Overall Loss 0.207706    Objective Loss 0.207706    Top1 92.993750    Top5 99.893750    LR 0.003000    Time 0.023650    
2018-10-27 23:24:41,478 - Epoch: [222][  300/  391]    Overall Loss 0.208800    Objective Loss 0.208800    Top1 92.924479    Top5 99.890625    LR 0.003000    Time 0.023618    
2018-10-27 23:24:42,650 - Epoch: [222][  350/  391]    Overall Loss 0.210120    Objective Loss 0.210120    Top1 92.843750    Top5 99.888393    LR 0.003000    Time 0.023589    
2018-10-27 23:24:43,699 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32293 | -0.00308 |    0.12097 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09954 | -0.00359 |    0.02208 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09329 |  0.00103 |    0.02272 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09698 | -0.00228 |    0.02309 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07568 | -0.00192 |    0.01499 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10154 | -0.00353 |    0.02806 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07444 | -0.00088 |    0.01665 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10640 | -0.00326 |    0.03734 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08500 | -0.00212 |    0.02623 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11672 | -0.00613 |    0.03701 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07378 | -0.00112 |    0.02043 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05944 | -0.00063 |    0.01439 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07955 | -0.00191 |    0.02346 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06042 | -0.00313 |    0.01539 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07318 | -0.00105 |    0.02449 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07046 | -0.00265 |    0.02407 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06427 |  0.00035 |    0.01649 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06584 | -0.00171 |    0.02027 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05427 | -0.00171 |    0.01403 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05201 | -0.00027 |    0.01366 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03191 |  0.00123 |    0.00569 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53077 | -0.07396 |    0.26225 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:24:43,699 - Total sparsity: 88.62

2018-10-27 23:24:43,699 - --- validate (epoch=222)-----------
2018-10-27 23:24:43,699 - 10000 samples (128 per mini-batch)
2018-10-27 23:24:44,435 - Epoch: [222][   50/   78]    Loss 0.386616    Top1 87.640625    Top5 99.546875    
2018-10-27 23:24:44,827 - ==> Top1: 87.770    Top5: 99.590    Loss: 0.381

2018-10-27 23:24:44,828 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:24:44,828 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:24:44,836 - 

2018-10-27 23:24:44,837 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:24:46,066 - Epoch: [223][   50/  391]    Overall Loss 0.210951    Objective Loss 0.210951    Top1 92.390625    Top5 99.843750    LR 0.003000    Time 0.024556    
2018-10-27 23:24:47,236 - Epoch: [223][  100/  391]    Overall Loss 0.209277    Objective Loss 0.209277    Top1 92.460938    Top5 99.890625    LR 0.003000    Time 0.023955    
2018-10-27 23:24:48,407 - Epoch: [223][  150/  391]    Overall Loss 0.212614    Objective Loss 0.212614    Top1 92.416667    Top5 99.875000    LR 0.003000    Time 0.023768    
2018-10-27 23:24:49,578 - Epoch: [223][  200/  391]    Overall Loss 0.212272    Objective Loss 0.212272    Top1 92.441406    Top5 99.863281    LR 0.003000    Time 0.023676    
2018-10-27 23:24:50,750 - Epoch: [223][  250/  391]    Overall Loss 0.209063    Objective Loss 0.209063    Top1 92.631250    Top5 99.871875    LR 0.003000    Time 0.023622    
2018-10-27 23:24:51,922 - Epoch: [223][  300/  391]    Overall Loss 0.210014    Objective Loss 0.210014    Top1 92.638021    Top5 99.875000    LR 0.003000    Time 0.023588    
2018-10-27 23:24:53,093 - Epoch: [223][  350/  391]    Overall Loss 0.212517    Objective Loss 0.212517    Top1 92.560268    Top5 99.875000    LR 0.003000    Time 0.023560    
2018-10-27 23:24:54,132 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32260 | -0.00315 |    0.12086 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09944 | -0.00357 |    0.02205 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09320 |  0.00096 |    0.02270 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09688 | -0.00227 |    0.02307 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07560 | -0.00192 |    0.01497 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10143 | -0.00356 |    0.02802 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07437 | -0.00091 |    0.01663 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10628 | -0.00327 |    0.03730 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08491 | -0.00213 |    0.02619 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11660 | -0.00608 |    0.03694 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07370 | -0.00112 |    0.02041 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05938 | -0.00063 |    0.01436 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07947 | -0.00191 |    0.02345 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06036 | -0.00315 |    0.01536 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07310 | -0.00106 |    0.02447 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07039 | -0.00265 |    0.02404 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06420 |  0.00035 |    0.01648 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06577 | -0.00170 |    0.02024 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05421 | -0.00171 |    0.01401 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05195 | -0.00028 |    0.01365 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03188 |  0.00122 |    0.00568 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53123 | -0.07403 |    0.26248 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:24:54,132 - Total sparsity: 88.62

2018-10-27 23:24:54,132 - --- validate (epoch=223)-----------
2018-10-27 23:24:54,132 - 10000 samples (128 per mini-batch)
2018-10-27 23:24:54,861 - Epoch: [223][   50/   78]    Loss 0.385913    Top1 87.531250    Top5 99.500000    
2018-10-27 23:24:55,255 - ==> Top1: 87.690    Top5: 99.560    Loss: 0.378

2018-10-27 23:24:55,256 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:24:55,256 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:24:55,265 - 

2018-10-27 23:24:55,265 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:24:56,494 - Epoch: [224][   50/  391]    Overall Loss 0.218787    Objective Loss 0.218787    Top1 92.546875    Top5 99.859375    LR 0.003000    Time 0.024549    
2018-10-27 23:24:57,666 - Epoch: [224][  100/  391]    Overall Loss 0.213938    Objective Loss 0.213938    Top1 92.632812    Top5 99.867188    LR 0.003000    Time 0.023979    
2018-10-27 23:24:58,839 - Epoch: [224][  150/  391]    Overall Loss 0.209120    Objective Loss 0.209120    Top1 92.760417    Top5 99.864583    LR 0.003000    Time 0.023795    
2018-10-27 23:25:00,014 - Epoch: [224][  200/  391]    Overall Loss 0.208517    Objective Loss 0.208517    Top1 92.769531    Top5 99.855469    LR 0.003000    Time 0.023714    
2018-10-27 23:25:01,190 - Epoch: [224][  250/  391]    Overall Loss 0.206630    Objective Loss 0.206630    Top1 92.881250    Top5 99.859375    LR 0.003000    Time 0.023673    
2018-10-27 23:25:02,363 - Epoch: [224][  300/  391]    Overall Loss 0.208991    Objective Loss 0.208991    Top1 92.742188    Top5 99.835938    LR 0.003000    Time 0.023631    
2018-10-27 23:25:03,536 - Epoch: [224][  350/  391]    Overall Loss 0.208666    Objective Loss 0.208666    Top1 92.732143    Top5 99.839286    LR 0.003000    Time 0.023603    
2018-10-27 23:25:04,577 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32226 | -0.00336 |    0.12069 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09934 | -0.00354 |    0.02202 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09310 |  0.00095 |    0.02267 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09678 | -0.00224 |    0.02305 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07552 | -0.00193 |    0.01494 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10133 | -0.00353 |    0.02797 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07429 | -0.00090 |    0.01660 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10617 | -0.00325 |    0.03726 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08482 | -0.00212 |    0.02615 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11647 | -0.00614 |    0.03694 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07363 | -0.00112 |    0.02038 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05931 | -0.00064 |    0.01435 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07939 | -0.00190 |    0.02342 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06030 | -0.00312 |    0.01534 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07302 | -0.00105 |    0.02443 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07032 | -0.00264 |    0.02401 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06413 |  0.00031 |    0.01645 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06570 | -0.00170 |    0.02022 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05416 | -0.00171 |    0.01400 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05190 | -0.00028 |    0.01363 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03184 |  0.00122 |    0.00568 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53178 | -0.07402 |    0.26272 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:25:04,577 - Total sparsity: 88.62

2018-10-27 23:25:04,577 - --- validate (epoch=224)-----------
2018-10-27 23:25:04,577 - 10000 samples (128 per mini-batch)
2018-10-27 23:25:05,308 - Epoch: [224][   50/   78]    Loss 0.386834    Top1 87.625000    Top5 99.484375    
2018-10-27 23:25:05,705 - ==> Top1: 87.780    Top5: 99.520    Loss: 0.380

2018-10-27 23:25:05,705 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:25:05,705 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:25:05,716 - 

2018-10-27 23:25:05,717 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:25:06,945 - Epoch: [225][   50/  391]    Overall Loss 0.203409    Objective Loss 0.203409    Top1 92.765625    Top5 99.843750    LR 0.003000    Time 0.024537    
2018-10-27 23:25:08,119 - Epoch: [225][  100/  391]    Overall Loss 0.205420    Objective Loss 0.205420    Top1 92.828125    Top5 99.867188    LR 0.003000    Time 0.023993    
2018-10-27 23:25:09,290 - Epoch: [225][  150/  391]    Overall Loss 0.210862    Objective Loss 0.210862    Top1 92.588542    Top5 99.859375    LR 0.003000    Time 0.023793    
2018-10-27 23:25:10,463 - Epoch: [225][  200/  391]    Overall Loss 0.210400    Objective Loss 0.210400    Top1 92.714844    Top5 99.847656    LR 0.003000    Time 0.023704    
2018-10-27 23:25:11,636 - Epoch: [225][  250/  391]    Overall Loss 0.209678    Objective Loss 0.209678    Top1 92.715625    Top5 99.843750    LR 0.003000    Time 0.023650    
2018-10-27 23:25:12,810 - Epoch: [225][  300/  391]    Overall Loss 0.210266    Objective Loss 0.210266    Top1 92.713542    Top5 99.848958    LR 0.003000    Time 0.023615    
2018-10-27 23:25:13,981 - Epoch: [225][  350/  391]    Overall Loss 0.209786    Objective Loss 0.209786    Top1 92.691964    Top5 99.859375    LR 0.003000    Time 0.023584    
2018-10-27 23:25:15,023 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32193 | -0.00324 |    0.12057 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09923 | -0.00354 |    0.02198 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09300 |  0.00095 |    0.02263 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09668 | -0.00223 |    0.02299 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07544 | -0.00192 |    0.01492 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10122 | -0.00348 |    0.02796 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07421 | -0.00088 |    0.01657 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10606 | -0.00322 |    0.03723 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08473 | -0.00212 |    0.02613 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11635 | -0.00615 |    0.03689 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07355 | -0.00110 |    0.02037 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05925 | -0.00064 |    0.01433 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07931 | -0.00190 |    0.02340 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06024 | -0.00311 |    0.01532 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07295 | -0.00106 |    0.02441 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07024 | -0.00265 |    0.02399 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06406 |  0.00032 |    0.01644 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06563 | -0.00169 |    0.02020 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05410 | -0.00170 |    0.01398 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05184 | -0.00029 |    0.01362 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03181 |  0.00122 |    0.00567 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53223 | -0.07402 |    0.26293 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:25:15,023 - Total sparsity: 88.62

2018-10-27 23:25:15,023 - --- validate (epoch=225)-----------
2018-10-27 23:25:15,023 - 10000 samples (128 per mini-batch)
2018-10-27 23:25:15,752 - Epoch: [225][   50/   78]    Loss 0.386291    Top1 87.750000    Top5 99.500000    
2018-10-27 23:25:16,153 - ==> Top1: 87.970    Top5: 99.510    Loss: 0.382

2018-10-27 23:25:16,154 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:25:16,154 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:25:16,164 - 

2018-10-27 23:25:16,164 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:25:17,396 - Epoch: [226][   50/  391]    Overall Loss 0.208558    Objective Loss 0.208558    Top1 92.843750    Top5 99.875000    LR 0.003000    Time 0.024601    
2018-10-27 23:25:18,568 - Epoch: [226][  100/  391]    Overall Loss 0.210122    Objective Loss 0.210122    Top1 92.695312    Top5 99.859375    LR 0.003000    Time 0.024005    
2018-10-27 23:25:19,744 - Epoch: [226][  150/  391]    Overall Loss 0.213050    Objective Loss 0.213050    Top1 92.494792    Top5 99.864583    LR 0.003000    Time 0.023835    
2018-10-27 23:25:20,920 - Epoch: [226][  200/  391]    Overall Loss 0.210524    Objective Loss 0.210524    Top1 92.664062    Top5 99.859375    LR 0.003000    Time 0.023749    
2018-10-27 23:25:22,095 - Epoch: [226][  250/  391]    Overall Loss 0.211017    Objective Loss 0.211017    Top1 92.650000    Top5 99.862500    LR 0.003000    Time 0.023695    
2018-10-27 23:25:23,267 - Epoch: [226][  300/  391]    Overall Loss 0.210743    Objective Loss 0.210743    Top1 92.625000    Top5 99.875000    LR 0.003000    Time 0.023646    
2018-10-27 23:25:24,441 - Epoch: [226][  350/  391]    Overall Loss 0.209717    Objective Loss 0.209717    Top1 92.676339    Top5 99.875000    LR 0.003000    Time 0.023620    
2018-10-27 23:25:25,488 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32160 | -0.00273 |    0.12048 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09913 | -0.00353 |    0.02195 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09291 |  0.00096 |    0.02261 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09658 | -0.00221 |    0.02297 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07536 | -0.00190 |    0.01490 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10112 | -0.00347 |    0.02792 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07413 | -0.00092 |    0.01656 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10595 | -0.00319 |    0.03718 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08465 | -0.00211 |    0.02610 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11623 | -0.00613 |    0.03679 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07348 | -0.00109 |    0.02035 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05919 | -0.00064 |    0.01431 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07923 | -0.00190 |    0.02337 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06017 | -0.00311 |    0.01530 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07287 | -0.00107 |    0.02438 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07017 | -0.00263 |    0.02396 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06399 |  0.00033 |    0.01642 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06557 | -0.00169 |    0.02018 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05405 | -0.00170 |    0.01397 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05179 | -0.00028 |    0.01360 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03178 |  0.00122 |    0.00566 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53266 | -0.07408 |    0.26311 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:25:25,489 - Total sparsity: 88.62

2018-10-27 23:25:25,489 - --- validate (epoch=226)-----------
2018-10-27 23:25:25,489 - 10000 samples (128 per mini-batch)
2018-10-27 23:25:26,231 - Epoch: [226][   50/   78]    Loss 0.390672    Top1 87.531250    Top5 99.484375    
2018-10-27 23:25:26,629 - ==> Top1: 87.790    Top5: 99.530    Loss: 0.383

2018-10-27 23:25:26,630 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:25:26,630 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:25:26,641 - 

2018-10-27 23:25:26,641 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:25:27,872 - Epoch: [227][   50/  391]    Overall Loss 0.207348    Objective Loss 0.207348    Top1 93.296875    Top5 99.796875    LR 0.003000    Time 0.024577    
2018-10-27 23:25:29,042 - Epoch: [227][  100/  391]    Overall Loss 0.206753    Objective Loss 0.206753    Top1 93.031250    Top5 99.820312    LR 0.003000    Time 0.023971    
2018-10-27 23:25:30,214 - Epoch: [227][  150/  391]    Overall Loss 0.203139    Objective Loss 0.203139    Top1 93.000000    Top5 99.854167    LR 0.003000    Time 0.023785    
2018-10-27 23:25:31,384 - Epoch: [227][  200/  391]    Overall Loss 0.207371    Objective Loss 0.207371    Top1 92.800781    Top5 99.851562    LR 0.003000    Time 0.023682    
2018-10-27 23:25:32,552 - Epoch: [227][  250/  391]    Overall Loss 0.207900    Objective Loss 0.207900    Top1 92.768750    Top5 99.859375    LR 0.003000    Time 0.023613    
2018-10-27 23:25:33,725 - Epoch: [227][  300/  391]    Overall Loss 0.208625    Objective Loss 0.208625    Top1 92.708333    Top5 99.867188    LR 0.003000    Time 0.023584    
2018-10-27 23:25:34,897 - Epoch: [227][  350/  391]    Overall Loss 0.206985    Objective Loss 0.206985    Top1 92.799107    Top5 99.877232    LR 0.003000    Time 0.023559    
2018-10-27 23:25:35,942 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32126 | -0.00272 |    0.12032 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09903 | -0.00351 |    0.02194 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09281 |  0.00091 |    0.02257 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09648 | -0.00224 |    0.02295 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07528 | -0.00193 |    0.01488 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10102 | -0.00347 |    0.02788 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07406 | -0.00092 |    0.01653 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10584 | -0.00318 |    0.03715 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08456 | -0.00210 |    0.02608 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11611 | -0.00612 |    0.03675 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07340 | -0.00111 |    0.02032 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05913 | -0.00062 |    0.01429 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07914 | -0.00189 |    0.02335 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06011 | -0.00312 |    0.01528 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07280 | -0.00107 |    0.02434 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07010 | -0.00262 |    0.02393 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06392 |  0.00034 |    0.01640 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06550 | -0.00168 |    0.02015 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05399 | -0.00170 |    0.01395 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05174 | -0.00029 |    0.01358 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03174 |  0.00122 |    0.00566 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53313 | -0.07405 |    0.26333 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:25:35,942 - Total sparsity: 88.62

2018-10-27 23:25:35,942 - --- validate (epoch=227)-----------
2018-10-27 23:25:35,942 - 10000 samples (128 per mini-batch)
2018-10-27 23:25:36,678 - Epoch: [227][   50/   78]    Loss 0.389335    Top1 87.734375    Top5 99.406250    
2018-10-27 23:25:37,075 - ==> Top1: 87.780    Top5: 99.480    Loss: 0.383

2018-10-27 23:25:37,075 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:25:37,076 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:25:37,086 - 

2018-10-27 23:25:37,087 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:25:38,315 - Epoch: [228][   50/  391]    Overall Loss 0.200570    Objective Loss 0.200570    Top1 92.906250    Top5 99.812500    LR 0.003000    Time 0.024518    
2018-10-27 23:25:39,494 - Epoch: [228][  100/  391]    Overall Loss 0.199855    Objective Loss 0.199855    Top1 93.109375    Top5 99.851562    LR 0.003000    Time 0.024041    
2018-10-27 23:25:40,668 - Epoch: [228][  150/  391]    Overall Loss 0.203880    Objective Loss 0.203880    Top1 92.927083    Top5 99.859375    LR 0.003000    Time 0.023840    
2018-10-27 23:25:41,846 - Epoch: [228][  200/  391]    Overall Loss 0.203689    Objective Loss 0.203689    Top1 92.910156    Top5 99.851562    LR 0.003000    Time 0.023747    
2018-10-27 23:25:43,023 - Epoch: [228][  250/  391]    Overall Loss 0.205418    Objective Loss 0.205418    Top1 92.828125    Top5 99.859375    LR 0.003000    Time 0.023700    
2018-10-27 23:25:44,202 - Epoch: [228][  300/  391]    Overall Loss 0.204083    Objective Loss 0.204083    Top1 92.890625    Top5 99.867188    LR 0.003000    Time 0.023676    
2018-10-27 23:25:45,375 - Epoch: [228][  350/  391]    Overall Loss 0.205514    Objective Loss 0.205514    Top1 92.863839    Top5 99.870536    LR 0.003000    Time 0.023639    
2018-10-27 23:25:46,418 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32092 | -0.00313 |    0.12020 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09892 | -0.00352 |    0.02191 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09271 |  0.00091 |    0.02256 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09638 | -0.00224 |    0.02294 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07521 | -0.00191 |    0.01487 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10091 | -0.00353 |    0.02782 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07398 | -0.00093 |    0.01653 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10573 | -0.00317 |    0.03710 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08447 | -0.00209 |    0.02604 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11598 | -0.00616 |    0.03671 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07332 | -0.00110 |    0.02031 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05907 | -0.00062 |    0.01427 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07906 | -0.00189 |    0.02333 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.06005 | -0.00311 |    0.01526 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07272 | -0.00107 |    0.02431 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.07003 | -0.00260 |    0.02390 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06385 |  0.00032 |    0.01639 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06543 | -0.00168 |    0.02012 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05394 | -0.00169 |    0.01394 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05168 | -0.00028 |    0.01357 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03171 |  0.00122 |    0.00565 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53362 | -0.07410 |    0.26356 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:25:46,418 - Total sparsity: 88.62

2018-10-27 23:25:46,418 - --- validate (epoch=228)-----------
2018-10-27 23:25:46,418 - 10000 samples (128 per mini-batch)
2018-10-27 23:25:47,162 - Epoch: [228][   50/   78]    Loss 0.391996    Top1 87.546875    Top5 99.515625    
2018-10-27 23:25:47,555 - ==> Top1: 87.740    Top5: 99.540    Loss: 0.384

2018-10-27 23:25:47,556 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:25:47,556 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:25:47,566 - 

2018-10-27 23:25:47,567 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:25:48,794 - Epoch: [229][   50/  391]    Overall Loss 0.197852    Objective Loss 0.197852    Top1 93.156250    Top5 99.781250    LR 0.003000    Time 0.024512    
2018-10-27 23:25:49,963 - Epoch: [229][  100/  391]    Overall Loss 0.207993    Objective Loss 0.207993    Top1 92.757812    Top5 99.820312    LR 0.003000    Time 0.023930    
2018-10-27 23:25:51,141 - Epoch: [229][  150/  391]    Overall Loss 0.209210    Objective Loss 0.209210    Top1 92.723958    Top5 99.833333    LR 0.003000    Time 0.023801    
2018-10-27 23:25:52,314 - Epoch: [229][  200/  391]    Overall Loss 0.210757    Objective Loss 0.210757    Top1 92.660156    Top5 99.855469    LR 0.003000    Time 0.023707    
2018-10-27 23:25:53,487 - Epoch: [229][  250/  391]    Overall Loss 0.209699    Objective Loss 0.209699    Top1 92.753125    Top5 99.853125    LR 0.003000    Time 0.023651    
2018-10-27 23:25:54,667 - Epoch: [229][  300/  391]    Overall Loss 0.208120    Objective Loss 0.208120    Top1 92.809896    Top5 99.854167    LR 0.003000    Time 0.023638    
2018-10-27 23:25:55,847 - Epoch: [229][  350/  391]    Overall Loss 0.207654    Objective Loss 0.207654    Top1 92.821429    Top5 99.852679    LR 0.003000    Time 0.023631    
2018-10-27 23:25:56,897 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32058 | -0.00321 |    0.12009 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09882 | -0.00347 |    0.02188 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09262 |  0.00095 |    0.02254 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09628 | -0.00227 |    0.02290 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07513 | -0.00193 |    0.01485 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10080 | -0.00358 |    0.02780 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07391 | -0.00097 |    0.01652 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10562 | -0.00318 |    0.03706 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08438 | -0.00212 |    0.02601 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11586 | -0.00612 |    0.03669 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07325 | -0.00110 |    0.02029 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05901 | -0.00061 |    0.01425 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07898 | -0.00190 |    0.02330 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05999 | -0.00311 |    0.01525 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07264 | -0.00106 |    0.02429 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06996 | -0.00260 |    0.02387 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06379 |  0.00033 |    0.01637 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06536 | -0.00168 |    0.02010 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05388 | -0.00169 |    0.01392 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05163 | -0.00028 |    0.01355 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03168 |  0.00122 |    0.00564 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53404 | -0.07413 |    0.26378 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:25:56,897 - Total sparsity: 88.62

2018-10-27 23:25:56,897 - --- validate (epoch=229)-----------
2018-10-27 23:25:56,897 - 10000 samples (128 per mini-batch)
2018-10-27 23:25:57,623 - Epoch: [229][   50/   78]    Loss 0.385373    Top1 87.750000    Top5 99.500000    
2018-10-27 23:25:58,018 - ==> Top1: 87.920    Top5: 99.510    Loss: 0.378

2018-10-27 23:25:58,019 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:25:58,019 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:25:58,029 - 

2018-10-27 23:25:58,029 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:25:59,260 - Epoch: [230][   50/  391]    Overall Loss 0.208391    Objective Loss 0.208391    Top1 92.734375    Top5 99.875000    LR 0.003000    Time 0.024570    
2018-10-27 23:26:00,432 - Epoch: [230][  100/  391]    Overall Loss 0.210030    Objective Loss 0.210030    Top1 92.820312    Top5 99.851562    LR 0.003000    Time 0.023993    
2018-10-27 23:26:01,597 - Epoch: [230][  150/  391]    Overall Loss 0.208167    Objective Loss 0.208167    Top1 92.828125    Top5 99.869792    LR 0.003000    Time 0.023751    
2018-10-27 23:26:02,737 - Epoch: [230][  200/  391]    Overall Loss 0.205828    Objective Loss 0.205828    Top1 92.886719    Top5 99.882812    LR 0.003000    Time 0.023507    
2018-10-27 23:26:03,877 - Epoch: [230][  250/  391]    Overall Loss 0.206301    Objective Loss 0.206301    Top1 92.859375    Top5 99.862500    LR 0.003000    Time 0.023361    
2018-10-27 23:26:05,020 - Epoch: [230][  300/  391]    Overall Loss 0.208402    Objective Loss 0.208402    Top1 92.791667    Top5 99.864583    LR 0.003000    Time 0.023273    
2018-10-27 23:26:06,165 - Epoch: [230][  350/  391]    Overall Loss 0.206797    Objective Loss 0.206797    Top1 92.863839    Top5 99.872768    LR 0.003000    Time 0.023215    
2018-10-27 23:26:07,180 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.32025 | -0.00345 |    0.12012 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09872 | -0.00349 |    0.02185 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09252 |  0.00097 |    0.02253 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09618 | -0.00226 |    0.02285 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07505 | -0.00195 |    0.01484 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10070 | -0.00360 |    0.02779 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07383 | -0.00097 |    0.01650 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10551 | -0.00318 |    0.03702 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08429 | -0.00215 |    0.02598 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11573 | -0.00614 |    0.03667 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07317 | -0.00109 |    0.02026 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05895 | -0.00061 |    0.01423 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07890 | -0.00187 |    0.02328 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05993 | -0.00310 |    0.01523 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07257 | -0.00105 |    0.02425 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06988 | -0.00260 |    0.02385 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06372 |  0.00033 |    0.01637 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06530 | -0.00167 |    0.02008 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05383 | -0.00168 |    0.01391 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05158 | -0.00029 |    0.01354 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03164 |  0.00122 |    0.00564 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53451 | -0.07412 |    0.26399 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:26:07,180 - Total sparsity: 88.62

2018-10-27 23:26:07,180 - --- validate (epoch=230)-----------
2018-10-27 23:26:07,181 - 10000 samples (128 per mini-batch)
2018-10-27 23:26:07,890 - Epoch: [230][   50/   78]    Loss 0.385732    Top1 87.546875    Top5 99.531250    
2018-10-27 23:26:08,272 - ==> Top1: 87.710    Top5: 99.550    Loss: 0.380

2018-10-27 23:26:08,273 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:26:08,273 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:26:08,290 - 

2018-10-27 23:26:08,290 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:26:09,461 - Epoch: [231][   50/  391]    Overall Loss 0.203177    Objective Loss 0.203177    Top1 93.000000    Top5 99.828125    LR 0.003000    Time 0.023379    
2018-10-27 23:26:10,602 - Epoch: [231][  100/  391]    Overall Loss 0.203755    Objective Loss 0.203755    Top1 92.929688    Top5 99.843750    LR 0.003000    Time 0.023086    
2018-10-27 23:26:11,747 - Epoch: [231][  150/  391]    Overall Loss 0.202281    Objective Loss 0.202281    Top1 92.968750    Top5 99.885417    LR 0.003000    Time 0.023015    
2018-10-27 23:26:12,891 - Epoch: [231][  200/  391]    Overall Loss 0.205901    Objective Loss 0.205901    Top1 92.890625    Top5 99.882812    LR 0.003000    Time 0.022975    
2018-10-27 23:26:14,034 - Epoch: [231][  250/  391]    Overall Loss 0.205043    Objective Loss 0.205043    Top1 92.931250    Top5 99.884375    LR 0.003000    Time 0.022947    
2018-10-27 23:26:15,178 - Epoch: [231][  300/  391]    Overall Loss 0.207476    Objective Loss 0.207476    Top1 92.817708    Top5 99.880208    LR 0.003000    Time 0.022933    
2018-10-27 23:26:16,322 - Epoch: [231][  350/  391]    Overall Loss 0.205322    Objective Loss 0.205322    Top1 92.895089    Top5 99.888393    LR 0.003000    Time 0.022921    
2018-10-27 23:26:17,337 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31992 | -0.00346 |    0.11994 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09862 | -0.00348 |    0.02183 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09242 |  0.00098 |    0.02247 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09608 | -0.00226 |    0.02285 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07497 | -0.00193 |    0.01479 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10060 | -0.00358 |    0.02773 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07375 | -0.00094 |    0.01649 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10540 | -0.00319 |    0.03696 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08421 | -0.00214 |    0.02595 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11562 | -0.00606 |    0.03657 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07310 | -0.00109 |    0.02025 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05889 | -0.00061 |    0.01422 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07882 | -0.00187 |    0.02325 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05986 | -0.00309 |    0.01521 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07249 | -0.00103 |    0.02422 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06981 | -0.00259 |    0.02382 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06365 |  0.00034 |    0.01634 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06523 | -0.00165 |    0.02005 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05377 | -0.00168 |    0.01389 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05152 | -0.00030 |    0.01352 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03161 |  0.00121 |    0.00563 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53492 | -0.07416 |    0.26417 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:26:17,338 - Total sparsity: 88.62

2018-10-27 23:26:17,338 - --- validate (epoch=231)-----------
2018-10-27 23:26:17,338 - 10000 samples (128 per mini-batch)
2018-10-27 23:26:18,063 - Epoch: [231][   50/   78]    Loss 0.387065    Top1 87.625000    Top5 99.500000    
2018-10-27 23:26:18,453 - ==> Top1: 87.720    Top5: 99.520    Loss: 0.385

2018-10-27 23:26:18,454 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:26:18,454 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:26:18,465 - 

2018-10-27 23:26:18,465 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:26:19,667 - Epoch: [232][   50/  391]    Overall Loss 0.203254    Objective Loss 0.203254    Top1 93.015625    Top5 99.921875    LR 0.003000    Time 0.023999    
2018-10-27 23:26:20,810 - Epoch: [232][  100/  391]    Overall Loss 0.206574    Objective Loss 0.206574    Top1 93.039062    Top5 99.914062    LR 0.003000    Time 0.023416    
2018-10-27 23:26:21,953 - Epoch: [232][  150/  391]    Overall Loss 0.206035    Objective Loss 0.206035    Top1 92.895833    Top5 99.890625    LR 0.003000    Time 0.023220    
2018-10-27 23:26:23,094 - Epoch: [232][  200/  391]    Overall Loss 0.202969    Objective Loss 0.202969    Top1 92.933594    Top5 99.878906    LR 0.003000    Time 0.023114    
2018-10-27 23:26:24,235 - Epoch: [232][  250/  391]    Overall Loss 0.204400    Objective Loss 0.204400    Top1 92.850000    Top5 99.868750    LR 0.003000    Time 0.023051    
2018-10-27 23:26:25,375 - Epoch: [232][  300/  391]    Overall Loss 0.204788    Objective Loss 0.204788    Top1 92.802083    Top5 99.861979    LR 0.003000    Time 0.023008    
2018-10-27 23:26:26,517 - Epoch: [232][  350/  391]    Overall Loss 0.206733    Objective Loss 0.206733    Top1 92.729911    Top5 99.857143    LR 0.003000    Time 0.022979    
2018-10-27 23:26:27,531 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31960 | -0.00325 |    0.11969 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09852 | -0.00346 |    0.02183 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09233 |  0.00103 |    0.02244 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09598 | -0.00227 |    0.02281 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07490 | -0.00192 |    0.01478 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10049 | -0.00357 |    0.02772 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07368 | -0.00091 |    0.01648 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10529 | -0.00320 |    0.03693 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08412 | -0.00212 |    0.02592 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11549 | -0.00608 |    0.03652 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07302 | -0.00108 |    0.02021 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05883 | -0.00061 |    0.01421 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07874 | -0.00186 |    0.02322 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05980 | -0.00311 |    0.01520 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07242 | -0.00102 |    0.02420 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06974 | -0.00258 |    0.02379 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06358 |  0.00035 |    0.01632 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06517 | -0.00164 |    0.02003 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05372 | -0.00168 |    0.01388 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05147 | -0.00030 |    0.01351 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03158 |  0.00121 |    0.00562 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53527 | -0.07416 |    0.26434 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:26:27,531 - Total sparsity: 88.62

2018-10-27 23:26:27,531 - --- validate (epoch=232)-----------
2018-10-27 23:26:27,531 - 10000 samples (128 per mini-batch)
2018-10-27 23:26:28,251 - Epoch: [232][   50/   78]    Loss 0.394774    Top1 87.453125    Top5 99.515625    
2018-10-27 23:26:28,642 - ==> Top1: 87.690    Top5: 99.530    Loss: 0.387

2018-10-27 23:26:28,643 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:26:28,643 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:26:28,653 - 

2018-10-27 23:26:28,653 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:26:29,852 - Epoch: [233][   50/  391]    Overall Loss 0.200382    Objective Loss 0.200382    Top1 92.968750    Top5 99.906250    LR 0.003000    Time 0.023931    
2018-10-27 23:26:30,995 - Epoch: [233][  100/  391]    Overall Loss 0.201364    Objective Loss 0.201364    Top1 92.804688    Top5 99.921875    LR 0.003000    Time 0.023381    
2018-10-27 23:26:32,137 - Epoch: [233][  150/  391]    Overall Loss 0.201830    Objective Loss 0.201830    Top1 92.828125    Top5 99.895833    LR 0.003000    Time 0.023191    
2018-10-27 23:26:33,279 - Epoch: [233][  200/  391]    Overall Loss 0.205349    Objective Loss 0.205349    Top1 92.734375    Top5 99.890625    LR 0.003000    Time 0.023097    
2018-10-27 23:26:34,421 - Epoch: [233][  250/  391]    Overall Loss 0.207780    Objective Loss 0.207780    Top1 92.621875    Top5 99.875000    LR 0.003000    Time 0.023040    
2018-10-27 23:26:35,562 - Epoch: [233][  300/  391]    Overall Loss 0.208753    Objective Loss 0.208753    Top1 92.638021    Top5 99.880208    LR 0.003000    Time 0.023000    
2018-10-27 23:26:36,703 - Epoch: [233][  350/  391]    Overall Loss 0.208986    Objective Loss 0.208986    Top1 92.618304    Top5 99.879464    LR 0.003000    Time 0.022971    
2018-10-27 23:26:37,715 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31927 | -0.00298 |    0.11963 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09841 | -0.00348 |    0.02183 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09223 |  0.00100 |    0.02241 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09588 | -0.00229 |    0.02279 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07482 | -0.00193 |    0.01478 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10039 | -0.00358 |    0.02767 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07360 | -0.00089 |    0.01647 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10519 | -0.00315 |    0.03688 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08404 | -0.00210 |    0.02588 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11537 | -0.00604 |    0.03647 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07295 | -0.00108 |    0.02020 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05877 | -0.00061 |    0.01419 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07866 | -0.00186 |    0.02319 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05974 | -0.00312 |    0.01518 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07234 | -0.00103 |    0.02417 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06967 | -0.00257 |    0.02377 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06352 |  0.00035 |    0.01631 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06510 | -0.00164 |    0.02001 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05366 | -0.00168 |    0.01386 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05142 | -0.00030 |    0.01349 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03154 |  0.00120 |    0.00562 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53557 | -0.07417 |    0.26446 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:26:37,715 - Total sparsity: 88.62

2018-10-27 23:26:37,715 - --- validate (epoch=233)-----------
2018-10-27 23:26:37,715 - 10000 samples (128 per mini-batch)
2018-10-27 23:26:38,441 - Epoch: [233][   50/   78]    Loss 0.389620    Top1 87.562500    Top5 99.484375    
2018-10-27 23:26:38,835 - ==> Top1: 87.730    Top5: 99.530    Loss: 0.385

2018-10-27 23:26:38,836 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:26:38,836 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:26:38,846 - 

2018-10-27 23:26:38,847 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:26:40,044 - Epoch: [234][   50/  391]    Overall Loss 0.212839    Objective Loss 0.212839    Top1 92.296875    Top5 99.906250    LR 0.003000    Time 0.023916    
2018-10-27 23:26:41,188 - Epoch: [234][  100/  391]    Overall Loss 0.211050    Objective Loss 0.211050    Top1 92.585938    Top5 99.898438    LR 0.003000    Time 0.023381    
2018-10-27 23:26:42,331 - Epoch: [234][  150/  391]    Overall Loss 0.202639    Objective Loss 0.202639    Top1 92.729167    Top5 99.916667    LR 0.003000    Time 0.023199    
2018-10-27 23:26:43,473 - Epoch: [234][  200/  391]    Overall Loss 0.205452    Objective Loss 0.205452    Top1 92.734375    Top5 99.917969    LR 0.003000    Time 0.023104    
2018-10-27 23:26:44,617 - Epoch: [234][  250/  391]    Overall Loss 0.204414    Objective Loss 0.204414    Top1 92.765625    Top5 99.915625    LR 0.003000    Time 0.023053    
2018-10-27 23:26:45,760 - Epoch: [234][  300/  391]    Overall Loss 0.204241    Objective Loss 0.204241    Top1 92.763021    Top5 99.908854    LR 0.003000    Time 0.023015    
2018-10-27 23:26:46,903 - Epoch: [234][  350/  391]    Overall Loss 0.205927    Objective Loss 0.205927    Top1 92.689732    Top5 99.904018    LR 0.003000    Time 0.022988    
2018-10-27 23:26:47,917 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31894 | -0.00301 |    0.11942 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09831 | -0.00349 |    0.02181 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09214 |  0.00100 |    0.02237 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09578 | -0.00228 |    0.02275 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07474 | -0.00193 |    0.01476 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10029 | -0.00355 |    0.02764 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07353 | -0.00094 |    0.01646 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10508 | -0.00315 |    0.03684 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08395 | -0.00210 |    0.02585 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11525 | -0.00599 |    0.03644 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07288 | -0.00108 |    0.02018 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05871 | -0.00061 |    0.01417 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07858 | -0.00184 |    0.02317 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05968 | -0.00311 |    0.01516 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07227 | -0.00104 |    0.02414 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06960 | -0.00256 |    0.02374 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06345 |  0.00036 |    0.01628 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06503 | -0.00164 |    0.01999 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05361 | -0.00168 |    0.01385 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05137 | -0.00031 |    0.01348 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03151 |  0.00120 |    0.00561 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53602 | -0.07419 |    0.26466 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:26:47,918 - Total sparsity: 88.62

2018-10-27 23:26:47,918 - --- validate (epoch=234)-----------
2018-10-27 23:26:47,918 - 10000 samples (128 per mini-batch)
2018-10-27 23:26:48,639 - Epoch: [234][   50/   78]    Loss 0.389070    Top1 87.734375    Top5 99.500000    
2018-10-27 23:26:49,029 - ==> Top1: 87.880    Top5: 99.530    Loss: 0.383

2018-10-27 23:26:49,030 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:26:49,030 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:26:49,040 - 

2018-10-27 23:26:49,040 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:26:50,239 - Epoch: [235][   50/  391]    Overall Loss 0.210318    Objective Loss 0.210318    Top1 93.078125    Top5 99.750000    LR 0.003000    Time 0.023939    
2018-10-27 23:26:51,380 - Epoch: [235][  100/  391]    Overall Loss 0.206173    Objective Loss 0.206173    Top1 92.968750    Top5 99.812500    LR 0.003000    Time 0.023367    
2018-10-27 23:26:52,520 - Epoch: [235][  150/  391]    Overall Loss 0.209570    Objective Loss 0.209570    Top1 92.750000    Top5 99.838542    LR 0.003000    Time 0.023170    
2018-10-27 23:26:53,661 - Epoch: [235][  200/  391]    Overall Loss 0.209929    Objective Loss 0.209929    Top1 92.722656    Top5 99.847656    LR 0.003000    Time 0.023078    
2018-10-27 23:26:54,804 - Epoch: [235][  250/  391]    Overall Loss 0.208898    Objective Loss 0.208898    Top1 92.690625    Top5 99.853125    LR 0.003000    Time 0.023016    
2018-10-27 23:26:55,944 - Epoch: [235][  300/  391]    Overall Loss 0.205767    Objective Loss 0.205767    Top1 92.750000    Top5 99.856771    LR 0.003000    Time 0.022974    
2018-10-27 23:26:57,083 - Epoch: [235][  350/  391]    Overall Loss 0.206105    Objective Loss 0.206105    Top1 92.738839    Top5 99.859375    LR 0.003000    Time 0.022942    
2018-10-27 23:26:58,097 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31861 | -0.00288 |    0.11934 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09821 | -0.00344 |    0.02178 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09204 |  0.00101 |    0.02236 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09569 | -0.00227 |    0.02274 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07466 | -0.00195 |    0.01475 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10019 | -0.00352 |    0.02762 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07345 | -0.00096 |    0.01645 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10497 | -0.00315 |    0.03678 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08386 | -0.00210 |    0.02583 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11513 | -0.00604 |    0.03638 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07280 | -0.00106 |    0.02015 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05865 | -0.00063 |    0.01414 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07850 | -0.00183 |    0.02314 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05962 | -0.00310 |    0.01514 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07220 | -0.00102 |    0.02411 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06953 | -0.00257 |    0.02372 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06338 |  0.00035 |    0.01626 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06497 | -0.00165 |    0.01996 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05355 | -0.00168 |    0.01383 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05131 | -0.00032 |    0.01346 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03148 |  0.00120 |    0.00560 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53642 | -0.07422 |    0.26487 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:26:58,097 - Total sparsity: 88.62

2018-10-27 23:26:58,097 - --- validate (epoch=235)-----------
2018-10-27 23:26:58,097 - 10000 samples (128 per mini-batch)
2018-10-27 23:26:58,822 - Epoch: [235][   50/   78]    Loss 0.397190    Top1 87.343750    Top5 99.484375    
2018-10-27 23:26:59,215 - ==> Top1: 87.640    Top5: 99.510    Loss: 0.389

2018-10-27 23:26:59,216 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:26:59,216 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:26:59,227 - 

2018-10-27 23:26:59,227 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:27:00,428 - Epoch: [236][   50/  391]    Overall Loss 0.213846    Objective Loss 0.213846    Top1 92.359375    Top5 99.968750    LR 0.003000    Time 0.023981    
2018-10-27 23:27:01,569 - Epoch: [236][  100/  391]    Overall Loss 0.212201    Objective Loss 0.212201    Top1 92.617188    Top5 99.898438    LR 0.003000    Time 0.023391    
2018-10-27 23:27:02,712 - Epoch: [236][  150/  391]    Overall Loss 0.210956    Objective Loss 0.210956    Top1 92.614583    Top5 99.901042    LR 0.003000    Time 0.023204    
2018-10-27 23:27:03,854 - Epoch: [236][  200/  391]    Overall Loss 0.210807    Objective Loss 0.210807    Top1 92.566406    Top5 99.898438    LR 0.003000    Time 0.023109    
2018-10-27 23:27:04,997 - Epoch: [236][  250/  391]    Overall Loss 0.209297    Objective Loss 0.209297    Top1 92.609375    Top5 99.893750    LR 0.003000    Time 0.023053    
2018-10-27 23:27:06,141 - Epoch: [236][  300/  391]    Overall Loss 0.208450    Objective Loss 0.208450    Top1 92.684896    Top5 99.890625    LR 0.003000    Time 0.023020    
2018-10-27 23:27:07,284 - Epoch: [236][  350/  391]    Overall Loss 0.205100    Objective Loss 0.205100    Top1 92.859375    Top5 99.897321    LR 0.003000    Time 0.022992    
2018-10-27 23:27:08,297 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31828 | -0.00323 |    0.11914 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09811 | -0.00342 |    0.02177 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09195 |  0.00105 |    0.02233 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09559 | -0.00226 |    0.02271 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07459 | -0.00196 |    0.01472 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.10009 | -0.00352 |    0.02759 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07338 | -0.00092 |    0.01642 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10486 | -0.00315 |    0.03675 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08378 | -0.00210 |    0.02579 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11501 | -0.00598 |    0.03636 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07273 | -0.00108 |    0.02014 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05859 | -0.00063 |    0.01413 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07842 | -0.00183 |    0.02311 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05956 | -0.00310 |    0.01513 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07212 | -0.00102 |    0.02409 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06946 | -0.00256 |    0.02369 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06332 |  0.00035 |    0.01624 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06490 | -0.00163 |    0.01993 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05350 | -0.00168 |    0.01382 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05126 | -0.00032 |    0.01345 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03144 |  0.00120 |    0.00560 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53685 | -0.07421 |    0.26503 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:27:08,298 - Total sparsity: 88.62

2018-10-27 23:27:08,298 - --- validate (epoch=236)-----------
2018-10-27 23:27:08,298 - 10000 samples (128 per mini-batch)
2018-10-27 23:27:09,019 - Epoch: [236][   50/   78]    Loss 0.393224    Top1 87.703125    Top5 99.453125    
2018-10-27 23:27:09,408 - ==> Top1: 87.890    Top5: 99.470    Loss: 0.388

2018-10-27 23:27:09,409 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:27:09,409 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:27:09,423 - 

2018-10-27 23:27:09,424 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:27:10,621 - Epoch: [237][   50/  391]    Overall Loss 0.202165    Objective Loss 0.202165    Top1 93.234375    Top5 99.875000    LR 0.003000    Time 0.023918    
2018-10-27 23:27:11,765 - Epoch: [237][  100/  391]    Overall Loss 0.197155    Objective Loss 0.197155    Top1 93.351562    Top5 99.875000    LR 0.003000    Time 0.023383    
2018-10-27 23:27:12,908 - Epoch: [237][  150/  391]    Overall Loss 0.202699    Objective Loss 0.202699    Top1 93.000000    Top5 99.875000    LR 0.003000    Time 0.023202    
2018-10-27 23:27:14,054 - Epoch: [237][  200/  391]    Overall Loss 0.203116    Objective Loss 0.203116    Top1 92.972656    Top5 99.882812    LR 0.003000    Time 0.023120    
2018-10-27 23:27:15,196 - Epoch: [237][  250/  391]    Overall Loss 0.202105    Objective Loss 0.202105    Top1 92.956250    Top5 99.890625    LR 0.003000    Time 0.023061    
2018-10-27 23:27:16,341 - Epoch: [237][  300/  391]    Overall Loss 0.201188    Objective Loss 0.201188    Top1 93.026042    Top5 99.903646    LR 0.003000    Time 0.023030    
2018-10-27 23:27:17,485 - Epoch: [237][  350/  391]    Overall Loss 0.202190    Objective Loss 0.202190    Top1 92.988839    Top5 99.904018    LR 0.003000    Time 0.023005    
2018-10-27 23:27:18,502 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31795 | -0.00309 |    0.11888 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09801 | -0.00341 |    0.02174 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09185 |  0.00103 |    0.02232 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09549 | -0.00224 |    0.02265 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07451 | -0.00194 |    0.01467 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09998 | -0.00352 |    0.02754 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07330 | -0.00093 |    0.01641 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10475 | -0.00312 |    0.03670 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08369 | -0.00207 |    0.02576 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11490 | -0.00584 |    0.03627 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07265 | -0.00110 |    0.02012 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05853 | -0.00062 |    0.01411 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07834 | -0.00182 |    0.02309 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05950 | -0.00310 |    0.01511 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07205 | -0.00103 |    0.02406 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06938 | -0.00256 |    0.02367 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06325 |  0.00038 |    0.01622 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06484 | -0.00163 |    0.01991 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05344 | -0.00168 |    0.01381 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05121 | -0.00032 |    0.01343 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03141 |  0.00120 |    0.00559 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53731 | -0.07422 |    0.26523 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:27:18,502 - Total sparsity: 88.62

2018-10-27 23:27:18,502 - --- validate (epoch=237)-----------
2018-10-27 23:27:18,502 - 10000 samples (128 per mini-batch)
2018-10-27 23:27:19,229 - Epoch: [237][   50/   78]    Loss 0.396876    Top1 87.671875    Top5 99.406250    
2018-10-27 23:27:19,622 - ==> Top1: 87.810    Top5: 99.440    Loss: 0.391

2018-10-27 23:27:19,623 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:27:19,623 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:27:19,634 - 

2018-10-27 23:27:19,634 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:27:20,832 - Epoch: [238][   50/  391]    Overall Loss 0.200262    Objective Loss 0.200262    Top1 93.062500    Top5 99.875000    LR 0.003000    Time 0.023921    
2018-10-27 23:27:21,974 - Epoch: [238][  100/  391]    Overall Loss 0.203062    Objective Loss 0.203062    Top1 93.156250    Top5 99.875000    LR 0.003000    Time 0.023370    
2018-10-27 23:27:23,117 - Epoch: [238][  150/  391]    Overall Loss 0.204043    Objective Loss 0.204043    Top1 92.953125    Top5 99.838542    LR 0.003000    Time 0.023188    
2018-10-27 23:27:24,259 - Epoch: [238][  200/  391]    Overall Loss 0.205996    Objective Loss 0.205996    Top1 92.917969    Top5 99.839844    LR 0.003000    Time 0.023094    
2018-10-27 23:27:25,401 - Epoch: [238][  250/  391]    Overall Loss 0.206994    Objective Loss 0.206994    Top1 92.881250    Top5 99.850000    LR 0.003000    Time 0.023038    
2018-10-27 23:27:26,543 - Epoch: [238][  300/  391]    Overall Loss 0.205063    Objective Loss 0.205063    Top1 92.872396    Top5 99.859375    LR 0.003000    Time 0.023003    
2018-10-27 23:27:27,684 - Epoch: [238][  350/  391]    Overall Loss 0.205344    Objective Loss 0.205344    Top1 92.919643    Top5 99.852679    LR 0.003000    Time 0.022972    
2018-10-27 23:27:28,703 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31763 | -0.00286 |    0.11883 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09791 | -0.00341 |    0.02172 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09176 |  0.00100 |    0.02229 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09539 | -0.00223 |    0.02263 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07443 | -0.00193 |    0.01468 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09988 | -0.00349 |    0.02754 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07323 | -0.00093 |    0.01638 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10465 | -0.00312 |    0.03667 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08360 | -0.00206 |    0.02574 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11477 | -0.00598 |    0.03629 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07258 | -0.00111 |    0.02009 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05847 | -0.00061 |    0.01410 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07826 | -0.00184 |    0.02306 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05944 | -0.00309 |    0.01510 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07197 | -0.00103 |    0.02403 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06931 | -0.00255 |    0.02364 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06318 |  0.00041 |    0.01620 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06477 | -0.00161 |    0.01989 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05339 | -0.00167 |    0.01379 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05116 | -0.00032 |    0.01342 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03138 |  0.00120 |    0.00558 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53773 | -0.07426 |    0.26544 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:27:28,703 - Total sparsity: 88.62

2018-10-27 23:27:28,703 - --- validate (epoch=238)-----------
2018-10-27 23:27:28,704 - 10000 samples (128 per mini-batch)
2018-10-27 23:27:29,427 - Epoch: [238][   50/   78]    Loss 0.392162    Top1 87.593750    Top5 99.500000    
2018-10-27 23:27:29,819 - ==> Top1: 87.710    Top5: 99.540    Loss: 0.386

2018-10-27 23:27:29,819 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:27:29,820 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:27:29,830 - 

2018-10-27 23:27:29,830 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:27:31,031 - Epoch: [239][   50/  391]    Overall Loss 0.200577    Objective Loss 0.200577    Top1 93.109375    Top5 99.906250    LR 0.003000    Time 0.023965    
2018-10-27 23:27:32,171 - Epoch: [239][  100/  391]    Overall Loss 0.203149    Objective Loss 0.203149    Top1 93.007812    Top5 99.882812    LR 0.003000    Time 0.023374    
2018-10-27 23:27:33,311 - Epoch: [239][  150/  391]    Overall Loss 0.204385    Objective Loss 0.204385    Top1 92.906250    Top5 99.885417    LR 0.003000    Time 0.023174    
2018-10-27 23:27:34,454 - Epoch: [239][  200/  391]    Overall Loss 0.203759    Objective Loss 0.203759    Top1 92.968750    Top5 99.878906    LR 0.003000    Time 0.023087    
2018-10-27 23:27:35,596 - Epoch: [239][  250/  391]    Overall Loss 0.203997    Objective Loss 0.203997    Top1 92.937500    Top5 99.887500    LR 0.003000    Time 0.023032    
2018-10-27 23:27:36,738 - Epoch: [239][  300/  391]    Overall Loss 0.203457    Objective Loss 0.203457    Top1 92.945312    Top5 99.888021    LR 0.003000    Time 0.022998    
2018-10-27 23:27:37,879 - Epoch: [239][  350/  391]    Overall Loss 0.204530    Objective Loss 0.204530    Top1 92.859375    Top5 99.892857    LR 0.003000    Time 0.022969    
2018-10-27 23:27:38,894 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31730 | -0.00286 |    0.11865 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09781 | -0.00348 |    0.02168 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09166 |  0.00100 |    0.02225 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09529 | -0.00222 |    0.02259 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07436 | -0.00193 |    0.01467 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09978 | -0.00351 |    0.02749 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07315 | -0.00095 |    0.01636 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10454 | -0.00311 |    0.03663 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08352 | -0.00208 |    0.02571 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11465 | -0.00596 |    0.03624 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07251 | -0.00110 |    0.02007 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05841 | -0.00061 |    0.01408 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07818 | -0.00182 |    0.02304 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05938 | -0.00308 |    0.01508 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07190 | -0.00103 |    0.02401 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06924 | -0.00254 |    0.02362 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06311 |  0.00041 |    0.01618 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06471 | -0.00160 |    0.01987 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05334 | -0.00167 |    0.01378 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05111 | -0.00032 |    0.01340 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03135 |  0.00119 |    0.00558 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53813 | -0.07432 |    0.26565 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:27:38,895 - Total sparsity: 88.62

2018-10-27 23:27:38,895 - --- validate (epoch=239)-----------
2018-10-27 23:27:38,895 - 10000 samples (128 per mini-batch)
2018-10-27 23:27:39,621 - Epoch: [239][   50/   78]    Loss 0.396449    Top1 87.609375    Top5 99.546875    
2018-10-27 23:27:40,015 - ==> Top1: 87.610    Top5: 99.570    Loss: 0.387

2018-10-27 23:27:40,016 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:27:40,016 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:27:40,026 - 

2018-10-27 23:27:40,026 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:27:41,223 - Epoch: [240][   50/  391]    Overall Loss 0.208134    Objective Loss 0.208134    Top1 92.500000    Top5 99.921875    LR 0.003000    Time 0.023899    
2018-10-27 23:27:42,363 - Epoch: [240][  100/  391]    Overall Loss 0.204069    Objective Loss 0.204069    Top1 92.828125    Top5 99.937500    LR 0.003000    Time 0.023339    
2018-10-27 23:27:43,503 - Epoch: [240][  150/  391]    Overall Loss 0.205860    Objective Loss 0.205860    Top1 92.744792    Top5 99.921875    LR 0.003000    Time 0.023152    
2018-10-27 23:27:44,647 - Epoch: [240][  200/  391]    Overall Loss 0.206589    Objective Loss 0.206589    Top1 92.671875    Top5 99.910156    LR 0.003000    Time 0.023075    
2018-10-27 23:27:45,791 - Epoch: [240][  250/  391]    Overall Loss 0.207146    Objective Loss 0.207146    Top1 92.665625    Top5 99.896875    LR 0.003000    Time 0.023032    
2018-10-27 23:27:46,938 - Epoch: [240][  300/  391]    Overall Loss 0.205099    Objective Loss 0.205099    Top1 92.726562    Top5 99.885417    LR 0.003000    Time 0.023011    
2018-10-27 23:27:48,083 - Epoch: [240][  350/  391]    Overall Loss 0.204424    Objective Loss 0.204424    Top1 92.779018    Top5 99.883929    LR 0.003000    Time 0.022992    
2018-10-27 23:27:49,104 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31697 | -0.00305 |    0.11850 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09771 | -0.00344 |    0.02166 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09157 |  0.00100 |    0.02221 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09520 | -0.00221 |    0.02259 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07428 | -0.00193 |    0.01464 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09968 | -0.00352 |    0.02744 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07308 | -0.00095 |    0.01634 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10443 | -0.00313 |    0.03660 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08343 | -0.00207 |    0.02569 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11453 | -0.00588 |    0.03614 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07243 | -0.00109 |    0.02005 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05835 | -0.00062 |    0.01407 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07810 | -0.00181 |    0.02302 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05932 | -0.00308 |    0.01506 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07183 | -0.00103 |    0.02398 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06917 | -0.00253 |    0.02359 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06305 |  0.00041 |    0.01616 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06464 | -0.00159 |    0.01984 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05328 | -0.00165 |    0.01376 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05105 | -0.00032 |    0.01339 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03131 |  0.00119 |    0.00557 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53851 | -0.07432 |    0.26581 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:27:49,104 - Total sparsity: 88.62

2018-10-27 23:27:49,104 - --- validate (epoch=240)-----------
2018-10-27 23:27:49,104 - 10000 samples (128 per mini-batch)
2018-10-27 23:27:49,825 - Epoch: [240][   50/   78]    Loss 0.396688    Top1 87.656250    Top5 99.500000    
2018-10-27 23:27:50,215 - ==> Top1: 87.810    Top5: 99.520    Loss: 0.389

2018-10-27 23:27:50,216 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:27:50,216 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:27:50,236 - 

2018-10-27 23:27:50,236 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:27:51,406 - Epoch: [241][   50/  391]    Overall Loss 0.196603    Objective Loss 0.196603    Top1 93.062500    Top5 99.890625    LR 0.003000    Time 0.023365    
2018-10-27 23:27:52,548 - Epoch: [241][  100/  391]    Overall Loss 0.202519    Objective Loss 0.202519    Top1 92.960938    Top5 99.882812    LR 0.003000    Time 0.023087    
2018-10-27 23:27:53,688 - Epoch: [241][  150/  391]    Overall Loss 0.203284    Objective Loss 0.203284    Top1 92.942708    Top5 99.885417    LR 0.003000    Time 0.022985    
2018-10-27 23:27:54,832 - Epoch: [241][  200/  391]    Overall Loss 0.204166    Objective Loss 0.204166    Top1 92.808594    Top5 99.878906    LR 0.003000    Time 0.022952    
2018-10-27 23:27:55,974 - Epoch: [241][  250/  391]    Overall Loss 0.204375    Objective Loss 0.204375    Top1 92.865625    Top5 99.884375    LR 0.003000    Time 0.022925    
2018-10-27 23:27:57,114 - Epoch: [241][  300/  391]    Overall Loss 0.202650    Objective Loss 0.202650    Top1 92.942708    Top5 99.882812    LR 0.003000    Time 0.022900    
2018-10-27 23:27:58,254 - Epoch: [241][  350/  391]    Overall Loss 0.203650    Objective Loss 0.203650    Top1 92.915179    Top5 99.886161    LR 0.003000    Time 0.022881    
2018-10-27 23:27:59,266 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31665 | -0.00311 |    0.11840 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09761 | -0.00347 |    0.02163 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09148 |  0.00096 |    0.02219 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09510 | -0.00228 |    0.02257 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07421 | -0.00191 |    0.01462 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09958 | -0.00349 |    0.02742 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07300 | -0.00099 |    0.01633 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10432 | -0.00313 |    0.03656 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08335 | -0.00209 |    0.02566 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11441 | -0.00595 |    0.03610 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07236 | -0.00108 |    0.02003 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05829 | -0.00063 |    0.01404 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07802 | -0.00181 |    0.02298 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05926 | -0.00307 |    0.01505 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07175 | -0.00103 |    0.02395 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06910 | -0.00252 |    0.02357 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06298 |  0.00043 |    0.01615 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06458 | -0.00159 |    0.01982 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05323 | -0.00165 |    0.01375 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05100 | -0.00032 |    0.01337 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03128 |  0.00119 |    0.00557 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53889 | -0.07433 |    0.26599 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:27:59,266 - Total sparsity: 88.62

2018-10-27 23:27:59,267 - --- validate (epoch=241)-----------
2018-10-27 23:27:59,267 - 10000 samples (128 per mini-batch)
2018-10-27 23:27:59,993 - Epoch: [241][   50/   78]    Loss 0.394835    Top1 87.453125    Top5 99.500000    
2018-10-27 23:28:00,395 - ==> Top1: 87.590    Top5: 99.520    Loss: 0.386

2018-10-27 23:28:00,396 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:28:00,396 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:28:00,407 - 

2018-10-27 23:28:00,407 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:28:01,606 - Epoch: [242][   50/  391]    Overall Loss 0.206933    Objective Loss 0.206933    Top1 92.671875    Top5 99.906250    LR 0.003000    Time 0.023951    
2018-10-27 23:28:02,749 - Epoch: [242][  100/  391]    Overall Loss 0.205244    Objective Loss 0.205244    Top1 92.921875    Top5 99.890625    LR 0.003000    Time 0.023386    
2018-10-27 23:28:03,892 - Epoch: [242][  150/  391]    Overall Loss 0.203194    Objective Loss 0.203194    Top1 92.947917    Top5 99.916667    LR 0.003000    Time 0.023202    
2018-10-27 23:28:05,036 - Epoch: [242][  200/  391]    Overall Loss 0.201733    Objective Loss 0.201733    Top1 93.000000    Top5 99.898438    LR 0.003000    Time 0.023118    
2018-10-27 23:28:06,180 - Epoch: [242][  250/  391]    Overall Loss 0.201339    Objective Loss 0.201339    Top1 93.053125    Top5 99.887500    LR 0.003000    Time 0.023064    
2018-10-27 23:28:07,324 - Epoch: [242][  300/  391]    Overall Loss 0.201324    Objective Loss 0.201324    Top1 93.020833    Top5 99.882812    LR 0.003000    Time 0.023028    
2018-10-27 23:28:08,466 - Epoch: [242][  350/  391]    Overall Loss 0.202572    Objective Loss 0.202572    Top1 92.962054    Top5 99.883929    LR 0.003000    Time 0.022998    
2018-10-27 23:28:09,481 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31631 | -0.00346 |    0.11826 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09751 | -0.00349 |    0.02161 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09138 |  0.00101 |    0.02217 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09500 | -0.00225 |    0.02254 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07413 | -0.00187 |    0.01461 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09948 | -0.00352 |    0.02738 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07293 | -0.00101 |    0.01632 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10422 | -0.00313 |    0.03654 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08326 | -0.00207 |    0.02563 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11429 | -0.00596 |    0.03604 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07229 | -0.00109 |    0.02000 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05823 | -0.00063 |    0.01403 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07794 | -0.00182 |    0.02295 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05920 | -0.00309 |    0.01503 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07168 | -0.00102 |    0.02393 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06903 | -0.00253 |    0.02354 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06291 |  0.00043 |    0.01612 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06451 | -0.00159 |    0.01980 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05318 | -0.00165 |    0.01373 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05095 | -0.00033 |    0.01336 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03125 |  0.00119 |    0.00556 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53932 | -0.07440 |    0.26617 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:28:09,482 - Total sparsity: 88.62

2018-10-27 23:28:09,482 - --- validate (epoch=242)-----------
2018-10-27 23:28:09,482 - 10000 samples (128 per mini-batch)
2018-10-27 23:28:10,204 - Epoch: [242][   50/   78]    Loss 0.397872    Top1 87.250000    Top5 99.484375    
2018-10-27 23:28:10,597 - ==> Top1: 87.530    Top5: 99.510    Loss: 0.392

2018-10-27 23:28:10,598 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:28:10,598 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:28:10,609 - 

2018-10-27 23:28:10,609 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:28:11,805 - Epoch: [243][   50/  391]    Overall Loss 0.187499    Objective Loss 0.187499    Top1 93.718750    Top5 99.875000    LR 0.003000    Time 0.023890    
2018-10-27 23:28:12,948 - Epoch: [243][  100/  391]    Overall Loss 0.196018    Objective Loss 0.196018    Top1 93.335938    Top5 99.875000    LR 0.003000    Time 0.023356    
2018-10-27 23:28:14,091 - Epoch: [243][  150/  391]    Overall Loss 0.193746    Objective Loss 0.193746    Top1 93.296875    Top5 99.875000    LR 0.003000    Time 0.023185    
2018-10-27 23:28:15,231 - Epoch: [243][  200/  391]    Overall Loss 0.195130    Objective Loss 0.195130    Top1 93.277344    Top5 99.875000    LR 0.003000    Time 0.023082    
2018-10-27 23:28:16,371 - Epoch: [243][  250/  391]    Overall Loss 0.196821    Objective Loss 0.196821    Top1 93.234375    Top5 99.865625    LR 0.003000    Time 0.023022    
2018-10-27 23:28:17,511 - Epoch: [243][  300/  391]    Overall Loss 0.198225    Objective Loss 0.198225    Top1 93.182292    Top5 99.859375    LR 0.003000    Time 0.022981    
2018-10-27 23:28:18,649 - Epoch: [243][  350/  391]    Overall Loss 0.200869    Objective Loss 0.200869    Top1 93.084821    Top5 99.850446    LR 0.003000    Time 0.022946    
2018-10-27 23:28:19,661 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31600 | -0.00341 |    0.11812 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09741 | -0.00351 |    0.02159 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09129 |  0.00098 |    0.02216 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09490 | -0.00225 |    0.02252 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07406 | -0.00187 |    0.01459 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09938 | -0.00342 |    0.02735 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07286 | -0.00097 |    0.01630 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10411 | -0.00309 |    0.03650 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08318 | -0.00209 |    0.02560 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11417 | -0.00595 |    0.03598 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07221 | -0.00109 |    0.01998 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05817 | -0.00064 |    0.01401 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07787 | -0.00184 |    0.02292 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05914 | -0.00306 |    0.01501 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07160 | -0.00103 |    0.02390 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06896 | -0.00251 |    0.02352 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06285 |  0.00044 |    0.01611 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06445 | -0.00158 |    0.01978 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05312 | -0.00165 |    0.01372 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05090 | -0.00034 |    0.01335 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03122 |  0.00118 |    0.00555 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.53972 | -0.07440 |    0.26635 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:28:19,661 - Total sparsity: 88.62

2018-10-27 23:28:19,661 - --- validate (epoch=243)-----------
2018-10-27 23:28:19,661 - 10000 samples (128 per mini-batch)
2018-10-27 23:28:20,387 - Epoch: [243][   50/   78]    Loss 0.396786    Top1 87.296875    Top5 99.484375    
2018-10-27 23:28:20,779 - ==> Top1: 87.620    Top5: 99.500    Loss: 0.391

2018-10-27 23:28:20,780 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:28:20,780 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:28:20,791 - 

2018-10-27 23:28:20,791 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:28:21,990 - Epoch: [244][   50/  391]    Overall Loss 0.203973    Objective Loss 0.203973    Top1 92.718750    Top5 99.859375    LR 0.003000    Time 0.023950    
2018-10-27 23:28:23,134 - Epoch: [244][  100/  391]    Overall Loss 0.202393    Objective Loss 0.202393    Top1 92.851562    Top5 99.882812    LR 0.003000    Time 0.023400    
2018-10-27 23:28:24,277 - Epoch: [244][  150/  391]    Overall Loss 0.199688    Objective Loss 0.199688    Top1 92.916667    Top5 99.906250    LR 0.003000    Time 0.023209    
2018-10-27 23:28:25,416 - Epoch: [244][  200/  391]    Overall Loss 0.200461    Objective Loss 0.200461    Top1 92.867188    Top5 99.910156    LR 0.003000    Time 0.023097    
2018-10-27 23:28:26,559 - Epoch: [244][  250/  391]    Overall Loss 0.201415    Objective Loss 0.201415    Top1 92.828125    Top5 99.900000    LR 0.003000    Time 0.023046    
2018-10-27 23:28:27,703 - Epoch: [244][  300/  391]    Overall Loss 0.202600    Objective Loss 0.202600    Top1 92.807292    Top5 99.906250    LR 0.003000    Time 0.023011    
2018-10-27 23:28:28,846 - Epoch: [244][  350/  391]    Overall Loss 0.202904    Objective Loss 0.202904    Top1 92.794643    Top5 99.908482    LR 0.003000    Time 0.022987    
2018-10-27 23:28:29,861 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31568 | -0.00290 |    0.11805 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09731 | -0.00345 |    0.02157 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09119 |  0.00096 |    0.02212 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09481 | -0.00230 |    0.02250 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07398 | -0.00187 |    0.01458 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09928 | -0.00336 |    0.02732 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07278 | -0.00095 |    0.01628 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10400 | -0.00309 |    0.03645 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08309 | -0.00207 |    0.02557 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11405 | -0.00596 |    0.03594 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07214 | -0.00108 |    0.01996 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05811 | -0.00064 |    0.01400 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07779 | -0.00183 |    0.02289 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05908 | -0.00305 |    0.01499 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07153 | -0.00104 |    0.02387 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06889 | -0.00251 |    0.02349 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06278 |  0.00043 |    0.01609 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06438 | -0.00157 |    0.01975 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05307 | -0.00164 |    0.01370 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05085 | -0.00034 |    0.01333 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03118 |  0.00118 |    0.00555 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54010 | -0.07440 |    0.26651 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:28:29,861 - Total sparsity: 88.62

2018-10-27 23:28:29,861 - --- validate (epoch=244)-----------
2018-10-27 23:28:29,862 - 10000 samples (128 per mini-batch)
2018-10-27 23:28:30,582 - Epoch: [244][   50/   78]    Loss 0.397566    Top1 87.625000    Top5 99.468750    
2018-10-27 23:28:30,973 - ==> Top1: 87.760    Top5: 99.480    Loss: 0.394

2018-10-27 23:28:30,974 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:28:30,974 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:28:30,983 - 

2018-10-27 23:28:30,984 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:28:32,183 - Epoch: [245][   50/  391]    Overall Loss 0.198493    Objective Loss 0.198493    Top1 93.171875    Top5 99.906250    LR 0.003000    Time 0.023953    
2018-10-27 23:28:33,325 - Epoch: [245][  100/  391]    Overall Loss 0.199863    Objective Loss 0.199863    Top1 92.953125    Top5 99.906250    LR 0.003000    Time 0.023376    
2018-10-27 23:28:34,467 - Epoch: [245][  150/  391]    Overall Loss 0.200156    Objective Loss 0.200156    Top1 92.817708    Top5 99.906250    LR 0.003000    Time 0.023189    
2018-10-27 23:28:35,608 - Epoch: [245][  200/  391]    Overall Loss 0.201755    Objective Loss 0.201755    Top1 92.781250    Top5 99.890625    LR 0.003000    Time 0.023090    
2018-10-27 23:28:36,747 - Epoch: [245][  250/  391]    Overall Loss 0.202242    Objective Loss 0.202242    Top1 92.781250    Top5 99.881250    LR 0.003000    Time 0.023024    
2018-10-27 23:28:37,887 - Epoch: [245][  300/  391]    Overall Loss 0.200123    Objective Loss 0.200123    Top1 92.864583    Top5 99.893229    LR 0.003000    Time 0.022970    
2018-10-27 23:28:39,029 - Epoch: [245][  350/  391]    Overall Loss 0.200183    Objective Loss 0.200183    Top1 92.857143    Top5 99.892857    LR 0.003000    Time 0.022947    
2018-10-27 23:28:40,043 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31536 | -0.00284 |    0.11786 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09722 | -0.00343 |    0.02154 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09110 |  0.00094 |    0.02210 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09471 | -0.00229 |    0.02246 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07390 | -0.00186 |    0.01456 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09918 | -0.00335 |    0.02728 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07271 | -0.00096 |    0.01626 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10390 | -0.00309 |    0.03643 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08301 | -0.00207 |    0.02554 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11394 | -0.00590 |    0.03595 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07207 | -0.00108 |    0.01994 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05805 | -0.00062 |    0.01398 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07771 | -0.00182 |    0.02287 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05902 | -0.00303 |    0.01496 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07146 | -0.00102 |    0.02385 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06882 | -0.00251 |    0.02346 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06272 |  0.00046 |    0.01608 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06432 | -0.00159 |    0.01973 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05302 | -0.00163 |    0.01369 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05080 | -0.00035 |    0.01331 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03115 |  0.00118 |    0.00554 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54050 | -0.07441 |    0.26671 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:28:40,043 - Total sparsity: 88.62

2018-10-27 23:28:40,044 - --- validate (epoch=245)-----------
2018-10-27 23:28:40,044 - 10000 samples (128 per mini-batch)
2018-10-27 23:28:40,769 - Epoch: [245][   50/   78]    Loss 0.395667    Top1 87.515625    Top5 99.484375    
2018-10-27 23:28:41,163 - ==> Top1: 87.810    Top5: 99.520    Loss: 0.389

2018-10-27 23:28:41,163 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:28:41,163 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:28:41,174 - 

2018-10-27 23:28:41,174 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:28:42,372 - Epoch: [246][   50/  391]    Overall Loss 0.198846    Objective Loss 0.198846    Top1 93.031250    Top5 99.906250    LR 0.003000    Time 0.023921    
2018-10-27 23:28:43,512 - Epoch: [246][  100/  391]    Overall Loss 0.200909    Objective Loss 0.200909    Top1 93.023438    Top5 99.890625    LR 0.003000    Time 0.023346    
2018-10-27 23:28:44,654 - Epoch: [246][  150/  391]    Overall Loss 0.198909    Objective Loss 0.198909    Top1 93.109375    Top5 99.890625    LR 0.003000    Time 0.023166    
2018-10-27 23:28:45,795 - Epoch: [246][  200/  391]    Overall Loss 0.201034    Objective Loss 0.201034    Top1 93.046875    Top5 99.882812    LR 0.003000    Time 0.023073    
2018-10-27 23:28:46,937 - Epoch: [246][  250/  391]    Overall Loss 0.201995    Objective Loss 0.201995    Top1 92.934375    Top5 99.896875    LR 0.003000    Time 0.023022    
2018-10-27 23:28:48,076 - Epoch: [246][  300/  391]    Overall Loss 0.202010    Objective Loss 0.202010    Top1 92.908854    Top5 99.903646    LR 0.003000    Time 0.022976    
2018-10-27 23:28:49,216 - Epoch: [246][  350/  391]    Overall Loss 0.203477    Objective Loss 0.203477    Top1 92.839286    Top5 99.892857    LR 0.003000    Time 0.022948    
2018-10-27 23:28:50,228 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31504 | -0.00266 |    0.11774 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09712 | -0.00345 |    0.02154 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09101 |  0.00096 |    0.02208 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09462 | -0.00223 |    0.02243 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07383 | -0.00187 |    0.01453 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09908 | -0.00340 |    0.02724 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07264 | -0.00093 |    0.01622 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10379 | -0.00307 |    0.03640 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08293 | -0.00205 |    0.02552 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11382 | -0.00580 |    0.03593 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07200 | -0.00109 |    0.01993 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05800 | -0.00062 |    0.01397 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07763 | -0.00182 |    0.02285 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05896 | -0.00302 |    0.01494 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07139 | -0.00102 |    0.02383 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06876 | -0.00251 |    0.02344 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06265 |  0.00046 |    0.01605 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06425 | -0.00156 |    0.01971 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05296 | -0.00163 |    0.01367 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05075 | -0.00034 |    0.01330 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03112 |  0.00118 |    0.00554 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54080 | -0.07444 |    0.26687 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:28:50,228 - Total sparsity: 88.62

2018-10-27 23:28:50,228 - --- validate (epoch=246)-----------
2018-10-27 23:28:50,228 - 10000 samples (128 per mini-batch)
2018-10-27 23:28:50,943 - Epoch: [246][   50/   78]    Loss 0.397290    Top1 87.484375    Top5 99.500000    
2018-10-27 23:28:51,334 - ==> Top1: 87.650    Top5: 99.510    Loss: 0.392

2018-10-27 23:28:51,335 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:28:51,335 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:28:51,349 - 

2018-10-27 23:28:51,349 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:28:52,547 - Epoch: [247][   50/  391]    Overall Loss 0.189890    Objective Loss 0.189890    Top1 93.390625    Top5 99.843750    LR 0.003000    Time 0.023919    
2018-10-27 23:28:53,688 - Epoch: [247][  100/  391]    Overall Loss 0.198829    Objective Loss 0.198829    Top1 93.226562    Top5 99.851562    LR 0.003000    Time 0.023357    
2018-10-27 23:28:54,828 - Epoch: [247][  150/  391]    Overall Loss 0.199411    Objective Loss 0.199411    Top1 93.114583    Top5 99.864583    LR 0.003000    Time 0.023164    
2018-10-27 23:28:55,973 - Epoch: [247][  200/  391]    Overall Loss 0.201326    Objective Loss 0.201326    Top1 92.964844    Top5 99.867188    LR 0.003000    Time 0.023090    
2018-10-27 23:28:57,114 - Epoch: [247][  250/  391]    Overall Loss 0.201369    Objective Loss 0.201369    Top1 92.928125    Top5 99.875000    LR 0.003000    Time 0.023031    
2018-10-27 23:28:58,255 - Epoch: [247][  300/  391]    Overall Loss 0.201861    Objective Loss 0.201861    Top1 92.867188    Top5 99.872396    LR 0.003000    Time 0.022991    
2018-10-27 23:28:59,396 - Epoch: [247][  350/  391]    Overall Loss 0.202875    Objective Loss 0.202875    Top1 92.843750    Top5 99.870536    LR 0.003000    Time 0.022964    
2018-10-27 23:29:00,415 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31472 | -0.00249 |    0.11776 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09702 | -0.00344 |    0.02149 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09091 |  0.00103 |    0.02207 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09452 | -0.00219 |    0.02241 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07375 | -0.00187 |    0.01451 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09898 | -0.00333 |    0.02724 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07256 | -0.00093 |    0.01620 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10369 | -0.00308 |    0.03635 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08284 | -0.00204 |    0.02549 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11370 | -0.00586 |    0.03588 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07193 | -0.00109 |    0.01991 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05794 | -0.00063 |    0.01395 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07755 | -0.00180 |    0.02282 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05890 | -0.00300 |    0.01493 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07131 | -0.00102 |    0.02381 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06869 | -0.00251 |    0.02341 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06259 |  0.00043 |    0.01605 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06419 | -0.00155 |    0.01969 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05291 | -0.00162 |    0.01365 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05070 | -0.00033 |    0.01328 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03109 |  0.00118 |    0.00553 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54110 | -0.07445 |    0.26699 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:29:00,415 - Total sparsity: 88.62

2018-10-27 23:29:00,415 - --- validate (epoch=247)-----------
2018-10-27 23:29:00,415 - 10000 samples (128 per mini-batch)
2018-10-27 23:29:01,130 - Epoch: [247][   50/   78]    Loss 0.399910    Top1 87.359375    Top5 99.468750    
2018-10-27 23:29:01,517 - ==> Top1: 87.630    Top5: 99.490    Loss: 0.392

2018-10-27 23:29:01,518 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:29:01,518 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:29:01,532 - 

2018-10-27 23:29:01,533 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:29:02,732 - Epoch: [248][   50/  391]    Overall Loss 0.200606    Objective Loss 0.200606    Top1 92.750000    Top5 99.843750    LR 0.003000    Time 0.023945    
2018-10-27 23:29:03,875 - Epoch: [248][  100/  391]    Overall Loss 0.201447    Objective Loss 0.201447    Top1 92.812500    Top5 99.835938    LR 0.003000    Time 0.023385    
2018-10-27 23:29:05,015 - Epoch: [248][  150/  391]    Overall Loss 0.200833    Objective Loss 0.200833    Top1 92.859375    Top5 99.848958    LR 0.003000    Time 0.023188    
2018-10-27 23:29:06,158 - Epoch: [248][  200/  391]    Overall Loss 0.203201    Objective Loss 0.203201    Top1 92.828125    Top5 99.867188    LR 0.003000    Time 0.023097    
2018-10-27 23:29:07,300 - Epoch: [248][  250/  391]    Overall Loss 0.204621    Objective Loss 0.204621    Top1 92.759375    Top5 99.865625    LR 0.003000    Time 0.023040    
2018-10-27 23:29:08,443 - Epoch: [248][  300/  391]    Overall Loss 0.202991    Objective Loss 0.202991    Top1 92.838542    Top5 99.875000    LR 0.003000    Time 0.023007    
2018-10-27 23:29:09,588 - Epoch: [248][  350/  391]    Overall Loss 0.203474    Objective Loss 0.203474    Top1 92.805804    Top5 99.883929    LR 0.003000    Time 0.022987    
2018-10-27 23:29:10,607 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31440 | -0.00279 |    0.11762 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09692 | -0.00347 |    0.02147 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09082 |  0.00099 |    0.02202 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09443 | -0.00226 |    0.02238 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07368 | -0.00188 |    0.01449 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09888 | -0.00341 |    0.02721 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07249 | -0.00093 |    0.01621 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10358 | -0.00310 |    0.03629 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08276 | -0.00205 |    0.02546 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11359 | -0.00584 |    0.03584 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07186 | -0.00107 |    0.01989 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05788 | -0.00061 |    0.01394 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07748 | -0.00180 |    0.02280 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05884 | -0.00299 |    0.01491 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07124 | -0.00101 |    0.02377 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06862 | -0.00250 |    0.02338 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06252 |  0.00043 |    0.01603 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06413 | -0.00155 |    0.01967 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05286 | -0.00162 |    0.01364 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05065 | -0.00033 |    0.01327 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03106 |  0.00118 |    0.00553 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54144 | -0.07446 |    0.26715 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:29:10,607 - Total sparsity: 88.62

2018-10-27 23:29:10,607 - --- validate (epoch=248)-----------
2018-10-27 23:29:10,607 - 10000 samples (128 per mini-batch)
2018-10-27 23:29:11,333 - Epoch: [248][   50/   78]    Loss 0.394866    Top1 87.390625    Top5 99.546875    
2018-10-27 23:29:11,727 - ==> Top1: 87.730    Top5: 99.560    Loss: 0.387

2018-10-27 23:29:11,728 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:29:11,728 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:29:11,738 - 

2018-10-27 23:29:11,738 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:29:12,936 - Epoch: [249][   50/  391]    Overall Loss 0.205240    Objective Loss 0.205240    Top1 92.812500    Top5 99.921875    LR 0.003000    Time 0.023923    
2018-10-27 23:29:14,079 - Epoch: [249][  100/  391]    Overall Loss 0.208864    Objective Loss 0.208864    Top1 92.632812    Top5 99.875000    LR 0.003000    Time 0.023371    
2018-10-27 23:29:15,220 - Epoch: [249][  150/  391]    Overall Loss 0.208238    Objective Loss 0.208238    Top1 92.619792    Top5 99.895833    LR 0.003000    Time 0.023177    
2018-10-27 23:29:16,359 - Epoch: [249][  200/  391]    Overall Loss 0.206579    Objective Loss 0.206579    Top1 92.687500    Top5 99.894531    LR 0.003000    Time 0.023073    
2018-10-27 23:29:17,500 - Epoch: [249][  250/  391]    Overall Loss 0.206272    Objective Loss 0.206272    Top1 92.615625    Top5 99.906250    LR 0.003000    Time 0.023016    
2018-10-27 23:29:18,642 - Epoch: [249][  300/  391]    Overall Loss 0.203942    Objective Loss 0.203942    Top1 92.723958    Top5 99.903646    LR 0.003000    Time 0.022983    
2018-10-27 23:29:19,785 - Epoch: [249][  350/  391]    Overall Loss 0.202800    Objective Loss 0.202800    Top1 92.825893    Top5 99.906250    LR 0.003000    Time 0.022962    
2018-10-27 23:29:20,802 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31408 | -0.00297 |    0.11749 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09682 | -0.00349 |    0.02146 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09073 |  0.00101 |    0.02200 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09433 | -0.00226 |    0.02235 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07361 | -0.00188 |    0.01446 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09878 | -0.00344 |    0.02718 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07242 | -0.00090 |    0.01619 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10348 | -0.00312 |    0.03625 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08267 | -0.00205 |    0.02544 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11346 | -0.00593 |    0.03581 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07178 | -0.00107 |    0.01987 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05782 | -0.00061 |    0.01393 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07740 | -0.00180 |    0.02277 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05878 | -0.00299 |    0.01490 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07117 | -0.00102 |    0.02374 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06855 | -0.00250 |    0.02336 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06246 |  0.00042 |    0.01601 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06406 | -0.00154 |    0.01965 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05281 | -0.00161 |    0.01363 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05060 | -0.00032 |    0.01326 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03102 |  0.00118 |    0.00552 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54180 | -0.07448 |    0.26734 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:29:20,802 - Total sparsity: 88.62

2018-10-27 23:29:20,802 - --- validate (epoch=249)-----------
2018-10-27 23:29:20,802 - 10000 samples (128 per mini-batch)
2018-10-27 23:29:21,521 - Epoch: [249][   50/   78]    Loss 0.400939    Top1 87.250000    Top5 99.515625    
2018-10-27 23:29:21,912 - ==> Top1: 87.440    Top5: 99.540    Loss: 0.394

2018-10-27 23:29:21,913 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:29:21,913 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:29:21,923 - 

2018-10-27 23:29:21,923 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:29:23,124 - Epoch: [250][   50/  391]    Overall Loss 0.199634    Objective Loss 0.199634    Top1 92.734375    Top5 99.906250    LR 0.001500    Time 0.023983    
2018-10-27 23:29:24,269 - Epoch: [250][  100/  391]    Overall Loss 0.200509    Objective Loss 0.200509    Top1 92.835938    Top5 99.890625    LR 0.001500    Time 0.023425    
2018-10-27 23:29:25,413 - Epoch: [250][  150/  391]    Overall Loss 0.201755    Objective Loss 0.201755    Top1 92.869792    Top5 99.895833    LR 0.001500    Time 0.023235    
2018-10-27 23:29:26,556 - Epoch: [250][  200/  391]    Overall Loss 0.198423    Objective Loss 0.198423    Top1 93.015625    Top5 99.902344    LR 0.001500    Time 0.023132    
2018-10-27 23:29:27,700 - Epoch: [250][  250/  391]    Overall Loss 0.198907    Objective Loss 0.198907    Top1 93.009375    Top5 99.900000    LR 0.001500    Time 0.023077    
2018-10-27 23:29:28,843 - Epoch: [250][  300/  391]    Overall Loss 0.199633    Objective Loss 0.199633    Top1 92.973958    Top5 99.890625    LR 0.001500    Time 0.023039    
2018-10-27 23:29:29,987 - Epoch: [250][  350/  391]    Overall Loss 0.199502    Objective Loss 0.199502    Top1 92.975446    Top5 99.892857    LR 0.001500    Time 0.023012    
2018-10-27 23:29:31,005 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31390 | -0.00290 |    0.11747 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09677 | -0.00347 |    0.02144 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09068 |  0.00101 |    0.02199 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09428 | -0.00224 |    0.02234 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07357 | -0.00187 |    0.01445 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09872 | -0.00344 |    0.02715 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07238 | -0.00091 |    0.01618 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10342 | -0.00314 |    0.03623 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08263 | -0.00206 |    0.02542 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11340 | -0.00588 |    0.03577 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07175 | -0.00107 |    0.01986 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05779 | -0.00060 |    0.01392 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07736 | -0.00178 |    0.02277 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05875 | -0.00300 |    0.01489 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07113 | -0.00102 |    0.02373 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06851 | -0.00250 |    0.02335 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06242 |  0.00041 |    0.01601 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06403 | -0.00153 |    0.01964 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05278 | -0.00161 |    0.01362 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05057 | -0.00033 |    0.01325 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03101 |  0.00118 |    0.00552 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54197 | -0.07447 |    0.26739 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:29:31,005 - Total sparsity: 88.62

2018-10-27 23:29:31,005 - --- validate (epoch=250)-----------
2018-10-27 23:29:31,005 - 10000 samples (128 per mini-batch)
2018-10-27 23:29:31,729 - Epoch: [250][   50/   78]    Loss 0.399969    Top1 87.218750    Top5 99.515625    
2018-10-27 23:29:32,124 - ==> Top1: 87.510    Top5: 99.540    Loss: 0.392

2018-10-27 23:29:32,125 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:29:32,125 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:29:32,139 - 

2018-10-27 23:29:32,140 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:29:33,336 - Epoch: [251][   50/  391]    Overall Loss 0.197427    Objective Loss 0.197427    Top1 93.171875    Top5 99.968750    LR 0.001500    Time 0.023896    
2018-10-27 23:29:34,478 - Epoch: [251][  100/  391]    Overall Loss 0.199535    Objective Loss 0.199535    Top1 93.078125    Top5 99.890625    LR 0.001500    Time 0.023349    
2018-10-27 23:29:35,617 - Epoch: [251][  150/  391]    Overall Loss 0.194987    Objective Loss 0.194987    Top1 93.322917    Top5 99.885417    LR 0.001500    Time 0.023153    
2018-10-27 23:29:36,757 - Epoch: [251][  200/  391]    Overall Loss 0.198548    Objective Loss 0.198548    Top1 93.074219    Top5 99.890625    LR 0.001500    Time 0.023057    
2018-10-27 23:29:37,899 - Epoch: [251][  250/  391]    Overall Loss 0.197852    Objective Loss 0.197852    Top1 93.178125    Top5 99.890625    LR 0.001500    Time 0.023006    
2018-10-27 23:29:39,039 - Epoch: [251][  300/  391]    Overall Loss 0.198895    Objective Loss 0.198895    Top1 93.098958    Top5 99.893229    LR 0.001500    Time 0.022969    
2018-10-27 23:29:40,179 - Epoch: [251][  350/  391]    Overall Loss 0.198171    Objective Loss 0.198171    Top1 93.113839    Top5 99.895089    LR 0.001500    Time 0.022942    
2018-10-27 23:29:41,196 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31373 | -0.00273 |    0.11741 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09672 | -0.00346 |    0.02144 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09063 |  0.00100 |    0.02196 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09423 | -0.00223 |    0.02231 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07353 | -0.00185 |    0.01444 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09867 | -0.00344 |    0.02713 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07234 | -0.00090 |    0.01617 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10336 | -0.00314 |    0.03621 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08258 | -0.00205 |    0.02540 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11334 | -0.00590 |    0.03578 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07171 | -0.00106 |    0.01985 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05776 | -0.00060 |    0.01391 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07732 | -0.00176 |    0.02275 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05872 | -0.00299 |    0.01489 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07109 | -0.00102 |    0.02372 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06847 | -0.00250 |    0.02333 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06239 |  0.00040 |    0.01600 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06399 | -0.00153 |    0.01962 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05275 | -0.00160 |    0.01361 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05054 | -0.00033 |    0.01324 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03099 |  0.00117 |    0.00551 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54216 | -0.07449 |    0.26749 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:29:41,197 - Total sparsity: 88.62

2018-10-27 23:29:41,197 - --- validate (epoch=251)-----------
2018-10-27 23:29:41,197 - 10000 samples (128 per mini-batch)
2018-10-27 23:29:41,922 - Epoch: [251][   50/   78]    Loss 0.396354    Top1 87.500000    Top5 99.515625    
2018-10-27 23:29:42,314 - ==> Top1: 87.690    Top5: 99.570    Loss: 0.390

2018-10-27 23:29:42,315 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:29:42,315 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:29:42,334 - 

2018-10-27 23:29:42,335 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:29:43,508 - Epoch: [252][   50/  391]    Overall Loss 0.197981    Objective Loss 0.197981    Top1 93.140625    Top5 99.921875    LR 0.001500    Time 0.023422    
2018-10-27 23:29:44,648 - Epoch: [252][  100/  391]    Overall Loss 0.198019    Objective Loss 0.198019    Top1 93.164062    Top5 99.906250    LR 0.001500    Time 0.023103    
2018-10-27 23:29:45,791 - Epoch: [252][  150/  391]    Overall Loss 0.198041    Objective Loss 0.198041    Top1 93.255208    Top5 99.906250    LR 0.001500    Time 0.023012    
2018-10-27 23:29:46,934 - Epoch: [252][  200/  391]    Overall Loss 0.197691    Objective Loss 0.197691    Top1 93.226562    Top5 99.910156    LR 0.001500    Time 0.022965    
2018-10-27 23:29:48,076 - Epoch: [252][  250/  391]    Overall Loss 0.196189    Objective Loss 0.196189    Top1 93.215625    Top5 99.909375    LR 0.001500    Time 0.022936    
2018-10-27 23:29:49,218 - Epoch: [252][  300/  391]    Overall Loss 0.196536    Objective Loss 0.196536    Top1 93.213542    Top5 99.908854    LR 0.001500    Time 0.022917    
2018-10-27 23:29:50,363 - Epoch: [252][  350/  391]    Overall Loss 0.195929    Objective Loss 0.195929    Top1 93.227679    Top5 99.904018    LR 0.001500    Time 0.022910    
2018-10-27 23:29:51,379 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31356 | -0.00285 |    0.11727 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09667 | -0.00344 |    0.02143 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09058 |  0.00098 |    0.02194 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09418 | -0.00225 |    0.02231 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07349 | -0.00184 |    0.01443 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09862 | -0.00345 |    0.02712 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07230 | -0.00091 |    0.01616 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10331 | -0.00310 |    0.03618 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08254 | -0.00205 |    0.02539 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11328 | -0.00590 |    0.03579 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07167 | -0.00107 |    0.01983 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05773 | -0.00060 |    0.01390 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07728 | -0.00174 |    0.02274 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05869 | -0.00298 |    0.01488 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07105 | -0.00102 |    0.02370 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06844 | -0.00249 |    0.02332 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06235 |  0.00041 |    0.01599 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06396 | -0.00153 |    0.01961 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05272 | -0.00160 |    0.01360 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05051 | -0.00033 |    0.01323 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03097 |  0.00117 |    0.00551 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54236 | -0.07448 |    0.26758 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:29:51,380 - Total sparsity: 88.62

2018-10-27 23:29:51,380 - --- validate (epoch=252)-----------
2018-10-27 23:29:51,380 - 10000 samples (128 per mini-batch)
2018-10-27 23:29:52,100 - Epoch: [252][   50/   78]    Loss 0.397403    Top1 87.453125    Top5 99.484375    
2018-10-27 23:29:52,492 - ==> Top1: 87.620    Top5: 99.530    Loss: 0.392

2018-10-27 23:29:52,492 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:29:52,493 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:29:52,503 - 

2018-10-27 23:29:52,504 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:29:53,703 - Epoch: [253][   50/  391]    Overall Loss 0.193115    Objective Loss 0.193115    Top1 93.156250    Top5 99.875000    LR 0.001500    Time 0.023959    
2018-10-27 23:29:54,846 - Epoch: [253][  100/  391]    Overall Loss 0.192324    Objective Loss 0.192324    Top1 93.203125    Top5 99.882812    LR 0.001500    Time 0.023391    
2018-10-27 23:29:55,992 - Epoch: [253][  150/  391]    Overall Loss 0.194884    Objective Loss 0.194884    Top1 93.260417    Top5 99.864583    LR 0.001500    Time 0.023223    
2018-10-27 23:29:57,138 - Epoch: [253][  200/  391]    Overall Loss 0.194932    Objective Loss 0.194932    Top1 93.292969    Top5 99.863281    LR 0.001500    Time 0.023142    
2018-10-27 23:29:58,283 - Epoch: [253][  250/  391]    Overall Loss 0.193462    Objective Loss 0.193462    Top1 93.328125    Top5 99.865625    LR 0.001500    Time 0.023089    
2018-10-27 23:29:59,427 - Epoch: [253][  300/  391]    Overall Loss 0.192579    Objective Loss 0.192579    Top1 93.361979    Top5 99.877604    LR 0.001500    Time 0.023049    
2018-10-27 23:30:00,574 - Epoch: [253][  350/  391]    Overall Loss 0.193973    Objective Loss 0.193973    Top1 93.308036    Top5 99.883929    LR 0.001500    Time 0.023031    
2018-10-27 23:30:01,591 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31339 | -0.00284 |    0.11722 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09661 | -0.00345 |    0.02141 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09053 |  0.00097 |    0.02192 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09413 | -0.00226 |    0.02230 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07345 | -0.00184 |    0.01443 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09856 | -0.00344 |    0.02710 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07226 | -0.00093 |    0.01615 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10325 | -0.00308 |    0.03615 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08249 | -0.00204 |    0.02537 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11321 | -0.00588 |    0.03574 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07163 | -0.00107 |    0.01982 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05770 | -0.00059 |    0.01389 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07723 | -0.00174 |    0.02272 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05866 | -0.00298 |    0.01487 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07102 | -0.00102 |    0.02368 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06840 | -0.00249 |    0.02330 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06232 |  0.00041 |    0.01597 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06392 | -0.00153 |    0.01960 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05269 | -0.00161 |    0.01360 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05049 | -0.00033 |    0.01323 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03096 |  0.00117 |    0.00551 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54260 | -0.07449 |    0.26768 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:30:01,591 - Total sparsity: 88.62

2018-10-27 23:30:01,591 - --- validate (epoch=253)-----------
2018-10-27 23:30:01,591 - 10000 samples (128 per mini-batch)
2018-10-27 23:30:02,313 - Epoch: [253][   50/   78]    Loss 0.396459    Top1 87.312500    Top5 99.515625    
2018-10-27 23:30:02,703 - ==> Top1: 87.580    Top5: 99.540    Loss: 0.391

2018-10-27 23:30:02,704 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:30:02,704 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:30:02,714 - 

2018-10-27 23:30:02,714 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:30:03,913 - Epoch: [254][   50/  391]    Overall Loss 0.187238    Objective Loss 0.187238    Top1 93.546875    Top5 99.921875    LR 0.001500    Time 0.023934    
2018-10-27 23:30:05,057 - Epoch: [254][  100/  391]    Overall Loss 0.190240    Objective Loss 0.190240    Top1 93.312500    Top5 99.937500    LR 0.001500    Time 0.023398    
2018-10-27 23:30:06,197 - Epoch: [254][  150/  391]    Overall Loss 0.193104    Objective Loss 0.193104    Top1 93.250000    Top5 99.911458    LR 0.001500    Time 0.023192    
2018-10-27 23:30:07,340 - Epoch: [254][  200/  391]    Overall Loss 0.195281    Objective Loss 0.195281    Top1 93.078125    Top5 99.898438    LR 0.001500    Time 0.023102    
2018-10-27 23:30:08,480 - Epoch: [254][  250/  391]    Overall Loss 0.194526    Objective Loss 0.194526    Top1 93.125000    Top5 99.915625    LR 0.001500    Time 0.023036    
2018-10-27 23:30:09,621 - Epoch: [254][  300/  391]    Overall Loss 0.193616    Objective Loss 0.193616    Top1 93.132812    Top5 99.911458    LR 0.001500    Time 0.022997    
2018-10-27 23:30:10,762 - Epoch: [254][  350/  391]    Overall Loss 0.194939    Objective Loss 0.194939    Top1 93.073661    Top5 99.904018    LR 0.001500    Time 0.022967    
2018-10-27 23:30:11,784 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31322 | -0.00258 |    0.11724 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09656 | -0.00344 |    0.02140 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09048 |  0.00096 |    0.02191 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09407 | -0.00226 |    0.02228 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07341 | -0.00184 |    0.01442 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09851 | -0.00342 |    0.02708 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07222 | -0.00094 |    0.01614 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10319 | -0.00311 |    0.03614 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08245 | -0.00204 |    0.02536 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11315 | -0.00593 |    0.03573 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07159 | -0.00106 |    0.01981 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05767 | -0.00059 |    0.01389 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07719 | -0.00174 |    0.02271 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05862 | -0.00298 |    0.01486 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07098 | -0.00101 |    0.02367 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06836 | -0.00249 |    0.02329 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06228 |  0.00041 |    0.01595 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06389 | -0.00153 |    0.01959 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05266 | -0.00160 |    0.01359 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05046 | -0.00034 |    0.01322 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03094 |  0.00117 |    0.00550 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54284 | -0.07449 |    0.26778 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:30:11,784 - Total sparsity: 88.62

2018-10-27 23:30:11,784 - --- validate (epoch=254)-----------
2018-10-27 23:30:11,785 - 10000 samples (128 per mini-batch)
2018-10-27 23:30:12,507 - Epoch: [254][   50/   78]    Loss 0.397268    Top1 87.453125    Top5 99.468750    
2018-10-27 23:30:12,898 - ==> Top1: 87.640    Top5: 99.540    Loss: 0.391

2018-10-27 23:30:12,899 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:30:12,899 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:30:12,908 - 

2018-10-27 23:30:12,909 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:30:14,106 - Epoch: [255][   50/  391]    Overall Loss 0.183931    Objective Loss 0.183931    Top1 93.703125    Top5 99.890625    LR 0.001500    Time 0.023902    
2018-10-27 23:30:15,244 - Epoch: [255][  100/  391]    Overall Loss 0.182096    Objective Loss 0.182096    Top1 93.765625    Top5 99.898438    LR 0.001500    Time 0.023319    
2018-10-27 23:30:16,383 - Epoch: [255][  150/  391]    Overall Loss 0.185495    Objective Loss 0.185495    Top1 93.604167    Top5 99.906250    LR 0.001500    Time 0.023136    
2018-10-27 23:30:17,524 - Epoch: [255][  200/  391]    Overall Loss 0.188473    Objective Loss 0.188473    Top1 93.406250    Top5 99.902344    LR 0.001500    Time 0.023048    
2018-10-27 23:30:18,664 - Epoch: [255][  250/  391]    Overall Loss 0.190801    Objective Loss 0.190801    Top1 93.359375    Top5 99.909375    LR 0.001500    Time 0.022993    
2018-10-27 23:30:19,806 - Epoch: [255][  300/  391]    Overall Loss 0.192451    Objective Loss 0.192451    Top1 93.299479    Top5 99.903646    LR 0.001500    Time 0.022952    
2018-10-27 23:30:20,948 - Epoch: [255][  350/  391]    Overall Loss 0.193477    Objective Loss 0.193477    Top1 93.214286    Top5 99.897321    LR 0.001500    Time 0.022934    
2018-10-27 23:30:21,967 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31305 | -0.00294 |    0.11715 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09651 | -0.00343 |    0.02137 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09043 |  0.00095 |    0.02190 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09402 | -0.00227 |    0.02226 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07337 | -0.00183 |    0.01442 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09846 | -0.00337 |    0.02706 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07218 | -0.00092 |    0.01613 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10314 | -0.00308 |    0.03612 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08240 | -0.00203 |    0.02534 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11309 | -0.00594 |    0.03570 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07155 | -0.00107 |    0.01980 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05763 | -0.00059 |    0.01388 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07715 | -0.00174 |    0.02270 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05859 | -0.00298 |    0.01485 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07094 | -0.00102 |    0.02365 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06833 | -0.00249 |    0.02328 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06225 |  0.00040 |    0.01594 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06386 | -0.00152 |    0.01957 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05264 | -0.00160 |    0.01358 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05043 | -0.00033 |    0.01321 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03092 |  0.00117 |    0.00550 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54305 | -0.07449 |    0.26787 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:30:21,967 - Total sparsity: 88.62

2018-10-27 23:30:21,967 - --- validate (epoch=255)-----------
2018-10-27 23:30:21,967 - 10000 samples (128 per mini-batch)
2018-10-27 23:30:22,685 - Epoch: [255][   50/   78]    Loss 0.396733    Top1 87.421875    Top5 99.515625    
2018-10-27 23:30:23,071 - ==> Top1: 87.630    Top5: 99.560    Loss: 0.390

2018-10-27 23:30:23,072 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:30:23,072 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:30:23,089 - 

2018-10-27 23:30:23,089 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:30:24,262 - Epoch: [256][   50/  391]    Overall Loss 0.195222    Objective Loss 0.195222    Top1 92.968750    Top5 99.890625    LR 0.001500    Time 0.023432    
2018-10-27 23:30:25,406 - Epoch: [256][  100/  391]    Overall Loss 0.194082    Objective Loss 0.194082    Top1 93.218750    Top5 99.898438    LR 0.001500    Time 0.023137    
2018-10-27 23:30:26,546 - Epoch: [256][  150/  391]    Overall Loss 0.195981    Objective Loss 0.195981    Top1 93.078125    Top5 99.921875    LR 0.001500    Time 0.023021    
2018-10-27 23:30:27,687 - Epoch: [256][  200/  391]    Overall Loss 0.195051    Objective Loss 0.195051    Top1 93.167969    Top5 99.917969    LR 0.001500    Time 0.022964    
2018-10-27 23:30:28,830 - Epoch: [256][  250/  391]    Overall Loss 0.197187    Objective Loss 0.197187    Top1 93.071875    Top5 99.909375    LR 0.001500    Time 0.022936    
2018-10-27 23:30:29,973 - Epoch: [256][  300/  391]    Overall Loss 0.196328    Objective Loss 0.196328    Top1 93.083333    Top5 99.903646    LR 0.001500    Time 0.022920    
2018-10-27 23:30:31,113 - Epoch: [256][  350/  391]    Overall Loss 0.196479    Objective Loss 0.196479    Top1 93.084821    Top5 99.897321    LR 0.001500    Time 0.022899    
2018-10-27 23:30:32,131 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31288 | -0.00278 |    0.11706 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09645 | -0.00346 |    0.02137 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09039 |  0.00098 |    0.02187 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09397 | -0.00220 |    0.02225 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07333 | -0.00183 |    0.01441 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09840 | -0.00338 |    0.02703 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07214 | -0.00093 |    0.01612 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10308 | -0.00308 |    0.03610 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08236 | -0.00204 |    0.02533 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11303 | -0.00592 |    0.03566 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07151 | -0.00106 |    0.01979 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05760 | -0.00060 |    0.01387 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07711 | -0.00174 |    0.02269 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05856 | -0.00298 |    0.01485 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07090 | -0.00102 |    0.02364 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06829 | -0.00248 |    0.02326 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06221 |  0.00040 |    0.01594 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06382 | -0.00152 |    0.01956 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05261 | -0.00160 |    0.01357 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05040 | -0.00033 |    0.01320 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03091 |  0.00117 |    0.00550 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54324 | -0.07452 |    0.26797 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:30:32,131 - Total sparsity: 88.62

2018-10-27 23:30:32,131 - --- validate (epoch=256)-----------
2018-10-27 23:30:32,131 - 10000 samples (128 per mini-batch)
2018-10-27 23:30:32,855 - Epoch: [256][   50/   78]    Loss 0.396551    Top1 87.343750    Top5 99.562500    
2018-10-27 23:30:33,248 - ==> Top1: 87.570    Top5: 99.590    Loss: 0.390

2018-10-27 23:30:33,249 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:30:33,249 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:30:33,263 - 

2018-10-27 23:30:33,264 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:30:34,460 - Epoch: [257][   50/  391]    Overall Loss 0.197396    Objective Loss 0.197396    Top1 93.156250    Top5 99.890625    LR 0.001500    Time 0.023886    
2018-10-27 23:30:35,600 - Epoch: [257][  100/  391]    Overall Loss 0.193150    Objective Loss 0.193150    Top1 93.328125    Top5 99.890625    LR 0.001500    Time 0.023334    
2018-10-27 23:30:36,742 - Epoch: [257][  150/  391]    Overall Loss 0.193983    Objective Loss 0.193983    Top1 93.203125    Top5 99.890625    LR 0.001500    Time 0.023157    
2018-10-27 23:30:37,884 - Epoch: [257][  200/  391]    Overall Loss 0.194619    Objective Loss 0.194619    Top1 93.171875    Top5 99.910156    LR 0.001500    Time 0.023073    
2018-10-27 23:30:39,027 - Epoch: [257][  250/  391]    Overall Loss 0.192923    Objective Loss 0.192923    Top1 93.203125    Top5 99.918750    LR 0.001500    Time 0.023023    
2018-10-27 23:30:40,170 - Epoch: [257][  300/  391]    Overall Loss 0.194408    Objective Loss 0.194408    Top1 93.138021    Top5 99.906250    LR 0.001500    Time 0.022992    
2018-10-27 23:30:41,314 - Epoch: [257][  350/  391]    Overall Loss 0.195108    Objective Loss 0.195108    Top1 93.136161    Top5 99.899554    LR 0.001500    Time 0.022971    
2018-10-27 23:30:42,331 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31270 | -0.00292 |    0.11694 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09640 | -0.00342 |    0.02136 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09034 |  0.00097 |    0.02186 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09392 | -0.00221 |    0.02224 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07329 | -0.00183 |    0.01439 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09835 | -0.00334 |    0.02701 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07210 | -0.00092 |    0.01611 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10303 | -0.00306 |    0.03607 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08231 | -0.00202 |    0.02532 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11296 | -0.00589 |    0.03562 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07147 | -0.00106 |    0.01978 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05757 | -0.00060 |    0.01387 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07707 | -0.00174 |    0.02268 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05853 | -0.00298 |    0.01484 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07086 | -0.00102 |    0.02363 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06825 | -0.00248 |    0.02325 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06218 |  0.00040 |    0.01593 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06379 | -0.00153 |    0.01955 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05258 | -0.00160 |    0.01357 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05038 | -0.00033 |    0.01319 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03089 |  0.00117 |    0.00549 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54345 | -0.07452 |    0.26806 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:30:42,332 - Total sparsity: 88.62

2018-10-27 23:30:42,332 - --- validate (epoch=257)-----------
2018-10-27 23:30:42,332 - 10000 samples (128 per mini-batch)
2018-10-27 23:30:43,057 - Epoch: [257][   50/   78]    Loss 0.398160    Top1 87.421875    Top5 99.500000    
2018-10-27 23:30:43,447 - ==> Top1: 87.640    Top5: 99.520    Loss: 0.393

2018-10-27 23:30:43,448 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:30:43,448 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:30:43,462 - 

2018-10-27 23:30:43,463 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:30:44,663 - Epoch: [258][   50/  391]    Overall Loss 0.193233    Objective Loss 0.193233    Top1 93.375000    Top5 99.953125    LR 0.001500    Time 0.023963    
2018-10-27 23:30:45,805 - Epoch: [258][  100/  391]    Overall Loss 0.188146    Objective Loss 0.188146    Top1 93.632812    Top5 99.890625    LR 0.001500    Time 0.023394    
2018-10-27 23:30:46,948 - Epoch: [258][  150/  391]    Overall Loss 0.193437    Objective Loss 0.193437    Top1 93.338542    Top5 99.880208    LR 0.001500    Time 0.023202    
2018-10-27 23:30:48,091 - Epoch: [258][  200/  391]    Overall Loss 0.196474    Objective Loss 0.196474    Top1 93.210938    Top5 99.867188    LR 0.001500    Time 0.023092    
2018-10-27 23:30:49,234 - Epoch: [258][  250/  391]    Overall Loss 0.196956    Objective Loss 0.196956    Top1 93.250000    Top5 99.875000    LR 0.001500    Time 0.023043    
2018-10-27 23:30:50,377 - Epoch: [258][  300/  391]    Overall Loss 0.195748    Objective Loss 0.195748    Top1 93.278646    Top5 99.872396    LR 0.001500    Time 0.023007    
2018-10-27 23:30:51,519 - Epoch: [258][  350/  391]    Overall Loss 0.196619    Objective Loss 0.196619    Top1 93.229911    Top5 99.879464    LR 0.001500    Time 0.022979    
2018-10-27 23:30:52,540 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31253 | -0.00309 |    0.11686 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09635 | -0.00343 |    0.02136 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09029 |  0.00097 |    0.02186 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09387 | -0.00221 |    0.02221 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07325 | -0.00184 |    0.01440 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09830 | -0.00336 |    0.02699 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07207 | -0.00093 |    0.01610 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10297 | -0.00306 |    0.03605 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08227 | -0.00203 |    0.02530 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11290 | -0.00583 |    0.03560 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07144 | -0.00105 |    0.01977 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05754 | -0.00061 |    0.01386 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07702 | -0.00175 |    0.02267 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05850 | -0.00298 |    0.01483 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07082 | -0.00102 |    0.02361 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06821 | -0.00248 |    0.02324 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06214 |  0.00040 |    0.01592 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06375 | -0.00152 |    0.01954 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05255 | -0.00159 |    0.01356 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05035 | -0.00033 |    0.01319 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03087 |  0.00117 |    0.00549 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54362 | -0.07454 |    0.26813 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:30:52,540 - Total sparsity: 88.62

2018-10-27 23:30:52,540 - --- validate (epoch=258)-----------
2018-10-27 23:30:52,540 - 10000 samples (128 per mini-batch)
2018-10-27 23:30:53,261 - Epoch: [258][   50/   78]    Loss 0.397060    Top1 87.406250    Top5 99.484375    
2018-10-27 23:30:53,644 - ==> Top1: 87.540    Top5: 99.520    Loss: 0.390

2018-10-27 23:30:53,645 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:30:53,645 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:30:53,656 - 

2018-10-27 23:30:53,656 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:30:54,860 - Epoch: [259][   50/  391]    Overall Loss 0.186260    Objective Loss 0.186260    Top1 93.281250    Top5 99.921875    LR 0.001500    Time 0.024033    
2018-10-27 23:30:56,001 - Epoch: [259][  100/  391]    Overall Loss 0.188389    Objective Loss 0.188389    Top1 93.109375    Top5 99.906250    LR 0.001500    Time 0.023420    
2018-10-27 23:30:57,144 - Epoch: [259][  150/  391]    Overall Loss 0.191433    Objective Loss 0.191433    Top1 93.062500    Top5 99.875000    LR 0.001500    Time 0.023223    
2018-10-27 23:30:58,287 - Epoch: [259][  200/  391]    Overall Loss 0.191662    Objective Loss 0.191662    Top1 93.066406    Top5 99.878906    LR 0.001500    Time 0.023125    
2018-10-27 23:30:59,429 - Epoch: [259][  250/  391]    Overall Loss 0.193048    Objective Loss 0.193048    Top1 93.071875    Top5 99.890625    LR 0.001500    Time 0.023064    
2018-10-27 23:31:00,575 - Epoch: [259][  300/  391]    Overall Loss 0.193513    Objective Loss 0.193513    Top1 93.057292    Top5 99.888021    LR 0.001500    Time 0.023034    
2018-10-27 23:31:01,735 - Epoch: [259][  350/  391]    Overall Loss 0.192056    Objective Loss 0.192056    Top1 93.098214    Top5 99.890625    LR 0.001500    Time 0.023053    
2018-10-27 23:31:02,781 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31236 | -0.00334 |    0.11678 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09630 | -0.00344 |    0.02134 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09024 |  0.00099 |    0.02184 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09382 | -0.00224 |    0.02220 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07321 | -0.00184 |    0.01438 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09825 | -0.00335 |    0.02697 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07203 | -0.00093 |    0.01609 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10291 | -0.00305 |    0.03603 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08222 | -0.00202 |    0.02529 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11284 | -0.00582 |    0.03556 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07140 | -0.00106 |    0.01976 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05751 | -0.00060 |    0.01385 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07698 | -0.00175 |    0.02265 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05846 | -0.00298 |    0.01482 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07078 | -0.00101 |    0.02359 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06818 | -0.00247 |    0.02322 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06211 |  0.00041 |    0.01591 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06372 | -0.00151 |    0.01953 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05252 | -0.00159 |    0.01355 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05032 | -0.00034 |    0.01318 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03085 |  0.00117 |    0.00549 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54386 | -0.07453 |    0.26823 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:31:02,781 - Total sparsity: 88.62

2018-10-27 23:31:02,781 - --- validate (epoch=259)-----------
2018-10-27 23:31:02,781 - 10000 samples (128 per mini-batch)
2018-10-27 23:31:03,514 - Epoch: [259][   50/   78]    Loss 0.399259    Top1 87.546875    Top5 99.468750    
2018-10-27 23:31:03,909 - ==> Top1: 87.650    Top5: 99.490    Loss: 0.392

2018-10-27 23:31:03,910 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:31:03,910 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:31:03,924 - 

2018-10-27 23:31:03,925 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:31:05,160 - Epoch: [260][   50/  391]    Overall Loss 0.184594    Objective Loss 0.184594    Top1 93.343750    Top5 99.875000    LR 0.001500    Time 0.024653    
2018-10-27 23:31:06,333 - Epoch: [260][  100/  391]    Overall Loss 0.188536    Objective Loss 0.188536    Top1 93.328125    Top5 99.898438    LR 0.001500    Time 0.024042    
2018-10-27 23:31:07,508 - Epoch: [260][  150/  391]    Overall Loss 0.188577    Objective Loss 0.188577    Top1 93.322917    Top5 99.911458    LR 0.001500    Time 0.023854    
2018-10-27 23:31:08,682 - Epoch: [260][  200/  391]    Overall Loss 0.191588    Objective Loss 0.191588    Top1 93.195312    Top5 99.910156    LR 0.001500    Time 0.023754    
2018-10-27 23:31:09,854 - Epoch: [260][  250/  391]    Overall Loss 0.192745    Objective Loss 0.192745    Top1 93.190625    Top5 99.915625    LR 0.001500    Time 0.023687    
2018-10-27 23:31:11,026 - Epoch: [260][  300/  391]    Overall Loss 0.194027    Objective Loss 0.194027    Top1 93.177083    Top5 99.898438    LR 0.001500    Time 0.023629    
2018-10-27 23:31:12,203 - Epoch: [260][  350/  391]    Overall Loss 0.193427    Objective Loss 0.193427    Top1 93.223214    Top5 99.895089    LR 0.001500    Time 0.023612    
2018-10-27 23:31:13,254 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31219 | -0.00304 |    0.11669 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09625 | -0.00343 |    0.02133 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09019 |  0.00098 |    0.02184 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09377 | -0.00220 |    0.02217 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07317 | -0.00184 |    0.01437 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09819 | -0.00335 |    0.02697 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07199 | -0.00094 |    0.01608 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10286 | -0.00306 |    0.03601 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08218 | -0.00202 |    0.02527 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11278 | -0.00578 |    0.03555 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07136 | -0.00105 |    0.01975 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05748 | -0.00059 |    0.01385 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07694 | -0.00175 |    0.02264 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05843 | -0.00298 |    0.01481 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07074 | -0.00102 |    0.02358 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06814 | -0.00246 |    0.02321 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06208 |  0.00042 |    0.01590 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06368 | -0.00151 |    0.01952 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05249 | -0.00159 |    0.01354 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05029 | -0.00033 |    0.01317 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03084 |  0.00116 |    0.00548 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54405 | -0.07456 |    0.26831 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:31:13,254 - Total sparsity: 88.62

2018-10-27 23:31:13,254 - --- validate (epoch=260)-----------
2018-10-27 23:31:13,254 - 10000 samples (128 per mini-batch)
2018-10-27 23:31:13,982 - Epoch: [260][   50/   78]    Loss 0.397857    Top1 87.546875    Top5 99.484375    
2018-10-27 23:31:14,379 - ==> Top1: 87.660    Top5: 99.520    Loss: 0.392

2018-10-27 23:31:14,380 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:31:14,380 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:31:14,390 - 

2018-10-27 23:31:14,390 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:31:15,617 - Epoch: [261][   50/  391]    Overall Loss 0.180387    Objective Loss 0.180387    Top1 93.250000    Top5 99.968750    LR 0.001500    Time 0.024512    
2018-10-27 23:31:16,791 - Epoch: [261][  100/  391]    Overall Loss 0.186378    Objective Loss 0.186378    Top1 93.250000    Top5 99.953125    LR 0.001500    Time 0.023974    
2018-10-27 23:31:17,964 - Epoch: [261][  150/  391]    Overall Loss 0.190988    Objective Loss 0.190988    Top1 93.119792    Top5 99.937500    LR 0.001500    Time 0.023798    
2018-10-27 23:31:19,134 - Epoch: [261][  200/  391]    Overall Loss 0.192858    Objective Loss 0.192858    Top1 93.093750    Top5 99.906250    LR 0.001500    Time 0.023693    
2018-10-27 23:31:20,310 - Epoch: [261][  250/  391]    Overall Loss 0.192151    Objective Loss 0.192151    Top1 93.181250    Top5 99.903125    LR 0.001500    Time 0.023652    
2018-10-27 23:31:21,487 - Epoch: [261][  300/  391]    Overall Loss 0.191437    Objective Loss 0.191437    Top1 93.250000    Top5 99.901042    LR 0.001500    Time 0.023629    
2018-10-27 23:31:22,661 - Epoch: [261][  350/  391]    Overall Loss 0.193369    Objective Loss 0.193369    Top1 93.187500    Top5 99.892857    LR 0.001500    Time 0.023602    
2018-10-27 23:31:23,702 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31202 | -0.00327 |    0.11655 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09619 | -0.00344 |    0.02131 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09014 |  0.00100 |    0.02182 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09372 | -0.00219 |    0.02216 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07313 | -0.00184 |    0.01436 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09814 | -0.00335 |    0.02695 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07195 | -0.00091 |    0.01606 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10280 | -0.00307 |    0.03598 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08213 | -0.00203 |    0.02525 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11272 | -0.00578 |    0.03554 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07132 | -0.00105 |    0.01973 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05745 | -0.00059 |    0.01384 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07690 | -0.00175 |    0.02263 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05840 | -0.00296 |    0.01480 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07071 | -0.00102 |    0.02356 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06810 | -0.00246 |    0.02319 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06204 |  0.00042 |    0.01589 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06365 | -0.00151 |    0.01951 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05246 | -0.00159 |    0.01353 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05027 | -0.00033 |    0.01316 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03082 |  0.00116 |    0.00548 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54424 | -0.07456 |    0.26840 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:31:23,702 - Total sparsity: 88.62

2018-10-27 23:31:23,702 - --- validate (epoch=261)-----------
2018-10-27 23:31:23,702 - 10000 samples (128 per mini-batch)
2018-10-27 23:31:24,433 - Epoch: [261][   50/   78]    Loss 0.396322    Top1 87.562500    Top5 99.515625    
2018-10-27 23:31:24,828 - ==> Top1: 87.690    Top5: 99.540    Loss: 0.391

2018-10-27 23:31:24,829 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:31:24,829 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:31:24,846 - 

2018-10-27 23:31:24,846 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:31:26,048 - Epoch: [262][   50/  391]    Overall Loss 0.200353    Objective Loss 0.200353    Top1 93.234375    Top5 99.890625    LR 0.001500    Time 0.024014    
2018-10-27 23:31:27,218 - Epoch: [262][  100/  391]    Overall Loss 0.198331    Objective Loss 0.198331    Top1 92.984375    Top5 99.867188    LR 0.001500    Time 0.023694    
2018-10-27 23:31:28,384 - Epoch: [262][  150/  391]    Overall Loss 0.196420    Objective Loss 0.196420    Top1 93.052083    Top5 99.880208    LR 0.001500    Time 0.023556    
2018-10-27 23:31:29,554 - Epoch: [262][  200/  391]    Overall Loss 0.197257    Objective Loss 0.197257    Top1 93.039062    Top5 99.875000    LR 0.001500    Time 0.023509    
2018-10-27 23:31:30,723 - Epoch: [262][  250/  391]    Overall Loss 0.197656    Objective Loss 0.197656    Top1 93.028125    Top5 99.868750    LR 0.001500    Time 0.023478    
2018-10-27 23:31:31,897 - Epoch: [262][  300/  391]    Overall Loss 0.195515    Objective Loss 0.195515    Top1 93.085938    Top5 99.880208    LR 0.001500    Time 0.023476    
2018-10-27 23:31:33,072 - Epoch: [262][  350/  391]    Overall Loss 0.194412    Objective Loss 0.194412    Top1 93.098214    Top5 99.890625    LR 0.001500    Time 0.023464    
2018-10-27 23:31:34,116 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31185 | -0.00274 |    0.11658 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09614 | -0.00346 |    0.02129 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09009 |  0.00099 |    0.02181 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09367 | -0.00220 |    0.02216 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07309 | -0.00184 |    0.01435 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09809 | -0.00338 |    0.02694 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07191 | -0.00089 |    0.01605 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10275 | -0.00305 |    0.03595 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08209 | -0.00203 |    0.02524 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11266 | -0.00580 |    0.03552 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07128 | -0.00103 |    0.01972 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05742 | -0.00059 |    0.01383 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07686 | -0.00175 |    0.02261 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05837 | -0.00296 |    0.01479 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07067 | -0.00102 |    0.02355 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06807 | -0.00246 |    0.02318 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06201 |  0.00042 |    0.01588 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06361 | -0.00151 |    0.01950 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05244 | -0.00159 |    0.01352 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05024 | -0.00033 |    0.01315 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03080 |  0.00116 |    0.00548 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54442 | -0.07456 |    0.26849 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:31:34,116 - Total sparsity: 88.62

2018-10-27 23:31:34,116 - --- validate (epoch=262)-----------
2018-10-27 23:31:34,117 - 10000 samples (128 per mini-batch)
2018-10-27 23:31:34,845 - Epoch: [262][   50/   78]    Loss 0.396543    Top1 87.609375    Top5 99.515625    
2018-10-27 23:31:35,240 - ==> Top1: 87.700    Top5: 99.570    Loss: 0.390

2018-10-27 23:31:35,241 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:31:35,241 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:31:35,251 - 

2018-10-27 23:31:35,251 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:31:36,484 - Epoch: [263][   50/  391]    Overall Loss 0.192245    Objective Loss 0.192245    Top1 93.187500    Top5 99.890625    LR 0.001500    Time 0.024609    
2018-10-27 23:31:37,654 - Epoch: [263][  100/  391]    Overall Loss 0.197998    Objective Loss 0.197998    Top1 92.953125    Top5 99.921875    LR 0.001500    Time 0.023994    
2018-10-27 23:31:38,835 - Epoch: [263][  150/  391]    Overall Loss 0.192241    Objective Loss 0.192241    Top1 93.125000    Top5 99.921875    LR 0.001500    Time 0.023862    
2018-10-27 23:31:40,008 - Epoch: [263][  200/  391]    Overall Loss 0.194697    Objective Loss 0.194697    Top1 93.113281    Top5 99.921875    LR 0.001500    Time 0.023754    
2018-10-27 23:31:41,183 - Epoch: [263][  250/  391]    Overall Loss 0.195837    Objective Loss 0.195837    Top1 93.109375    Top5 99.909375    LR 0.001500    Time 0.023695    
2018-10-27 23:31:42,359 - Epoch: [263][  300/  391]    Overall Loss 0.195725    Objective Loss 0.195725    Top1 93.151042    Top5 99.908854    LR 0.001500    Time 0.023662    
2018-10-27 23:31:43,530 - Epoch: [263][  350/  391]    Overall Loss 0.196062    Objective Loss 0.196062    Top1 93.142857    Top5 99.906250    LR 0.001500    Time 0.023625    
2018-10-27 23:31:44,575 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31168 | -0.00294 |    0.11650 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09609 | -0.00345 |    0.02129 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.09004 |  0.00097 |    0.02180 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09361 | -0.00221 |    0.02214 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07305 | -0.00185 |    0.01435 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09803 | -0.00335 |    0.02692 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07187 | -0.00091 |    0.01604 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10269 | -0.00304 |    0.03595 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08205 | -0.00204 |    0.02524 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11260 | -0.00577 |    0.03549 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07124 | -0.00103 |    0.01971 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05738 | -0.00058 |    0.01382 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07682 | -0.00174 |    0.02260 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05834 | -0.00296 |    0.01478 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07063 | -0.00103 |    0.02354 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06803 | -0.00246 |    0.02317 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06197 |  0.00042 |    0.01587 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06358 | -0.00151 |    0.01948 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05241 | -0.00158 |    0.01352 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05021 | -0.00033 |    0.01314 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03079 |  0.00116 |    0.00547 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54460 | -0.07458 |    0.26857 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:31:44,575 - Total sparsity: 88.62

2018-10-27 23:31:44,575 - --- validate (epoch=263)-----------
2018-10-27 23:31:44,576 - 10000 samples (128 per mini-batch)
2018-10-27 23:31:45,303 - Epoch: [263][   50/   78]    Loss 0.395401    Top1 87.625000    Top5 99.515625    
2018-10-27 23:31:45,699 - ==> Top1: 87.780    Top5: 99.560    Loss: 0.388

2018-10-27 23:31:45,700 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:31:45,700 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:31:45,711 - 

2018-10-27 23:31:45,711 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:31:46,940 - Epoch: [264][   50/  391]    Overall Loss 0.191905    Objective Loss 0.191905    Top1 93.109375    Top5 99.937500    LR 0.001500    Time 0.024547    
2018-10-27 23:31:48,117 - Epoch: [264][  100/  391]    Overall Loss 0.195795    Objective Loss 0.195795    Top1 93.007812    Top5 99.906250    LR 0.001500    Time 0.024029    
2018-10-27 23:31:49,295 - Epoch: [264][  150/  391]    Overall Loss 0.197244    Objective Loss 0.197244    Top1 93.151042    Top5 99.895833    LR 0.001500    Time 0.023861    
2018-10-27 23:31:50,469 - Epoch: [264][  200/  391]    Overall Loss 0.194061    Objective Loss 0.194061    Top1 93.171875    Top5 99.914062    LR 0.001500    Time 0.023760    
2018-10-27 23:31:51,645 - Epoch: [264][  250/  391]    Overall Loss 0.196406    Objective Loss 0.196406    Top1 93.087500    Top5 99.903125    LR 0.001500    Time 0.023708    
2018-10-27 23:31:52,819 - Epoch: [264][  300/  391]    Overall Loss 0.196317    Objective Loss 0.196317    Top1 93.125000    Top5 99.895833    LR 0.001500    Time 0.023665    
2018-10-27 23:31:53,992 - Epoch: [264][  350/  391]    Overall Loss 0.195580    Objective Loss 0.195580    Top1 93.180804    Top5 99.897321    LR 0.001500    Time 0.023631    
2018-10-27 23:31:55,033 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31151 | -0.00293 |    0.11633 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09604 | -0.00343 |    0.02127 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08999 |  0.00097 |    0.02178 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09356 | -0.00219 |    0.02213 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07301 | -0.00183 |    0.01435 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09798 | -0.00334 |    0.02689 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07183 | -0.00094 |    0.01602 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10263 | -0.00304 |    0.03592 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08200 | -0.00202 |    0.02522 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11253 | -0.00581 |    0.03549 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07120 | -0.00102 |    0.01970 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05735 | -0.00058 |    0.01382 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07677 | -0.00172 |    0.02259 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05831 | -0.00296 |    0.01477 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07059 | -0.00102 |    0.02353 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06799 | -0.00246 |    0.02316 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06194 |  0.00041 |    0.01586 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06355 | -0.00151 |    0.01947 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05238 | -0.00158 |    0.01351 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05019 | -0.00033 |    0.01314 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03077 |  0.00115 |    0.00547 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54476 | -0.07458 |    0.26864 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:31:55,033 - Total sparsity: 88.62

2018-10-27 23:31:55,033 - --- validate (epoch=264)-----------
2018-10-27 23:31:55,034 - 10000 samples (128 per mini-batch)
2018-10-27 23:31:55,764 - Epoch: [264][   50/   78]    Loss 0.395613    Top1 87.515625    Top5 99.515625    
2018-10-27 23:31:56,169 - ==> Top1: 87.750    Top5: 99.530    Loss: 0.387

2018-10-27 23:31:56,169 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:31:56,169 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:31:56,180 - 

2018-10-27 23:31:56,180 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:31:57,406 - Epoch: [265][   50/  391]    Overall Loss 0.201409    Objective Loss 0.201409    Top1 92.906250    Top5 99.875000    LR 0.001500    Time 0.024471    
2018-10-27 23:31:58,579 - Epoch: [265][  100/  391]    Overall Loss 0.204393    Objective Loss 0.204393    Top1 92.757812    Top5 99.843750    LR 0.001500    Time 0.023950    
2018-10-27 23:31:59,750 - Epoch: [265][  150/  391]    Overall Loss 0.197582    Objective Loss 0.197582    Top1 93.026042    Top5 99.859375    LR 0.001500    Time 0.023765    
2018-10-27 23:32:00,927 - Epoch: [265][  200/  391]    Overall Loss 0.196575    Objective Loss 0.196575    Top1 93.117188    Top5 99.875000    LR 0.001500    Time 0.023701    
2018-10-27 23:32:02,099 - Epoch: [265][  250/  391]    Overall Loss 0.197042    Objective Loss 0.197042    Top1 93.062500    Top5 99.878125    LR 0.001500    Time 0.023646    
2018-10-27 23:32:03,272 - Epoch: [265][  300/  391]    Overall Loss 0.197987    Objective Loss 0.197987    Top1 93.005208    Top5 99.869792    LR 0.001500    Time 0.023609    
2018-10-27 23:32:04,448 - Epoch: [265][  350/  391]    Overall Loss 0.197751    Objective Loss 0.197751    Top1 93.022321    Top5 99.870536    LR 0.001500    Time 0.023593    
2018-10-27 23:32:05,493 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31134 | -0.00286 |    0.11637 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09598 | -0.00346 |    0.02128 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08994 |  0.00095 |    0.02176 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09351 | -0.00217 |    0.02211 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07297 | -0.00183 |    0.01434 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09793 | -0.00329 |    0.02688 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07179 | -0.00094 |    0.01601 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10258 | -0.00302 |    0.03590 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08196 | -0.00202 |    0.02521 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11247 | -0.00583 |    0.03546 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07117 | -0.00101 |    0.01968 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05732 | -0.00058 |    0.01381 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07673 | -0.00170 |    0.02257 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05828 | -0.00296 |    0.01476 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07055 | -0.00100 |    0.02351 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06796 | -0.00246 |    0.02315 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06190 |  0.00043 |    0.01584 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06351 | -0.00151 |    0.01946 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05235 | -0.00158 |    0.01350 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05016 | -0.00033 |    0.01313 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03075 |  0.00115 |    0.00547 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54491 | -0.07459 |    0.26870 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:32:05,493 - Total sparsity: 88.62

2018-10-27 23:32:05,493 - --- validate (epoch=265)-----------
2018-10-27 23:32:05,494 - 10000 samples (128 per mini-batch)
2018-10-27 23:32:06,224 - Epoch: [265][   50/   78]    Loss 0.394915    Top1 87.703125    Top5 99.500000    
2018-10-27 23:32:06,618 - ==> Top1: 87.860    Top5: 99.560    Loss: 0.388

2018-10-27 23:32:06,619 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:32:06,619 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:32:06,629 - 

2018-10-27 23:32:06,629 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:32:07,860 - Epoch: [266][   50/  391]    Overall Loss 0.193840    Objective Loss 0.193840    Top1 93.171875    Top5 99.875000    LR 0.001500    Time 0.024580    
2018-10-27 23:32:09,035 - Epoch: [266][  100/  391]    Overall Loss 0.195116    Objective Loss 0.195116    Top1 93.101562    Top5 99.875000    LR 0.001500    Time 0.024016    
2018-10-27 23:32:10,207 - Epoch: [266][  150/  391]    Overall Loss 0.194345    Objective Loss 0.194345    Top1 93.166667    Top5 99.885417    LR 0.001500    Time 0.023820    
2018-10-27 23:32:11,385 - Epoch: [266][  200/  391]    Overall Loss 0.193575    Objective Loss 0.193575    Top1 93.175781    Top5 99.890625    LR 0.001500    Time 0.023745    
2018-10-27 23:32:12,550 - Epoch: [266][  250/  391]    Overall Loss 0.195141    Objective Loss 0.195141    Top1 93.175000    Top5 99.890625    LR 0.001500    Time 0.023653    
2018-10-27 23:32:13,725 - Epoch: [266][  300/  391]    Overall Loss 0.194556    Objective Loss 0.194556    Top1 93.192708    Top5 99.893229    LR 0.001500    Time 0.023621    
2018-10-27 23:32:14,899 - Epoch: [266][  350/  391]    Overall Loss 0.195388    Objective Loss 0.195388    Top1 93.158482    Top5 99.892857    LR 0.001500    Time 0.023598    
2018-10-27 23:32:15,943 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31117 | -0.00286 |    0.11618 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09593 | -0.00342 |    0.02124 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08989 |  0.00094 |    0.02175 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09346 | -0.00218 |    0.02210 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07293 | -0.00182 |    0.01433 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09788 | -0.00332 |    0.02687 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07175 | -0.00095 |    0.01601 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10252 | -0.00304 |    0.03588 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08191 | -0.00202 |    0.02519 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11241 | -0.00579 |    0.03543 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07113 | -0.00101 |    0.01967 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05729 | -0.00060 |    0.01381 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07669 | -0.00170 |    0.02256 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05824 | -0.00296 |    0.01476 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07051 | -0.00100 |    0.02350 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06792 | -0.00246 |    0.02313 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06187 |  0.00044 |    0.01583 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06348 | -0.00150 |    0.01945 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05232 | -0.00158 |    0.01349 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05013 | -0.00033 |    0.01312 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03074 |  0.00115 |    0.00546 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54504 | -0.07458 |    0.26877 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:32:15,944 - Total sparsity: 88.62

2018-10-27 23:32:15,944 - --- validate (epoch=266)-----------
2018-10-27 23:32:15,944 - 10000 samples (128 per mini-batch)
2018-10-27 23:32:16,667 - Epoch: [266][   50/   78]    Loss 0.393618    Top1 87.500000    Top5 99.468750    
2018-10-27 23:32:17,060 - ==> Top1: 87.810    Top5: 99.540    Loss: 0.386

2018-10-27 23:32:17,060 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:32:17,060 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:32:17,070 - 

2018-10-27 23:32:17,071 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:32:18,299 - Epoch: [267][   50/  391]    Overall Loss 0.199842    Objective Loss 0.199842    Top1 93.046875    Top5 99.828125    LR 0.001500    Time 0.024533    
2018-10-27 23:32:19,472 - Epoch: [267][  100/  391]    Overall Loss 0.194255    Objective Loss 0.194255    Top1 93.125000    Top5 99.890625    LR 0.001500    Time 0.023983    
2018-10-27 23:32:20,644 - Epoch: [267][  150/  391]    Overall Loss 0.195379    Objective Loss 0.195379    Top1 93.093750    Top5 99.901042    LR 0.001500    Time 0.023794    
2018-10-27 23:32:21,818 - Epoch: [267][  200/  391]    Overall Loss 0.197131    Objective Loss 0.197131    Top1 93.035156    Top5 99.894531    LR 0.001500    Time 0.023704    
2018-10-27 23:32:22,991 - Epoch: [267][  250/  391]    Overall Loss 0.197242    Objective Loss 0.197242    Top1 92.993750    Top5 99.887500    LR 0.001500    Time 0.023635    
2018-10-27 23:32:24,162 - Epoch: [267][  300/  391]    Overall Loss 0.195591    Objective Loss 0.195591    Top1 93.062500    Top5 99.880208    LR 0.001500    Time 0.023596    
2018-10-27 23:32:25,334 - Epoch: [267][  350/  391]    Overall Loss 0.193513    Objective Loss 0.193513    Top1 93.198661    Top5 99.863839    LR 0.001500    Time 0.023570    
2018-10-27 23:32:26,374 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31100 | -0.00302 |    0.11616 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09588 | -0.00342 |    0.02124 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08985 |  0.00096 |    0.02173 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09341 | -0.00218 |    0.02208 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07289 | -0.00184 |    0.01431 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09782 | -0.00333 |    0.02685 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07172 | -0.00095 |    0.01600 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10247 | -0.00302 |    0.03585 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08187 | -0.00202 |    0.02517 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11235 | -0.00581 |    0.03538 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07109 | -0.00102 |    0.01966 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05726 | -0.00060 |    0.01380 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07665 | -0.00170 |    0.02255 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05821 | -0.00296 |    0.01475 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07048 | -0.00100 |    0.02348 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06788 | -0.00246 |    0.02312 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06183 |  0.00044 |    0.01582 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06344 | -0.00149 |    0.01944 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05230 | -0.00158 |    0.01348 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05010 | -0.00033 |    0.01311 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03072 |  0.00115 |    0.00546 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54525 | -0.07461 |    0.26887 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:32:26,375 - Total sparsity: 88.62

2018-10-27 23:32:26,375 - --- validate (epoch=267)-----------
2018-10-27 23:32:26,375 - 10000 samples (128 per mini-batch)
2018-10-27 23:32:27,106 - Epoch: [267][   50/   78]    Loss 0.398320    Top1 87.437500    Top5 99.500000    
2018-10-27 23:32:27,500 - ==> Top1: 87.710    Top5: 99.530    Loss: 0.390

2018-10-27 23:32:27,501 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:32:27,501 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:32:27,512 - 

2018-10-27 23:32:27,512 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:32:28,743 - Epoch: [268][   50/  391]    Overall Loss 0.189105    Objective Loss 0.189105    Top1 93.250000    Top5 99.937500    LR 0.001500    Time 0.024591    
2018-10-27 23:32:29,913 - Epoch: [268][  100/  391]    Overall Loss 0.190093    Objective Loss 0.190093    Top1 93.210938    Top5 99.906250    LR 0.001500    Time 0.023978    
2018-10-27 23:32:31,079 - Epoch: [268][  150/  391]    Overall Loss 0.192178    Objective Loss 0.192178    Top1 93.104167    Top5 99.901042    LR 0.001500    Time 0.023750    
2018-10-27 23:32:32,250 - Epoch: [268][  200/  391]    Overall Loss 0.191942    Objective Loss 0.191942    Top1 93.187500    Top5 99.882812    LR 0.001500    Time 0.023660    
2018-10-27 23:32:33,418 - Epoch: [268][  250/  391]    Overall Loss 0.193972    Objective Loss 0.193972    Top1 93.112500    Top5 99.871875    LR 0.001500    Time 0.023596    
2018-10-27 23:32:34,587 - Epoch: [268][  300/  391]    Overall Loss 0.194504    Objective Loss 0.194504    Top1 93.098958    Top5 99.880208    LR 0.001500    Time 0.023553    
2018-10-27 23:32:35,758 - Epoch: [268][  350/  391]    Overall Loss 0.197384    Objective Loss 0.197384    Top1 93.053571    Top5 99.877232    LR 0.001500    Time 0.023532    
2018-10-27 23:32:36,798 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31084 | -0.00273 |    0.11613 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09583 | -0.00344 |    0.02121 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08980 |  0.00098 |    0.02172 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09336 | -0.00218 |    0.02206 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07285 | -0.00183 |    0.01431 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09777 | -0.00332 |    0.02683 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07168 | -0.00095 |    0.01599 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10241 | -0.00302 |    0.03583 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08182 | -0.00202 |    0.02516 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11228 | -0.00581 |    0.03537 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07105 | -0.00104 |    0.01965 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05723 | -0.00061 |    0.01379 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07661 | -0.00170 |    0.02253 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05818 | -0.00296 |    0.01474 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07044 | -0.00100 |    0.02347 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06785 | -0.00246 |    0.02311 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06180 |  0.00044 |    0.01582 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06341 | -0.00150 |    0.01942 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05227 | -0.00157 |    0.01348 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05008 | -0.00033 |    0.01311 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03070 |  0.00115 |    0.00546 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54538 | -0.07459 |    0.26892 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:32:36,798 - Total sparsity: 88.62

2018-10-27 23:32:36,799 - --- validate (epoch=268)-----------
2018-10-27 23:32:36,799 - 10000 samples (128 per mini-batch)
2018-10-27 23:32:37,523 - Epoch: [268][   50/   78]    Loss 0.398839    Top1 87.234375    Top5 99.531250    
2018-10-27 23:32:37,918 - ==> Top1: 87.570    Top5: 99.580    Loss: 0.390

2018-10-27 23:32:37,918 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:32:37,919 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:32:37,929 - 

2018-10-27 23:32:37,930 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:32:39,160 - Epoch: [269][   50/  391]    Overall Loss 0.187149    Objective Loss 0.187149    Top1 93.562500    Top5 99.890625    LR 0.001500    Time 0.024575    
2018-10-27 23:32:40,332 - Epoch: [269][  100/  391]    Overall Loss 0.191555    Objective Loss 0.191555    Top1 93.406250    Top5 99.867188    LR 0.001500    Time 0.023993    
2018-10-27 23:32:41,512 - Epoch: [269][  150/  391]    Overall Loss 0.189879    Objective Loss 0.189879    Top1 93.369792    Top5 99.875000    LR 0.001500    Time 0.023857    
2018-10-27 23:32:42,689 - Epoch: [269][  200/  391]    Overall Loss 0.192544    Objective Loss 0.192544    Top1 93.234375    Top5 99.871094    LR 0.001500    Time 0.023770    
2018-10-27 23:32:43,859 - Epoch: [269][  250/  391]    Overall Loss 0.191541    Objective Loss 0.191541    Top1 93.303125    Top5 99.875000    LR 0.001500    Time 0.023690    
2018-10-27 23:32:45,033 - Epoch: [269][  300/  391]    Overall Loss 0.192285    Objective Loss 0.192285    Top1 93.273438    Top5 99.869792    LR 0.001500    Time 0.023637    
2018-10-27 23:32:46,205 - Epoch: [269][  350/  391]    Overall Loss 0.193719    Objective Loss 0.193719    Top1 93.229911    Top5 99.875000    LR 0.001500    Time 0.023605    
2018-10-27 23:32:47,248 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31066 | -0.00296 |    0.11608 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09578 | -0.00344 |    0.02122 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08975 |  0.00096 |    0.02170 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09331 | -0.00220 |    0.02204 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07281 | -0.00182 |    0.01430 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09772 | -0.00334 |    0.02680 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07164 | -0.00096 |    0.01599 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10236 | -0.00303 |    0.03582 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08178 | -0.00201 |    0.02515 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11222 | -0.00583 |    0.03534 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07101 | -0.00104 |    0.01963 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05720 | -0.00060 |    0.01378 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07657 | -0.00170 |    0.02252 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05815 | -0.00295 |    0.01473 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07040 | -0.00100 |    0.02345 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06781 | -0.00246 |    0.02309 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06177 |  0.00045 |    0.01580 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06337 | -0.00150 |    0.01941 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05224 | -0.00157 |    0.01347 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05005 | -0.00033 |    0.01310 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03069 |  0.00115 |    0.00546 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54556 | -0.07464 |    0.26902 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:32:47,248 - Total sparsity: 88.62

2018-10-27 23:32:47,248 - --- validate (epoch=269)-----------
2018-10-27 23:32:47,249 - 10000 samples (128 per mini-batch)
2018-10-27 23:32:47,981 - Epoch: [269][   50/   78]    Loss 0.398796    Top1 87.312500    Top5 99.546875    
2018-10-27 23:32:48,379 - ==> Top1: 87.640    Top5: 99.580    Loss: 0.391

2018-10-27 23:32:48,379 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:32:48,379 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:32:48,390 - 

2018-10-27 23:32:48,391 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:32:49,625 - Epoch: [270][   50/  391]    Overall Loss 0.205477    Objective Loss 0.205477    Top1 92.484375    Top5 99.921875    LR 0.001500    Time 0.024660    
2018-10-27 23:32:50,794 - Epoch: [270][  100/  391]    Overall Loss 0.193202    Objective Loss 0.193202    Top1 92.929688    Top5 99.921875    LR 0.001500    Time 0.024000    
2018-10-27 23:32:51,968 - Epoch: [270][  150/  391]    Overall Loss 0.191665    Objective Loss 0.191665    Top1 93.088542    Top5 99.921875    LR 0.001500    Time 0.023815    
2018-10-27 23:32:53,138 - Epoch: [270][  200/  391]    Overall Loss 0.190232    Objective Loss 0.190232    Top1 93.167969    Top5 99.914062    LR 0.001500    Time 0.023706    
2018-10-27 23:32:54,314 - Epoch: [270][  250/  391]    Overall Loss 0.191513    Objective Loss 0.191513    Top1 93.200000    Top5 99.906250    LR 0.001500    Time 0.023665    
2018-10-27 23:32:55,495 - Epoch: [270][  300/  391]    Overall Loss 0.193666    Objective Loss 0.193666    Top1 93.153646    Top5 99.901042    LR 0.001500    Time 0.023651    
2018-10-27 23:32:56,678 - Epoch: [270][  350/  391]    Overall Loss 0.193927    Objective Loss 0.193927    Top1 93.156250    Top5 99.892857    LR 0.001500    Time 0.023651    
2018-10-27 23:32:57,722 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31050 | -0.00304 |    0.11599 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09572 | -0.00343 |    0.02121 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08970 |  0.00096 |    0.02168 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09326 | -0.00222 |    0.02202 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07277 | -0.00183 |    0.01428 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09766 | -0.00334 |    0.02680 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07160 | -0.00095 |    0.01598 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10230 | -0.00305 |    0.03579 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08173 | -0.00199 |    0.02514 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11216 | -0.00585 |    0.03533 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07098 | -0.00104 |    0.01962 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05717 | -0.00061 |    0.01378 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07653 | -0.00170 |    0.02250 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05812 | -0.00295 |    0.01472 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07036 | -0.00101 |    0.02344 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06777 | -0.00246 |    0.02308 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06173 |  0.00045 |    0.01579 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06334 | -0.00150 |    0.01940 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05221 | -0.00157 |    0.01346 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05002 | -0.00033 |    0.01309 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03067 |  0.00115 |    0.00545 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54574 | -0.07462 |    0.26909 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:32:57,722 - Total sparsity: 88.62

2018-10-27 23:32:57,722 - --- validate (epoch=270)-----------
2018-10-27 23:32:57,723 - 10000 samples (128 per mini-batch)
2018-10-27 23:32:58,451 - Epoch: [270][   50/   78]    Loss 0.398702    Top1 87.375000    Top5 99.500000    
2018-10-27 23:32:58,849 - ==> Top1: 87.660    Top5: 99.550    Loss: 0.389

2018-10-27 23:32:58,850 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:32:58,850 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:32:58,864 - 

2018-10-27 23:32:58,865 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:33:00,096 - Epoch: [271][   50/  391]    Overall Loss 0.196976    Objective Loss 0.196976    Top1 93.093750    Top5 99.906250    LR 0.001500    Time 0.024591    
2018-10-27 23:33:01,269 - Epoch: [271][  100/  391]    Overall Loss 0.189747    Objective Loss 0.189747    Top1 93.265625    Top5 99.898438    LR 0.001500    Time 0.024008    
2018-10-27 23:33:02,440 - Epoch: [271][  150/  391]    Overall Loss 0.188921    Objective Loss 0.188921    Top1 93.296875    Top5 99.895833    LR 0.001500    Time 0.023802    
2018-10-27 23:33:03,614 - Epoch: [271][  200/  391]    Overall Loss 0.188707    Objective Loss 0.188707    Top1 93.367188    Top5 99.890625    LR 0.001500    Time 0.023716    
2018-10-27 23:33:04,786 - Epoch: [271][  250/  391]    Overall Loss 0.189527    Objective Loss 0.189527    Top1 93.406250    Top5 99.868750    LR 0.001500    Time 0.023656    
2018-10-27 23:33:05,962 - Epoch: [271][  300/  391]    Overall Loss 0.190720    Objective Loss 0.190720    Top1 93.375000    Top5 99.867188    LR 0.001500    Time 0.023627    
2018-10-27 23:33:07,138 - Epoch: [271][  350/  391]    Overall Loss 0.192260    Objective Loss 0.192260    Top1 93.292411    Top5 99.859375    LR 0.001500    Time 0.023608    
2018-10-27 23:33:08,181 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31033 | -0.00286 |    0.11599 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09567 | -0.00343 |    0.02120 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08965 |  0.00095 |    0.02167 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09321 | -0.00218 |    0.02201 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07273 | -0.00183 |    0.01427 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09761 | -0.00335 |    0.02677 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07156 | -0.00095 |    0.01596 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10224 | -0.00305 |    0.03578 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08169 | -0.00199 |    0.02513 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11210 | -0.00579 |    0.03529 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07094 | -0.00104 |    0.01961 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05714 | -0.00060 |    0.01377 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07649 | -0.00169 |    0.02249 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05809 | -0.00294 |    0.01471 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07032 | -0.00101 |    0.02343 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06773 | -0.00246 |    0.02306 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06170 |  0.00044 |    0.01578 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06331 | -0.00150 |    0.01939 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05218 | -0.00157 |    0.01345 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.05000 | -0.00033 |    0.01308 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03065 |  0.00115 |    0.00545 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54592 | -0.07464 |    0.26917 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:33:08,181 - Total sparsity: 88.62

2018-10-27 23:33:08,182 - --- validate (epoch=271)-----------
2018-10-27 23:33:08,182 - 10000 samples (128 per mini-batch)
2018-10-27 23:33:08,901 - Epoch: [271][   50/   78]    Loss 0.400519    Top1 87.500000    Top5 99.500000    
2018-10-27 23:33:09,293 - ==> Top1: 87.680    Top5: 99.540    Loss: 0.392

2018-10-27 23:33:09,294 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:33:09,294 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:33:09,309 - 

2018-10-27 23:33:09,309 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:33:10,538 - Epoch: [272][   50/  391]    Overall Loss 0.195216    Objective Loss 0.195216    Top1 93.312500    Top5 99.906250    LR 0.001500    Time 0.024541    
2018-10-27 23:33:11,713 - Epoch: [272][  100/  391]    Overall Loss 0.196138    Objective Loss 0.196138    Top1 93.273438    Top5 99.898438    LR 0.001500    Time 0.024008    
2018-10-27 23:33:12,886 - Epoch: [272][  150/  391]    Overall Loss 0.193554    Objective Loss 0.193554    Top1 93.375000    Top5 99.911458    LR 0.001500    Time 0.023815    
2018-10-27 23:33:14,060 - Epoch: [272][  200/  391]    Overall Loss 0.191605    Objective Loss 0.191605    Top1 93.394531    Top5 99.910156    LR 0.001500    Time 0.023725    
2018-10-27 23:33:15,231 - Epoch: [272][  250/  391]    Overall Loss 0.190464    Objective Loss 0.190464    Top1 93.468750    Top5 99.900000    LR 0.001500    Time 0.023656    
2018-10-27 23:33:16,403 - Epoch: [272][  300/  391]    Overall Loss 0.189978    Objective Loss 0.189978    Top1 93.484375    Top5 99.903646    LR 0.001500    Time 0.023616    
2018-10-27 23:33:17,573 - Epoch: [272][  350/  391]    Overall Loss 0.192402    Objective Loss 0.192402    Top1 93.390625    Top5 99.901786    LR 0.001500    Time 0.023579    
2018-10-27 23:33:18,616 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.31016 | -0.00302 |    0.11583 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09562 | -0.00344 |    0.02119 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08960 |  0.00095 |    0.02166 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09316 | -0.00218 |    0.02201 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07269 | -0.00179 |    0.01426 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09756 | -0.00334 |    0.02675 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07152 | -0.00095 |    0.01596 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10219 | -0.00302 |    0.03577 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08165 | -0.00199 |    0.02511 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11204 | -0.00579 |    0.03525 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07090 | -0.00104 |    0.01960 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05711 | -0.00060 |    0.01375 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07645 | -0.00168 |    0.02248 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05806 | -0.00294 |    0.01470 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07028 | -0.00101 |    0.02342 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06770 | -0.00245 |    0.02305 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06166 |  0.00044 |    0.01577 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06327 | -0.00150 |    0.01938 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05215 | -0.00157 |    0.01344 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04997 | -0.00032 |    0.01308 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03064 |  0.00115 |    0.00545 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54612 | -0.07463 |    0.26926 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:33:18,616 - Total sparsity: 88.62

2018-10-27 23:33:18,616 - --- validate (epoch=272)-----------
2018-10-27 23:33:18,616 - 10000 samples (128 per mini-batch)
2018-10-27 23:33:19,348 - Epoch: [272][   50/   78]    Loss 0.398045    Top1 87.453125    Top5 99.515625    
2018-10-27 23:33:19,738 - ==> Top1: 87.710    Top5: 99.550    Loss: 0.390

2018-10-27 23:33:19,739 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:33:19,739 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:33:19,750 - 

2018-10-27 23:33:19,751 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:33:20,979 - Epoch: [273][   50/  391]    Overall Loss 0.189667    Objective Loss 0.189667    Top1 93.218750    Top5 99.953125    LR 0.001500    Time 0.024523    
2018-10-27 23:33:22,152 - Epoch: [273][  100/  391]    Overall Loss 0.195426    Objective Loss 0.195426    Top1 92.937500    Top5 99.898438    LR 0.001500    Time 0.023986    
2018-10-27 23:33:23,325 - Epoch: [273][  150/  391]    Overall Loss 0.195511    Objective Loss 0.195511    Top1 92.947917    Top5 99.901042    LR 0.001500    Time 0.023798    
2018-10-27 23:33:24,500 - Epoch: [273][  200/  391]    Overall Loss 0.192788    Objective Loss 0.192788    Top1 93.093750    Top5 99.902344    LR 0.001500    Time 0.023720    
2018-10-27 23:33:25,669 - Epoch: [273][  250/  391]    Overall Loss 0.189580    Objective Loss 0.189580    Top1 93.237500    Top5 99.915625    LR 0.001500    Time 0.023644    
2018-10-27 23:33:26,836 - Epoch: [273][  300/  391]    Overall Loss 0.190248    Objective Loss 0.190248    Top1 93.239583    Top5 99.911458    LR 0.001500    Time 0.023589    
2018-10-27 23:33:28,002 - Epoch: [273][  350/  391]    Overall Loss 0.192310    Objective Loss 0.192310    Top1 93.214286    Top5 99.892857    LR 0.001500    Time 0.023548    
2018-10-27 23:33:29,042 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30999 | -0.00291 |    0.11587 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09557 | -0.00345 |    0.02118 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08955 |  0.00096 |    0.02165 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09311 | -0.00223 |    0.02200 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07265 | -0.00179 |    0.01425 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09751 | -0.00328 |    0.02672 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07148 | -0.00094 |    0.01595 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10213 | -0.00301 |    0.03575 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08160 | -0.00198 |    0.02509 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11198 | -0.00578 |    0.03525 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07086 | -0.00105 |    0.01959 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05707 | -0.00060 |    0.01374 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07640 | -0.00169 |    0.02247 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05802 | -0.00293 |    0.01469 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07025 | -0.00099 |    0.02340 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06766 | -0.00245 |    0.02304 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06163 |  0.00044 |    0.01576 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06324 | -0.00150 |    0.01937 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05213 | -0.00156 |    0.01343 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04994 | -0.00032 |    0.01307 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03062 |  0.00115 |    0.00544 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54629 | -0.07465 |    0.26934 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:33:29,043 - Total sparsity: 88.62

2018-10-27 23:33:29,043 - --- validate (epoch=273)-----------
2018-10-27 23:33:29,043 - 10000 samples (128 per mini-batch)
2018-10-27 23:33:29,774 - Epoch: [273][   50/   78]    Loss 0.398689    Top1 87.421875    Top5 99.500000    
2018-10-27 23:33:30,173 - ==> Top1: 87.700    Top5: 99.540    Loss: 0.389

2018-10-27 23:33:30,174 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:33:30,174 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:33:30,185 - 

2018-10-27 23:33:30,186 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:33:31,417 - Epoch: [274][   50/  391]    Overall Loss 0.193038    Objective Loss 0.193038    Top1 93.093750    Top5 99.968750    LR 0.001500    Time 0.024581    
2018-10-27 23:33:32,588 - Epoch: [274][  100/  391]    Overall Loss 0.196836    Objective Loss 0.196836    Top1 92.968750    Top5 99.945312    LR 0.001500    Time 0.023995    
2018-10-27 23:33:33,763 - Epoch: [274][  150/  391]    Overall Loss 0.193094    Objective Loss 0.193094    Top1 93.223958    Top5 99.921875    LR 0.001500    Time 0.023817    
2018-10-27 23:33:34,937 - Epoch: [274][  200/  391]    Overall Loss 0.193584    Objective Loss 0.193584    Top1 93.222656    Top5 99.902344    LR 0.001500    Time 0.023728    
2018-10-27 23:33:36,108 - Epoch: [274][  250/  391]    Overall Loss 0.195349    Objective Loss 0.195349    Top1 93.118750    Top5 99.915625    LR 0.001500    Time 0.023658    
2018-10-27 23:33:37,281 - Epoch: [274][  300/  391]    Overall Loss 0.193576    Objective Loss 0.193576    Top1 93.195312    Top5 99.916667    LR 0.001500    Time 0.023620    
2018-10-27 23:33:38,452 - Epoch: [274][  350/  391]    Overall Loss 0.193951    Objective Loss 0.193951    Top1 93.189732    Top5 99.910714    LR 0.001500    Time 0.023588    
2018-10-27 23:33:39,499 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30982 | -0.00282 |    0.11574 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09552 | -0.00345 |    0.02117 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08950 |  0.00096 |    0.02163 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09306 | -0.00220 |    0.02199 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07261 | -0.00180 |    0.01424 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09746 | -0.00326 |    0.02672 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07145 | -0.00094 |    0.01594 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10208 | -0.00300 |    0.03573 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08156 | -0.00198 |    0.02508 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11192 | -0.00576 |    0.03524 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07082 | -0.00104 |    0.01958 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05704 | -0.00061 |    0.01373 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07636 | -0.00168 |    0.02245 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05799 | -0.00293 |    0.01469 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07021 | -0.00100 |    0.02339 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06763 | -0.00244 |    0.02302 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06160 |  0.00044 |    0.01575 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06320 | -0.00150 |    0.01936 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05210 | -0.00156 |    0.01343 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04992 | -0.00032 |    0.01306 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03060 |  0.00114 |    0.00544 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54646 | -0.07468 |    0.26942 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:33:39,499 - Total sparsity: 88.62

2018-10-27 23:33:39,499 - --- validate (epoch=274)-----------
2018-10-27 23:33:39,499 - 10000 samples (128 per mini-batch)
2018-10-27 23:33:40,226 - Epoch: [274][   50/   78]    Loss 0.398881    Top1 87.515625    Top5 99.515625    
2018-10-27 23:33:40,621 - ==> Top1: 87.730    Top5: 99.560    Loss: 0.390

2018-10-27 23:33:40,622 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:33:40,622 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:33:40,632 - 

2018-10-27 23:33:40,632 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:33:41,863 - Epoch: [275][   50/  391]    Overall Loss 0.179197    Objective Loss 0.179197    Top1 93.687500    Top5 99.953125    LR 0.001500    Time 0.024584    
2018-10-27 23:33:43,035 - Epoch: [275][  100/  391]    Overall Loss 0.182320    Objective Loss 0.182320    Top1 93.296875    Top5 99.914062    LR 0.001500    Time 0.023997    
2018-10-27 23:33:44,204 - Epoch: [275][  150/  391]    Overall Loss 0.186672    Objective Loss 0.186672    Top1 93.416667    Top5 99.854167    LR 0.001500    Time 0.023783    
2018-10-27 23:33:45,370 - Epoch: [275][  200/  391]    Overall Loss 0.186633    Objective Loss 0.186633    Top1 93.453125    Top5 99.859375    LR 0.001500    Time 0.023659    
2018-10-27 23:33:46,541 - Epoch: [275][  250/  391]    Overall Loss 0.188846    Objective Loss 0.188846    Top1 93.371875    Top5 99.859375    LR 0.001500    Time 0.023608    
2018-10-27 23:33:47,715 - Epoch: [275][  300/  391]    Overall Loss 0.189287    Objective Loss 0.189287    Top1 93.388021    Top5 99.869792    LR 0.001500    Time 0.023582    
2018-10-27 23:33:48,890 - Epoch: [275][  350/  391]    Overall Loss 0.190769    Objective Loss 0.190769    Top1 93.343750    Top5 99.868304    LR 0.001500    Time 0.023567    
2018-10-27 23:33:49,934 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30966 | -0.00274 |    0.11567 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09547 | -0.00345 |    0.02114 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08946 |  0.00096 |    0.02161 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09301 | -0.00223 |    0.02198 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07257 | -0.00179 |    0.01423 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09740 | -0.00324 |    0.02672 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07141 | -0.00097 |    0.01593 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10202 | -0.00302 |    0.03571 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08151 | -0.00198 |    0.02507 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11186 | -0.00575 |    0.03520 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07078 | -0.00105 |    0.01957 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05701 | -0.00061 |    0.01373 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07632 | -0.00167 |    0.02244 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05796 | -0.00294 |    0.01468 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07017 | -0.00100 |    0.02338 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06759 | -0.00244 |    0.02301 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06156 |  0.00045 |    0.01574 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06317 | -0.00150 |    0.01934 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05207 | -0.00156 |    0.01342 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04989 | -0.00032 |    0.01306 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03059 |  0.00114 |    0.00544 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54662 | -0.07469 |    0.26950 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:33:49,934 - Total sparsity: 88.62

2018-10-27 23:33:49,935 - --- validate (epoch=275)-----------
2018-10-27 23:33:49,935 - 10000 samples (128 per mini-batch)
2018-10-27 23:33:50,662 - Epoch: [275][   50/   78]    Loss 0.400533    Top1 87.546875    Top5 99.515625    
2018-10-27 23:33:51,056 - ==> Top1: 87.820    Top5: 99.560    Loss: 0.390

2018-10-27 23:33:51,057 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:33:51,057 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:33:51,072 - 

2018-10-27 23:33:51,073 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:33:52,304 - Epoch: [276][   50/  391]    Overall Loss 0.198417    Objective Loss 0.198417    Top1 93.109375    Top5 99.890625    LR 0.001500    Time 0.024591    
2018-10-27 23:33:53,475 - Epoch: [276][  100/  391]    Overall Loss 0.198378    Objective Loss 0.198378    Top1 93.023438    Top5 99.898438    LR 0.001500    Time 0.023994    
2018-10-27 23:33:54,647 - Epoch: [276][  150/  391]    Overall Loss 0.197840    Objective Loss 0.197840    Top1 93.000000    Top5 99.885417    LR 0.001500    Time 0.023800    
2018-10-27 23:33:55,822 - Epoch: [276][  200/  391]    Overall Loss 0.196459    Objective Loss 0.196459    Top1 93.082031    Top5 99.890625    LR 0.001500    Time 0.023716    
2018-10-27 23:33:56,994 - Epoch: [276][  250/  391]    Overall Loss 0.196913    Objective Loss 0.196913    Top1 93.087500    Top5 99.878125    LR 0.001500    Time 0.023653    
2018-10-27 23:33:58,172 - Epoch: [276][  300/  391]    Overall Loss 0.195821    Objective Loss 0.195821    Top1 93.117188    Top5 99.885417    LR 0.001500    Time 0.023633    
2018-10-27 23:33:59,347 - Epoch: [276][  350/  391]    Overall Loss 0.192217    Objective Loss 0.192217    Top1 93.258929    Top5 99.895089    LR 0.001500    Time 0.023613    
2018-10-27 23:34:00,394 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30949 | -0.00306 |    0.11559 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09541 | -0.00342 |    0.02114 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08941 |  0.00097 |    0.02160 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09296 | -0.00223 |    0.02198 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07253 | -0.00178 |    0.01422 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09735 | -0.00325 |    0.02669 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07137 | -0.00099 |    0.01593 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10197 | -0.00301 |    0.03569 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08147 | -0.00198 |    0.02506 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11180 | -0.00573 |    0.03519 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07075 | -0.00105 |    0.01956 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05698 | -0.00061 |    0.01372 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07628 | -0.00166 |    0.02243 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05793 | -0.00293 |    0.01467 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07013 | -0.00100 |    0.02336 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06755 | -0.00243 |    0.02299 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06153 |  0.00044 |    0.01574 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06314 | -0.00150 |    0.01933 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05204 | -0.00156 |    0.01341 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04986 | -0.00032 |    0.01305 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03057 |  0.00114 |    0.00543 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54678 | -0.07468 |    0.26955 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:34:00,394 - Total sparsity: 88.62

2018-10-27 23:34:00,394 - --- validate (epoch=276)-----------
2018-10-27 23:34:00,394 - 10000 samples (128 per mini-batch)
2018-10-27 23:34:01,125 - Epoch: [276][   50/   78]    Loss 0.397546    Top1 87.406250    Top5 99.515625    
2018-10-27 23:34:01,517 - ==> Top1: 87.570    Top5: 99.540    Loss: 0.391

2018-10-27 23:34:01,518 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:34:01,518 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:34:01,532 - 

2018-10-27 23:34:01,532 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:34:02,731 - Epoch: [277][   50/  391]    Overall Loss 0.181476    Objective Loss 0.181476    Top1 93.578125    Top5 99.921875    LR 0.001500    Time 0.023930    
2018-10-27 23:34:03,871 - Epoch: [277][  100/  391]    Overall Loss 0.187906    Objective Loss 0.187906    Top1 93.320312    Top5 99.921875    LR 0.001500    Time 0.023358    
2018-10-27 23:34:05,011 - Epoch: [277][  150/  391]    Overall Loss 0.190041    Objective Loss 0.190041    Top1 93.250000    Top5 99.906250    LR 0.001500    Time 0.023160    
2018-10-27 23:34:06,151 - Epoch: [277][  200/  391]    Overall Loss 0.191504    Objective Loss 0.191504    Top1 93.171875    Top5 99.898438    LR 0.001500    Time 0.023061    
2018-10-27 23:34:07,290 - Epoch: [277][  250/  391]    Overall Loss 0.193383    Objective Loss 0.193383    Top1 93.090625    Top5 99.881250    LR 0.001500    Time 0.023001    
2018-10-27 23:34:08,428 - Epoch: [277][  300/  391]    Overall Loss 0.192971    Objective Loss 0.192971    Top1 93.122396    Top5 99.885417    LR 0.001500    Time 0.022958    
2018-10-27 23:34:09,568 - Epoch: [277][  350/  391]    Overall Loss 0.192463    Objective Loss 0.192463    Top1 93.183036    Top5 99.890625    LR 0.001500    Time 0.022932    
2018-10-27 23:34:10,583 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30932 | -0.00297 |    0.11546 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09536 | -0.00343 |    0.02113 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08936 |  0.00093 |    0.02158 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09291 | -0.00225 |    0.02197 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07249 | -0.00177 |    0.01421 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09730 | -0.00325 |    0.02667 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07133 | -0.00096 |    0.01592 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10191 | -0.00300 |    0.03566 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08143 | -0.00199 |    0.02504 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11173 | -0.00573 |    0.03517 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07071 | -0.00105 |    0.01955 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05695 | -0.00060 |    0.01371 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07624 | -0.00166 |    0.02242 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05790 | -0.00294 |    0.01466 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07009 | -0.00100 |    0.02335 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06752 | -0.00243 |    0.02298 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06149 |  0.00047 |    0.01572 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06310 | -0.00149 |    0.01932 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05201 | -0.00155 |    0.01340 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04984 | -0.00032 |    0.01304 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03055 |  0.00114 |    0.00543 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54694 | -0.07472 |    0.26963 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:34:10,583 - Total sparsity: 88.62

2018-10-27 23:34:10,583 - --- validate (epoch=277)-----------
2018-10-27 23:34:10,583 - 10000 samples (128 per mini-batch)
2018-10-27 23:34:11,295 - Epoch: [277][   50/   78]    Loss 0.395970    Top1 87.671875    Top5 99.484375    
2018-10-27 23:34:11,678 - ==> Top1: 87.750    Top5: 99.520    Loss: 0.389

2018-10-27 23:34:11,679 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:34:11,679 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:34:11,690 - 

2018-10-27 23:34:11,690 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:34:12,891 - Epoch: [278][   50/  391]    Overall Loss 0.192379    Objective Loss 0.192379    Top1 93.078125    Top5 99.890625    LR 0.001500    Time 0.023981    
2018-10-27 23:34:14,034 - Epoch: [278][  100/  391]    Overall Loss 0.189769    Objective Loss 0.189769    Top1 93.132812    Top5 99.929688    LR 0.001500    Time 0.023403    
2018-10-27 23:34:15,177 - Epoch: [278][  150/  391]    Overall Loss 0.188907    Objective Loss 0.188907    Top1 93.260417    Top5 99.906250    LR 0.001500    Time 0.023213    
2018-10-27 23:34:16,319 - Epoch: [278][  200/  391]    Overall Loss 0.190836    Objective Loss 0.190836    Top1 93.285156    Top5 99.910156    LR 0.001500    Time 0.023115    
2018-10-27 23:34:17,464 - Epoch: [278][  250/  391]    Overall Loss 0.193448    Objective Loss 0.193448    Top1 93.162500    Top5 99.903125    LR 0.001500    Time 0.023064    
2018-10-27 23:34:18,605 - Epoch: [278][  300/  391]    Overall Loss 0.192460    Objective Loss 0.192460    Top1 93.234375    Top5 99.901042    LR 0.001500    Time 0.023020    
2018-10-27 23:34:19,747 - Epoch: [278][  350/  391]    Overall Loss 0.193155    Objective Loss 0.193155    Top1 93.245536    Top5 99.908482    LR 0.001500    Time 0.022992    
2018-10-27 23:34:20,763 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30915 | -0.00320 |    0.11544 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09531 | -0.00346 |    0.02112 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08931 |  0.00093 |    0.02156 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09286 | -0.00224 |    0.02196 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07246 | -0.00179 |    0.01420 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09725 | -0.00325 |    0.02666 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07129 | -0.00098 |    0.01591 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10186 | -0.00298 |    0.03565 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08138 | -0.00198 |    0.02503 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11167 | -0.00573 |    0.03516 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07067 | -0.00105 |    0.01954 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05692 | -0.00059 |    0.01370 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07620 | -0.00166 |    0.02240 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05787 | -0.00294 |    0.01465 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07006 | -0.00101 |    0.02334 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06748 | -0.00243 |    0.02297 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06146 |  0.00046 |    0.01571 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06307 | -0.00149 |    0.01931 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05199 | -0.00155 |    0.01340 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04981 | -0.00032 |    0.01303 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03053 |  0.00114 |    0.00543 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54715 | -0.07473 |    0.26973 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:34:20,763 - Total sparsity: 88.62

2018-10-27 23:34:20,763 - --- validate (epoch=278)-----------
2018-10-27 23:34:20,763 - 10000 samples (128 per mini-batch)
2018-10-27 23:34:21,491 - Epoch: [278][   50/   78]    Loss 0.396540    Top1 87.406250    Top5 99.468750    
2018-10-27 23:34:21,885 - ==> Top1: 87.580    Top5: 99.530    Loss: 0.391

2018-10-27 23:34:21,886 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:34:21,886 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:34:21,896 - 

2018-10-27 23:34:21,897 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:34:23,097 - Epoch: [279][   50/  391]    Overall Loss 0.190274    Objective Loss 0.190274    Top1 93.109375    Top5 99.906250    LR 0.001500    Time 0.023961    
2018-10-27 23:34:24,236 - Epoch: [279][  100/  391]    Overall Loss 0.185151    Objective Loss 0.185151    Top1 93.570312    Top5 99.890625    LR 0.001500    Time 0.023363    
2018-10-27 23:34:25,376 - Epoch: [279][  150/  391]    Overall Loss 0.188852    Objective Loss 0.188852    Top1 93.427083    Top5 99.901042    LR 0.001500    Time 0.023163    
2018-10-27 23:34:26,516 - Epoch: [279][  200/  391]    Overall Loss 0.192317    Objective Loss 0.192317    Top1 93.292969    Top5 99.886719    LR 0.001500    Time 0.023067    
2018-10-27 23:34:27,655 - Epoch: [279][  250/  391]    Overall Loss 0.193739    Objective Loss 0.193739    Top1 93.250000    Top5 99.884375    LR 0.001500    Time 0.023004    
2018-10-27 23:34:28,796 - Epoch: [279][  300/  391]    Overall Loss 0.194468    Objective Loss 0.194468    Top1 93.231771    Top5 99.895833    LR 0.001500    Time 0.022969    
2018-10-27 23:34:29,936 - Epoch: [279][  350/  391]    Overall Loss 0.193341    Objective Loss 0.193341    Top1 93.203125    Top5 99.897321    LR 0.001500    Time 0.022941    
2018-10-27 23:34:30,954 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30899 | -0.00314 |    0.11542 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09526 | -0.00347 |    0.02109 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08926 |  0.00097 |    0.02155 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09281 | -0.00220 |    0.02195 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07242 | -0.00179 |    0.01419 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09719 | -0.00326 |    0.02666 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07125 | -0.00098 |    0.01591 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10180 | -0.00299 |    0.03561 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08134 | -0.00198 |    0.02501 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11161 | -0.00571 |    0.03513 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07063 | -0.00104 |    0.01953 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05689 | -0.00059 |    0.01369 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07616 | -0.00167 |    0.02239 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05784 | -0.00294 |    0.01464 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.07002 | -0.00102 |    0.02332 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06744 | -0.00243 |    0.02296 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06143 |  0.00046 |    0.01571 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06304 | -0.00148 |    0.01930 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05196 | -0.00155 |    0.01339 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04978 | -0.00032 |    0.01302 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03052 |  0.00114 |    0.00542 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54730 | -0.07474 |    0.26981 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:34:30,954 - Total sparsity: 88.62

2018-10-27 23:34:30,954 - --- validate (epoch=279)-----------
2018-10-27 23:34:30,954 - 10000 samples (128 per mini-batch)
2018-10-27 23:34:31,673 - Epoch: [279][   50/   78]    Loss 0.396524    Top1 87.562500    Top5 99.468750    
2018-10-27 23:34:32,056 - ==> Top1: 87.700    Top5: 99.520    Loss: 0.390

2018-10-27 23:34:32,057 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:34:32,057 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:34:32,067 - 

2018-10-27 23:34:32,068 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:34:33,265 - Epoch: [280][   50/  391]    Overall Loss 0.179897    Objective Loss 0.179897    Top1 93.515625    Top5 99.906250    LR 0.001500    Time 0.023919    
2018-10-27 23:34:34,405 - Epoch: [280][  100/  391]    Overall Loss 0.183621    Objective Loss 0.183621    Top1 93.507812    Top5 99.921875    LR 0.001500    Time 0.023343    
2018-10-27 23:34:35,546 - Epoch: [280][  150/  391]    Overall Loss 0.184672    Objective Loss 0.184672    Top1 93.479167    Top5 99.927083    LR 0.001500    Time 0.023162    
2018-10-27 23:34:36,687 - Epoch: [280][  200/  391]    Overall Loss 0.188828    Objective Loss 0.188828    Top1 93.367188    Top5 99.917969    LR 0.001500    Time 0.023069    
2018-10-27 23:34:37,828 - Epoch: [280][  250/  391]    Overall Loss 0.190901    Objective Loss 0.190901    Top1 93.271875    Top5 99.906250    LR 0.001500    Time 0.023014    
2018-10-27 23:34:38,971 - Epoch: [280][  300/  391]    Overall Loss 0.188631    Objective Loss 0.188631    Top1 93.315104    Top5 99.914062    LR 0.001500    Time 0.022970    
2018-10-27 23:34:40,111 - Epoch: [280][  350/  391]    Overall Loss 0.190107    Objective Loss 0.190107    Top1 93.294643    Top5 99.908482    LR 0.001500    Time 0.022943    
2018-10-27 23:34:41,126 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30882 | -0.00295 |    0.11536 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09521 | -0.00345 |    0.02109 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08921 |  0.00097 |    0.02154 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09276 | -0.00223 |    0.02193 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07238 | -0.00178 |    0.01418 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09714 | -0.00324 |    0.02664 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07121 | -0.00099 |    0.01590 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10175 | -0.00297 |    0.03559 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08129 | -0.00196 |    0.02499 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11155 | -0.00575 |    0.03511 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07060 | -0.00103 |    0.01952 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05686 | -0.00060 |    0.01368 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07612 | -0.00167 |    0.02237 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05781 | -0.00294 |    0.01463 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.06998 | -0.00102 |    0.02331 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06741 | -0.00243 |    0.02294 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06139 |  0.00045 |    0.01570 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06300 | -0.00148 |    0.01929 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05193 | -0.00155 |    0.01338 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04976 | -0.00032 |    0.01302 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03050 |  0.00114 |    0.00542 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54748 | -0.07473 |    0.26989 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:34:41,127 - Total sparsity: 88.62

2018-10-27 23:34:41,127 - --- validate (epoch=280)-----------
2018-10-27 23:34:41,127 - 10000 samples (128 per mini-batch)
2018-10-27 23:34:41,853 - Epoch: [280][   50/   78]    Loss 0.400900    Top1 87.421875    Top5 99.468750    
2018-10-27 23:34:42,247 - ==> Top1: 87.700    Top5: 99.540    Loss: 0.392

2018-10-27 23:34:42,248 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:34:42,248 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:34:42,258 - 

2018-10-27 23:34:42,259 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:34:43,456 - Epoch: [281][   50/  391]    Overall Loss 0.213098    Objective Loss 0.213098    Top1 92.734375    Top5 99.875000    LR 0.001500    Time 0.023903    
2018-10-27 23:34:44,596 - Epoch: [281][  100/  391]    Overall Loss 0.204024    Objective Loss 0.204024    Top1 92.828125    Top5 99.875000    LR 0.001500    Time 0.023340    
2018-10-27 23:34:45,735 - Epoch: [281][  150/  391]    Overall Loss 0.203737    Objective Loss 0.203737    Top1 92.854167    Top5 99.890625    LR 0.001500    Time 0.023149    
2018-10-27 23:34:46,876 - Epoch: [281][  200/  391]    Overall Loss 0.198919    Objective Loss 0.198919    Top1 93.019531    Top5 99.894531    LR 0.001500    Time 0.023056    
2018-10-27 23:34:48,016 - Epoch: [281][  250/  391]    Overall Loss 0.197571    Objective Loss 0.197571    Top1 93.046875    Top5 99.900000    LR 0.001500    Time 0.023002    
2018-10-27 23:34:49,156 - Epoch: [281][  300/  391]    Overall Loss 0.196202    Objective Loss 0.196202    Top1 93.122396    Top5 99.895833    LR 0.001500    Time 0.022964    
2018-10-27 23:34:50,297 - Epoch: [281][  350/  391]    Overall Loss 0.195967    Objective Loss 0.195967    Top1 93.098214    Top5 99.901786    LR 0.001500    Time 0.022939    
2018-10-27 23:34:51,316 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30865 | -0.00298 |    0.11524 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09516 | -0.00345 |    0.02107 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08917 |  0.00097 |    0.02153 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09271 | -0.00220 |    0.02191 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07234 | -0.00178 |    0.01417 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09709 | -0.00323 |    0.02662 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07118 | -0.00100 |    0.01589 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10169 | -0.00295 |    0.03557 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08125 | -0.00196 |    0.02498 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11149 | -0.00576 |    0.03511 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07056 | -0.00103 |    0.01950 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05683 | -0.00059 |    0.01368 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07608 | -0.00167 |    0.02235 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05777 | -0.00294 |    0.01462 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.06994 | -0.00101 |    0.02330 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06737 | -0.00243 |    0.02293 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06136 |  0.00045 |    0.01570 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06297 | -0.00149 |    0.01928 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05190 | -0.00154 |    0.01337 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04973 | -0.00033 |    0.01301 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03048 |  0.00114 |    0.00542 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54759 | -0.07473 |    0.26993 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:34:51,316 - Total sparsity: 88.62

2018-10-27 23:34:51,316 - --- validate (epoch=281)-----------
2018-10-27 23:34:51,316 - 10000 samples (128 per mini-batch)
2018-10-27 23:34:52,053 - Epoch: [281][   50/   78]    Loss 0.399169    Top1 87.578125    Top5 99.484375    
2018-10-27 23:34:52,447 - ==> Top1: 87.730    Top5: 99.540    Loss: 0.391

2018-10-27 23:34:52,448 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:34:52,448 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:34:52,458 - 

2018-10-27 23:34:52,459 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:34:53,658 - Epoch: [282][   50/  391]    Overall Loss 0.192437    Objective Loss 0.192437    Top1 93.187500    Top5 99.890625    LR 0.001500    Time 0.023955    
2018-10-27 23:34:54,803 - Epoch: [282][  100/  391]    Overall Loss 0.190593    Objective Loss 0.190593    Top1 93.312500    Top5 99.882812    LR 0.001500    Time 0.023414    
2018-10-27 23:34:55,950 - Epoch: [282][  150/  391]    Overall Loss 0.190819    Objective Loss 0.190819    Top1 93.302083    Top5 99.890625    LR 0.001500    Time 0.023241    
2018-10-27 23:34:57,094 - Epoch: [282][  200/  391]    Overall Loss 0.190275    Objective Loss 0.190275    Top1 93.328125    Top5 99.882812    LR 0.001500    Time 0.023145    
2018-10-27 23:34:58,238 - Epoch: [282][  250/  391]    Overall Loss 0.191109    Objective Loss 0.191109    Top1 93.221875    Top5 99.884375    LR 0.001500    Time 0.023086    
2018-10-27 23:34:59,383 - Epoch: [282][  300/  391]    Overall Loss 0.191328    Objective Loss 0.191328    Top1 93.250000    Top5 99.885417    LR 0.001500    Time 0.023053    
2018-10-27 23:35:00,530 - Epoch: [282][  350/  391]    Overall Loss 0.192151    Objective Loss 0.192151    Top1 93.283482    Top5 99.892857    LR 0.001500    Time 0.023033    
2018-10-27 23:35:01,547 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30849 | -0.00311 |    0.11520 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09510 | -0.00344 |    0.02106 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08912 |  0.00099 |    0.02152 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09266 | -0.00224 |    0.02190 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07230 | -0.00179 |    0.01417 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09704 | -0.00322 |    0.02659 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07114 | -0.00097 |    0.01588 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10164 | -0.00300 |    0.03557 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08121 | -0.00195 |    0.02496 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11143 | -0.00573 |    0.03510 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07052 | -0.00104 |    0.01949 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05680 | -0.00059 |    0.01368 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07604 | -0.00168 |    0.02234 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05774 | -0.00294 |    0.01461 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.06990 | -0.00101 |    0.02328 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06733 | -0.00242 |    0.02292 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06132 |  0.00045 |    0.01568 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06293 | -0.00148 |    0.01927 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05188 | -0.00154 |    0.01337 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04970 | -0.00032 |    0.01300 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03047 |  0.00114 |    0.00542 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54777 | -0.07473 |    0.27001 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:35:01,547 - Total sparsity: 88.62

2018-10-27 23:35:01,547 - --- validate (epoch=282)-----------
2018-10-27 23:35:01,547 - 10000 samples (128 per mini-batch)
2018-10-27 23:35:02,270 - Epoch: [282][   50/   78]    Loss 0.402947    Top1 87.562500    Top5 99.515625    
2018-10-27 23:35:02,657 - ==> Top1: 87.660    Top5: 99.540    Loss: 0.394

2018-10-27 23:35:02,658 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:35:02,658 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:35:02,668 - 

2018-10-27 23:35:02,669 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:35:03,867 - Epoch: [283][   50/  391]    Overall Loss 0.184887    Objective Loss 0.184887    Top1 93.484375    Top5 99.890625    LR 0.001500    Time 0.023938    
2018-10-27 23:35:05,009 - Epoch: [283][  100/  391]    Overall Loss 0.181353    Objective Loss 0.181353    Top1 93.695312    Top5 99.929688    LR 0.001500    Time 0.023370    
2018-10-27 23:35:06,150 - Epoch: [283][  150/  391]    Overall Loss 0.183272    Objective Loss 0.183272    Top1 93.645833    Top5 99.916667    LR 0.001500    Time 0.023176    
2018-10-27 23:35:07,292 - Epoch: [283][  200/  391]    Overall Loss 0.186769    Objective Loss 0.186769    Top1 93.468750    Top5 99.902344    LR 0.001500    Time 0.023086    
2018-10-27 23:35:08,436 - Epoch: [283][  250/  391]    Overall Loss 0.188120    Objective Loss 0.188120    Top1 93.409375    Top5 99.884375    LR 0.001500    Time 0.023041    
2018-10-27 23:35:09,578 - Epoch: [283][  300/  391]    Overall Loss 0.191929    Objective Loss 0.191929    Top1 93.244792    Top5 99.882812    LR 0.001500    Time 0.023003    
2018-10-27 23:35:10,723 - Epoch: [283][  350/  391]    Overall Loss 0.192553    Objective Loss 0.192553    Top1 93.203125    Top5 99.886161    LR 0.001500    Time 0.022982    
2018-10-27 23:35:11,739 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30832 | -0.00307 |    0.11511 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09505 | -0.00343 |    0.02104 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08907 |  0.00097 |    0.02150 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09261 | -0.00224 |    0.02189 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07226 | -0.00177 |    0.01417 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09699 | -0.00321 |    0.02659 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07110 | -0.00098 |    0.01588 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10158 | -0.00299 |    0.03555 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08116 | -0.00195 |    0.02495 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11137 | -0.00571 |    0.03509 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07048 | -0.00103 |    0.01949 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05677 | -0.00059 |    0.01366 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07599 | -0.00167 |    0.02232 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05771 | -0.00294 |    0.01459 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.06987 | -0.00100 |    0.02326 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06730 | -0.00242 |    0.02291 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06129 |  0.00045 |    0.01567 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06290 | -0.00147 |    0.01925 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05185 | -0.00154 |    0.01336 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04968 | -0.00033 |    0.01299 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03045 |  0.00114 |    0.00541 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54796 | -0.07473 |    0.27009 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:35:11,739 - Total sparsity: 88.62

2018-10-27 23:35:11,739 - --- validate (epoch=283)-----------
2018-10-27 23:35:11,739 - 10000 samples (128 per mini-batch)
2018-10-27 23:35:12,467 - Epoch: [283][   50/   78]    Loss 0.402679    Top1 87.375000    Top5 99.468750    
2018-10-27 23:35:12,861 - ==> Top1: 87.610    Top5: 99.530    Loss: 0.395

2018-10-27 23:35:12,862 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:35:12,862 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:35:12,873 - 

2018-10-27 23:35:12,873 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:35:14,074 - Epoch: [284][   50/  391]    Overall Loss 0.191498    Objective Loss 0.191498    Top1 93.312500    Top5 99.843750    LR 0.001500    Time 0.023979    
2018-10-27 23:35:15,215 - Epoch: [284][  100/  391]    Overall Loss 0.194948    Objective Loss 0.194948    Top1 93.179688    Top5 99.812500    LR 0.001500    Time 0.023383    
2018-10-27 23:35:16,356 - Epoch: [284][  150/  391]    Overall Loss 0.194429    Objective Loss 0.194429    Top1 93.114583    Top5 99.854167    LR 0.001500    Time 0.023186    
2018-10-27 23:35:17,499 - Epoch: [284][  200/  391]    Overall Loss 0.195226    Objective Loss 0.195226    Top1 93.097656    Top5 99.863281    LR 0.001500    Time 0.023098    
2018-10-27 23:35:18,641 - Epoch: [284][  250/  391]    Overall Loss 0.195244    Objective Loss 0.195244    Top1 93.128125    Top5 99.865625    LR 0.001500    Time 0.023041    
2018-10-27 23:35:19,780 - Epoch: [284][  300/  391]    Overall Loss 0.195539    Objective Loss 0.195539    Top1 93.072917    Top5 99.875000    LR 0.001500    Time 0.022995    
2018-10-27 23:35:20,923 - Epoch: [284][  350/  391]    Overall Loss 0.194349    Objective Loss 0.194349    Top1 93.120536    Top5 99.881696    LR 0.001500    Time 0.022970    
2018-10-27 23:35:21,941 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30816 | -0.00291 |    0.11501 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09500 | -0.00341 |    0.02104 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08902 |  0.00096 |    0.02150 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09256 | -0.00224 |    0.02188 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07222 | -0.00173 |    0.01417 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09693 | -0.00320 |    0.02657 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07106 | -0.00098 |    0.01587 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10153 | -0.00299 |    0.03553 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08112 | -0.00195 |    0.02493 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11131 | -0.00571 |    0.03508 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07044 | -0.00102 |    0.01947 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05674 | -0.00059 |    0.01366 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07595 | -0.00167 |    0.02232 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05768 | -0.00293 |    0.01458 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.06983 | -0.00099 |    0.02325 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06726 | -0.00241 |    0.02289 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06126 |  0.00046 |    0.01567 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06287 | -0.00147 |    0.01924 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05182 | -0.00154 |    0.01335 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04965 | -0.00032 |    0.01298 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03044 |  0.00114 |    0.00541 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54811 | -0.07475 |    0.27016 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:35:21,942 - Total sparsity: 88.62

2018-10-27 23:35:21,942 - --- validate (epoch=284)-----------
2018-10-27 23:35:21,942 - 10000 samples (128 per mini-batch)
2018-10-27 23:35:22,665 - Epoch: [284][   50/   78]    Loss 0.401563    Top1 87.468750    Top5 99.468750    
2018-10-27 23:35:23,055 - ==> Top1: 87.530    Top5: 99.500    Loss: 0.394

2018-10-27 23:35:23,056 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:35:23,056 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:35:23,066 - 

2018-10-27 23:35:23,066 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:35:24,267 - Epoch: [285][   50/  391]    Overall Loss 0.201979    Objective Loss 0.201979    Top1 92.703125    Top5 99.843750    LR 0.001500    Time 0.023994    
2018-10-27 23:35:25,409 - Epoch: [285][  100/  391]    Overall Loss 0.195003    Objective Loss 0.195003    Top1 93.070312    Top5 99.843750    LR 0.001500    Time 0.023398    
2018-10-27 23:35:26,546 - Epoch: [285][  150/  391]    Overall Loss 0.197706    Objective Loss 0.197706    Top1 93.000000    Top5 99.864583    LR 0.001500    Time 0.023172    
2018-10-27 23:35:27,687 - Epoch: [285][  200/  391]    Overall Loss 0.195444    Objective Loss 0.195444    Top1 93.078125    Top5 99.867188    LR 0.001500    Time 0.023077    
2018-10-27 23:35:28,828 - Epoch: [285][  250/  391]    Overall Loss 0.194409    Objective Loss 0.194409    Top1 93.115625    Top5 99.859375    LR 0.001500    Time 0.023020    
2018-10-27 23:35:29,971 - Epoch: [285][  300/  391]    Overall Loss 0.192435    Objective Loss 0.192435    Top1 93.190104    Top5 99.869792    LR 0.001500    Time 0.022978    
2018-10-27 23:35:31,113 - Epoch: [285][  350/  391]    Overall Loss 0.192803    Objective Loss 0.192803    Top1 93.118304    Top5 99.879464    LR 0.001500    Time 0.022953    
2018-10-27 23:35:32,129 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30799 | -0.00297 |    0.11494 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09495 | -0.00341 |    0.02101 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08897 |  0.00093 |    0.02147 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09251 | -0.00225 |    0.02186 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07218 | -0.00174 |    0.01416 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09688 | -0.00321 |    0.02656 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07102 | -0.00099 |    0.01587 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10147 | -0.00299 |    0.03552 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08107 | -0.00193 |    0.02492 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11124 | -0.00574 |    0.03507 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07041 | -0.00104 |    0.01946 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05671 | -0.00060 |    0.01365 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07591 | -0.00168 |    0.02231 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05765 | -0.00293 |    0.01458 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.06979 | -0.00100 |    0.02323 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06723 | -0.00242 |    0.02288 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06122 |  0.00046 |    0.01566 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06283 | -0.00147 |    0.01923 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05179 | -0.00154 |    0.01334 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04962 | -0.00032 |    0.01298 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03042 |  0.00114 |    0.00541 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54829 | -0.07476 |    0.27025 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:35:32,129 - Total sparsity: 88.62

2018-10-27 23:35:32,129 - --- validate (epoch=285)-----------
2018-10-27 23:35:32,129 - 10000 samples (128 per mini-batch)
2018-10-27 23:35:32,851 - Epoch: [285][   50/   78]    Loss 0.402538    Top1 87.437500    Top5 99.500000    
2018-10-27 23:35:33,242 - ==> Top1: 87.690    Top5: 99.520    Loss: 0.396

2018-10-27 23:35:33,243 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:35:33,243 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:35:33,254 - 

2018-10-27 23:35:33,254 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:35:34,452 - Epoch: [286][   50/  391]    Overall Loss 0.190666    Objective Loss 0.190666    Top1 93.328125    Top5 99.859375    LR 0.001500    Time 0.023922    
2018-10-27 23:35:35,595 - Epoch: [286][  100/  391]    Overall Loss 0.194047    Objective Loss 0.194047    Top1 93.242188    Top5 99.859375    LR 0.001500    Time 0.023378    
2018-10-27 23:35:36,738 - Epoch: [286][  150/  391]    Overall Loss 0.198830    Objective Loss 0.198830    Top1 92.947917    Top5 99.859375    LR 0.001500    Time 0.023193    
2018-10-27 23:35:37,880 - Epoch: [286][  200/  391]    Overall Loss 0.195023    Objective Loss 0.195023    Top1 93.175781    Top5 99.863281    LR 0.001500    Time 0.023100    
2018-10-27 23:35:39,021 - Epoch: [286][  250/  391]    Overall Loss 0.195041    Objective Loss 0.195041    Top1 93.209375    Top5 99.859375    LR 0.001500    Time 0.023038    
2018-10-27 23:35:40,164 - Epoch: [286][  300/  391]    Overall Loss 0.195189    Objective Loss 0.195189    Top1 93.229167    Top5 99.867188    LR 0.001500    Time 0.023004    
2018-10-27 23:35:41,309 - Epoch: [286][  350/  391]    Overall Loss 0.194235    Objective Loss 0.194235    Top1 93.258929    Top5 99.872768    LR 0.001500    Time 0.022983    
2018-10-27 23:35:42,325 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30782 | -0.00286 |    0.11489 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09490 | -0.00343 |    0.02100 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08893 |  0.00094 |    0.02147 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09246 | -0.00226 |    0.02185 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07214 | -0.00175 |    0.01416 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09683 | -0.00318 |    0.02655 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07099 | -0.00099 |    0.01586 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10142 | -0.00298 |    0.03549 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08103 | -0.00193 |    0.02491 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11118 | -0.00572 |    0.03504 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07037 | -0.00103 |    0.01944 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05668 | -0.00060 |    0.01365 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07587 | -0.00168 |    0.02230 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05762 | -0.00293 |    0.01457 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.06975 | -0.00099 |    0.02322 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06719 | -0.00242 |    0.02287 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06119 |  0.00045 |    0.01565 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06280 | -0.00147 |    0.01922 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05177 | -0.00153 |    0.01334 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04960 | -0.00032 |    0.01297 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03040 |  0.00114 |    0.00540 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54845 | -0.07475 |    0.27032 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:35:42,325 - Total sparsity: 88.62

2018-10-27 23:35:42,326 - --- validate (epoch=286)-----------
2018-10-27 23:35:42,326 - 10000 samples (128 per mini-batch)
2018-10-27 23:35:43,051 - Epoch: [286][   50/   78]    Loss 0.403098    Top1 87.359375    Top5 99.546875    
2018-10-27 23:35:43,444 - ==> Top1: 87.600    Top5: 99.560    Loss: 0.395

2018-10-27 23:35:43,445 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:35:43,445 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:35:43,456 - 

2018-10-27 23:35:43,456 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:35:44,658 - Epoch: [287][   50/  391]    Overall Loss 0.183843    Objective Loss 0.183843    Top1 93.656250    Top5 99.906250    LR 0.001500    Time 0.024010    
2018-10-27 23:35:45,796 - Epoch: [287][  100/  391]    Overall Loss 0.190641    Objective Loss 0.190641    Top1 93.265625    Top5 99.882812    LR 0.001500    Time 0.023369    
2018-10-27 23:35:46,940 - Epoch: [287][  150/  391]    Overall Loss 0.194944    Objective Loss 0.194944    Top1 93.119792    Top5 99.869792    LR 0.001500    Time 0.023194    
2018-10-27 23:35:48,084 - Epoch: [287][  200/  391]    Overall Loss 0.194031    Objective Loss 0.194031    Top1 93.210938    Top5 99.894531    LR 0.001500    Time 0.023110    
2018-10-27 23:35:49,227 - Epoch: [287][  250/  391]    Overall Loss 0.191572    Objective Loss 0.191572    Top1 93.281250    Top5 99.900000    LR 0.001500    Time 0.023057    
2018-10-27 23:35:50,370 - Epoch: [287][  300/  391]    Overall Loss 0.192196    Objective Loss 0.192196    Top1 93.263021    Top5 99.901042    LR 0.001500    Time 0.023018    
2018-10-27 23:35:51,513 - Epoch: [287][  350/  391]    Overall Loss 0.191771    Objective Loss 0.191771    Top1 93.207589    Top5 99.906250    LR 0.001500    Time 0.022991    
2018-10-27 23:35:52,527 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30766 | -0.00264 |    0.11483 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09485 | -0.00342 |    0.02099 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08888 |  0.00093 |    0.02146 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09241 | -0.00227 |    0.02184 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07210 | -0.00174 |    0.01415 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09678 | -0.00319 |    0.02653 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07095 | -0.00102 |    0.01585 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10136 | -0.00298 |    0.03548 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08099 | -0.00191 |    0.02490 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11112 | -0.00571 |    0.03503 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07033 | -0.00103 |    0.01943 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05665 | -0.00059 |    0.01364 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07583 | -0.00167 |    0.02229 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05759 | -0.00293 |    0.01456 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.06971 | -0.00099 |    0.02320 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06715 | -0.00241 |    0.02286 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06116 |  0.00046 |    0.01564 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06277 | -0.00148 |    0.01921 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05174 | -0.00153 |    0.01333 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04957 | -0.00032 |    0.01296 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03039 |  0.00114 |    0.00540 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54859 | -0.07477 |    0.27039 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:35:52,527 - Total sparsity: 88.62

2018-10-27 23:35:52,528 - --- validate (epoch=287)-----------
2018-10-27 23:35:52,528 - 10000 samples (128 per mini-batch)
2018-10-27 23:35:53,253 - Epoch: [287][   50/   78]    Loss 0.398060    Top1 87.531250    Top5 99.531250    
2018-10-27 23:35:53,637 - ==> Top1: 87.650    Top5: 99.570    Loss: 0.392

2018-10-27 23:35:53,638 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:35:53,638 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:35:53,649 - 

2018-10-27 23:35:53,650 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:35:54,847 - Epoch: [288][   50/  391]    Overall Loss 0.180152    Objective Loss 0.180152    Top1 93.562500    Top5 99.937500    LR 0.001500    Time 0.023913    
2018-10-27 23:35:55,988 - Epoch: [288][  100/  391]    Overall Loss 0.190932    Objective Loss 0.190932    Top1 93.257812    Top5 99.906250    LR 0.001500    Time 0.023357    
2018-10-27 23:35:57,130 - Epoch: [288][  150/  391]    Overall Loss 0.192463    Objective Loss 0.192463    Top1 93.270833    Top5 99.906250    LR 0.001500    Time 0.023172    
2018-10-27 23:35:58,273 - Epoch: [288][  200/  391]    Overall Loss 0.191219    Objective Loss 0.191219    Top1 93.347656    Top5 99.906250    LR 0.001500    Time 0.023088    
2018-10-27 23:35:59,414 - Epoch: [288][  250/  391]    Overall Loss 0.192745    Objective Loss 0.192745    Top1 93.278125    Top5 99.909375    LR 0.001500    Time 0.023028    
2018-10-27 23:36:00,561 - Epoch: [288][  300/  391]    Overall Loss 0.192136    Objective Loss 0.192136    Top1 93.299479    Top5 99.901042    LR 0.001500    Time 0.023008    
2018-10-27 23:36:01,703 - Epoch: [288][  350/  391]    Overall Loss 0.191483    Objective Loss 0.191483    Top1 93.316964    Top5 99.901786    LR 0.001500    Time 0.022983    
2018-10-27 23:36:02,724 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30749 | -0.00296 |    0.11469 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09480 | -0.00340 |    0.02097 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08883 |  0.00094 |    0.02144 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09236 | -0.00226 |    0.02183 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07206 | -0.00173 |    0.01413 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09673 | -0.00318 |    0.02651 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07091 | -0.00101 |    0.01584 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10131 | -0.00295 |    0.03547 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08094 | -0.00191 |    0.02488 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11106 | -0.00569 |    0.03500 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07029 | -0.00102 |    0.01942 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05662 | -0.00060 |    0.01363 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07579 | -0.00166 |    0.02228 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05756 | -0.00293 |    0.01455 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.06968 | -0.00098 |    0.02319 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06712 | -0.00240 |    0.02284 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06112 |  0.00046 |    0.01563 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06273 | -0.00147 |    0.01920 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05171 | -0.00153 |    0.01333 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04954 | -0.00033 |    0.01295 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03037 |  0.00114 |    0.00540 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54877 | -0.07477 |    0.27047 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:36:02,724 - Total sparsity: 88.62

2018-10-27 23:36:02,724 - --- validate (epoch=288)-----------
2018-10-27 23:36:02,725 - 10000 samples (128 per mini-batch)
2018-10-27 23:36:03,443 - Epoch: [288][   50/   78]    Loss 0.398945    Top1 87.500000    Top5 99.515625    
2018-10-27 23:36:03,833 - ==> Top1: 87.630    Top5: 99.530    Loss: 0.391

2018-10-27 23:36:03,834 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:36:03,834 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:36:03,848 - 

2018-10-27 23:36:03,849 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:36:05,048 - Epoch: [289][   50/  391]    Overall Loss 0.196649    Objective Loss 0.196649    Top1 93.000000    Top5 99.906250    LR 0.001500    Time 0.023960    
2018-10-27 23:36:06,189 - Epoch: [289][  100/  391]    Overall Loss 0.195990    Objective Loss 0.195990    Top1 93.171875    Top5 99.890625    LR 0.001500    Time 0.023375    
2018-10-27 23:36:07,330 - Epoch: [289][  150/  391]    Overall Loss 0.192761    Objective Loss 0.192761    Top1 93.317708    Top5 99.890625    LR 0.001500    Time 0.023181    
2018-10-27 23:36:08,472 - Epoch: [289][  200/  391]    Overall Loss 0.193020    Objective Loss 0.193020    Top1 93.253906    Top5 99.906250    LR 0.001500    Time 0.023087    
2018-10-27 23:36:09,615 - Epoch: [289][  250/  391]    Overall Loss 0.193656    Objective Loss 0.193656    Top1 93.171875    Top5 99.906250    LR 0.001500    Time 0.023039    
2018-10-27 23:36:10,759 - Epoch: [289][  300/  391]    Overall Loss 0.191761    Objective Loss 0.191761    Top1 93.218750    Top5 99.916667    LR 0.001500    Time 0.023006    
2018-10-27 23:36:11,902 - Epoch: [289][  350/  391]    Overall Loss 0.192019    Objective Loss 0.192019    Top1 93.223214    Top5 99.915179    LR 0.001500    Time 0.022982    
2018-10-27 23:36:12,920 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30732 | -0.00276 |    0.11466 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09475 | -0.00342 |    0.02097 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08878 |  0.00094 |    0.02143 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09231 | -0.00223 |    0.02182 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07203 | -0.00170 |    0.01413 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09667 | -0.00319 |    0.02648 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07087 | -0.00102 |    0.01584 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10125 | -0.00296 |    0.03544 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08090 | -0.00192 |    0.02487 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11100 | -0.00568 |    0.03499 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07026 | -0.00102 |    0.01941 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05659 | -0.00060 |    0.01362 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07575 | -0.00167 |    0.02227 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05753 | -0.00293 |    0.01454 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.06964 | -0.00098 |    0.02317 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06708 | -0.00240 |    0.02283 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06109 |  0.00046 |    0.01563 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06270 | -0.00147 |    0.01919 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05168 | -0.00153 |    0.01332 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04952 | -0.00033 |    0.01294 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03035 |  0.00113 |    0.00540 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54894 | -0.07479 |    0.27055 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:36:12,920 - Total sparsity: 88.62

2018-10-27 23:36:12,920 - --- validate (epoch=289)-----------
2018-10-27 23:36:12,920 - 10000 samples (128 per mini-batch)
2018-10-27 23:36:13,645 - Epoch: [289][   50/   78]    Loss 0.403037    Top1 87.328125    Top5 99.500000    
2018-10-27 23:36:14,039 - ==> Top1: 87.500    Top5: 99.540    Loss: 0.394

2018-10-27 23:36:14,040 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:36:14,040 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:36:14,050 - 

2018-10-27 23:36:14,050 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:36:15,248 - Epoch: [290][   50/  391]    Overall Loss 0.204213    Objective Loss 0.204213    Top1 92.890625    Top5 99.890625    LR 0.001500    Time 0.023932    
2018-10-27 23:36:16,391 - Epoch: [290][  100/  391]    Overall Loss 0.197525    Objective Loss 0.197525    Top1 93.015625    Top5 99.890625    LR 0.001500    Time 0.023383    
2018-10-27 23:36:17,534 - Epoch: [290][  150/  391]    Overall Loss 0.198738    Objective Loss 0.198738    Top1 93.057292    Top5 99.901042    LR 0.001500    Time 0.023197    
2018-10-27 23:36:18,676 - Epoch: [290][  200/  391]    Overall Loss 0.195692    Objective Loss 0.195692    Top1 93.167969    Top5 99.898438    LR 0.001500    Time 0.023100    
2018-10-27 23:36:19,820 - Epoch: [290][  250/  391]    Overall Loss 0.194004    Objective Loss 0.194004    Top1 93.162500    Top5 99.915625    LR 0.001500    Time 0.023050    
2018-10-27 23:36:20,963 - Epoch: [290][  300/  391]    Overall Loss 0.192365    Objective Loss 0.192365    Top1 93.221354    Top5 99.919271    LR 0.001500    Time 0.023017    
2018-10-27 23:36:22,108 - Epoch: [290][  350/  391]    Overall Loss 0.191603    Objective Loss 0.191603    Top1 93.292411    Top5 99.919643    LR 0.001500    Time 0.022997    
2018-10-27 23:36:23,124 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30715 | -0.00287 |    0.11454 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09470 | -0.00339 |    0.02095 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08873 |  0.00093 |    0.02142 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09226 | -0.00228 |    0.02181 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07199 | -0.00169 |    0.01413 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09662 | -0.00318 |    0.02646 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07083 | -0.00099 |    0.01583 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10120 | -0.00296 |    0.03543 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08086 | -0.00192 |    0.02486 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11094 | -0.00570 |    0.03497 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07022 | -0.00102 |    0.01940 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05655 | -0.00060 |    0.01361 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07571 | -0.00166 |    0.02226 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05750 | -0.00292 |    0.01454 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.06960 | -0.00099 |    0.02316 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06705 | -0.00240 |    0.02282 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06105 |  0.00046 |    0.01562 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06267 | -0.00147 |    0.01918 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05165 | -0.00152 |    0.01331 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04949 | -0.00033 |    0.01294 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03034 |  0.00113 |    0.00539 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54911 | -0.07480 |    0.27063 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:36:23,124 - Total sparsity: 88.62

2018-10-27 23:36:23,124 - --- validate (epoch=290)-----------
2018-10-27 23:36:23,124 - 10000 samples (128 per mini-batch)
2018-10-27 23:36:23,859 - Epoch: [290][   50/   78]    Loss 0.403634    Top1 87.296875    Top5 99.546875    
2018-10-27 23:36:24,254 - ==> Top1: 87.350    Top5: 99.580    Loss: 0.397

2018-10-27 23:36:24,255 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:36:24,255 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:36:24,270 - 

2018-10-27 23:36:24,270 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:36:25,471 - Epoch: [291][   50/  391]    Overall Loss 0.190113    Objective Loss 0.190113    Top1 93.453125    Top5 99.859375    LR 0.001500    Time 0.023985    
2018-10-27 23:36:26,615 - Epoch: [291][  100/  391]    Overall Loss 0.188550    Objective Loss 0.188550    Top1 93.507812    Top5 99.882812    LR 0.001500    Time 0.023415    
2018-10-27 23:36:27,758 - Epoch: [291][  150/  391]    Overall Loss 0.188177    Objective Loss 0.188177    Top1 93.515625    Top5 99.895833    LR 0.001500    Time 0.023220    
2018-10-27 23:36:28,901 - Epoch: [291][  200/  391]    Overall Loss 0.190607    Objective Loss 0.190607    Top1 93.351562    Top5 99.894531    LR 0.001500    Time 0.023128    
2018-10-27 23:36:30,045 - Epoch: [291][  250/  391]    Overall Loss 0.188738    Objective Loss 0.188738    Top1 93.371875    Top5 99.896875    LR 0.001500    Time 0.023073    
2018-10-27 23:36:31,191 - Epoch: [291][  300/  391]    Overall Loss 0.189500    Objective Loss 0.189500    Top1 93.346354    Top5 99.890625    LR 0.001500    Time 0.023041    
2018-10-27 23:36:32,334 - Epoch: [291][  350/  391]    Overall Loss 0.190443    Objective Loss 0.190443    Top1 93.276786    Top5 99.888393    LR 0.001500    Time 0.023014    
2018-10-27 23:36:33,354 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30699 | -0.00314 |    0.11455 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09464 | -0.00341 |    0.02094 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08868 |  0.00096 |    0.02141 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09221 | -0.00229 |    0.02179 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07195 | -0.00170 |    0.01412 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09657 | -0.00319 |    0.02644 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07080 | -0.00098 |    0.01581 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10114 | -0.00297 |    0.03541 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08081 | -0.00193 |    0.02484 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11088 | -0.00568 |    0.03494 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07018 | -0.00102 |    0.01938 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05652 | -0.00060 |    0.01360 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07567 | -0.00167 |    0.02225 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05746 | -0.00292 |    0.01452 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.06956 | -0.00099 |    0.02315 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06701 | -0.00241 |    0.02280 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06102 |  0.00046 |    0.01562 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06263 | -0.00147 |    0.01917 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05163 | -0.00152 |    0.01330 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04946 | -0.00033 |    0.01293 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03032 |  0.00113 |    0.00539 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54928 | -0.07482 |    0.27070 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:36:33,355 - Total sparsity: 88.62

2018-10-27 23:36:33,355 - --- validate (epoch=291)-----------
2018-10-27 23:36:33,355 - 10000 samples (128 per mini-batch)
2018-10-27 23:36:34,072 - Epoch: [291][   50/   78]    Loss 0.398776    Top1 87.593750    Top5 99.515625    
2018-10-27 23:36:34,459 - ==> Top1: 87.720    Top5: 99.530    Loss: 0.392

2018-10-27 23:36:34,460 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:36:34,460 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:36:34,477 - 

2018-10-27 23:36:34,477 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:36:35,648 - Epoch: [292][   50/  391]    Overall Loss 0.190232    Objective Loss 0.190232    Top1 93.437500    Top5 99.906250    LR 0.001500    Time 0.023382    
2018-10-27 23:36:36,789 - Epoch: [292][  100/  391]    Overall Loss 0.191823    Objective Loss 0.191823    Top1 93.523438    Top5 99.898438    LR 0.001500    Time 0.023085    
2018-10-27 23:36:37,927 - Epoch: [292][  150/  391]    Overall Loss 0.190227    Objective Loss 0.190227    Top1 93.416667    Top5 99.921875    LR 0.001500    Time 0.022966    
2018-10-27 23:36:39,067 - Epoch: [292][  200/  391]    Overall Loss 0.192810    Objective Loss 0.192810    Top1 93.316406    Top5 99.917969    LR 0.001500    Time 0.022919    
2018-10-27 23:36:40,207 - Epoch: [292][  250/  391]    Overall Loss 0.191163    Objective Loss 0.191163    Top1 93.362500    Top5 99.921875    LR 0.001500    Time 0.022889    
2018-10-27 23:36:41,346 - Epoch: [292][  300/  391]    Overall Loss 0.191117    Objective Loss 0.191117    Top1 93.312500    Top5 99.916667    LR 0.001500    Time 0.022867    
2018-10-27 23:36:42,487 - Epoch: [292][  350/  391]    Overall Loss 0.192723    Objective Loss 0.192723    Top1 93.256696    Top5 99.919643    LR 0.001500    Time 0.022856    
2018-10-27 23:36:43,502 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30682 | -0.00312 |    0.11454 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09459 | -0.00342 |    0.02094 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08864 |  0.00096 |    0.02139 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09216 | -0.00230 |    0.02178 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07191 | -0.00170 |    0.01411 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09652 | -0.00320 |    0.02644 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07076 | -0.00100 |    0.01581 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10109 | -0.00297 |    0.03540 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08077 | -0.00193 |    0.02483 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11082 | -0.00567 |    0.03489 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07014 | -0.00102 |    0.01937 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05649 | -0.00061 |    0.01359 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07563 | -0.00168 |    0.02223 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05743 | -0.00292 |    0.01451 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.06953 | -0.00100 |    0.02313 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06697 | -0.00241 |    0.02279 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06099 |  0.00047 |    0.01561 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06260 | -0.00147 |    0.01916 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05160 | -0.00152 |    0.01329 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04944 | -0.00033 |    0.01292 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03030 |  0.00113 |    0.00539 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54945 | -0.07482 |    0.27077 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:36:43,502 - Total sparsity: 88.62

2018-10-27 23:36:43,502 - --- validate (epoch=292)-----------
2018-10-27 23:36:43,502 - 10000 samples (128 per mini-batch)
2018-10-27 23:36:44,227 - Epoch: [292][   50/   78]    Loss 0.404653    Top1 87.421875    Top5 99.468750    
2018-10-27 23:36:44,616 - ==> Top1: 87.630    Top5: 99.510    Loss: 0.396

2018-10-27 23:36:44,617 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:36:44,617 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:36:44,631 - 

2018-10-27 23:36:44,632 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:36:45,828 - Epoch: [293][   50/  391]    Overall Loss 0.184444    Objective Loss 0.184444    Top1 93.328125    Top5 99.921875    LR 0.001500    Time 0.023876    
2018-10-27 23:36:46,972 - Epoch: [293][  100/  391]    Overall Loss 0.190927    Objective Loss 0.190927    Top1 93.296875    Top5 99.890625    LR 0.001500    Time 0.023366    
2018-10-27 23:36:48,111 - Epoch: [293][  150/  391]    Overall Loss 0.191149    Objective Loss 0.191149    Top1 93.255208    Top5 99.890625    LR 0.001500    Time 0.023166    
2018-10-27 23:36:49,256 - Epoch: [293][  200/  391]    Overall Loss 0.191284    Objective Loss 0.191284    Top1 93.218750    Top5 99.906250    LR 0.001500    Time 0.023092    
2018-10-27 23:36:50,400 - Epoch: [293][  250/  391]    Overall Loss 0.193470    Objective Loss 0.193470    Top1 93.128125    Top5 99.909375    LR 0.001500    Time 0.023044    
2018-10-27 23:36:51,544 - Epoch: [293][  300/  391]    Overall Loss 0.193780    Objective Loss 0.193780    Top1 93.135417    Top5 99.906250    LR 0.001500    Time 0.023013    
2018-10-27 23:36:52,688 - Epoch: [293][  350/  391]    Overall Loss 0.192405    Objective Loss 0.192405    Top1 93.236607    Top5 99.901786    LR 0.001500    Time 0.022990    
2018-10-27 23:36:53,705 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30665 | -0.00304 |    0.11444 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09454 | -0.00342 |    0.02093 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08859 |  0.00095 |    0.02137 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09211 | -0.00223 |    0.02175 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07187 | -0.00171 |    0.01409 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09647 | -0.00321 |    0.02641 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07072 | -0.00101 |    0.01581 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10103 | -0.00295 |    0.03538 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08073 | -0.00193 |    0.02481 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11076 | -0.00568 |    0.03486 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07011 | -0.00101 |    0.01936 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05646 | -0.00059 |    0.01359 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07559 | -0.00167 |    0.02222 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05740 | -0.00292 |    0.01451 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.06949 | -0.00099 |    0.02312 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06694 | -0.00240 |    0.02278 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06095 |  0.00047 |    0.01561 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06257 | -0.00146 |    0.01914 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05157 | -0.00152 |    0.01329 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04941 | -0.00033 |    0.01292 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03029 |  0.00113 |    0.00538 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54961 | -0.07483 |    0.27084 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:36:53,706 - Total sparsity: 88.62

2018-10-27 23:36:53,706 - --- validate (epoch=293)-----------
2018-10-27 23:36:53,706 - 10000 samples (128 per mini-batch)
2018-10-27 23:36:54,429 - Epoch: [293][   50/   78]    Loss 0.405091    Top1 87.250000    Top5 99.453125    
2018-10-27 23:36:54,819 - ==> Top1: 87.400    Top5: 99.490    Loss: 0.398

2018-10-27 23:36:54,820 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:36:54,820 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:36:54,831 - 

2018-10-27 23:36:54,831 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:36:56,031 - Epoch: [294][   50/  391]    Overall Loss 0.195936    Objective Loss 0.195936    Top1 93.328125    Top5 99.828125    LR 0.001500    Time 0.023962    
2018-10-27 23:36:57,173 - Epoch: [294][  100/  391]    Overall Loss 0.193005    Objective Loss 0.193005    Top1 93.382812    Top5 99.875000    LR 0.001500    Time 0.023385    
2018-10-27 23:36:58,318 - Epoch: [294][  150/  391]    Overall Loss 0.193124    Objective Loss 0.193124    Top1 93.302083    Top5 99.890625    LR 0.001500    Time 0.023215    
2018-10-27 23:36:59,456 - Epoch: [294][  200/  391]    Overall Loss 0.192967    Objective Loss 0.192967    Top1 93.292969    Top5 99.902344    LR 0.001500    Time 0.023095    
2018-10-27 23:37:00,599 - Epoch: [294][  250/  391]    Overall Loss 0.194682    Objective Loss 0.194682    Top1 93.206250    Top5 99.896875    LR 0.001500    Time 0.023044    
2018-10-27 23:37:01,741 - Epoch: [294][  300/  391]    Overall Loss 0.194683    Objective Loss 0.194683    Top1 93.182292    Top5 99.890625    LR 0.001500    Time 0.023005    
2018-10-27 23:37:02,887 - Epoch: [294][  350/  391]    Overall Loss 0.193902    Objective Loss 0.193902    Top1 93.194196    Top5 99.892857    LR 0.001500    Time 0.022989    
2018-10-27 23:37:03,899 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30649 | -0.00293 |    0.11436 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09449 | -0.00336 |    0.02092 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08854 |  0.00095 |    0.02137 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09206 | -0.00224 |    0.02175 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07183 | -0.00169 |    0.01408 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09642 | -0.00317 |    0.02639 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07068 | -0.00102 |    0.01580 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10098 | -0.00294 |    0.03534 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08068 | -0.00192 |    0.02480 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11070 | -0.00567 |    0.03482 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07007 | -0.00101 |    0.01935 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05643 | -0.00060 |    0.01358 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07555 | -0.00166 |    0.02221 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05737 | -0.00291 |    0.01450 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.06945 | -0.00099 |    0.02311 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06690 | -0.00240 |    0.02277 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06092 |  0.00046 |    0.01560 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06253 | -0.00146 |    0.01913 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05154 | -0.00151 |    0.01328 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04938 | -0.00032 |    0.01291 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03027 |  0.00113 |    0.00538 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54972 | -0.07483 |    0.27089 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:37:03,899 - Total sparsity: 88.62

2018-10-27 23:37:03,899 - --- validate (epoch=294)-----------
2018-10-27 23:37:03,899 - 10000 samples (128 per mini-batch)
2018-10-27 23:37:04,630 - Epoch: [294][   50/   78]    Loss 0.404742    Top1 87.312500    Top5 99.484375    
2018-10-27 23:37:05,023 - ==> Top1: 87.520    Top5: 99.510    Loss: 0.398

2018-10-27 23:37:05,024 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:37:05,024 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:37:05,034 - 

2018-10-27 23:37:05,034 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:37:06,234 - Epoch: [295][   50/  391]    Overall Loss 0.181674    Objective Loss 0.181674    Top1 93.375000    Top5 99.906250    LR 0.001500    Time 0.023956    
2018-10-27 23:37:07,376 - Epoch: [295][  100/  391]    Overall Loss 0.187097    Objective Loss 0.187097    Top1 93.312500    Top5 99.875000    LR 0.001500    Time 0.023383    
2018-10-27 23:37:08,518 - Epoch: [295][  150/  391]    Overall Loss 0.189709    Objective Loss 0.189709    Top1 93.223958    Top5 99.875000    LR 0.001500    Time 0.023198    
2018-10-27 23:37:09,660 - Epoch: [295][  200/  391]    Overall Loss 0.186481    Objective Loss 0.186481    Top1 93.410156    Top5 99.875000    LR 0.001500    Time 0.023103    
2018-10-27 23:37:10,804 - Epoch: [295][  250/  391]    Overall Loss 0.187766    Objective Loss 0.187766    Top1 93.378125    Top5 99.871875    LR 0.001500    Time 0.023051    
2018-10-27 23:37:11,948 - Epoch: [295][  300/  391]    Overall Loss 0.187375    Objective Loss 0.187375    Top1 93.408854    Top5 99.880208    LR 0.001500    Time 0.023007    
2018-10-27 23:37:13,092 - Epoch: [295][  350/  391]    Overall Loss 0.187628    Objective Loss 0.187628    Top1 93.435268    Top5 99.886161    LR 0.001500    Time 0.022984    
2018-10-27 23:37:14,110 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30632 | -0.00285 |    0.11438 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09444 | -0.00334 |    0.02091 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08849 |  0.00095 |    0.02135 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09201 | -0.00223 |    0.02175 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07179 | -0.00170 |    0.01407 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09636 | -0.00315 |    0.02636 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07064 | -0.00099 |    0.01579 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10092 | -0.00297 |    0.03532 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08064 | -0.00191 |    0.02479 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11064 | -0.00564 |    0.03479 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07003 | -0.00100 |    0.01934 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05640 | -0.00060 |    0.01358 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07551 | -0.00167 |    0.02219 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05734 | -0.00291 |    0.01449 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.06941 | -0.00099 |    0.02309 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06687 | -0.00240 |    0.02275 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06089 |  0.00046 |    0.01559 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06250 | -0.00146 |    0.01912 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05152 | -0.00151 |    0.01327 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04936 | -0.00032 |    0.01290 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03025 |  0.00113 |    0.00538 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.54988 | -0.07482 |    0.27095 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:37:14,110 - Total sparsity: 88.62

2018-10-27 23:37:14,110 - --- validate (epoch=295)-----------
2018-10-27 23:37:14,110 - 10000 samples (128 per mini-batch)
2018-10-27 23:37:14,839 - Epoch: [295][   50/   78]    Loss 0.404362    Top1 87.531250    Top5 99.500000    
2018-10-27 23:37:15,233 - ==> Top1: 87.660    Top5: 99.530    Loss: 0.396

2018-10-27 23:37:15,234 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:37:15,234 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:37:15,250 - 

2018-10-27 23:37:15,251 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:37:16,422 - Epoch: [296][   50/  391]    Overall Loss 0.185525    Objective Loss 0.185525    Top1 93.546875    Top5 99.921875    LR 0.001500    Time 0.023400    
2018-10-27 23:37:17,564 - Epoch: [296][  100/  391]    Overall Loss 0.190454    Objective Loss 0.190454    Top1 93.398438    Top5 99.898438    LR 0.001500    Time 0.023102    
2018-10-27 23:37:18,706 - Epoch: [296][  150/  391]    Overall Loss 0.191162    Objective Loss 0.191162    Top1 93.354167    Top5 99.901042    LR 0.001500    Time 0.023004    
2018-10-27 23:37:19,848 - Epoch: [296][  200/  391]    Overall Loss 0.189574    Objective Loss 0.189574    Top1 93.394531    Top5 99.902344    LR 0.001500    Time 0.022958    
2018-10-27 23:37:20,992 - Epoch: [296][  250/  391]    Overall Loss 0.188882    Objective Loss 0.188882    Top1 93.406250    Top5 99.906250    LR 0.001500    Time 0.022938    
2018-10-27 23:37:22,135 - Epoch: [296][  300/  391]    Overall Loss 0.190042    Objective Loss 0.190042    Top1 93.335938    Top5 99.901042    LR 0.001500    Time 0.022920    
2018-10-27 23:37:23,276 - Epoch: [296][  350/  391]    Overall Loss 0.190680    Objective Loss 0.190680    Top1 93.308036    Top5 99.904018    LR 0.001500    Time 0.022902    
2018-10-27 23:37:24,294 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30615 | -0.00314 |    0.11427 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09439 | -0.00332 |    0.02091 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08845 |  0.00095 |    0.02133 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09196 | -0.00228 |    0.02173 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07175 | -0.00170 |    0.01407 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09631 | -0.00316 |    0.02635 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07061 | -0.00102 |    0.01579 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10087 | -0.00296 |    0.03529 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08059 | -0.00191 |    0.02477 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11058 | -0.00566 |    0.03478 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.07000 | -0.00100 |    0.01933 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05637 | -0.00060 |    0.01357 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07547 | -0.00167 |    0.02218 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05731 | -0.00291 |    0.01448 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.06938 | -0.00099 |    0.02308 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06683 | -0.00240 |    0.02274 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06085 |  0.00047 |    0.01558 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06247 | -0.00146 |    0.01911 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05149 | -0.00151 |    0.01326 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04933 | -0.00032 |    0.01289 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03024 |  0.00113 |    0.00537 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.55003 | -0.07484 |    0.27101 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:37:24,294 - Total sparsity: 88.62

2018-10-27 23:37:24,294 - --- validate (epoch=296)-----------
2018-10-27 23:37:24,294 - 10000 samples (128 per mini-batch)
2018-10-27 23:37:25,023 - Epoch: [296][   50/   78]    Loss 0.403725    Top1 87.484375    Top5 99.531250    
2018-10-27 23:37:25,415 - ==> Top1: 87.640    Top5: 99.560    Loss: 0.398

2018-10-27 23:37:25,415 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:37:25,416 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:37:25,432 - 

2018-10-27 23:37:25,432 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:37:26,603 - Epoch: [297][   50/  391]    Overall Loss 0.191789    Objective Loss 0.191789    Top1 93.281250    Top5 99.937500    LR 0.001500    Time 0.023393    
2018-10-27 23:37:27,746 - Epoch: [297][  100/  391]    Overall Loss 0.194432    Objective Loss 0.194432    Top1 93.000000    Top5 99.937500    LR 0.001500    Time 0.023112    
2018-10-27 23:37:28,889 - Epoch: [297][  150/  391]    Overall Loss 0.193617    Objective Loss 0.193617    Top1 93.218750    Top5 99.911458    LR 0.001500    Time 0.023016    
2018-10-27 23:37:30,032 - Epoch: [297][  200/  391]    Overall Loss 0.193897    Objective Loss 0.193897    Top1 93.226562    Top5 99.914062    LR 0.001500    Time 0.022970    
2018-10-27 23:37:31,176 - Epoch: [297][  250/  391]    Overall Loss 0.193421    Objective Loss 0.193421    Top1 93.168750    Top5 99.909375    LR 0.001500    Time 0.022948    
2018-10-27 23:37:32,321 - Epoch: [297][  300/  391]    Overall Loss 0.192283    Objective Loss 0.192283    Top1 93.192708    Top5 99.906250    LR 0.001500    Time 0.022936    
2018-10-27 23:37:33,463 - Epoch: [297][  350/  391]    Overall Loss 0.192115    Objective Loss 0.192115    Top1 93.218750    Top5 99.904018    LR 0.001500    Time 0.022916    
2018-10-27 23:37:34,477 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30599 | -0.00323 |    0.11413 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09434 | -0.00333 |    0.02089 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08840 |  0.00094 |    0.02131 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09191 | -0.00228 |    0.02172 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07172 | -0.00171 |    0.01406 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09626 | -0.00314 |    0.02633 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07057 | -0.00102 |    0.01578 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10082 | -0.00296 |    0.03526 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08055 | -0.00191 |    0.02476 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11051 | -0.00570 |    0.03474 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.06996 | -0.00100 |    0.01932 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05634 | -0.00060 |    0.01356 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07543 | -0.00167 |    0.02216 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05728 | -0.00292 |    0.01447 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.06934 | -0.00099 |    0.02307 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06679 | -0.00239 |    0.02273 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06082 |  0.00045 |    0.01557 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06243 | -0.00145 |    0.01909 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05146 | -0.00151 |    0.01325 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04930 | -0.00032 |    0.01289 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03022 |  0.00112 |    0.00537 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.55016 | -0.07486 |    0.27108 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:37:34,477 - Total sparsity: 88.62

2018-10-27 23:37:34,477 - --- validate (epoch=297)-----------
2018-10-27 23:37:34,477 - 10000 samples (128 per mini-batch)
2018-10-27 23:37:35,203 - Epoch: [297][   50/   78]    Loss 0.403000    Top1 87.421875    Top5 99.515625    
2018-10-27 23:37:35,598 - ==> Top1: 87.610    Top5: 99.540    Loss: 0.396

2018-10-27 23:37:35,599 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:37:35,599 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:37:35,609 - 

2018-10-27 23:37:35,610 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:37:36,807 - Epoch: [298][   50/  391]    Overall Loss 0.193636    Objective Loss 0.193636    Top1 92.968750    Top5 99.890625    LR 0.001500    Time 0.023918    
2018-10-27 23:37:37,952 - Epoch: [298][  100/  391]    Overall Loss 0.195155    Objective Loss 0.195155    Top1 93.117188    Top5 99.906250    LR 0.001500    Time 0.023386    
2018-10-27 23:37:39,094 - Epoch: [298][  150/  391]    Overall Loss 0.195114    Objective Loss 0.195114    Top1 93.041667    Top5 99.885417    LR 0.001500    Time 0.023197    
2018-10-27 23:37:40,238 - Epoch: [298][  200/  391]    Overall Loss 0.191480    Objective Loss 0.191480    Top1 93.160156    Top5 99.898438    LR 0.001500    Time 0.023112    
2018-10-27 23:37:41,381 - Epoch: [298][  250/  391]    Overall Loss 0.189518    Objective Loss 0.189518    Top1 93.225000    Top5 99.896875    LR 0.001500    Time 0.023056    
2018-10-27 23:37:42,524 - Epoch: [298][  300/  391]    Overall Loss 0.188866    Objective Loss 0.188866    Top1 93.309896    Top5 99.893229    LR 0.001500    Time 0.023018    
2018-10-27 23:37:43,666 - Epoch: [298][  350/  391]    Overall Loss 0.189860    Objective Loss 0.189860    Top1 93.343750    Top5 99.895089    LR 0.001500    Time 0.022990    
2018-10-27 23:37:44,683 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30583 | -0.00307 |    0.11411 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09429 | -0.00334 |    0.02088 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08835 |  0.00091 |    0.02130 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09186 | -0.00230 |    0.02170 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07168 | -0.00171 |    0.01405 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09621 | -0.00317 |    0.02632 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07053 | -0.00100 |    0.01577 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10076 | -0.00296 |    0.03523 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08051 | -0.00191 |    0.02474 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11045 | -0.00569 |    0.03475 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.06992 | -0.00099 |    0.01932 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05631 | -0.00059 |    0.01355 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07539 | -0.00167 |    0.02215 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05725 | -0.00292 |    0.01446 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.06930 | -0.00099 |    0.02306 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06676 | -0.00238 |    0.02272 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06079 |  0.00046 |    0.01556 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06240 | -0.00145 |    0.01908 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05143 | -0.00152 |    0.01324 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04928 | -0.00032 |    0.01288 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03020 |  0.00112 |    0.00537 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.55033 | -0.07483 |    0.27116 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:37:44,684 - Total sparsity: 88.62

2018-10-27 23:37:44,684 - --- validate (epoch=298)-----------
2018-10-27 23:37:44,684 - 10000 samples (128 per mini-batch)
2018-10-27 23:37:45,405 - Epoch: [298][   50/   78]    Loss 0.401577    Top1 87.281250    Top5 99.468750    
2018-10-27 23:37:45,795 - ==> Top1: 87.510    Top5: 99.520    Loss: 0.396

2018-10-27 23:37:45,795 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:37:45,796 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:37:45,806 - 

2018-10-27 23:37:45,807 - Training epoch: 50000 samples (128 per mini-batch)
2018-10-27 23:37:47,008 - Epoch: [299][   50/  391]    Overall Loss 0.181062    Objective Loss 0.181062    Top1 93.765625    Top5 99.812500    LR 0.001500    Time 0.023992    
2018-10-27 23:37:48,148 - Epoch: [299][  100/  391]    Overall Loss 0.182516    Objective Loss 0.182516    Top1 93.671875    Top5 99.843750    LR 0.001500    Time 0.023385    
2018-10-27 23:37:49,290 - Epoch: [299][  150/  391]    Overall Loss 0.185474    Objective Loss 0.185474    Top1 93.494792    Top5 99.848958    LR 0.001500    Time 0.023196    
2018-10-27 23:37:50,434 - Epoch: [299][  200/  391]    Overall Loss 0.184457    Objective Loss 0.184457    Top1 93.488281    Top5 99.878906    LR 0.001500    Time 0.023110    
2018-10-27 23:37:51,579 - Epoch: [299][  250/  391]    Overall Loss 0.184954    Objective Loss 0.184954    Top1 93.546875    Top5 99.884375    LR 0.001500    Time 0.023060    
2018-10-27 23:37:52,723 - Epoch: [299][  300/  391]    Overall Loss 0.185913    Objective Loss 0.185913    Top1 93.507812    Top5 99.893229    LR 0.001500    Time 0.023025    
2018-10-27 23:37:53,866 - Epoch: [299][  350/  391]    Overall Loss 0.186554    Objective Loss 0.186554    Top1 93.435268    Top5 99.901786    LR 0.001500    Time 0.023000    
2018-10-27 23:37:54,886 - 
Parameters:
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
|    | Name                                | Shape          |   NNZ (dense) |   NNZ (sparse) |   Cols (%) |   Rows (%) |   Ch (%) |   2D (%) |   3D (%) |   Fine (%) |     Std |     Mean |   Abs-Mean |
|----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------|
|  0 | module.conv1.weight                 | (16, 3, 3, 3)  |           432 |             95 |    0.00000 |    0.00000 |  0.00000 | 22.91667 | 18.75000 |   78.00926 | 0.30566 | -0.00288 |    0.11410 |
|  1 | module.layer1.0.conv1.weight        | (16, 16, 3, 3) |          2304 |            164 |    0.00000 |    0.00000 | 18.75000 | 60.15625 |  0.00000 |   92.88194 | 0.09424 | -0.00334 |    0.02087 |
|  2 | module.layer1.0.conv2.weight        | (16, 16, 3, 3) |          2304 |            192 |    0.00000 |    0.00000 |  0.00000 | 51.95312 |  0.00000 |   91.66667 | 0.08830 |  0.00092 |    0.02128 |
|  3 | module.layer1.1.conv1.weight        | (16, 16, 3, 3) |          2304 |            189 |    0.00000 |    0.00000 |  0.00000 | 52.34375 |  0.00000 |   91.79688 | 0.09181 | -0.00229 |    0.02169 |
|  4 | module.layer1.1.conv2.weight        | (16, 16, 3, 3) |          2304 |            130 |    0.00000 |    0.00000 |  0.00000 | 62.89062 |  0.00000 |   94.35764 | 0.07164 | -0.00171 |    0.01404 |
|  5 | module.layer1.2.conv1.weight        | (16, 16, 3, 3) |          2304 |            276 |    0.00000 |    0.00000 |  0.00000 | 42.57812 |  0.00000 |   88.02083 | 0.09616 | -0.00321 |    0.02629 |
|  6 | module.layer1.2.conv2.weight        | (16, 16, 3, 3) |          2304 |            168 |    0.00000 |    0.00000 |  0.00000 | 62.10938 |  0.00000 |   92.70833 | 0.07049 | -0.00100 |    0.01575 |
|  7 | module.layer2.0.conv1.weight        | (32, 16, 3, 3) |          4608 |            879 |    0.00000 |    0.00000 |  0.00000 | 29.10156 |  0.00000 |   80.92448 | 0.10071 | -0.00296 |    0.03521 |
|  8 | module.layer2.0.conv2.weight        | (32, 32, 3, 3) |          9216 |           1303 |    0.00000 |    0.00000 |  0.00000 | 30.37109 |  0.00000 |   85.86155 | 0.08046 | -0.00193 |    0.02472 |
|  9 | module.layer2.0.downsample.0.weight | (32, 16, 1, 1) |           512 |             68 |    0.00000 |    0.00000 |  0.00000 | 86.71875 | 21.87500 |   86.71875 | 0.11039 | -0.00570 |    0.03474 |
| 10 | module.layer2.1.conv1.weight        | (32, 32, 3, 3) |          9216 |           1018 |    0.00000 |    0.00000 |  0.00000 | 40.33203 |  0.00000 |   88.95399 | 0.06988 | -0.00098 |    0.01930 |
| 11 | module.layer2.1.conv2.weight        | (32, 32, 3, 3) |          9216 |            731 |    0.00000 |    0.00000 |  0.00000 | 53.02734 |  0.00000 |   92.06814 | 0.05628 | -0.00059 |    0.01354 |
| 12 | module.layer2.2.conv1.weight        | (32, 32, 3, 3) |          9216 |           1149 |    0.00000 |    0.00000 |  0.00000 | 38.08594 |  0.00000 |   87.53255 | 0.07535 | -0.00165 |    0.02214 |
| 13 | module.layer2.2.conv2.weight        | (32, 32, 3, 3) |          9216 |            802 |    0.00000 |    0.00000 |  0.00000 | 50.19531 |  0.00000 |   91.29774 | 0.05722 | -0.00291 |    0.01445 |
| 14 | module.layer3.0.conv1.weight        | (64, 32, 3, 3) |         18432 |           2879 |    0.00000 |    0.00000 |  0.00000 | 29.78516 |  0.00000 |   84.38043 | 0.06926 | -0.00098 |    0.02304 |
| 15 | module.layer3.0.conv2.weight        | (64, 64, 3, 3) |         36864 |           6285 |    0.00000 |    0.00000 |  0.00000 | 21.14258 |  0.00000 |   82.95085 | 0.06672 | -0.00239 |    0.02270 |
| 16 | module.layer3.0.downsample.0.weight | (64, 32, 1, 1) |          2048 |            168 |    0.00000 |    0.00000 |  3.12500 | 91.79688 | 10.93750 |   91.79688 | 0.06075 |  0.00045 |    0.01554 |
| 17 | module.layer3.1.conv1.weight        | (64, 64, 3, 3) |         36864 |           5070 |    0.00000 |    0.00000 |  0.00000 | 30.54199 |  0.00000 |   86.24674 | 0.06237 | -0.00145 |    0.01907 |
| 18 | module.layer3.1.conv2.weight        | (64, 64, 3, 3) |         36864 |           3556 |    0.00000 |    0.00000 |  0.00000 | 44.99512 |  0.00000 |   90.35373 | 0.05141 | -0.00151 |    0.01324 |
| 19 | module.layer3.2.conv1.weight        | (64, 64, 3, 3) |         36864 |           3725 |    0.00000 |    0.00000 |  0.00000 | 45.82520 |  0.00000 |   89.89529 | 0.04925 | -0.00032 |    0.01287 |
| 20 | module.layer3.2.conv2.weight        | (64, 64, 3, 3) |         36864 |           1746 |    0.00000 |    0.00000 |  0.00000 | 71.72852 |  0.00000 |   95.26367 | 0.03019 |  0.00112 |    0.00537 |
| 21 | module.fc.weight                    | (10, 64)       |           640 |            230 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   64.06250 | 0.55053 | -0.07486 |    0.27124 |
| 22 | Total sparsity:                     | -              |        270896 |          30823 |    0.00000 |    0.00000 |  0.00000 |  0.00000 |  0.00000 |   88.62183 | 0.00000 |  0.00000 |    0.00000 |
+----+-------------------------------------+----------------+---------------+----------------+------------+------------+----------+----------+----------+------------+---------+----------+------------+
2018-10-27 23:37:54,886 - Total sparsity: 88.62

2018-10-27 23:37:54,886 - --- validate (epoch=299)-----------
2018-10-27 23:37:54,886 - 10000 samples (128 per mini-batch)
2018-10-27 23:37:55,607 - Epoch: [299][   50/   78]    Loss 0.404475    Top1 87.281250    Top5 99.562500    
2018-10-27 23:37:56,001 - ==> Top1: 87.490    Top5: 99.550    Loss: 0.399

2018-10-27 23:37:56,001 - ==> Best Top1: 88.120   On Epoch: 218

2018-10-27 23:37:56,002 - Saving checkpoint to: logs/2018.10.27-224628/checkpoint.pth.tar
2018-10-27 23:37:56,012 - --- test ---------------------
2018-10-27 23:37:56,012 - 10000 samples (128 per mini-batch)
2018-10-27 23:37:56,765 - Test: [   50/   78]    Loss 0.404475    Top1 87.281250    Top5 99.562500    
2018-10-27 23:37:57,156 - ==> Top1: 87.490    Top5: 99.550    Loss: 0.399

2018-10-27 23:37:57,161 - 
2018-10-27 23:37:57,161 - Log file for this run: /home/ccma/Chilung/1022/distiller/examples/classifier_compression/logs/2018.10.27-224628/2018.10.27-224628.log
